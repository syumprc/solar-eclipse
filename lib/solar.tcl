# Filename: solar.tcl
# Purpose:  standard scripts for use with solar
# Authors:  Charles Peterson and Tom Dyer
# Date:     February 20, 1998 (started)
#
# Copyright (C) Southwest Foundation for Biomedical Research, 1998-2011.
# Copyright (C) Texas Biomedical Research Institute, 2011-present
#
# All rights reserved.

proc solar_tcl_version {} {
    return "8.1.2 (General)"
}

proc solar_up_date {} {
    global env
    global auto_path
    global SOLAR_Date
    set fullpath [lindex $auto_path 0]/solar.tcl
    set SOLAR_Date [file mtime $fullpath]
    set bins [glob $env(SOLAR_BIN)/*]
    foreach bin $bins {
	if {[file mtime $bin] > $SOLAR_Date} {
	    set SOLAR_Date [file mtime $bin]
	}
    }
    set fdate [clock format $SOLAR_Date -format "%B %d"]
    return $fdate
}

proc solar_up_year {} {
    global SOLAR_Date
    set year [clock format $SOLAR_Date -format %Y]
    return $year
}

# solar::about --
#
# Purpose:  Copyright, authors, and disclaimers
#
# SOLAR is Copyright (c) 1995-2014 Southwest Foundation for Biomedical
# Research.  All rights reserved.
#
# The authors are John Blangero, Kenneth Lange, Laura Almasy, Harald Goring,
# Jeff Williams, Tom Dyer, Michael Boehnke, and Charles Peterson.  Parts of
# SOLAR consist of software developed by others; a complete list is provided
# in Appendix Four of the documentation included with this package (use the
# "doc" command to find out where that is).
#
# Use of this program should be acknowledged in scientific publications.
# 
# Commands, features, performance, and availability are subject to change.
# There is absolutely no warranty, express or implied.
# There is no committment to support scripts written using current commands
#   in future releases.
# -

proc about {} {
    return [helpscript about]
}

# solar::register --
#
# Purpose:  Create registration key file
#
# Usage:    register <key>
#
# Notes:    This creates a file ~/.solar_reg containing the key.  Do
#           not delete this file.  You may copy this file to your
#           home directory on other systems to run SOLAR if the
#           same username is used.  (Each key matches only one
#           username.)
#
#           To obtain a key, please send a request to solar@txbiomedgenetics.org.
#           specifing the username(s) under which you will be using the
#           program, the email addresses of the users, and the NIH grant
#           numbers (if any) that apply to the work for which SOLAR may
#           be used.
# -

proc register {key} {
    if {[key -check]} {
	puts "You already have a valid ~/.solar_reg key file installed."
	puts "Are you sure you want to do this?  Y or N:"
	gets stdin yorn
	if {"y" != [string tolower $yorn]} {
	    return ""
	}
    }
    exec rm -f ~/.solar_reg
    exec echo $key >~/.solar_reg
    set status [key -validate]
    if {!$status} {
	error "register: Incorrect key was entered.  Try again."
    }
    return "Registration Successful!"
}

# solar::please_register -- private
#
# Please register SOLAR by sending an email to solar@txbiomedgenetics.org
# asking for a solar key(s) and providing the following information:
#
#  1.  login name(s) (on the machine(s) on which SOLAR will be run).  We need
#      the short form of your login name, usually less than 8 characters.
#-

# solar::please_register_2 -- private
#  2.  email address
#
#  3.  if used in research funded by the NIH, please specify grant number(s)
#
# If you have not gotten your registration key in a few days, please ask again.
# Meanwhile, even without registration you can use all SOLAR commands except
# those which maximize models (such as the polygenic and multipoint commands).
# -

proc please_register {} {
    helpscript please_register
    catch {
	set loginname [exec whoami]
	puts "     Your short login name appears to be: $loginname\n"
    }
    helpscript please_register_2
    return ""
}

# solar::doc --
#
# Purpose:  Find the SOLAR documentation
#
# Usage:    doc [-whereis]
#
#           doc               show official documentation URL and
#                             location of documentation files on this system
#
# Notes:    This command now tells you the URL where SOLAR may be seen with
#           any available browser.  Previously, it would run Netscape.
# -

proc doco {args} {
    return [eval doc -old $args]
}

proc doc {args} {

    global env
    set dirname [file dirname $env(SOLAR_LIB)]/doc
    
    set whereis 0
    set chapter 0
    set appendix 0
    set preface 0
    set contents 0
    set oldsession 0
    set badargs [read_arglist $args \
        -chapter chapter \
	-appendix appendix \
	-old {set oldsession 1} \
        -whereis {set whereis 1} \
    ]

    return [docwhere $dirname]
}


proc docwhere {dirname} {
    return "Point your browser to http://solar.txbiomedgenetics.org/doc/index.html\n\nOn this computer documentation files are located at $dirname"
}


# solar::example --
#
# Purpose:  Copy the SOLAR example to the current working directory
#
# Usage:    example
#
# Notes:    The example may be used in conjunction with the SOLAR tutorial
#           in Chapter 3.  To read that, give the command "doc -chapter 3"
#
#           The example files are actually located in the doc/Example
#           subdirectory of the SOLAR installation.  To find the "doc"
#           subdirectory, give the command "doc -whereis"
# -

proc example {} {

    global env
    set docdir "[file dirname $env(SOLAR_LIB)]/doc/Example"
    puts "Copying example files to working directory"
    set files [glob $docdir/*]
    foreach fi $files {
	if {![file isdirectory $fi]} {
	    file copy -force $fi .
	}
    }
    newtcl
}

proc untagify {infile outfile} {
    set ifile [open $infile r]
    set ofile [open $outfile w]
    while {-1 != [gets $ifile line]} {
	if {[string length $line] == 0} {
	    puts $ofile $line
	} else {
	    while {-1 != [string first < $line]} {
		if {-1 == [string first > $line]} {
		    break
		}
		set beginb [string first < $line]
		set endb [string first > $line]
		if {0 == $beginb} {
		    set line [string range $line [expr $endb + 1] end]
		    if {0 == [string length $line]} {
			break
		    }
		} else {
		    set prefix [string range $line 0 [expr $beginb - 1]]
		    set postfix [string range $line [expr $endb + 1] end]
		    set line [catenate $prefix $postfix]
		}
	    }
	    if {[string length $line] != 0} {  ;# if wasn't only tags
		puts $ofile $line
	    }
	}
    }
    close $ifile
    close $ofile
}


proc dominance-notes {} {
    return [helpscript dominance-notes]
}

# solar::dominance-notes --
#
# Purpose:  Find dominance documentation
#
# Dominance analysis is documented in section 9.4 of the SOLAR
# manual.  Use the "doc" command or point your browser to
# the "doc" directory of your SOLAR directory, then select
# "Go to full SOLAR manual", then select Chapter 9.
#
# Dominance analysis is made possible by the "delta7" and "d7" columns
# in SOLAR phi2.gz and ibd matrices.  For polygenic models, the delta7
# matrix column is loaded, a d2r parameter is created and added to the
# "e2" constraint, then a delta7*d2r term is added to the omega.  The
# commands required to do this are described in Section 9.4
# -


proc discrete-notes {} {
    return [helpscript discrete-notes]
}

# solar::discrete-notes --
#
# Purpose:  Describe support for discrete traits
#
# Usage:    discrete-notes
#
# Discrete traits are detected automatically by SOLAR.  They must be
# coded as two integer values separated by exactly 1.  Typical codings
# are 0,1 or 1,2.  If you specify two values that are not separated by
# exactly 1, this will be detected as an error.  If you specify more than
# two values, your trait will not be determined to be discrete.  For
# this reason, DO NOT specify missing values with a third number. 
# Missing values should always be coded as blank (" ") or null with no
# number or character.  DO NOT use "0" to signify missing values.  See
# toward the bottom of this note for advice regarding analyzing traits
# with two values quantitatively.
#
# Discrete traits having more than 2 values are not supported by SOLAR.
#
# (This is also true for discrete phenotypic covariates: if discrete, they
#  should not have more than 2 values.  If you have such data, they
#  should be recoded into N-1 discrete binary covariates or recast into
#  "household groups."  See the documentation for the "house" command.)
#
# Models with discrete traits may be used with any command in SOLAR such as
# polygenic, twopoint, multipoint, maximize, etc.  Sometimes the
# information returned by SOLAR differs.  For example, while the
# "polygenic" command normally returns "proportion of variance due to all
# covariates" when used with a quantitative trait, it instead returns the
# "Kullback-Leibler R-squared" when used with a discrete trait.  (For
# technical reasons, the proportion of variance due to all covariates is
# not available for discrete trait models in SOLAR.)
#
# By examining the maximization output files you can determine unambiguously
# whether discrete or quantitative methods were used.  (An example of
# a maximization output file is "null0.out" saved in the maximization
# output directory after running "polygenic".)  In this file, immediately
# after the "Descriptive Statistics" and immediately before the "Model
# Parameter Starting Points and Boundaries" there will be one of two
# comments, either:
#
#                Using SOLAR Quantitative Trait Modeling
#
# or
#
#                  Using SOLAR Discrete Trait Modeling
#
# When a model with a discrete trait is maximized, special discrete trait
# algorithms are used. Unfortunately, these methods are much more prone
# to numerical problems than the usual "quantiative trait" methods.
# Numerical problems lead to faulty parameter estimates and convergence
# failures.
#
# The default descrete method is relatively robust and only infrequently
# has the problem where the heritability erroneously gets close to 1.0.
#
# Even if the polygenic heritability (h2r) goes to 1.0, you may still be
# able to run a "multipoint" linkage analysis to find important locii.
# The heritibilities will be wrong, and the LOD scores will be wrong,
# but the "peaks" may be at or near the correct locations.
#
# It is not recommended to use the optional second discrete method set by
# giving the command "option DiscreteMethod 2" prior to running
# polygenic.  Although it was intended to be more accurate, it more
# frequently fails with convergence errors or having the heritability go
# to 1.0, and at this time it is not recommended.
#
# Some people also try analyzing their discrete trait as quantitative.
# This can be done by giving the command "option EnableDiscrete 0".
# The likelihoods, LODS, and parameter estimates may be inaccurate, but the
# LOD peaks should be in the correct places.  Better convergence is
# sometimes obtained, however, than when using the discrete method.
#
# Beware that there is a fundamental error when analyzing a discrete trait
# as quantitative.  There are not truly two degrees of freedom for the
# mean and SD.  Therefore, convergence failure is still more common with these
# models than with true quantitative models.
#
# Also beware that if you had previously analyzed the trait as discrete,
# and then changed the EnableDiscrete option to 0 without exiting SOLAR
# or giving the "model new" command, you will still have parameter SD
# constrained to 1.0, which is probably NOT what you need to do.  SD is
# properly constrained to 1.0 only when you are analyzing a discrete trait
# as discrete (and, perhaps, in a few other esoteric cases).
#
# Because of all the pitfalls in using discrete traits, we try to find and
# use relevant quantitative traits whenever possible.
#-

# solar::file-matrix --
#
# Purpose:  Describe csv matrix file format requirements
#
# CSV matrix files were introduced in SOLAR version 7.5.0, and it is
# conditionally recommended that all users who are creating their own matrix
# files use this format, as it is more easily understood and created than the
# previous format.  Matrix writers should beware that there are many possible
# pitfalls in hand writing a matrix file, and matrix files should at minimum
# be checked with the "matrix debug" command after loading the first time.
#
# The original space column delimited matrix file is still created and used by
# SOLAR itself and documented in the manual, and if you merely wish to modify
# an existing matrix file, it may still be easiest to use the original format.
# The format of the phi2.gz kinship matrix file is described in Section 8.3
# of the manual, and the same rules would apply to any original format matrix
# file.
#
# The csv matrix file is an ordinary comma separated variable file with the
# first line being a header which names all the fields used, as is common
# with CSV files.  However SOLAR requires that all matrix files, including
# csv matrix files, be compressed using the gzip program.  Thus all matrix
# files will have the final filename extension ".gz".
#
# All csv matrix files must have the following three required fields:
# id1, id2, matrix1.  id1 and id2 are user assigned ID's, as used in pedigree
# and phenotypes files.  matrix1 is typically the primary matrix in the file,
# for example the primary matrix in the phi2.gz file is the "phi2" matrix.  A
# csv matrix file may also have a second matrix named matrix2.  If FAMID's are
# required to disambiguate ID's in your dataset, you must also include
# famid1 and famid2, the famid's corresponding to id1 and id2.  SOLAR will
# determine whether famid's are needed or not from the pedigree file.  If
# famid's are not needed, famid1 and famid2 will be ignored if they are
# present.  If famid's are needed and not present in the matrix file, the
# load matrix command will generate an error and the matrix will not be loaded.
#
# Note that is the "load matrix" command which actually assigns meaningful
# names to matrixes for use in the SOLAR omega.  For example, the phi2
# matrix is usually loaded with the following command:
#
#   load matrix phi2.gz phi2
#
# Alternatively, an analysis examining dominance would require a second
# matrix, delta7:
#
#   load matrix phi2.gz phi2 delta7
#
# In both cases the first matrix (called matrix1 in a CSV matrix file) will
# be associated with the name phi2 in the omega.  The second command will
# also associate the second matrix (matrix2) with the name delta7.  Neither
# command shown let you directly use the names matrix1 and matrix2 in the
# omega, unless those names were also specified in the load matrix command.
# The CSV naming scheme is intended to allow the "load matrix" command to be
# used exactly as it was before.
#
# Variable names other than id1, id2, matrix1, matrix2, famid1, and famid2
# in a CSV matrix file will be (in this version) ignored.
#
# There are other semantic requirements for matrix files, depending
# on the type of matrix involved.  One is that every individual in an analysis
# must be included in the matrix file regardless of whether they have pairwise
# relationships with others in the file.  At minimum every individual has a
# self relationship of value 1.0.  With version 7.5.0 it is required that you
# include these "diagonal" matrix values of 1.0 for every individual in your
# sample, and possibly everyone in your pedigree file.  Otherwise, diagonal
# values will default to -1 which could have bad consequences.  (Note: in
# version 7.5.0 it is not possible to check this with matrix debug because
# it only checks values in the input file, not defaulted values in the
# matrix itself.)
#
# It is not permitted to have individuals in the CSV matrix file who are not
# defined in the pedigree file.
#
# Historically matrix files were dependent on the pedindex.out files created
# when the pedigree was loaded.  This is because the very IBDID's used in
# original format matrix files might be assigned to different actual ID's
# if the pedigree is changed.  CSV format matrix files are less dependent,
# but it is still likely that changes to a pedigree file would require
# corresponding changes in the matrix file.  And sometimes this may be
# overlooked, causing disasterous results.  SOLAR has long prepended a
# pedigree checksum record to all the matrix files it creates which are checked
# against the pedigree file when the matrix is loaded.  Changes to the
# pedigree after the matrix file was created will cause an error to
# be raised when attempting to load that matrix file.
#
# The same checksum checking features can optionally be used in CSV matrix
# files.  Once the matrix has been created and compressed using gzip, the
# procedure "matcrc" can be run on them, for example:
#
# solar> matcrc phi2.csv.gz
#
# This will determine a checksum value from the currently loaded pedigree
# file, and prepend this to the matrix file in a record with id1=checksum
# and id2=checksum.  It also will gunzip the matrix file to perform this
# change, and gzip after the change has been made.
# 
# If the names assigned to matrixes (in the load matrix command) begin with
# "ibd" or "mibd" a special defaulting rule applies.  -1 values found in these
# matrixes mean that the actual value should be taken from the phi2 matrix
# (for the first matrix) or delta7 (for the second matrix).  Furthermore, if
# -1 is READ FROM THE FILE for a diagonal matrix entry, the default is at
# that time applied to every pair including that individual.  (On the other
# hand, if a -1 occurs on the diagonal only because the diagonal entry was
# missing from the file, defaulting would occur for the missing diagonal but
# not for every other pair including that individual, the default value for
# those other pairs would be zero.)  This feature had historical importance
# but is considered obsolescent now and is not recommended for use in new
# matrix files.
#- 

proc file-matrix {} {
    return [helpscript file-matrix]
}

# solar::file-phenotypes --
#
# Purpose:  Describe phenotypes data file requirements
#
# The phenotypes file may be in either PEDSYS or Comma Delimited format.
#
# The phenotypes file consists of one record for each individual.
# Each record must include an ego ID and one or more phenotypic
# values (which may be blank to signify missing data).
#
#    ego ID, phen 1, phen 2, ...
#
# (The phenotypes file may also contain other data, such as pedigree
# data.  You could use one file as both your phenotype and your
# pedigree file, though that is not necessarily recommended.  There
# are fewer possible problems with separate files.)
#
# Just as with the pedigree file, a field name FAMID is required when
# IDs are not unique across the entire data set.  (If your ego IDs
# are unique, it is probably better _not_ to include a family ID,
# as it just complicates things slightly.)
#
# If your data has probands and you wish to employ ascertainment
# correction, the phenotypes file must have a proband field.  In this
# field, blank ( ) or zero (0) signifies non-proband, and anything
# else signifies proband.  A decimal point is _not_ permitted after
# the zero.  The presence of a proband field automatically turns on
# ascertainment correction.
#
# The default field names are ID, FAMID, and PROBND.  You can set up
# SOLAR to use different field names by using the field command.
#
# The phenotype field names may be anything within certain rules.
# (no spaces, tabs, or slashes; also certain special characters such
# as *#,^/-+ can cause problems in the names of phenotypes used
# as covariates).  If you stick with alphabetic characters, numeric
# characters, and underscores you will be safe.
#
# The phenotype data fields must be numbers, either with or without
# decimal points.  Zero (0) is always considered a permissible value;
# blank ( ) or null (e.g. no value in between the commas ",," in a
# comma delimited file) must be used to signify missing values.
#
# Floating or fixed point numbers must always include a decimal
# point; numbers without a decimal point are assumed to be integers.
# Binary, discrete or categorical values should be indicated with
# consecutive integers (e.g. 0,1 or 1,2 or 2,3).  SOLAR checks all
# phenotype fields to see if they contain only two consecutive
# integers and judges them "binary" if they do.  Binary traits are
# automatically handled by the SOLAR discrete trait liability
# threshold modeling code; you don't need to do anything special.
# See Chapter 9 for discussion on what to do with "categorical"
# data that has more than two categories.
#
# Without special scripting, categorical phenotypes with more than two
# categories should not be used in SOLAR.  (SOLAR will not identify
# categorical phenotypes with more than two categories and instead
# treat them as quantitative phenotypes.)
#
# The 'load phenotypes' command creates a file named phenotypes.info
# in the working directory.  Once a phenotypes file has been loaded,
# it need not be loaded again in the same working directory, unless
# you change the file itself.
#
# SOLAR automatically removes pedigrees in which no non-proband has
# all required phenotypic data from the analysis.  You need not
# remove these pedigrees yourself.  You will get a full accounting of
# pedigrees and individuals included and excluded in the maximization
# output files (described below) , by running the 'maximize' command,
# or giving the 'verbosity max' command prior to other commands.
#-

proc file-phenotypes {} {
    return [helpscript file-phenotypes]
}

# solar::file-pedigree --
#
# Purpose:  Describe pedigree data file requirements
#
# The pedigree file consists of one record for each individual in the data
# set.  Each record must include the following fields:
#
#     ego ID, father ID, mother ID, sex
#
# In addition, a family ID is required when ego IDs are not unique across
# the entire data set.  If the data set contains genetically identical
# individuals, an MZ-twin ID must be present (as described below).  If an
# analysis of household effects is planned, a household ID can be included
# (also described below).
#
# The default field names are ID, FA, MO, SEX, FAMID, MZTWIN, and HHID.
# EGO, SIRE, and DAM are also accepted by default.  You can set up SOLAR to
# use different field names by using the field command (see 'help field').
# You do not necessarily need to change your names to match ours.
#
# A blank parental ID or a parental ID of 0 (zero) signifies a missing
# parent.  SOLAR requires that either both parents are unknown, i.e. the
# individual is a founder, or both parents are known.
#
# If the pedigree data consists of unrelated individuals with no parental
# data, then the father ID and mother ID fields are not required. If there
# are parents for whom pedigree file records do not exist, then records
# are created internally for those parents, who are assumed to be founders.
#
# Sex may be encoded as M, m, or 1 for males and F, f, or 2 for females.
# The missing value for sex is 0, U, u, or blank.
#
# The MZ-twin ID is used to designate genetically identical individuals,
# e.g. monozygotic twins or triplets.  Each member of a group of identical
# individuals should be assigned the same MZ-twin ID.  Twin IDs must be
# unique across the entire data set.  If there are no genetically identical
# individuals in the data set, this field need not be present in the
# pedigree file.
#
# The household ID, if present, will be used to generate a matrix file
# (house.gz) that can be used later to include a variance component for
# household effects.  Household IDs must be unique across the entire data
# set.
#
# The family ID field is required only when ego IDs are not unique across
# the entire data set.  For example, if a data set consists of nuclear
# families, and the same ego ID may appear in more than one family, then
# the family ID must be included.  Or if, for example, IDs are sequential
# integers unique only within pedigrees, then the pedigree ID must be
# included.
#
# At the time the pedigree file is loaded, SOLAR indexes the data set.
# This indexing is internal and should not be confused with any external
# indexing the user may have imposed upon the data set.  This indexing
# information is stored in a file named 'pedindex.out' in the directory
# where SOLAR is running when the pedigree data is loaded.  Be careful
# about deleting files unless you are sure they are not needed by SOLAR!  
#
# Once a pedigree file has been loaded, it is not necessary to load
# it again in subsequent SOLAR runs from the same working directory.
# -

proc file-pedigree {} {
    return [helpscript file-pedigree]
}

# solar::file-marker --
#
# Purpose:  Describe marker data file requirements
#
# The marker file contains genotype data for one or more marker loci.
# The file consists of one record for each individual who has been typed
# for one or more of these markers.  Each record must contain the following
# fields:
#
#     ego ID, genotype1, genotype2, ...
#
# In addition, a family ID field must be included when ego IDs are not
# unique across the entire data set.  If, however, each ego ID is unique
# to an individual and an individual may appear multiple times in the
# data set, then the family ID should not be included.  The same genotypic
# data is then associated with every occurrence of an individual.
#
# The default field names are ID and FAMID.  EGO is also accepted by
# default.  You can set up SOLAR to use different field names by using
# the field command (see 'help field').  You do not necessarily need to
# change your names to match ours.
#
# Fields with names other than ID and FAMID are assumed to contain marker
# data, with the exception of the following names: FA, MO, SEX, MZTWIN,
# HHID, AGE, PEDNO, and GEN. Fields having one of these names are ignored.
#
# The scheme used to encode genotypes may vary from field to field.
# SOLAR recognizes many standard coding schemes, but the safest way to
# code genotypes is with the forward slash to separate the alleles.
#
# Ex: AB
#     E1 E3
#     123/456
#
# A blank genotype field denotes missing data, as do the genotypes 0/0
# and -/-.  SOLAR requires that either both alleles are typed or both
# alleles are missing, except for male genotypes at X-linked marker loci.
# In that case, either a single allele is specified (the other allele is
# blank, 0, or -), or the genotype is coded as a "homozygote".
#
# Ex: 237/243   valid female X-linked marker genotype
#        /251   valid male X-linked marker genotype
#       251/0   valid male X-linked marker genotype
#       -/251   valid male X-linked marker genotype
#     251/251   valid male X-linked marker genotype
#
# The marker loci in the marker file must all be autosomal or all be
# X-linked.  By default, SOLAR assumes that the markers are autosomal.
# If the markers are X-linked, then either the XLinked option must be
# set with the ibdoption command prior to loading the marker file, or
# the -xlinked option must be given in the load marker command.
#
# Once a marker file has been loaded, it is not necessary to load it
# again in subsequent SOLAR runs from the same working directory.
# -


proc file-marker {} {
    return [helpscript file-marker]
}

# solar::file-freq --
#
# Purpose:  Describe frequency data file requirements
#
# The freq file contains allele frequency data for a set of marker loci,
# one line per marker.  Each line consists of the following space-delimited
# fields:
#
#     marker name, all_1 name, all_1 freq, all_2 name, all_2 freq, ...
#
# The allele frequencies for a marker must sum to 1 (a small roundoff error
# is tolerated.)
#
# Allele frequency information is used when IBDs are computed for a marker
# that is not completely typed, i.e. there are individuals for whom genotype
# data is not available.
#
# Example:
#
# D20S101 123 0.2457 127 0.1648 133 0.5895
# IGF1 A 0.4 B 0.3 C 0.1 F 0.2
# ApoE E1 .125 E2 .25 E3 .625
#
# Once a freq file has been loaded, it is not necessary to load it again
# in subsequent SOLAR runs from the same working directory.
# -

proc file-freq {} {
    return [helpscript file-freq]
}


# solar::file-map --
#
# Purpose:  Describe map data file requirements
#
# The map file contains chromosomal locations for a set of marker loci
# on a single chromosome.  Typically, marker locations are given in cM
# and a mapping function is used to convert inter-marker distances to
# recombination fractions.  Currently, the Kosambi and Haldane mapping
# functions are allowed.  Marker locations can also be specified in
# basepairs.  While cM locations can be floating point numbers, basepair
# locations must be integers; non-integer locations are truncated to
# integers.  When basepair locations are used, the mapping function is
# called "basepair" rather than Kosambi or Haldane, but in fact there
# is no mapping provided from basepairs to recombination fractions and
# such maps cannot be used to compute multipoint IBDs.  The first line
# of the map file contains the chromosome number, and (optionally) the
# name of the mapping function.  If no mapping function is specified,
# the mapping is assumed to be Kosambi.  The chromosome number can be
# any character string not containing a blank or a forward slash (/),
# although the use of integers is recommended.  For example, the strings
# '01' and '10q' are allowed.  Each line after the first line consists
# of the following space-delimited fields:
#
#     marker name, marker location
#
# Examples:
#
# 20
# D20S101         0.0
# D20S202        34.2
# D20S303        57.5
#
# TCF basepair
# 2448b   19828941
# 380659  19829489
#
# -

proc file-map {} {
    return [helpscript file-map]
}


# Check phenotypes file for some errors:
#   File not found
#   Missing FAMID when required

proc check_phenotypes {args} {

# If pedigree file not loaded, the safer assumption is that it does
# have FAMID field.  If it doesn't, ID's must be unique anyway

    set pedfilefam 1
    catch {
	if {[file exists pedindex.out]} {
	    set pedfile [tablefile open pedindex.out]
	    set pedfilefam [tablefile $pedfile test_name famid]
	    tablefile $pedfile close
	}
    }

# See if each phenotypes file exists, then see if it has famid
# If any phenotypes files are missing famid, can't use famid

    set phenfilefam 1
    foreach phenfilename $args {

	if {![file exists $phenfilename]} {
	    error "phenotypes: File $phenfilename not found"
	}
	set phenfile [solarfile open $phenfilename]
	set testfilefam [solarfile $phenfile test_name famid]
	if {!$testfilefam} {
	    set phenfilefam 0
	}
	solarfile $phenfile close
    }

    ifdebug puts "Using famid: pedfile: $pedfilefam    phenfile: $phenfilefam"

    set outfilefam $phenfilefam
    if {$phenfilefam != $pedfilefam} {
	set outfilefam 0
	foreach phenfilename $args {
	    set ids {}
	    set phenfile [solarfile open $phenfilename]
	    solarfile $phenfile start_setup
	    solarfile $phenfile setup id
	    while {{} != [set record [solarfile $phenfile get]]} {
		lappend ids [lindex $record 0]
	    }
	    set idss [lsort $ids]
	    set index 0
	    while {{}  != [set next [lindex $idss [expr 1 + $index]]]} {
		set this [lindex $idss $index]
		if {![string compare $this $next]} {
		    solarfile $phenfile close
		    error \
	"Duplicate ID's in $phenfilename; have you mislabeled FAMID field?"
		}
		incr index
	    }
	    solarfile $phenfile close
	}
    }
    return $outfilefam
}

# solar::ibddir --
#
# Purpose:  Set directory in which IBD matrix files are stored
#            (twopoint only; use mibddir to set up multipoint)
#
# Usage:    ibddir <dirname>     ; set director for IBD files
#           ibddir               ; show current ibddir
#           ibddir -session      ; show ibddir entered in this session
#
# Notes:    The ibddir selected is saved in file ibddir.info for
#           future SOLAR sessions.  Once a midddir is selected, it
#           need not be selected again within the same working directory,
#           EXCEPT for the purposes of writing out ibd files.  To
#           prevent accidentally overwriting pre-existing ibd files,
#           it is necessary to explicitly enter the ibddir
#           command before using the ibd command or other commands
#           which write files into the ibddir.
# -

# solar::mibddir --
#
# Purpose:  Set directory in which MIBD matrix files are stored
#            (multipoint only; use ibddir to set up twopoint)
#
# Usage:    mibddir <dirname>     ; set directory for MIBD files
#           mibddir               ; show current mibddir
#           mibddir -session      ; show mibddir entered in this session
#
# Notes:    The mibddir selected is saved in file mibddir.info for
#           future SOLAR sessions.  Once a midddir is selected, it
#           need not be selected again within the same working directory,
#           EXCEPT for the purposes of writing out mibd files.  To
#           prevent accidentally overwriting pre-existing mibd files,
#           it is necessary to explicitly enter the mibddir
#           command before using the mibd command or other commands
#           which write files into the mibddir.
# -

# solar::snpdir -- private
#
# Purpose:  Set directory in which snp related files are to be put or found
#           (See "snp".)  NOTE: This command is not supported for any
#           useful purpose.
#
# Usage:    snpdir <dirname>     ; set directory for MIBD files
#           snpdir               ; show current mibddir
#           snpdir -session      ; show mibddir entered in this session
#
# Notes:    The snpdir selected is saved in file snpdir.info for
#           future SOLAR sessions.  Once a midddir is selected, it
#           need not be selected again within the same working directory,
#           EXCEPT for the purposes of writing out mibd files.  To
#           prevent accidentally overwriting pre-existing mibd files,
#           it is necessary to explicitly enter the snpdir
#           command before using the mibd command or other commands
#           which write files into the snpdir.
# -

proc ibddir {args} {
    return [eval ibdmibddir ibddir $args]
}

proc mibddir {args} {
    return [eval ibdmibddir mibddir $args]
}

proc snpdir {args} {
    return [eval ibdmibddir snpdir $args]
}


proc ibdmibddir {dirt args} {
    set save 1
    set session 0
    set args [read_arglist $args -nosave {set save 0} \
		  -session {set session 1}]

    global Solar_$dirt
    if {$args == {}} {
	if {![if_global_exists Solar_$dirt]} {
	    error "No $dirt specification has been given"
	}
	if {$session && ![if_global_exists Solar_Session_$dirt]} {
	    error "No $dirt specification has been given in this session"
	}
	eval return \$Solar_$dirt
    }

# IMPORTANT!!! BEYOND THIS POINT WE DELETE $dirt.info for all errors!!!
# (unless -nosave option)

    purge_global Solar_$dirt
    if {$save} {
	global Solar_Session_$dirt
	set Solar_Session_$dirt 1
    }
    set dir [lindex $args 0]

    global Solar_Forbidden_Ibd_Root
    if {[if_global_exists Solar_Forbidden_Ibd_Root]} {
	set fsir $Solar_Forbidden_Ibd_Root
	set fsirend [expr [string length $fsir] - 1]
	if {-1 < $fsirend} {
	    if {![string compare [string range $dir 0 $fsirend] $fsir]} {
		set froot [string range $fsir 0 [expr $fsirend - 1]]
		if {$save} {catch {file delete $dirt.info}}
		error "$dirt must not be a subdirectory of $froot"
	    }
	}
    }
    if {![file isdirectory $dir]} {
	if {$save} {
	    catch {file delete $dirt.info}
	    error "No such directory: $dir"
	}
    }
    set absolute_dir [make_absolute_pathname $dir]
    set Solar_$dirt $absolute_dir

# Now, save mibddir to a state file, but only if different
# from previous state file

    if {$save} {
	set changed 1
	if {0==[catch {set testfile [open $dirt.info]}]} {
	    if {-1 != [gets $testfile line]} {
		if {0 == [string compare $line $absolute_dir]} {
		    set changed 0
		}
	    }
	    close $testfile
	}
	if {$changed} {
	    exec echo $absolute_dir > $dirt.info
	}
    }
    return ""
}

# Invoked by solar.cc to initialize ibddir and mibddir

proc Start_Dirs {} {

    foreach dir {ibddir mibddir snpdir} {
	if {[file exists $dir.info]} {
	    set dfile [open $dir.info r]
	    if {[gets $dfile line]} {
		if {[catch {$dir -nosave $line} errmes]} {
#		    puts "\nError initializing $dir: $errmes"
		}
	    }
	    close $dfile
	}
    }
    return ""
}


# solar::chromosome --
#
# Purpose:  Select chromosome(s) for multipoint scan
#
# Usage:   chromosome [<number>|<name>|<low>-<high>|all|*]+    ;select
#          chromosome                  ; show currently selected chromosomes
#          chromosome show             ; show all available chromosomes
#          chromosome showm            ; show mibd's in pass (see note 2)
#
# Examples: 
#          chromosome 10
#          chromosome 10-13 15-17 20
#          chromosome 11 11p
#          chromosome all              ; select all available chromosomes
#          chromosome *                ; select all available chromosomes
#
# Notes:   Use in conjunction with mibddir, interval, multipoint commands.
#
#  (2)  The showm option lists the mibds's that will be selected by
#       the current "chromosome" and "interval" commands.
#
#  (3)  Alphanumeric chromosomes may not be in <low>-<high> ranges, but may
#       be selected individually (for example, 11p), or with "all" or *.
#
#  (4)  The chromosome specification is not saved from one solar session
#       to the next unless put in a .solar file.
#
#  (5)  For convenience, you may specify a chromosome or range of
#       chromosomes whose mibds are not actually present, and
#       the gap is ignored silently, as long as there are some mibds
#       available for other specified chromosomes.  The chromosome
#       command acts as a filter applied to the mibd data actually
#       available.
# -

proc chromosome {args} {
    if {$args == {}} {
	if {0 == [llength [info globals Solar_Chromosomes]]} {
	    error "No chromosome specification has been given"
	}
	global Solar_Chromosomes
	return $Solar_Chromosomes
    }
    if {$args == "show"} {
	return [get_all_chromos [mibddir]]
    }
    if {$args == "showm"} {
	return [global_mibd_list]
    }
    purge_global Solar_Chromosomes
    expand_ranges $args    ;# this is only to test for errors now
    global Solar_Chromosomes
    set Solar_Chromosomes $args
    return {}
}

# solar::finemap
#
# Purpose:  Set fine mapping threshold for multipoint
#
# Usage:    finemap <LOD> [<LOD> ...]
#           finemap default
#           finemap off
#           finemap                  {displays current finemap setting}
#
# Example:  finemap 0.588
#
# Notes:    After each multipoint pass when the interval is greater than 1
#           SOLAR will examine all points in the regions around points
#           higher than some threshold.  This threshold is set with the
#           finemap command.
#
#           The default is 0.588.
#
#           Finemapping can also be turned off.  The finemap setting is
#           unimportant when the interval is 1.  (Note: versions of SOLAR
#           prior to 1.1.0 did finemapping only around the single highest
#           peak by default.)
#           
# -

proc finemap {args} {
    if {$args == {}} {
	if {0 == [if_global_exists Solar_Fine_Map]} {
	    return 0.588
	} else {
	    global Solar_Fine_Map
	    return $Solar_Fine_Map
	}
    }
    global Solar_Fine_Map
    if {0==[string compare $args default]} {
	if {[if_global_exists Solar_Fine_Map]} {
	    unset Solar_Fine_Map
	}
    } elseif {0==[string compare $args off]} {
	set Solar_Fine_Map off
    } else {
	foreach arg $args {
	    ensure_float $arg
	}
	set Solar_Fine_Map $args
    }
    return ""
}


# solar::interval --
#
# Purpose:  Set cM interval and range for multipoint scanning each chromosome
#
# Usage:      interval <count> <range> ; set increment count and range
#             interval <count>         ; default range is 0-* (* means last)
#             interval                 ; displays current setting
#
# Examples:   interval 5               ; Check every 5 cM
#             interval 1 101-109       ; Check every 1 cM between 101 and 109
#             interval 10 200-*        ; Check every 10 cM after <200 cM>
#             interval 0 100           ; Check at position <100 cM>
#             interval -5 *-100        ; Check every 5 cM from last to 100
# -

proc interval {args} {
    if {$args == {}} {
	if {0 == [llength [info globals Solar_Interval]] || \
		0 == [llength [info globals Solar_Interval_Range]]} {
	    error "No interval specification has been given"
	}
	global Solar_Interval
	global Solar_Interval_Range

	return [format "%d %s-%s" \
	        $Solar_Interval \
		[lindex $Solar_Interval_Range 0] \
		[lindex $Solar_Interval_Range 1]]
    }

# Clear out old values
	
    purge_global Solar_Interval
    purge_global Solar_Interval_Range

# Get increment_count

    set argc [llength $args]
    if {1 != $argc && 2 != $argc} {
	error "Invalid interval command"
    }
    set increment_count [lindex $args 0]
    ensure_integer $increment_count

# Get first and last

    if {$argc == 1} {
	if {$increment_count >= 0} {
	    set first 0
	    set last *
	} else {
	    set first *
	    set last 0
	}
    } else {
	set range [lindex $args 1]
	set hyphen [string first "-" $range]
	if {-1 == $hyphen} {
	    ensure_integer $range
	    set first $range
	    set last $range
	    if {$increment_count != 0} {
		error "Non-zero count with single marker"
	    }
	} else {
	    set first_and_last [split $range "-"]
	    if {2 != [llength $first_and_last]} {
		error "Range should be first-last"
	    }
	    set first [lindex $first_and_last 0]
	    set last [lindex $first_and_last 1]
	    if {0 == [string compare $first *]} {
		ensure_integer $last
		if {$increment_count >= 0} {
		    error "Interval direction inconsistent"
		}
	    } elseif {0 == [string compare $last *]} {
		ensure_integer $first
		if {$increment_count <= 0} {
		    error "Interval direction inconsistent"
		}
	    } else {
		ensure_integer $first
		ensure_integer $last
		if {$increment_count > 0} {
		    if {$first >= $last} {
			error "Interval direction inconsistent"
		    }
		} elseif {$increment_count < 0} {
		    if {$first <= $last} {
			error "Interval direction inconsistent"
		    }
		} else {
		    if {$first != $last} {
			error "Interval direction inconsistent"
		    }
		}
	    }
	}
    }
    global Solar_Interval
    global Solar_Interval_Range
    set Solar_Interval $increment_count
    set Solar_Interval_Range [list $first $last]
    return {}
}

proc global_mibd_list {} {
    if {[catch {mibddir}]} {
#	puts "Info level is [info level]"
	if {2<[info level]} {
	    error "First use mibddir command to set mibd directory"
	}
	puts -nonewline "Enter mibddir: "
        flush stdout
	gets stdin m
	mibddir $m
    }
    if {0 == [llength [info globals Solar_Chromosomes]]} {
	if {2<[info level]} {
	    error "Use chromosome command to select chromosomes"
	}
	puts -nonewline "Enter chromosomes: "
        flush stdout
	gets stdin c
	eval chromosome $c
    }
    if {0 == [llength [info globals Solar_Interval]]} {
	if {2<[info level]} {
	    error "Use interval command to set interval(s)"
	}
	puts -nonewline "Specify interval (also range if desired): "
        flush stdout
	gets stdin i
	eval interval $i
    }
    if {0 == [llength [info globals Solar_Interval_Range]]} {
	error "Use interval command to set interval(s)"
    }
    global Solar_Chromosomes
    global Solar_Interval
    global Solar_Interval_Range
    global Solar_Mibd_List
    set chromolist [expand_ranges $Solar_Chromosomes]
    set begin_range [lindex $Solar_Interval_Range 0]
    set end_range [lindex $Solar_Interval_Range 1]

    return [mibd_list [mibddir] $chromolist $Solar_Interval \
	    $begin_range $end_range]
}

proc get_all_chromos {mdir} {
    set chromolist {}
    set all_mibds [glob -nocomplain $mdir/mibd.*.*.gz]
    set plength [string length "$mdir/mibd."]
    foreach mibd $all_mibds {
	set tail [string range $mibd $plength end]
	set dotpos [string first . $tail]
	if {!$dotpos} {
	    error "Invalid mibd name $mibd"
	}
	set c [string range $tail 0 [expr $dotpos - 1]]
	set chromolist [setappend chromolist $c]
    }
    return $chromolist
}

proc mibd_list {mibddir chromolist increment begin_range end_range} {

    set mlist {}
    if {[catch {set mdir [glob $mibddir]}]} {
	error "mibddir $mibddir is empty or unavailable"
    }

    if {-1 != [lsearch $chromolist all] || \
	    -1 != [lsearch -exact $chromolist *]} {
	set chromolist [get_all_chromos $mdir]
    }

    foreach chromo $chromolist {
	set wildcard [format "%s/mibd.%s.*.gz" $mdir $chromo]
	set full_vector [glob -nocomplain $wildcard]
	
	set flength [llength $full_vector]
	if {0 == $flength} continue
	
# Find/Set begin and end numbers

	set marker_list {}
	set file_format [format "%s/mibd.%s.%s.gz" $mdir $chromo "%d"]
	foreach file $full_vector {
	    if {0 < [scan $file $file_format marker_number]} {
		lappend marker_list $marker_number
	    } else {
		error "Invalid loc name in mibddir filename [file tail $file]"
	    }
	}
	set sorted_list [lsort -integer $marker_list]
	set highest_marker_found [lindex $sorted_list [expr $flength - 1]]

	if {0 == [string compare $end_range *]} {
	    set end_marker $highest_marker_found
	} else {
	    set end_marker $end_range
	}

	if {0 == [string compare $begin_range *]} {
	    set begin_marker $highest_marker_found
	} else {
	    set begin_marker $begin_range
	}

	if {$increment >= 0} {
	    set test {$marker <= $end_marker}
	} else {
	    set test {$marker >= $end_marker}
	}

	if {$increment == 0} {set increment 1}

	for {set marker $begin_marker } $test {incr marker $increment} {
	    set testname [format "%s/mibd.%s.%d.gz" $mdir $chromo \
	                  $marker]
	    if {[file exists $testname]} {
		lappend mlist $testname
	    }
	}
    }
    return $mlist
}

# expand ranges permits alphanumeric singletons (e.g. 1,2p,2q,3-23)
proc expand_ranges {ranges} {
    set final_list {}
    foreach range $ranges {
	set hyphen [string first "-" $range]
	if {-1 == $hyphen} {
#	    ensure_integer $range
	    lappend final_list $range
	} else {
	    set first_and_last [split $range "-"]
	    if {2 != [llength $first_and_last]} {
		error "Invalid range specification"
	    }
	    set first [lindex $first_and_last 0]
	    set last [lindex $first_and_last 1]
	    ensure_integer $first
	    ensure_integer $last
	    if {$first > $last} {error "Invalid range specification"}
	    for {set i $first} {$i <= $last} {incr i} {
		lappend final_list $i
	    }
	}
    }
    return $final_list
}

# solar::deputy --
#
# Purpose:  Make limited user key (for deputy registrars)
#
# Usage::   deputy register <deputy-key>
#           deputy make <access-code> <username>
#
# Notes:
#
# 1) Deputy registrar must obtain deputy-key and access-code from
#    solar@txbiomedgenetics.org.  Key is granted for critical collaborators
#    only for use in cluster systems where normal registration process
#    is unwieldy.
#
# 2) Deputy registrar uses "deputy register" command to register as
#    deputy.  This creates a file named .solar_deputy in home directory.
#    (Note: It does not move the .solar_deputy file to SOLAR_DEPUTY_HOME
#    if that is different from the deputy's home directory.)
#
# 3) The .solar_deputy file must be copied to a user to a deputy
#    directory on all systems.  This can be done in one of two ways.
#    The default way is to access the .solar_deputy file in the home
#    directory of the deputy, which must be found in a pathname
#    with the deputy's username replacing the current username.  For
#    example if deputy registrar jsmith has registered the name pmiller,
#    and the home directory for pmiller is:
#
#        /home/pmiller
#
#    Then the .solar_deputy file must be found in directory named:
#
#        /home/jsmith
#
#    If this default method cannot be used, there is an alternate
#    method involving creating a shell variable SOLAR_DEPUTY_HOME
#    giving the path to the .solar_deputy file.  For example, the
#    following line could be added to the "solar" startup script:
#
#        export SOLAR_DEPUTY_HOME=/home/admin/jsmith
#    
# 4) The deputy registrar can now make a limited range key for each
#    user using the "deputy make" command.  The user uses the
#    normal "register" command to install the key into a file named
#    .solar_reg in the user's home directory.  The .solar_reg file
#    AND the .solar_deputy file (located as described in note 3)
#    must be found on each system where SOLAR is to be run because
#    both are used in the validation process for keys created by
#    deputy registrars.
#
# 5) The "deputy make" command adds the usernames registered to a file
#    named "solar_registrations" in your home directory.  The contents
#    of this file should be sent to solar@txbiomedgenetics.org on at least
#    a biannual basis.
#
# 6) Username must be 2 characters or longer.
# -

proc deputy {args} {
    if {"register" == [lindex $args 0]} {
	if {[llength $args] < 2} {
	    error "deputy: must specify <deputy-key>"
	}
	if {[file exists ~/.solar_deputy]} {
	    puts ""
	    puts "Note: Use the 'deputy make' command to make user keys."
	    puts "You already have a .solar_deputy file in your home directory"
	    puts "Changing the file will invalidate existing user keys !!!"
	    error "Error! You must delete or rename the old .solar_deputy first.\n"
	}
	exec echo [lindex $args 1] >~/.solar_deputy
	return ""
    }
    if {"make" == [lindex $args 0]} {
	if {[llength $args] < 3} {
	    error "deputy: must specify access code and username"
	}
	set access_code [lindex $args 1]
	set foruser [lindex $args 2]
	exec echo $foruser >>~/solar_registrations
	return [key deputy $access_code $foruser]
    }
    error "invalid arguments to deputy command"
}


# solar::outdir --
#
# Purpose:  Set maximization output directory (overriding default)
#
# Usage:    outdir <dirname>
#           outdir                   ; shows current outdir
#           outdir -default          ; restore default: (trait name)
#
# Notes:   By default, solar models and related output are written to
#          the maximization output directory.  By default, that directory
#          is named after the trait.*  For bivariate models, the trait
#          names are separated by a period (".").
#
#          The default output directory can be overridden by this command.
#          Once set, it stays overridden until the "outdir -default" command
#          is given, or a new SOLAR session is started.
#
#          (*The directory will be named after the trait as entered in the
#           trait command, rather than as it exists in the phenotypes file.
#           For example, it will be named 'foo' if the command 'trait foo'
#           has been given, even if the variable is actually named FOO.)
#
#          To prepend the name of the maximization output directory to
#          any filename, use the "full_filename" command.
# -

proc outdir {args} {
    if {{} == $args} {
	if {0 == [llength [info globals Solar_Out_Dir]]} {
	    if {![catch {trait}]} {
		error \
	"Maximization output directory is defaulting to trait name: [full_filename ""]"
	    } else {
		error \
   "Maximization output directory is defaulting to trait name (none selected)."
	    }
	}
	global Solar_Out_Dir
	return "outdir $Solar_Out_Dir"
    } elseif {"-default" == $args} {
	purge_global Solar_Out_Dir
	return ""
    }
	
    purge_global Solar_Out_Dir
    set dir [lindex $args 0]
    if {[file exists $dir] && ![file isdirectory $dir]} {
	error "File $dir exists but is not directory"
    }
    global Solar_Out_Dir
    set Solar_Out_Dir $dir
    return ""
}


# solar::allsnp
#
# Purpose:  Include all snps as covariates in current model
#
# Usage:    allsnp
#
# Notes:    allsnp includes all the phenotypes prefixed with snp_ or
#           hap_ as covariates in the current model.  This is often the
#           first step in a qtn analysis.  Afterwards, you can remove
#           some snps using the "covariate delete" command.
#
#           It is OK if you have already selected other covariates,
#           including some of the snps.  Every covariate is only added
#           once no matter what.
#
#           allsnp looks at all currently loaded phenotype files.
# -

proc allsnp {} {

    set phens [phenotypes]
    foreach phen $phens {

	if {[string_imatch snp_ [string range $phen 0 3]] || \
		[string_imatch hap_ [string range $phen 0 3]]} {

	    if  {![string_imatch ":" [string range $phen end end]]} {
		covariate $phen
	    }
	}
    }
}


# solar::automodel --
#
# Purpose:  Default model setup
#
# Usage:    automodel <phenotypes> <trait>
#              phenotypes is the name of phenotype file
#              trait is the name of trait
#                  (all other variables will be used as covariates)
#
# Notes:   1.  Automodel will create a new model, with all non-trait and
#              non-pedigree variables as covariates (see note 2).
#
#          2.  The pedigree-related fields listed by the 'field' command
#              will not be used as covariates (except for SEX, which will be).
#              Certain other standard PEDSYS names are in the default exclude
#              list.  You can add additional items to the exclude list with
#              the exclude command.  See 'exclude' and 'allcovar' help.
#
#          3.  Boundaries and starting points are set automatically by the
#              maximize command.
#
#          4.  You can pick and choose from the commands that automodel uses
#              if you want to do things differently. Here is the body of 
#              automodel:
#
#                  model new                   ;# Start a new model
#                  phenotypes load filename    ;# load phenotypes file
#                  trait traitname             ;# assign trait variable
#                  allcovar                    ;# assign covariates
#                  polymod                     ;# set polygenic model type
# -

proc automodel {phenotypesname traitname} {
    model new
    phenotypes load $phenotypesname
    trait $traitname
    allcovar
    polymod
    return {}
}

proc get_chromosome {mibdfilename} {
    set mibdfilename [file tail $mibdfilename]
    set strings [split $mibdfilename .]
    if {4>[llength $strings]} {
	error "Invalid mibdfilename $mibdfilename"
    }
    return [lindex $strings 1]
}


proc get_locus {mibdfilename} {
    set mibdfilename [file tail $mibdfilename]
    set strings [split $mibdfilename .]
    set length [llength $strings]
    if {4>$length || 5<$length} {
	error "Invalid mibdfilename $mibdfilename"
    }
    set locus [lindex $strings 2]
    if {5 == $length} {
	set locus $locus.[lindex $strings 3]
    }
    return $locus
}


# solar::multipoint --
#
# Purpose:  Perform a multipoint analysis.
#             Scan loci on selected chromosomes at selected interval
#             (use chromosome, interval, and finemap commands beforehand)
#
# Usage:   multipoint [<LOD1> [<LOD2> [<LOD3> ...]]] [-overwrite] [-restart]
#                     [-renew mod] [-nullbase] [-plot] [-score]
#                     [-cparm <plist>] [-rhoq <fixed value>] [-saveall]
#                     [-ctparm <plist>] [-se]
#
#          Zero or more criterion LOD scores may be specified.  If none are
#          specified, multipoint will make one full scan and then stop.  If
#          one LOD score is specified, multipoint will continue scanning
#          until the highest LOD found in the last scan is no longer
#          greater than or equal to the LOD score specified.  If more than
#          one LOD score is specified, each LOD will apply after one scan
#          has been completed.  Then the last LOD specified will remain in
#          effect.
#
#          -overwrite  (or -ov) Overwrite existing multipoint output files.
#
#          -restart    (or -r) Restart previous multipoint run
#
#          -nose       Don't bother computing standard errors in best
#                      models (S.E.'s are not normally computing while
#                      scanning anyway).  This should not be combined with
#                      -slod option.
#
#          -plot       plot each point while scanning (uses plot -quick)
#                        Shows the current chromosome in the current pass,
#                        ending with the last chromosome in the last pass.
#                        To view previous passes, or for best quality plot,
#                        use the plot command.  The plot command may be run
#                        simultaneously in other SOLAR sessions plotting the
#                        same data from the multipoint*.out files.  For more
#                        information, see help for the plot command.
#
#          -score        Use Score based LOD (S-LOD) defined as:
#                        SLOD (score(i)^2 * SE(i))/(2 ln (10)) (where i is 
#                        the index of the new parameter).
#
#          -cparm <plist>  Custom parameters.  (See also -ctparm.)  This is
#                          discussed in Section 9.5 of the manual.  Scanning
#                          will consist of replacing one matrix with another
#                          matrix, everything else is unchanged.  The
#                          starting model MUST be a "prototype" linkage model
#                          will all the desired parameters, omega, and
#                          constraints.  Starting points and boundaries for
#                          all variance parameters must be explicitly
#                          specified.  Following the -cparm tag, there must be
#                          a list of parameters in curly braces that you want
#                          printed out for each model.  The list can be empty
#                          as indicated by an empty pair of curly braces {}.
#                          The matrix to be replaced must have name mibd or
#                          mibd1, mibd2, etc.  The highest such mibd will be
#                          replaced.  If the loaded matrix has two columns,
#                          each succeeding matrix will also be loaded with two
#                          columns.  There must be a model named null0 in
#                          the maximization output directory for LOD
#                          computation.  See section 9.5 for an example of
#                          custom parameterization.  Note: the user's
#                          starting model is saved in the output directory
#                          as multipoint.template.mod.  Any or all parameters
#                          in the <plist> may also be multiple-term
#                          expressions.  See second example below.
#
#                          After revision in version 4.1.5, -cparm now
#                          reloads the prototype model at the beginning of
#                          each chromosome or during finemapping if there is
#                          a gap greater than 11cm.  This provides much more
#                          stable operation of -cparm and fixes the problems
#                          that led most people to use -ctparm.  However,
#                          -ctparm may be preferable in some cases where
#                          there are convergence errors.  Or vice versa.
#                          Another strategy is to set the interval to 1.
#                        
#          -ctparm <plist> Custom parameters, as -cparm, but rebuilding each
#                          model from the "prototype" linkage model.  This
#                          might be slower, but it has the advantage of
#                          greater reliability.  If any model ends up with
#                          parameter(s) on boundaries, it has no ill effect
#                          on the remaining models.
#
#          -se           Calculate standard errors in all linkage models.
#                        Otherwise, they are always NOT calculated.  This
#                        is mainly useful in conjunction with -cparm
#                        and -ctparm.  See second example below.
#                        
#
#          -link <proc>  Use specified (by name) procedure instead of
#                        linkmod (default) to move to the next locus.
#                        The procedure requires 1 argument which is the
#                        full relative or absolute pathname to the mibd file.
#                        For now, it should ignore additional arguments
#                        (use a trailing "args" argument to do this).
#
#          -nullbase     Reload null model as base for each linkage model.  
#                        The default is to start from the previous linkage
#                        model if on the same chromosome.
#
#          -epistasis N   Use current loaded model as the base for a one-pass
#                         epistasis scan.  N is the index of the mibdN to
#                         be included in epistatic interactions (e.g. 1 for
#                         mibd1).  An additional parameter H2qE1 will be
#                         added for the interaction term (for mibdN and
#                         mibd<scan>).  Output files will be named
#                         multipointe.out and multipointe1.out.  Only one
#                         epistasis pass is currently supported; if
#                         oligogenic scanning is desired that should be
#                         done first before running an epistasis scan.  At the
#                         one QTL where mibdN and mibd<scan> are the same,
#                         h2q<scan> is constrained to zero (it is not and
#                         should not be constrained to zero elsewhere).
#
#          -rhoq <value>  Constrain rhoq parameters to <value>.
#
#          -saveall       Save the multipoint linkage models for every locus
#                         tested, not just the best ones.  The filenames look
#                         like this: multi.pass1.2.3.mod for pass 1, chromosome
#                         2, locus 3.  The maximization output files are saved
#                         also following the same naming convention but with
#                         a .out suffix.  Warning!  This can fill up a lot of
#                         harddrive space quickly.  It is recommended to
#                         restrict this to a chromosome and/or range (set with
#                         the interval command) of interest.
#                         
# Examples: multipoint 3 1.9
#
#          This will first do a full scan and zero-in scan once, then, if the
#          highest LOD >= 3, it will scan again.  If the next highest 
#          LOD >= 1.9, it will continue scanning until the last highest
#          LOD < 1.9.
#
#          trait q4
#          polymod
#          maximize
#          save model q4/null0
#          linkmod gaw10mibd/mibd.9.1.gz
#          option standerr 1
#          multipoint -ctparm {h2r h2q1 {par h2q1 se}} -se
#
#          This illustrates a simple use of the "custom parameterization"
#          option.  Note that unlike the typical use of the multipoint
#          command, it is necessary to create a "prototype" linkage model
#          first (here it is done with the linkmod command, but one might
#          also use linkqsd or build the model "by hand" setting up the
#          parameters and omega).  The list of parameters following -ctparm
#          may also include commands enclosed in a second level of braces.
#          The command must include more than one element as it is not
#          the braces but the element length that determines whether the
#          element is interpreted as a parameter or a command.
#          In this example, a command extracts the standard error of h2q1.
#          
# Requires:    mibddir, chromosome, and interval commands must have been
#              given to select mibd files to use.  finemap may be adjusted
#              with finemap command.
#
#              There must be a null0.mod model in the trait or outdir
#              directory.  This can be created with the polygenic command
#              prior to running multipoint.  (This model may include
#              household and covariate effects.  See the help for the
#              polygenic command for more information.)
#
# IMPORTANT NOTE:  In most cases, multipoint starts by loading a model named
#                  null0.mod from the current output directory.  The
#                  model currently in memory is ignored.  This is done
#                  because it is absolutely essentially that the null0
#                  model be the basis to build all multipoint models.  However,
#                  some options, such as -ctparm, use the model currently
#                  in memory when multipoint is invoked because all
#                  models are derived from a custom linkage model that
#                  the multipoint command does not necessarily know how
#                  to build.
#
# Notes:   1.  Summary output is written to multipoint.out in a subdirectory
#              named after the trait variable.  You can set another output
#              directory with the outdir command.  Contents of the output
#              directory will be purged of previous files at the beginning
#              of each invocation if -overwrite is used.
#
#          2.  The final "best" linkage model is link.mod.  In addition,
#              a series of additional "null" models is produced, starting
#              with null1.mod (containing 1 QTL), null2.mod, etc.  These
#              models are produced only if a LOD criterion is specified
#              and satisfied (so there is more than one pass).
#
#          3.  If a LOD adjustment is in effect (see lodadj command) it
#              is applied here.
#
#          4.  If models have two traits, the 2df LOD scores will be
#              converted to 1df effective LOD scores.  To override this, 
#              use the lodp command (see).  This feature
#              was first included with SOLAR beta version 2.0.1.
#
#          5.  At the beginning of each pass through the selected genome,
#              multipoint calls a user script named multipoint_user_start_pass
#              which takes one argument, the pass number (which starts at 1
#              for the first pass).  Within this routine, the user can change
#              the selected chromosomes or interval.
# -

proc multipoint {args} {

    set verbose 0
    set qu -q
    ifverbplus set verbose 1
    ifverbplus set qu ""

    set ts [trait]
    if {[llength $ts] == 1} {
	set multi 0
    } else {
	set multi 1
    }
    set nts [llength $ts]

# Process arguments

    set plist \'
    set ptlist \'
    set mplot 0
    set reuse_null 0
    set force_overwrite 0
    set re_start 0
    set lod_criteria {}
    set restart_model ""
    set restart_model_required 0
    set lod_format [use_global_if_defined Solar_Lod_Format "%.4f"]
    set slod 0
    set lod_type LOD
    set scoredebug 0
    set epistasis 0
    set save_all_models 0
    set nose 0
    set FLOAT_RHOQ 2
    set con_rhoq $FLOAT_RHOQ
    set linkproc linkmod
    set atemplate 0
    set searg ""
    set nullifneg 0

    set lod_criteria [read_arglist $args \
	    -overwrite {set force_overwrite 1} -ov {set force_overwrite 1} \
	    -restart {set re_start 1} -r {set re_start 1} \
	    -renew restart_model \
	    -plot {set mplot 1} \
	    -epistasis epistasis \
	    -score {set slod 1; set lod_type S-LOD} \
	    -scoredebug  {set slod 1; set lod_type S-LOD; set scoredebug 1} \
	    -saveall {set save_all_models 1} \
	    -nose {set nose 1} \
	    -parm plist \
            -cparm plist \
            -ctparm ptlist \
	    -rhoq con_rhoq \
	    -link linkproc \
	    -se {set searg -se} \
	    -nullifneg {set nullifneg 1} \
	    -nullbase {set reuse_null 1}]

    ensure_integer $epistasis

    foreach lod $lod_criteria {
	ensure_float $lod
    }

    if {0<[llength $restart_model]} {
	set re_start 1
	if {"." != $restart_model} {
	    load model $restart_model
	}
    }

    if {$re_start && $force_overwrite} {
		error "Arguments -restart and -overwrite are incompatible."
    }

    if {"\'" != $plist} {
	set noparama "-cparm"
	set aparama 1
    } elseif {"\'" != $ptlist} {
	set plist $ptlist
	set noparama "-cparm"
	set aparama 1
	set atemplate 1
    } else {
	set noparama ""
	set aparama 0
	set plist ""
    }

# Check for existing multipoint output files

    if {$force_overwrite} {
	eval delete_files_forcibly [glob -nocomplain [full_filename multipoint*.out]]
	eval delete_files_forcibly [glob -nocomplain [full_filename multipoint*.save]]
	purge_multipoint_output_directory
    } elseif {!$re_start && $epistasis} {
	eval delete_files_forcibly [glob -nocomplain [full_filename multipointe*.out]]
    } elseif {!$re_start && !$epistasis} {
	if {0<[llength [glob -nocomplain [full_filename multipoint*.out]]]} {
    error "Multipoint output files exist.  Use -overwrite or -restart option."
	}
    }

# If restarting, check previous lodadj (if any) with current lodadj

    if {$re_start} {
	if {[file exists [full_filename multipoint.out]]} {
	    set mfile [open [full_filename multipoint.out] r]
	    set found 1.0
	    while {-1 != [gets $mfile line]} {
		if {-1 != [string first "*** Using LOD Adjustment:" $line]} {
		    set found [lindex $line 4]
		    break
		}
	    }
	    close $mfile
	    set format_newlodadj [lodadj -query]
	    catch {set format_newlodadj [format %.5f $format_newlodadj]}
	    if {$found != $format_newlodadj} {
		puts "old lodadj: $found"
		puts "new lodadj: $format_newlodadj"
		error \
 "lodadj has changed.  Use 'madj' command to readjust old multipoint files."
	    }
	}
    }

    global Solar_Mibd_List
    set Solar_Mibd_List [global_mibd_list]
    global Solar_Fixed_Loci
    set Solar_Fixed_Loci 0
    global Solar_Interval
    set major_chromosome_list {}
    set major_locus_list {}
    set major_lod_list {}
    set last_chromosome -1
    global Solar_Plot_Last_Chrom
    set Solar_Plot_Last_Chrom -1

# Start from current model (including linkage) if epistasis

    set epiarg ""
    set el ""
    if {$epistasis} {
	set Solar_Fixed_Loci [h2qcount]
	set rootname nulle$Solar_Fixed_Loci
	maximize_quietly $rootname.out
	save model [full_filename $rootname.mod]
	outheader multipointe.out 1 $lod_type 0
	outresults multipointe.out $rootname "" [loglike] 1
	lodadj -query -inform stdout
	lodadj -query -inform multipoint.out
	set epiarg "-epistasis $epistasis"
	set el e

# start from existing model if arbitrary parameterization

    } elseif {$aparama} {
	set basemodel [full_filename multipoint.template.mod]
	save model $basemodel
	set Solar_Fixed_Loci 0

# Setup minimum resultfile and report null model

	load model [full_filename null0]
	set headings "Model LOD Loglike"
	set formats "%19s %11s %12.3f"
	set expressions "\$modelname \$lodscore \[loglike\]"
	set modelname polygenic
	set lodscore ""
	foreach par $plist {
	    lappend formats %9.6f
	    lappend headings $par
	    if {1<[llength $par]} {
		lappend expressions \[$par\]
	    } else {
		lappend expressions "\[parameter $par =\]"
	    }

# Parameters which do not exist in null model are forced to 0

	    if {![if_parameter_exists $par]} {
		parameter $par = 0
	    }
	}
	set open_option -create
	if {$re_start} {
	    set open_option -append
	    putsout multipoint.out "\n    *** Restarting multipoint scan"
	}
	set resultf [resultfile $open_option [full_filename multipoint.out] \
			 -headings $headings \
			 -formats $formats \
			 -expressions $expressions -display]
	if {!$re_start} {
	    resultfile $resultf -header
	    resultfile $resultf -write
	}
	load model $basemodel

    } else {

# No epistasis, must have previously stored null model

	if {![file exists [full_filename null0.mod]]} {
	    error "Model [full_filename null0] not found.\
\nThis can be created with polygenic command."
        }

# Load null model and display its results
# First, for univariate case (uses older output routines)

	if {!$multi} {
	outheader multipoint.out 1 $lod_type 0
	load model [full_filename null0]
	set modelname polygenic
	if {[check_house]} {
	    set modelname "household polygenic"
	}
	outresults multipoint.out $modelname "" [loglike] 0

# Check for certain warning conditions related to the null model

	catch {
	set as_quantitative 0
	set constrained_to_1 0
        if {{} != [find_string [full_filename null0.out] "quantitative!"]} {
	    set as_quantitative 1
	}
	if {![catch {set cval [find_simple_constraint sd]}] && $cval == 1} {
	    set constrained_to_1 1
	}
	if {$constrained_to_1} {
	    set discrete_trait 0
	    if {!$as_quantitative} {
		set tstats [stats -return -q]
		set discrete_trait [stats_get $tstats discrete]
	    }
	    if {!$discrete_trait || $as_quantitative} {
		putsout multipoint.out \
"\n\tWARNING!  YOU HAVE PARAMETER SD CONSTRAINED TO 1.0 !"
		putsout multipoint.out \
"\tThis is probably not what you intended."
		putsout multipoint.out \
"\tYou need to use command \"model new\" before changing from discrete to"
		putsout multipoint.out \
"\tquantitative trait analysis."
	    }
	}
	if {$as_quantitative} {
	    putsout multipoint.out \
"\n\tWARNING!  You are analyzing a discrete trait as quantitative!"
	    putsout multipoint.out \
"\tSee \"help discrete-notes\" for discussion."
	}
	}

	} else {
#	    countmax

# Set up resultfile for bivariate output

	    load model [full_filename null0]
	    set headings "Model LOD Loglike"
	    set formats "%19s %11s %12.3f"
	    set expressions "\$modelname \$lodscore \[loglike\]"

# Set modelname of base model, and, if house, set upper c2 bounds also

	    set modelname polygenic
	    if {[check_house]} {
		set modelname "household polygenic"
	    }

# OK, back to resultfile

	    set lodscore ""
	    foreach tr $ts {
		lappend headings H2r($tr)
		lappend formats %9.6f
		lappend expressions "\[parameter H2r($tr) =\]"
	    }
	    lappend headings RhoG
	    lappend formats %9.6f
	    lappend expressions "\[parameter rhog =\]"

	    set error_status ""
	    lappend headings ""
	    lappend formats "%s"
	    lappend expressions "\$error_status"

	    set open_option -create
	    if {$re_start} {
		set open_option -append
		putsout multipoint.out "\n    *** Restarting multipoint scan"
	    }
	    set resultf [resultfile $open_option \
			     [full_filename multipoint.out] \
			     -headings $headings \
			     -formats $formats \
			     -expressions $expressions -display]
	    if {!$re_start} {
		resultfile $resultf -header
		resultfile $resultf -write
	    }
	}
	lodadj -query -inform stdout
	lodadj -query -inform multipoint.out

    }  ;# End if _not_ epistasis

# Continue scanning while current lod_criteria is satisfied
#   (This is a "do-while"...test is near bottom...one pass assured)

    while {1} {
	
# Define scanning procedure for each linkage model

# (Keep sub-scripts un-indented for proper autoloading
proc scanpass {mibdlist} {
            upvar mplot mplot
	    upvar highest_lod hlod
	    upvar highest_mibdfile hmibd
            upvar verbose verbose
	    global Solar_Fixed_Loci
            upvar last_chromosome last_chromosome
            upvar reuse_null reuse_null
            upvar finemap_level finemap_level
            upvar finemap_list finemap_list
            upvar re_start re_start
            upvar bnd_or_conv_err_in_pass bnd_or_conv_err_in_pass
            upvar slod slod
            upvar scoredebug scoredebug
            upvar previous_h2q_value previous_h2q_value
            upvar epiarg epiarg
            upvar noparama noparama
            upvar aparama aparama
            upvar atemplate atemplate
            upvar el el
            upvar save_all_models save_all_models
            upvar multi multi
            upvar resultf resultf
            upvar con_rhoq con_rhoq
            upvar FLOAT_RHOQ FLOAT_RHOQ
            upvar linkproc linkproc
            upvar searg searg

            set h2q_index [expr $Solar_Fixed_Loci + 1]
            set minlike ""
	    foreach mibdfile $mibdlist {

# Set new_chromosome and locus; loading null model if necessary

		set retry_mode 3
		set new_chromosome [get_chromosome $mibdfile]
		set locus [get_locus $mibdfile]

		if {!$atemplate && !$aparama && ($reuse_null || \
		     $last_chromosome != $new_chromosome || $slod==1)} {
		    set retry_mode 1
		    set last_chromosome $new_chromosome
		    model load [full_filename null$Solar_Fixed_Loci]
		    set minlike [loglike]
		}
# New template reloading rules for -cparm (4.1.5)
#   If chromosome is different, or if locus is > 11 + previous locus
#   Reload template model
# Previously, and very badly, template was never reloaded, leading to frequent
#   convergence errors
		if {!$atemplate && $aparama} {
# Determine chrom,locus of currently loaded chromosome by inspection
# This is really "a better way" of finding out that trying to keep track of
#   all changes through a "last" variable, and the "last" variable used here
#   isn't reliable anyway, it's reset to -1 on each finemap segment.
		    set current_mibd [get_current_mibd]
		    if {$current_mibd == ""} {
			set current_chrom -1
			set current_locus -1
		    } else {
			set current_chrom [get_chromosome $current_mibd]
			set current_locus [get_locus $current_mibd]
		    }
		    if 	{$current_chrom != $new_chromosome || \
			     ![is_integer $locus] || \
			     ![is_integer $current_locus] || \
			     $current_locus + 11 < $locus || \
			     $current_locus > $locus + 11} {
			ifdebug puts "Reloading template model"
			load model [full_filename multipoint.template.mod]
			set minlike ""
			set last_chromosome $new_chromosome
		    }
		}

# If restarting, see if this locus already done

		if {$re_start && ![catch {set lodlist \
			[get_prev_lod_and_h2q $h2q_index $new_chromosome \
			$locus]}]} {
		    set h2q_value [lindex $lodlist 1]
		    set olod [lindex $lodlist 0]
		    if {![is_nan $olod]} {
			if {$olod > $hlod} {
			    set hlod $olod
			    set hmibd $mibdfile
			}
			if {$olod >= $finemap_level} {
			    lappend finemap_list [list $mibdfile $h2q_value]
			}
		    }
		    if {$verbose} {
			puts \
       "\n    *** Using previous results for Chrom $new_chromosome  Loc $locus"
		    }
		} else {
		    if {$verbose} {
			puts \
                        "\n    *** Analyzing Chrom $new_chromosome  Loc $locus"
		    }
		    if {$atemplate} {
			load model [full_filename multipoint.template.mod]
			set minlike ""
		    }
		    if {!$slod} {
			eval $linkproc $mibdfile $noparama $epiarg -lasth2q \
				$previous_h2q_value $searg
			set previous_h2q_value none
		    } else {
			eval $linkproc $mibdfile $noparama $epiarg -zerostart \
			    $searg
			option StandErr 1
			option ScoreOnlyIndex $h2q_index
			option MaxIter 1
		    }
		    if {$con_rhoq != $FLOAT_RHOQ} {
			parameter rhoq1 = $con_rhoq
			constraint rhoq1 = $con_rhoq
		    }
		    set boundary_error 0
		    if {[catch {set newchf [format "%02d" $new_chromosome]}]} {
			set newchf [format "%2s" $new_chromosome]
			set char0 [string range $new_chromosome 0 0]
			set char1 [string range $new_chromosome 1 1]
			if {[is_integer $char0] && ![is_integer $char1]} {
			    set newchf [format "%2s" 0$new_chromosome]
			}
		    }
		    set locf [format "%5s" $locus]
		    if {$save_all_models} {
			set outfilename \
			  multi.pass$h2q_index.$new_chromosome.$locus
		    } else {
			set outfilename temp
		    }
		    set max_status [maxtry $retry_mode $h2q_index $mibdfile \
			    [full_filename $outfilename.out] $verbose $minlike]
		    if {$save_all_models} {
			save model [full_filename $outfilename]
		    }
		    if {"" != $max_status && "ConsRhoq" != $max_status && \
			"ZeroLodX" != $max_status} {
			set bnd_or_conv_err_in_pass 2
			set last_chromosome -1
			delete_files_forcibly [full_filename temp.out]
			if {!$multi && !$aparama} {
			    outresults multipoint$el$h2q_index.out \
				"chrom $newchf  loc $locf" \
				"" NaN $h2q_index 2 none -none
			} else {
			    set modelname "chrom $newchf  loc $locf"
			    set lodscore ""
			    set error_status "ConvrgErr"
			    resultfile $resultf -write
			}
			continue
		    }
		    delete_files_forcibly [full_filename temp.out]
		    if {[catch {set olod [lod_or_slod_n $slod \
			    $Solar_Fixed_Loci]}]} {
			set boundary_error 3
			set bnd_or_conv_err_in_pass 3
			puts "catch failed, now setting olod NAN"
			set olod NaN
		    } else {
			if {$olod > $hlod} {
			    set hlod $olod
			    set hmibd $mibdfile
			    model save [full_filename best$h2q_index]
			}
			if {$olod >= $finemap_level} {
			    if {!$multi && !$aparama} {
				set h2q_value [parameter h2q$h2q_index start]
			    } else {
				set h2q_value 0.01  ;# lots of work req. for bi
			    }
			    lappend finemap_list [list $mibdfile $h2q_value]
			}
			if {$save_all_models} {
			    save model [full_filename \
			      multi.pass$h2q_index.$new_chromosome.$locus]
			}
		    }
		    if {0==[string compare [verbosity] "verbosity max"]} {
			scanputs "    *** Total process memory is [memory]"
		    }
		    if {$slod && $scoredebug} {
			set score [parameter h2q$h2q_index score]
			set lcse [parameter h2q$h2q_index se]
		    } else {
			set score none
			set lcse none
		    }
		    if {!$multi && !$aparama} {
		    outresults multipoint$el$h2q_index.out \
			    "chrom $newchf  loc $locf" \
		      $olod [loglike] $h2q_index $boundary_error none -none \
		      $score $lcse
		    } else {
			set modelname "chrom $newchf  loc $locf"
			if {[catch {set lodscore [format %11.4f $olod]}]} {
			    set lodscore [format %11s $olod]
			}
			set error_status $max_status
			resultfile $resultf -write
		    }
		}
		if {$mplot} {
		    catch {plot -quick -chrom $new_chromosome -pass $h2q_index}
		    ifverbplus puts \
		     "plot -quick -chrom $new_chromosome -pass $h2q_index"
		}
	    }

# Do replot without "-quick" option now that chromosome is done
#   (if any chromosomes were done!)

	    if {$mplot && 0<[llength $mibdlist]} {
		catch {plot -chrom $new_chromosome -pass $h2q_index}
		ifverbplus puts \
		 "plot -chrom $new_chromosome -pass $h2q_index"
	    }
        }


# Do regular scan through using established interval and range

	set previous_h2q_value none
	set bnd_or_conv_err_in_pass 0
	set h2q_index [expr $Solar_Fixed_Loci + 1]
	set highest_lod -10000
	set highest_mibdfile ""
	set finemap_list {}
	set finemap_level 1000
	if {0!=[string compare [finemap] off] && \
		0!=[string compare [finemap] default]} {
	    set rindex [expr [lowest [llength [finemap]] $h2q_index] - 1]
	    set finemap_level [lindex [finemap] $rindex]
	}
	if {![file exists [full_filename multipoint$el$h2q_index.out]]} {
	    if {!$verbose} {puts ""}
	    if {!$multi && !$aparama} {
		outheader multipoint$el$h2q_index.out $h2q_index $lod_type 0 \
		    $epistasis
	    } else {
		set open_option -create
	    }
	} 

# Multivariate uses generic "resultfile"

        if {$multi && !$aparama} {
# Remove previous error_status

	    set headings [lrange $headings 0 [expr [llength $headings] - 2]]
	    set expressions [lrange $expressions 0 \
				 [expr [llength $expressions] - 2]]
	    set formats [lrange $formats 0 [expr [llength $formats] - 2]]

# Add values for new traits
	    
	    foreach tr $ts {
		set newterm [catenate H2q$h2q_index ($tr)]
		lappend headings $newterm
		lappend expressions "\[parameter $newterm =\]"
		lappend formats %9.6f
	    }
	    if {$nts == 2} {
		lappend headings RhoQ$h2q_index
		lappend expressions "\[parameter rhoq$h2q_index =\]"
		lappend formats %9.6f
	    } else {
		for {set iti 1} {$iti < $nts} {incr iti} {
		    for {set itj [expr $iti + 1]} {$itj <= $nts} {incr itj} {
			set pname [catenate RhoQ$h2q_index _ $iti$itj]
			lappend headings $pname
			lappend expressions "\[parameter $pname =\]"
			lappend formats %9.6f
		    }
		}
	    }
			

# Add error_status

	    lappend headings ""
	    lappend expressions "\$error_status"
	    lappend formats "%s"

# Update resultfile

	    set resultf [resultfile $open_option \
			     [full_filename multipoint$h2q_index.out] \
			     -headings $headings \
			     -formats $formats \
			     -expressions $expressions -display]
	} elseif {$aparama} {
	    set resultf [resultfile $open_option \
			     [full_filename multipoint1.out] \
			     -headings $headings \
			     -formats $formats \
			     -expressions $expressions -display]
	}
        if {$multi || $aparama} {
            if {"-create" == $open_option} {
		resultfile $resultf -header
	    } else {
		resultfile $resultf -header -displayonly
	    }
	}
	    
        catch {multipoint_user_start_pass [expr $Solar_Fixed_Loci + 1]}
	set chrom [chromosome]
	set int [interval]
	scanputs \
"\n    *** Pass $h2q_index:  Chrom $chrom;  Interval $int"
	scanpass $Solar_Mibd_List

# Do finemapping to find the real peak

	if {$highest_lod > -10000} {
	    if {0!=[string compare [finemap] off]} {
		if {$Solar_Interval > 1} {
		    if {[llength $finemap_list]} {
			set last_chrom -1
			set last_locus -1
			set test_list {}

# Scan all points around entries in finemap_list

                        foreach elem $finemap_list {
			    set mibdfile [lindex $elem 0]
			    set h2q_value [lindex $elem 1]
			    set cnum [get_chromosome $mibdfile]
			    set lnum [get_locus $mibdfile]
			    if {$last_chrom == $cnum && \
			      [expr $last_locus + $Solar_Interval] == $lnum} {
			        set lm [expr $lnum + 1]
		            } else {
				set lm [expr $lnum + 1 - $Solar_Interval]
			    }
			    set last_chrom $cnum
			    set last_locus $lnum
			    set hm [expr $lnum + $Solar_Interval - 1]
			    set sublist [mibd_list [mibddir] $cnum 1 $lm $hm]
			    set center [lsearch $sublist $mibdfile]
			    if {$center != -1} {
			      set sublist [lreplace $sublist $center $center]
			    }

# For each sublist of new mibd's, a previous_h2q_value is included

			    set tlist [list $h2q_value $sublist]
			    lappend test_list $tlist
			}
			set last_chrom -1
			set col 1
                        scanputs -nonewline \
			"    *** Fine Map over the following locations:"
			foreach tlist $test_list {
			    set mlist [lindex $tlist 1]
			    foreach m $mlist {
				set cnum [get_chromosome $m]
				set lnum [get_locus $m]
				if {$cnum != $last_chrom} {
				    set last_chrom $cnum
			            scanputs -nonewline \
					    "\n        Chrom $cnum  Loc"
				    set col 22
				}
				if {$col > 71} {
				    scanputs -nonewline \
					    "\n                      "
				    set col 22
				}
				scanputs -nonewline " $lnum"
				set col [expr $col + [string length " $lnum"]]
			    }
			}
			scanputs ""
			foreach tlist $test_list {
			    set last_chromosome -1
			    set previous_h2q_value [lindex $tlist 0]
			    set mlist [lindex $tlist 1]
			    scanpass $mlist
			}
		    } else {
			scanputs -nonewline \
		"    *** No LOD met finemap criterion of $finemap_level"
		    }
		}
	    }

# Re-do final linkage model with standard errors and saving output
	    
	    set last_chromosome -1
	    set got_se 0
	    set h2q_index [expr $Solar_Fixed_Loci + 1]

# *** This section applies to restart only

	    if {$re_start} {
		if {[file exists [full_filename null$h2q_index.mod]]} {
		    set got_se 1
		    if {$verbose} {
			puts "    *** Re-using previous null$h2q_index.mod"
		    }
		} elseif {!$slod && \
		    ![file exists [full_filename best$h2q_index.mod]]} {
		    if {$verbose} {
			puts \
	         "    *** Maximizing model with highest lod found in prior run"
		    }
		    if {$aparama || $atemplate} {
			load model [full_filename multipoint.template.mod]
		    } else {
			load model [full_filename null$Solar_Fixed_Loci]
		    }
		    eval $linkproc $highest_mibdfile $noparama $epiarg $searg
		    if {$con_rhoq != $FLOAT_RHOQ} {
			parameter rhoq1 = $con_rhoq
			constraint rhoq1 = $con_rhoq
		    }
		    model save [full_filename best$h2q_index.mod]
		    if {"" != [maxtry 1 $h2q_index $highest_mibdfile \
			    [full_filename null$h2q_index] $verbose $minlike]} {
			if {$aparama || $atemplate} {
			    load model [full_filename multipoint.template.mod]
			} else {
			    load model [full_filename null$Solar_Fixed_Loci]
			}
			eval $linkproc $noparama $epiarg $highest_mibdfile \
			    $searg
			if {$con_rhoq != $FLOAT_RHOQ} {
			    parameter rhoq1 = $con_rhoq
			    constraint rhoq1 = $con_rhoq
			}
               error "Can't maximize highest lod model: best$h2q_index.mod"
		    }
		    model save best$h2q_index
		}
	    }

#  OK, now we've got bestX.mod, regardless of restart or regular
#  If we're not restarting, or we haven't got S.E. model,
#  compute S.E.

	    if {!$got_se} {
		if {$nose && !$slod} {
		    model load [full_filename best$h2q_index]
		    model save [full_filename null$h2q_index]
		} else {

		if {$verbose} {
		    puts \
"    *** Reloading best linkage model in this pass to compute standard errors"
                }
		if {$slod} {
		    if {$aparama || $atemplate} {
			load model [full_filename multipoint.template.mod]
		    } else {
			load model [full_filename null$Solar_Fixed_Loci]
		    }
		    eval $linkproc $highest_mibdfile $searg
		    if {$con_rhoq != $FLOAT_RHOQ} {
			parameter rhoq1 = $con_rhoq
			constraint rhoq1 = $con_rhoq
		    }
#		    option maxiter 1000
#                   option ScoreOnlyIndex -1
		} else {
		    model load [full_filename best$h2q_index]
		    perturb
		}
		option standerr 1
		if {0 == [catch {set maxmsg [maxtry 3 $h2q_index \
			$highest_mibdfile \
			[full_filename null$h2q_index] $verbose $minlike]}] \
			&& $maxmsg == ""} {
		    model save [full_filename null$h2q_index]
		    if {!$slod} {
			set hlodf [format %.2f $highest_lod]
			set lodf 0.0
			catch {set lodf [format %.2f \
				[lod_or_slod_n $slod $Solar_Fixed_Loci]]}
			if {$hlodf !=  $lodf} {
			    set bnd_or_conv_err_in_pass -1
			}
		    }
		} else {

# Maximization of best model with S.E. failed
		    if {$verbose} {
			puts "    *** Couldn't compute standard errors."
		    }

# If slod, try again but w/o S.E.
# For slod, we NEED this for next pass

		    if {$slod} {
			if {$aparama || $atemplate} {
			    load model [full_filename multipoint.template.mod]
			} else {
			    load model [full_filename null$Solar_Fixed_Loci]
			}
			eval $linkproc $highest_mibdfile $searg
			if {$con_rhoq != $FLOAT_RHOQ} {
			    parameter rhoq1 = $con_rhoq
			    constraint rhoq1 = $con_rhoq
			}
#			option maxiter 1000
#                       option ScoreOnlyIndex -1
			option standerr 0
			if {0 == [catch {set maxmsg [maxtry 3 $h2q_index \
				$highest_mibdfile \
				[full_filename null$h2q_index] $verbose \
							$minlike]}] && \
				$maxmsg == ""} {
			    model save [full_filename null$h2q_index]
			} else {
			    set bnd_or_conv_err_in_pass -2
			}
		    } else {


# If not slod, we'll just have to go with previously saved bestX model
# without standard errors

			model load [full_filename best$h2q_index]
			model save [full_filename null$h2q_index]
		    }
		}
		if {$bnd_or_conv_err_in_pass >= 0} {
		    delete_files_forcibly [full_filename best$h2q_index.mod]
		}
	    }   
	}
	}
	
# Sort output file

        set multipoint_index [expr $Solar_Fixed_Loci + 1]
        if {0 != [string length [usort]]} {
	    set mname [full_filename multipoint$multipoint_index.out]
	    if {$verbose} {puts "\n    *** Sorting $mname"}
	    catch {
		exec head -2 $mname >$mname.tmp
		if {[exec uname] == "SunOS"} {
		    exec tail +3 $mname | [usort] -k 2,2 -k 4,4n >>$mname.tmp
		} else {
		    exec tail -n +3 $mname | [usort] -k 2,2 -k 4,4n >>$mname.tmp
		}		    
		file rename -force $mname.tmp $mname
	    }
	}

# If restart, rescan output file and remove duplicates

        if {$re_start} {
	    set resinfilename [full_filename multipoint$multipoint_index.out]
	    set resinfile [open $resinfilename]
	    set resoutfile [open $resinfilename.tmp w]
	    gets $resinfile resline ;# copy header
	    puts $resoutfile $resline
	    gets $resinfile resline
	    puts $resoutfile $resline
	    set last_reschrom ""
	    set last_resloc ""
	    set lastresline ""
	    while {-1 != [gets $resinfile resline]} {
		set reschrom [lindex $resline 1]
		set resloc [lindex $resline 3]
		if {[string compare $reschrom $last_reschrom] || \
			[string compare $resloc $last_resloc]} {
		    if {"" != $lastresline} {
			puts $resoutfile $lastresline
		    }
		    set lastresline $resline
		} else {
		    if {[is_float [lindex $resline end]] || \
			    ![is_float [lindex $lastresline end]]} {
			set lastresline $resline  ;# only change if not worse
		    }
		}
		set last_reschrom $reschrom
		set last_resloc $resloc
	    }
	    if {"" != $lastresline} {
		puts $resoutfile $lastresline
	    }
	    close $resinfile
	    close $resoutfile
	    file rename -force $resinfilename.tmp $resinfilename
	}

# Report best location found (regardless of verbosity)
# UNLESS there was a convergence error

	if {[llength $highest_mibdfile] && \
		$bnd_or_conv_err_in_pass == 0} {
	    set chromosome [get_chromosome $highest_mibdfile]
	    set locus [get_locus $highest_mibdfile]
	    set old_verbose $verbose
	    set verbose 1
	    set flod [format $lod_format $highest_lod]
	    set pno [expr $Solar_Fixed_Loci + 1]
	    scanputs \
"\n    *** Highest $lod_type in pass $pno was $flod at Chrom $chromosome Loc $locus"
            set verbose $old_verbose
        }

# See if another scan is required

	if {$bnd_or_conv_err_in_pass > 1 && \
	    $Solar_Fixed_Loci <=  0} {
	    set old_verbose $verbose
	    set verbose 1
            scanputs \
"\n    *** Exiting because convergence errors occurred in first pass"
            scanputs \
"    *** Use 'help boundary' for help resolving such errors"
            set verbose $old_verbose
            break
        }

	if {0 == [llength $lod_criteria]} {
	    scanputs \
"\n    *** No $lod_type criterion specified...exiting after one pass"
            break
        }

	set lod_criterion [lindex $lod_criteria 0]
	if {1 < [llength $lod_criteria]} {
	    set lod_criteria [lrange $lod_criteria 1 end]
	}

# We exit here either if LOD criterion not satisfied or if bnd/conv error

 	if {$highest_lod < $lod_criterion || \
	    $bnd_or_conv_err_in_pass} {

	    if {!$bnd_or_conv_err_in_pass && $verbose} {
	        puts "\n    *** Current LOD criterion is $lod_criterion"
	        puts "    *** LOD criterion was not satisfied in last scan"
	    }
	    if {$Solar_Fixed_Loci > 0} {
		set old_verbose $verbose
		set verbose 1
		if {$Solar_Fixed_Loci > 1} {
		    scanputs \
		    "\n    *** $Solar_Fixed_Loci loci have satisfied criteria:"
		} else {
		    scanputs \
		    "\n    *** $Solar_Fixed_Loci locus has satisfied criteria:"
		}

		for {set i 0} {$i < $Solar_Fixed_Loci} {incr i} {
		    set chromosome [lindex $major_chromosome_list $i]
		    set locus [lindex $major_locus_list $i]
		    set lod [lindex $major_lod_list $i]
		    set flod [format $lod_format $lod]
		    scanputs \
              "        Chromosome: $chromosome   Loc: $locus   $lod_type: $flod"
		}
		set verbose $old_verbose
		catch {eval exec ln [full_filename null$Solar_Fixed_Loci.mod] \
			[full_filename link.mod]}
		scanputs \
                    "\n    *** Best model is link.mod, now being re-loaded..."
		model load [full_filename link.mod]
	    }
	    scanputs \
		    "    *** Pedigree:    [topline pedigree.info]"
	    scanputs \
		    "    *** Phenotypes:  [phenotypes -files]"
	    set rfilename [full_filename multipoint*.out]
	    if {!$verbose} {puts ""}
	    puts "    *** Additional information is in files named $rfilename"
	    if {$bnd_or_conv_err_in_pass > 0} {
		set old_verbose $verbose
		set verbose 1
		scanputs \
"\n    *** Exiting because convergence errors occurred in last pass"
                scanputs \
"    *** Use 'help boundary' for help resolving such errors"
                set verbose $old_verbose
            } elseif {$bnd_or_conv_err_in_pass == -1} {
		set old_verbose $verbose
		set verbose 1
		scanputs \
"\n    *** Exiting because LOD changed when computing Std Errors on best model"
                set h2q_ind [expr $Solar_Fixed_Loci + 1]
                scanputs \
"    *** Compare null$h2q_ind.mod and best$h2q_ind.mod"
                scanputs \
"    *** This indicates poor convergence"
                set verbose $old_verbose
            } elseif {$bnd_or_conv_err_in_pass == -2} {
		set old_verbose $verbose
		set verbose 1
		scanputs \
"\n    *** Exiting because we failed to get MLE for best model in pass"
                set verbose $old_verbose
            }
	    break
	}

# Add results to lists

	set highest_chromosome [get_chromosome $highest_mibdfile]
	set highest_locus [get_locus $highest_mibdfile]
    	lappend major_lod_list $highest_lod
	lappend major_chromosome_list $highest_chromosome
	lappend major_locus_list $highest_locus

# Linkage model becomes new null model

	set Solar_Fixed_Loci [expr 1 + $Solar_Fixed_Loci]
	if {$verbose} {puts \
          "    *** Repeating scan with $Solar_Fixed_Loci fixed loci\n"}
    }
}

proc scanputs {args} {
    upvar el el
    set nonewline 0
    set message [lindex $args 0]
    if {0==[string compare [lindex $args 0] -nonewline]} {
	set nonewline 1
	set message [lindex $args 1]
    }
    set sfile [open [full_filename multipoint$el.out] a]
    if {$nonewline} {
	puts -nonewline $sfile $message
    } else {
	puts $sfile $message
    }
    close $sfile

    upvar verbose verbose
    if {$verbose} {
	if {$nonewline} {
	    puts -nonewline $message
	} else {
	    puts $message
	}
    }
}

proc get_current_mibd {} {
    set matlist [matrix]
    set len [llength $matlist]
    for {set i [expr $len - 1]} {$i > 0} {incr i -1} {
	set test [lindex $matlist $i]
	if {[file extension $test] == ".gz"} {
	    set filename [file tail $test]
	    if {[string range $filename 0 4] == "mibd."} {
		return $test
	    }
	}
    }
    return ""
}
	

# solar::madj --
#
# Purpose:  Apply current lodadj to a previous multipoint run
#
# Usage:    madj
#           madj -restore    ;# restore ORIGINAL multipoint files
#           madj -undo       ;# restore previous multipoint files
#
# Notes: trait or outdir must already have been selected.
#
#        madj applies loadadj from lodadj.info file in trait/outdir.
#
#        madj may be used on incomplete multipoint runs prior to restarting.
#
#        It is not necessary to -restore before applying another lodadj.
#        Some roundoff errors occur in last decimal place, but do not
#        "accumulate" over multiple runs because LOD's are calculated
#        from loglikelihood values, not previous LOD's.  NULL models must
#        be present.
#
#        If there is an error, there should either be a "no files modified"
#        or "restoring previous files" message.  If not, or if it is
#        desired to restore the ORIGINAL multipoint files for any
#        reason, use the command "madj -restore."  The FIRST time madj is
#        run, those files were saved as multipoint*.save.  (The PREVIOUS
#        set of multipoint files were also saved as multipoint*.tmp, and
#        may also be restored with the "madj -undo" command.)
# -

proc madj {args} {

# ensure trait/outdir

    full_filename foo
#
# recalc is now a fixed option; LOD's are recalculated using null models
#     and stored loglikelihood values.
#   If recalc is 0, LOD's would be scaled from previous LOD scores.
#

    set recalc 1 

# Process arguments: -restore and -undo options are separate procs

    set restore 0
    set undo 0
    set badargs [read_arglist $args -restore {set restore 1} \
	    -undo {set undo 1} -* foo]
    if {"" != $badargs} {
	puts "No files modified"
	error "Invalid arguments: $badargs"
    }
    if {$restore} {
	return [madj_restore]
    }
    if {$undo} {
	return [madj_undo]
    }


# Check for all the files we need and saved files

    set backup_already_done 0
    if {![file exists [full_filename multipoint.out]]} {
	puts "No files modified"
	error "Missing file [full_filename multipoint.out]"
    }
    if {[file exists [full_filename multipoint.save]]} {
	set backup_already_done 1
    }
    set t_file [open [full_filename multipoint.out] r]
    set last_pass 0
    while {-1 != [gets $t_file line]} {
	if {-1 != [string first "*** Highest LOD in pass" $line]} {
	    if {![file exists [full_filename null$last_pass.mod]]} {
		puts "No files modified"
		error "Missing model [full_filename null$last_pass.mod]"
	    }
	    incr last_pass
	    if {![file exists [full_filename multipoint$last_pass.out]]} {
		puts "No files modified"
		error "Missing file [full_filename multipoint$last_pass.out]"
	    }

# Check that save file exists or does not exist as expected

	    if {[file exists [full_filename multipoint$last_pass.save]]} {
		if {!$backup_already_done} {
		    puts "No files modified"
		    puts \
		      "Found multipoint$last_pass.save but not multipoint.save"
		    error "Missing multipoint.save file?"
		}
	    } elseif {$backup_already_done} {
		puts "No files modified"
		error "Missing multipoint$last_pass.save file"
	    }
	}
    }
    close $t_file

# Now check for additional multipointN.out files not yet in multipoint.out

    while {[file exists [full_filename multipoint[expr $last_pass+1].out]]} {
	if {![file exists [full_filename null$last_pass.mod]]} {
	    puts "No files modified"
	    error "Missing model [full_filename null$last_pass.mod]"
	}
	incr last_pass
	if {[file exists [full_filename multipoint$last_pass.save]]} {
	    if {!$backup_already_done} {
		puts "No files modified"
		puts \
		  "Found multipoint$last_pass.save but not multipoint.save"
		error "Missing multipoint.save file?"
	    }
	} elseif {$backup_already_done} {
	    puts "No files modified"
	    error "Missing multipoint$last_pass.save file"
	}
    }

# Backup if backup not done before

    if {!$backup_already_done} {
	file copy -force [full_filename multipoint.out] [full_filename multipoint.save]
	for {set i 1} {$i <= $last_pass} {incr i} {
	    file copy -force [full_filename multipoint$i.out] \
		    [full_filename multipoint$i.save]
	}
    }


# Start reading and copying multipoint.out file

    set new_adj [lodadj -query]
    file rename -force [full_filename multipoint.out] [full_filename multipoint.tmp]
    set m_file [open [full_filename multipoint.tmp] r]
    set o_file [open [full_filename multipoint.out] w]
    set found ""
    set old_adj 1.0

# Find and replace LOD Adj line (if any) in multipoint.out file

    while {-1 != [gets $m_file line]} {
	if {-1 != [string first "*** Using LOD Adjustment:" $line]} {
	    set found [lindex $line 4]
	    break
	} elseif {-1 != [string first "*** Pass 1:" $line]} {
	    break
	}
	puts $o_file $line
    }
    if {"" == $found} {
	puts $o_file "    *** Using LOD Adjustment:  [format %.5f $new_adj]\n"
	puts $o_file $line
    } else {
	set old_adj $found
	puts $o_file "    *** Using LOD Adjustment:  [format %.5f $new_adj]"
    }

# Determine adj_factor (not actually used unless recalc is 0)

    set adj_factor [expr $new_adj / $old_adj]

# Now, update all multipointN.out files
# Keep tabs of highest lods for final update of multipoint.out file

    set highest_lods {}

    for {set N 1} {$N <= $last_pass} {incr N} {

	set null_loglike [oldmodel null[expr $N - 1] loglike]
	file rename -force [full_filename multipoint$N.out] \
		[full_filename multipoint$N.tmp]
	set mfile [open [full_filename multipoint$N.tmp] r]
	set ofile [open [full_filename multipoint$N.out] w]

# Copy "header" lines

	for {set i 0} {$i < 2} {incr i} {
	    if {-1 != [gets $mfile line]} {
		puts $ofile $line
	    }
	}

# Copy rest of file, adjusting LOD all the way

	set highest_lod 0.0
	while {-1 != [gets $mfile line]} {
	    set old_lod [lindex $line 4]
	    set current_loglike [lindex $line 5]

# New LOD calculated from loglikelihoods (lodadj built-in to lod)

	    if {$recalc} {
		if {![is_nan $current_loglike] && ![is_nan $null_loglike]} {
		    set new_lod [lod $current_loglike $null_loglike]
		    if {$new_lod < 0.0} {
			set new_lod 0.0
		    }
		    if {$new_lod > $highest_lod} {
			set highest_lod $new_lod
		    }
		    set new_lod [format %.4f $new_lod]
		} else {
		    set new_lod NaN
		}
	    } else {

# LOD scaled from old lod

		if {![is_nan $old_lod]} {
		    set new_lod [format %.4f [expr $old_lod * $adj_factor]]
		} else {
		    set new_lod NaN
		}
	    }

# output new LOD to file

	    set line [format "%s%12s%s" \
		    [string range $line 0 18] $new_lod \
		    [string range $line 31 end]]
	    puts $ofile $line
	}
	close $mfile
	close $ofile
	lappend highest_lods $highest_lod
    }

# Now copy rest of multipoint.out file
# Updating LOD scores as we go...

    set final_index 0
    set ending 0
    while {-1 != [gets $m_file line]} {
	if {-1 != [string first "*** Highest LOD in pass" $line]} {
	    set passno [lindex $line 5]
	    set chromno [lindex $line 10]
	    set locno [lindex $line 12]
	    set newlod [lindex $highest_lods [expr $passno - 1]]
	    puts $o_file [format \
              "    *** Highest LOD in pass %d was %.4f at Chrom %d Loc %d" \
              $passno $newlod $chromno $locno]
	} elseif {-1 != [string first "satisfied criter" $line]} {
	    set ending 1
	    puts $o_file $line
	} elseif {-1 != [string first "Chromosome:" $line]} {
	    if {!$ending} {
		close $o_file
		close $m_file
		madj_undo "Parsing error: Chromosome: in multipoint.out "
	    }
	    set chromosome [lindex $line 1]
	    set locus [lindex $line 3]
	    set flod [format %.4f [lindex $highest_lods $final_index]]
	    incr final_index
	    set lod_type LOD
	    puts $o_file \
             "        Chromosome: $chromosome   Loc: $locus   $lod_type: $flod"
        } else {
	    puts $o_file $line
	}
    }
    close $o_file
    close $m_file
    return "DONE:  $old_adj  ->  $new_adj"
}

proc madj_restore {args} {
    puts "Restoring original files..."
    file copy -force [full_filename multipoint.save] [full_filename multipoint.out]
    for {set i 1} {[file exists [full_filename multipoint$i.save]]} {incr i} {
	file copy -force [full_filename multipoint$i.save] \
		[full_filename multipoint$i.out]
    }
    if {"" != $args} {
	error $args
    }
}

proc madj_undo {args} {
    puts "Restoring previous files..."
    file copy -force [full_filename multipoint.tmp] [full_filename multipoint.out]
    for {set i 1} {[file exists [full_filename multipoint$i.tmp]]} {incr i} {
	file copy -force [full_filename multipoint$i.tmp] \
		[full_filename multipoint$i.out]
    }
    if {"" != $args} {
	error $args
    }
}


# solar::stringplot
#
# Purpose:  String plot of entire genome scan
#
# Usage:    multipoint
#           stringplot [-pass pass] [-allpass] [-title] [-lod <lod>] [-lodmark]
#                      [-color <name>] [-noconv] [-date] [-nomark]
#                      [-font <X-font-spec>] [-titlefont <X-font-spec>]
#                      [-dash <dash spec>] [-linestyle <dash spec>]
#                      [-mibddir <mibddir>]
#
# Notes:    You can also use the command "plot -string" which has the
#           same options and works identically.  For further information on
#           the options, see "help plot", where all the options are
#           described.  Here are the more important ones.  No options are
#           usually needed, they are usually for fine-tuning the display.
#
#           -pass       Multipoint oligogenic pass number, "1" is default
#           -allpass    Plot all multipoint passes (in separate plots)
#           -title      Title of plot
#           -lod <lod>  Show LOD scale for lods this high (default is highest)
#           -lodmark    Put marker ticks ON TOP of LOD curve (default is axis)
#           -color      Takes standard names like "blue" and "red"
#           -noconv     Do not mark convergence errors
#           -date       Datestamp plot
#           -nomark     Do not show marker ticks (very useful for GWAS)
#           -font       X font for text (see xlsfonts | more)
#           -titlefont  X font for title only
#           -dash       Line style (see "help plot" for description of spec)
#           -linestyle  Line style (same as -dash)
#           -mibddir    specify mibddir (default is set with mibddir command)
#           -mapfile    User mapfile
#           -layers     Method of using multiple colors.  See help plot.
#
#           mibddir and trait (or outdir) must have been specified previously.
#
#           String plot graph will be both displayed on screen and written
#           to file.  If you are running on a remote system, you will
#           need to enable X window forwarding by setting DISPLAY variable
#           to point back to X display, and enabling acceptance of X
#           protocol with xhost + command, as described in section
#           3.8.3.1 of the SOLAR documentation.  Sorry, there is no possible
#           way to write the the file without displaying the plot, the
#           underlying "tk/wish" program does not allow that.
#
#           An encapsulated postscript file is written to the trait/outdir
#           with the name str.passN.ps where N is the pass number,
#           such as str.pass01.ps
#
#           If a copy of the string plot script, which is named
#           "stringplotk", is found in the current working directory, that
#           will be used in place of the standard version in the
#           SOLAR bin directory.  You can customize stringplotk as you
#           wish.  (It is a "wish" script, after all.)  Good luck!
# -

proc stringplot {args} {

# Check for outdir and mibddir

    full_filename foo
    mibddir

    set main_title ""
    if {[catch {set main_title [trait]}]} {
	catch {set main_title [lindex [outdir] 1]}
    }

# Capitalize first letter

    if {0<[string length $main_title]} {
	set main_title \
"[string toupper [string range $main_title 0 0]][string range $main_title 1 end]"
    }

# See if there is a copy of stringplotk in local directory

    set program stringplotk
    if {[file exists stringplotk]} {
        puts "\nUsing stringplotk in working directory...\n"
        set program ./stringplotk
    }


# Read arguments

    set no_such_title "!>/?*&%@^qawitmbp<zv239485721029751238941236743@"

    set pass 1
    set usertitle $no_such_title
    set layername ""
    set coloropt ""
    set moreargs [read_arglist $args -pass pass -allpass {set pass -1} \
	    -title usertitle -name layername -color coloropt -* foo]

# Tk intercepts -name so we need to use -layername

    if {"" != $layername} {
        set layername "-layername $layername"
    }

    if {"" != $coloropt} {
        set coloropt "-layercolor $coloropt"
    }

    if {$pass == -1} {
	set hightest 0
	for {set i 1} {[file exists [full_filename multipoint$i.out]]} \
		{incr i} {
	    set highest $i
	}
	for {set i $highest} {$i >= 1} {incr i -1} {
	    set infile [full_filename multipoint$i.out]
	    set outfile [full_filename pass[format %02d $i].str.ps]
	    if {$usertitle != $no_such_title} {
		set use_title $usertitle
	    } else {
		set use_title "$main_title  (Pass $i)"
	    }
	    eval exec $program -mibddir [mibddir] -in $infile \
		    -o $outfile -title \"$use_title\" $layername $coloropt \
                    $moreargs &
	    after 1000
	}
    } else {
	set infile [full_filename multipoint$pass.out]
	set outfile [full_filename pass[format %02d $pass].str.ps]
	if {$usertitle != $no_such_title} {
	    set use_title $usertitle
	} else {
	    set use_title "$main_title  (Pass $pass)"
	}
	eval exec $program -mibddir [mibddir] -in $infile \
		-o $outfile -title \"$use_title\" $layername $coloropt \
                $moreargs &
    }
    
    return ""
}


# solar::plot --
#
# Purpose:  Plot multipoint LOD scores, empirical LOD adjustments, or power
#
# Usage:    plot [<chromnum>] [-pass <passnum>] [-write]
#                     [-color <colornum>] [-overlay]
#                     [-title <title>] [-subtitle <subtitle>]
#                     [-yscale <maxy>] [-map <user_map>] [-lodadj]
#                     [-min x] [-max x] [-nomark] [-nomarklab]
#                     [-all | -allpass [-nodisplay] [-nomini]]
#                     [-string [-allpass] [-lod <lod>] [-lodmark] [-lodscale]
#                       [-color <colorname>] [-noconv] [-date] [-name <name>]
#                       [-font <X-font-spec>] [-titlefont <X-font-spec]
#                       [-layers {{<layername> [-color <colorname>]} ... }
#                       [-replay {{<layername> [-color <colorname>]} ... }
#                       [-title <title>] [-dash 1/2] [-linestyle 1/2]
#                     [-liability [-model <name>]]
#                     [-power [-title <plot_title>]]
#
#           plot -purge
#           plot -close
#
# Examples: plot                    plot chromosome with highest LOD in pass 1
#           plot 9                  plot chromosome 9 in pass 1
#           plot 9 -pass 2          plot chromosome 9 in pass 2
#           plot -all               plot all chromosomes in pass 1
#           plot -all -pass 2       plot all chromosomes in pass 2
#           plot -allpass           plot all chromosomes in all passes
#           plot -string            plot all chromosomes in pass 1 using 
#                                     "string" plot format
#           plot -string -allpass   plot all chromosomes in all passes using
#                                     "string" plot format
#
# If postscript output files are saved, they are written to the current
# trait or outdir directory with names like these:
#
#   chr01.ps              chromosome 1 (pass 1)
#   chr01.pass02.ps       chromosome 1 (pass 2)
#   pass01.ps             Miniplot of chromosomes in pass 1 (plot -all -pass 1)
#   pass01.str.ps         String plot of pass 1
#
#
#           chromnum  [1-29] Set chromosome number for plotting.  The default
#                     is to plot chromosome with highest LOD score.
#
#           -pass     Set multipoint pass number for plotting.  "1" would
#                     mean the first pass in which all models have one
#                     QTL.  1 is the default.
#
#           -close  Close the XMGR plot window.  The miniplot and string plot
#                   display windows must be closed with their close buttons,
#                   but it is better if you close XMGR from the SOLAR
#                   command line.  Otherwise, on your next plot, there will
#                   be a delay until SOLAR determines that it cannot
#                   communicate with the old XMGR session.  Then, it will
#                   time-out and tell you to use the "tclgr close" command,
#                   which does the same thing as "plot -close".
#
#           -write    Write postscript output file for plot(s).  If there are
#                     no other arguments and if a plot was done previously,
#                     the output file for the previous plot is written.  
#                     Miniplot and stringplot files are always written
#                     by default.  For plots drawn using XMGR, you can
#                     also choose to write the postscript file from the
#                     XMGR graphical interface, which give you more options.
#                     See note 8 below.
#
#           -nomark     Do not show ticks or labels for markers.  (This works
#                       for both regular and -string plots.)  Unless this
#                       option is selected, there must be a mibddir
#                       selection in the current directory so that SOLAR
#                       can find the map files.
#
#           -nomarklab  Do not show labels for markers (still show ticks).
#
#           -title      Set plot title.  Title may be blanked with
#                       -title "" or -title " ".  This is supported
#                       by regular plots, string plots, and power
#                       plots only.  Plots made through XMGR may also
#                       have title set through graphical interface or
#                       by editing .gr file such as multipoint.gr.
#
#           -subtitle   Set plot subtitle.  Supported by regular
#                       multipoint plots only.  Subtitle may be blanked
#                       with -subtitle "" or -subtitle " ".
#
#           -color  Use this color for curve (overrides multipoint.gr default)
#
#                   For regular plots, this must be integer from 1-15; 
#                   colors are defined by XMGR:
#
#                   0:White 1:Black 2:Red 3:Green 4:Blue 5:Yellow
#                   6:Brown 7:Gray 8:Violet 9:Cyan 10:Magenta 11:Orange
#                   12:Indigo 13:Maroon 14:Turquoise 15:Green4
#
#                   For string plots, the X11 color names are used.  Typical
#                   color names are:
#
#                   black white blue red green grey orange purple brown violet
#                   magenta yellow cyan
#
#                   Many mixtures and shades are also available.  Find the
#                   rgb.txt file in your X11 installation for a complete list.
#
#           -overlay  Plot this curve on top of the current graph, which may
#                     already include more than one curve.  (Each curve
#                     corresponds to a distinct XMGR set, of which 30 are
#                     available in the custom version of XMGR used by SOLAR.
#                     To control order of sets in Legend, use the -set
#                     option for every plot.)
#
#           -purge    Delete all previously created plotfiles (not valid with
#                     other options; only valid for multipoint plots).
#
#           -string     Plot all chromosomes (in pass 1 unless otherwise
#                       specified) using "string plot" format.  (An
#                       alternative page of plots in xmgr format can be
#                       produced by with plot -all command.)
#
#           -name       Name this plot for later use (-string plots only).
#
#           -layers <layerlist>  Add one or more previous plots to this plot.
#                       This is either a simple list of previous names, or a
#                       nested list of names with other options, in either case
#                       each element of <layerlist> specifies a single layer.
#                       See extended example below under replay.
#                      (-string plots only).
#
#           -replay <layerlist>  Draw previous plots only, otherwise this is
#                       the same as -layers.  (-string plots only) Example:
#
#     trait q1
#     plot -string -name A1
#     trait q2
#     plot -string -name A2 -layers {{A1 -color green}}
#     trait q3
#     plot -string -name A3 -layers {{A2 -color blue} {A1 -color green}}
#     plot -string -replay {{A3 -color grey} {A2 -color blue} {A1 -color red}}
#     plot -string -replay {A3 A2 A1}  ;# just default colors
#
#                  Note that spaces between close and open braces, as
#                  shown above, is required.
#
#                  You can specify -color for the new top level plot and/or
#                  for layers in the -layers or -replay list.  Any unspecified
#                  colors will default to a built-in set of defaults.
#
#           -lod lod    Add horizontal scales above this lodscore (for string
#                         plot only)
#
#           -noconv     Do not mark convergence errors (string plot only)
#
#           -date       Datestamp (string plot only)
#
#           -lodmark    Put marker ticks ON TOP of LOD curve (default is to the
#                         left of the plot axis)  String plot only.
#
#           -lodscale   Show the LOD scale when this LOD is exceeded (default
#                       is for the scale only to appear for the highest LOD).
#                       String plot only.
#
#           -font       (String plot only!) Specify X font to use for title.
#                       Use command "xlsfonts | more" to list X fonts.
#                       Wildcards may be used, however, results are
#                       sometimes unexpected.  For example *bold-r* will
#                       match first bold,roman font in list, whatever
#                       that happens to be.
#
#           -dash 5/5   (String plot only!) Specify line style dot and
#                        dash.  Two or more integers are specified separated
#                        by slash.  The first and all odd position numbers
#                        specify the length of drawn segments, whereas the
#                        second and all even position numbers specify
#                        the transparent segments.  Results are approximate
#                        depending on system used.
#
#           -linestyle 5/5  (String plot only!) Same as -dash (see above).
#
#                        Note that for regular plots, linestyle can be
#                        changed by editing the linestyle parameter in the
#                        applicable .gr file such as multipoint.gr.
#
#           -titlefont  (String plot only!) Same as -font, except applies
#                       to title only.  Supercedes -font for title only.
#
#           -all        Plot all chromosomes in xmgr postscript format (in
#                       the first pass only unless -allpass specified). A page
#                       of miniature chromosome plots in postscript is
#                       created (if a python interpreter is available).  The
#                       names of all postscript files are listed, and any
#                       of them may be printed with the lp command.  Nothing
#                       is displayed on your desktop with this command. An
#                       alternative genome plot is available with
#                       "plot -string".
#
#           -allpass    Plot all chromosomes in all passes, producing either
#                       miniplots or "string" plots (if -string).
#
#           -nodisplay  Used with -all or -allpass to skip displaying 
#                       miniplots on-screen (has no effect on xmgr graphs).
#
#           -nomini     Used with -all or -allpass to skip making miniplots.
#                       Automatically sets the "-write" option to write all
#                       individual chromosome plots.  Miniplots can always
#                       be made later, and with more options, using the 
#                       separate "miniplot" command.
#
#           -yscale  [NOTE: Ordinarily you do not need to use this.]
#                    This sets the smallest LOD scaling if there is no
#                    LOD above 4.99.  Autoscaling will not apply for smaller
#                    values to prevent confusion (e.g. seeing what looks
#                    like a large peak but isn't because the y scale is
#                    is so small).  The default value is 4.99.  You can
#                    set this to 0 if you really want to look at tiny LOD
#                    curves.  Larger scaling is applied automatically, as
#                    is the adjustment to allow space for marker labels.
#
#           -map    Use user_map file (in user map format).  By default,
#                   the map information processed by the 'load map' command
#                   (and saved in the mibddir) is used to display marker
#                   labels and locations.  However, you can substitute a user
#                   map file (set file-map) by using this argument.  This 
#                   allows you to delete unimportant markers and add QTL's of
#                   particular interest.
#                
#           -min m  location at which to start plotting
#           -max m  location at which to end plotting
#                   (min and/or max may be used to restrict interval.
#                    These apply to ordinary chromosome plots only.)
#
#           -quick  Save time when plotting by re-using marker ticks and
#                   labels from previous plot.  This option is used
#                   automatically when you are plotting from the multipoint
#                   command using the "-plot" argument.
#
#           -lodadj  Plot empirical LOD adjustment scores.  None of the above
#                    arguments except -close and -color are applicable in
#                    this case.  The format file lodadj.gr is used instead
#                    of multipoint.gr, but the rules are applied in the
#                    same way (see notes below).
#
#           -liability   Plot discrete trait liability function (a different
#                        kind of plot, not related to above).  "polygenic"
#                        command must have been run first, and the following
#                        covariates must have been included:
#
#                          sex age age*sex age^2 age^2*sex
#
#                        The xmgr parameter file for -liability is liability.gr
#
#           -model name  Specify a different modelname for the -liability
#                        option.  There must be a companion maximization
#                        output file (maximize -output name.out) so, for
#                        example, there is name.mod and name.out.  The
#                        default is poly (poly.mod and poly.out are created
#                        by the polygenic command).
#
#           -power  Plot power versus QTL heritability.  Only the -title
#                   argument is applicable in this case.  The format file
#                   power.gr is used instead of multipoint.gr.
#
# Notes:
#
#          1.  The trait or outdir must have previously been specified so
#              the applicable multipoint file can be found.
#
#          2.  Marker labels and ticks are taken from the mibdchrN.loc file
#              (where N is the chromosome number) created from the user's
#              map file during mibd file creation.  These files should be
#              stored in the mibddir (and the mibddir should be specified
#              before running plot).  If the mibdchrN.loc file(s) cannot
#              be found, marker ticks and labels will not be shown.  In
#              SOLAR releases 1.1.2-1.2.0 the 'load map' command will create
#              a mibdchrN.loc file in the current directory.
#
#              There will be a tick for each marker, and up to 42 marker
#              labels will be displayed.  If there are more than 42
#              markers, some labels will not be displayed.  Labels are
#              prioritized based on LOD score and distance from nonzero
#              value.  By plotting after the multipoint session has
#              completed, one gets the best benefit from this label
#              prioritization.  Marker ticks are always drawn vertically;
#              add additional line (which might be diagonal) joins the
#              label to its tick.
#
#              You can eliminate the need for the map file by using the
#              -nomark option.
#
#          3.  XMGR (ACE/gr) is used for most plotting, using tclgr command.
#              Each SOLAR process can have only one tclgr session open.  You
#              can change the plot command used with the 'tclgr syscommand'
#              command (it must be XMGR pipe command compatible).
#              If SOLAR is exited before closing the plot session, the plot
#              session may remain active (however, it may print a warning
#              about not being able to access the named pipe).  If the user
#              terminates the XMGR session through its graphical interface, 
#              the command 'plot -close' must be given to reset it before
#              another plot command can be given.
#
#           4. The XMGR parameter setup file multipoint.gr is loaded.
#              First, the multipoint.gr from SOLAR_LIB is loaded, then the
#              multipoint.gr from ~/lib (if any), then the multipoint.gr
#              from the working directory (if any).  You need only include
#              the parameters you want to change in your local copy.
#
#           5. When SOLAR exits, the XMGR session will be terminated.  If
#              the automatic termination of XMGR should fail, the user
#              should terminate XMGR anyway to prevent it from hogging CPU.
#              (The custom XMGR in SOLAR_BIN prevents CPU hogging.)
#
#           6. NaN's are indicated by X's on the curve.  Areas of the curve
#              in between multiple X's may be invalid.  (NaN's are Not A Number
#              which means maximization failed to arrive at a valid likelihood
#              estimate.
#
#           7. There are two additional options, -set and -graph, whose usage
#              is discouraged except under exceptional circumstances.  They
#              might force the set and graph numbers to specific values.
#              By default, the set number is 1 (changed in version 1.6.0)
#              except for overlays.  Overlays use the first available set
#              number counting backwards from 29.  The graph number (except
#              for overlays) is the same as the set number (overlays must use
#              the preceding graph number).  Fooling with these can get you
#              into trouble, but under difficult circumstances they might
#              help.
#
#          8.  Standard postscript landscape mode is used in all output files.
#              If you want to choose any other output features, such as
#              Encapsulated Postscript (EPS), portrait mode, etc., for
#              those plots made by XMGR, you can open the "Printer Setup"
#              dialog (under the "File" menu).  There you can select
#              portrait output in a pulldown menu, check a "Generate EPS"
#              check box, etc.  Then, to write the file, select the
#              "File" option in the "Print to:" pulldown, and then press
#              the "Print" button at the bottom of the dialog box.  You
#              need not go to the separate "Print" option in the file menu,
#              and sometimes it seems to work better to print directly
#              from the Printer Setup dialog anyway.  All postscript files
#              can be printed using "lp" command.  Displaying postscript
#              or editing on screen depends on locally available software.
# -

# Obsolescent command name plotmulti handled through make_solar_aliases

proc plot {args} {

# -liability is special

    if {-1 != [lsearch -exact $args -liability]} {
	set largs [read_arglist $args -liability {set foo 1} -* foo]
	return [eval plot_liability $largs]
    }

# -close is special

    if {-1 != [lsearch -exact $args -close]} {
	if {[string compare -close $args]} {
	    error "-close is not valid with other arguments"
	}
	tclgr close
	return ""
    }

# -lodadj is special

    if {-1 != [lsearch -exact $args -lodadj]} {
	return [eval plot_lodadj $args]
    }

# -power is special

    if {-1 != [lsearch -exact $args -power]} {
	return [eval plot_power $args]
    }

# -h2power is special

    if {-1 != [lsearch -exact $args -h2power]} {
	return [eval plot_h2power $args]
    }


# -purge is special

    if {-1 != [lsearch -exact $args -purge]} {
	if {[string compare -purge $args]} {
	    error "-purge is not valid with other arguments"
	}
	full_filename foo
	catch {eval delete_files_forcibly [eval glob [full_filename chr*.ps \
		pass??.ps pass??.str.ps]]}
	return ""
    }

# -write is special if by itself and if a plot was done previously

    if {![string compare -write $args] && \
	    [if_global_exists Solar_Plot_Last_Chrom]} {
	return [plotwrite]
    }

# If "-string", this is a string plot

    if {-1 != [lsearch -exact $args -string]} {
	set args [read_arglist $args -string {set foo 1} -all {set foo 1} \
		-* foo]
	eval stringplot $args
	return ""
    }

# If -all or -allpass, this is a plot_all* of some kind

    if {-1 != [lsearch -exact $args -all] || \
	    -1 != [lsearch -exact $args -allpass]} {

# Remove -all or -allpass from arg list; also check for nomini and nodisplay

	set mini 1
	set display ""
        set pass 1
	set saveplots 0
	set args [read_arglist $args -all {set foo 1} -allpass {set pass -1} \
		-nodisplay {set display -nodisplay} \
		-write {set saveplots 1} \
		-nomini {set mini 0; set saveplots 1} -* foo]
	read_arglist $args -pass pass -* foo

# -allpass

	if {$pass == -1} {

# Delete earlier files

	    full_filename foo
	    if {$mini} {
		catch {eval delete_files_forcibly [eval glob [full_filename pass??.ps \
			chr*.ps]]}
	    } else {
		catch {eval delete_files_forcibly [eval glob [full_filename \
			chr*.ps]]}
	    }
# Plot
	    eval plot_all_pass $args
	    if {$mini} {
		eval miniplot -allpass $display
	    }

# Delete output files, unless -write requested

	    if {!$saveplots} {
		ifdebug puts "deleting all chr*.ps files"
		catch {eval delete_files_forcibly [eval glob [full_filename chr*.ps]]}
	    }
	} else {

# -all

# Delete earlier files

	    full_filename foo
	    if {$mini} {
		set oname [format "pass%02d.ps" $pass]
		delete_files_forcibly [full_filename $oname]
	    }
	    if {$pass == 1} {
		set testonames [glob -nocomplain [full_filename chr*.ps]]
		set onames {}
		foreach testoname $testonames {
		    if {-1 == [string first pass $testoname]} {
			lappend onames $testoname
		    }
		}
	    } else {
		set onames [glob -nocomplain [full_filename \
			[format "chr*.pass%02d.ps" $pass]]]
	    }
	    if {{} != $onames} {
		catch {eval delete_files_forcibly $onames}
	    }
# Plot
	    eval plot_all $args
	    if {$mini} {
		eval miniplot -pass $pass $display
	    }

# OBSOLETE: Delete output files unless -write requested

	    if {0} {
		if {$pass == 1} {
		    set tfilenames [glob -nocomplain [full_filename chr*.ps]]
		    set dfilenames {}
		    foreach tfilename $tfilenames {
			if {-1 == [string first pass $tfilename]} {
			    lappend dfilenames $tfilename
			}
		    }
		} else {
		    set dfilenames [eval glob -nocomplain [full_filename \
			    "chr*.pass[format %02d $pass].ps"]]
		}
		ifdebug puts "Deleting files: $dfilenames"
		if {{} != $dfilenames} {
		    catch {eval delete_files_forcibly $dfilenames}
		}
	    }
	}
	return ""
    }

# ensure trait/outdir has been specified
    full_filename test_trait_or_outdir

# set defaults and parse arguments (-1 will mean not specified)
    set write_flag 0
    set user_map ""
    set color -1
    set min_y_range 4.99
    set quick 0
    set num_points 0
    set passnum 1
    set chromnum -1
    set setnum -1      
    set graphnum -1
    set overlay 0
    set marker_ticks {}
    set marker_labels {}
    set max_locnum -1.0
    set max_olod 0
    set max_mrk_location 0
    set user_min 0
    set user_max 0
    set no_markers 0
    set no_marker_labels 0
    set no_such_title "!>/?*&%@^qawitmbp<zv239485721029751238941236743@"
    set user_title $no_such_title
    set user_subtitle $no_such_title

    set max_mrk_name_len  \
	    [use_global_if_defined Solar_Plot_Last_Max_Chars 0]
    set last_chrom_plotted \
	    [use_global_if_defined Solar_Plot_Last_Chrom -1]
    set last_max_loc \
	    [use_global_if_defined Solar_Plot_Last_Max_Loc -1]

    set moreargs [read_arglist $args \
	    -pass passnum \
	    -chrom chromnum \
	    -set setnum \
	    -graph graphnum \
	    -overlay {set overlay 1} \
	    -quick {set quick 1} \
	    -yscale min_y_range \
	    -color color \
	    -write {set write_flag 1} \
	    -min user_min \
	    -max user_max \
	    -nomark {set no_markers 1} \
	    -nomarker {set no_markers 1} \
	    -nomarkers {set no_markers 1} \
	    -nomarklab {set no_marker_labels 1} \
	    -title user_title \
            -subtitle user_subtitle \
	    -map user_map]
    
    if {$moreargs != ""} {
	if {1 != [llength $moreargs]} {
	    error "Invalid arguments or more than one chromosome: $moreargs"
	}
	set chromnum $moreargs
    }
    ensure_integer $passnum
    ensure_integer $setnum
    ensure_integer $graphnum
    ensure_integer $color
    ensure_float $min_y_range
    ensure_float $user_min
    ensure_float $user_max

# Ensure mibddir available if required

    if {!$no_markers && 0==[string length $user_map]} {
	mibddir
    }

# Determine if modified xmgr is available

    set modified_xmgr 0
    set max_ticks_modified_xmgr 1000
    set u_name [string tolower [exec uname]]
    if {![string compare $u_name sunos] || \
	    ![string compare $u_name osf1] || \
	    [string match *linux* $u_name] || \
	    [string match *irix* $u_name]} {
	set modified_xmgr 1
    }

    if {$user_min < 0 || $user_max < 0} {
	error "Negative -min or -max not possible"
    }
    if {$user_min != 0} {
	if {$user_max != 0} {
	    if {$user_min >= $user_max} {
		error "-min must be less than -max"
	    }
	}
    }

    if {0.0001 > $min_y_range} {set min_y_range 0.0001}

    if {$color != -1} {
	if {$color < 1 || $color > 15} {
	    error "Color must be 1..15"
	}
    }

    if {$passnum < 0} {
	error "Invalid pass number"
    }

# Choose setnum and graphnum:
#   If this is an overlay, choose an unused set
#     starting from end (29) going backwards to not interfere with chromosomes
#     unless very daring user is using -set command to set set
#   If this is not an overlay, we default setnum to 1 and graphnum to setnum

    if {$overlay} {
	set sets_in_use \
		[use_global_if_defined Solar_Plot_Sets_In_Use {}]
	if {$setnum == -1} {
	    for {set i 29} {$i > 0} {set i [expr $i - 1]} {
		if {-1==[lsearch $sets_in_use $i]} {
		    set setnum $i
		    break
		}
	    }
	    if {$setnum == -1} {
		error "All 30 possible sets are in use; overlay not possible"
	    }
	} elseif {$setnum < 1 || $setnum > 29} {
	    error \
		    "Invalid setnum chosen:  1-29 allowed"
	}
	set graphnum [use_global_if_defined Solar_Plot_Last_Graphnum -1]
	if {$graphnum == -1} {
	    error "Overlay adds to an existing plot; apparently there is none"
	}
    } else {

# No overlay, set set number to chromosome number
#   unless very daring user is using -set command

	set sets_in_use {}
	if {$setnum == -1} {
	    set setnum 1
	} elseif {$setnum < 1 || $setnum > 29} {
	    error "Invalid setnum chosen:  1-29 allowed"
	}
    }
    if {$graphnum == -1} {
	set graphnum $setnum
    }

#
# Read multipointN.out file
#
    set nans {}
    set all_points {}
    set notkilled 1
    set infilename [full_filename multipoint$passnum.out]
    set infile [open $infilename r]
    gets $infile line
    set ylabel [lindex $line 1]
    gets $infile 


    if {$chromnum == -1} {

# We need to scan file for chromosome with highest LOD score first
#   to set defaulted chromosome number

	set highest_lod -1
	set chrom_with_highest_lod -1
	while {-1 != [gets $infile line]} {
	    if {5 != [scan $line "%s %s %s %d %s" cid chrom lid locnum olod]} {
		error "Invalid line in multipoint$passnum.out: \n$line"
	    }
	    if {[string compare $cid "chrom"] || \
		    [string compare $lid "loc"]} {
		error "Invalid line in multipoint$passnum.out: \n$line"
	    }
	    if {![is_nan $olod]} {
		if {$olod > $highest_lod} {
		    set highest_lod $olod
		    set chrom_with_highest_lod $chrom
		}
	    }
	}
	set chromnum $chrom_with_highest_lod
	seek $infile 0 start
	gets $infile
	gets $infile
    }

# Remove leading zero from chromnum

    if {![string compare "0" [string index $chromnum 0]]} {
	set chromnum [string range $chromnum 1 end]
    }

# Now we can read the file for real

    set sort_points 0
    set highest_chromosome 0
    while {-1 != [gets $infile line]} {
	if {5 != [scan $line "%s %s %s %d %s" cid chrom lid locnum olod]} {
	    error "Invalid line in multipoint$passnum.out: \n$line"
	}
	if {![string compare "0" [string index $chrom 0]]} {
	    set chrom [string range $chrom 1 end]
	}
	if {[string compare $cid "chrom"] || \
		[string compare $lid "loc"]} {
	    error "Invalid line in multipoint$passnum.out: \n$line"
	}
	if {$chrom > $highest_chromosome} {
	    set highest_chromosome $chrom
	}
	if {[string compare $chromnum $chrom]} {
	    continue
	}
	if {[is_nan $olod]} {
	    lappend nans  $locnum
	} else {
	    lappend all_points [list $locnum $olod]
	    incr num_points
	    if {$locnum > $max_locnum} {
		set max_locnum $locnum
	    } else {
		set sort_points 1
	    }
	    if {$olod > $max_olod} {set max_olod $olod}
	}
    }
    close $infile

    if {$num_points < 1} {
	error "Chromosome $chromnum not found in $infilename"
    }

    set num_nans [llength $nans]
    global Solar_Plot_If_Nans
    if {![if_global_exists Solar_Plot_If_Nans]} {
	set Solar_Plot_If_Nans 0
    }
    if {$num_nans > 0 || \
	    ($overlay && $Solar_Plot_If_Nans)} {
	set if_nans 1
    } else {
	set if_nans 0
    }

# Now, we've read all the points
#   we also now know the selected or defaulted setnum and graphnum
#   so we can see now if graph can be reused
#   puts "setnum: $setnum  graphnum: $graphnum  chromnum: $chromnum"

    set reuse_graph 0
    if {$quick && ![string compare $last_chrom_plotted $chromnum] && \
	    $max_locnum <= $last_max_loc} {
	set reuse_graph 1
	set max_locnum $last_max_loc
    }
    if {$overlay && [string compare $last_chrom_plotted $chromnum]} {
	error "Cannot overlay a different chromosome"
    }

    if {$overlay} {
	set test_max $max_locnum
	if {$user_max != 0} {
	    set test_max $user_max
	}
	if {$last_max_loc < $test_max} {
	    error "Cannot overlay when previous graph had smaller width (maximum chrom position)\nTry putting widest graph first"
	}
    }

# Read the map file
    
    if {!$no_markers} {
	set sort_markers 0
	set last_location -100.0
	if {!$reuse_graph && !$overlay} {
	    set max_mrk_name_len 0
	    if {"" != $user_map} {
		set map_name $user_map
	    } else {
		set map_name1 [mibddir]/mibdchr$chromnum.loc
		set map_name2 [mibddir]/mibdchr0$chromnum.loc
		set exists1 [file exists $map_name1]
		set exists2 [file exists $map_name2]
		if {$exists1 && $exists2} {
		    puts \
                  "For chromosome $chromlabel found both $exists1 and $exists2"
		    break
		}
		if {$exists2} {
		    set map_name $map_name2
		} else {
		    set map_name $map_name1
		}
	    }
	    if {![catch {set mapfile [open  $map_name r]}]}  {
		set old_format 0
		set instring [gets $mapfile]
		set word [string tolower [lindex $instring 0]]
		if {$word == "nloci"} {
		    gets $mapfile
		}

# Regardless of format (2 or 3 columns) marker name is first column, location
#   is last column.  Assume at least one space between columns.

		while {-1 != [gets $mapfile mapline]} {
		    set mrkname [lindex $mapline 0]
		    set mrk_location [lindex $mapline [expr \
			    [llength $mapline] -1]]
		    if {![is_float $mrk_location]} {
			error "Invalid line in map file: $mapline"
		    }

# Round marker location to nearest 0.000000001 cM
# (Actually, it is probably in 1/10 anyway, but we must be certain of
#  level of precision in order for marker moving algorithm to work)

		    set mrk_location [format "%.9f" $mrk_location]
		    if {$mrk_location < $last_location} {set sort_markers 1}
		    set last_location $mrk_location
		    lappend marker_ticks $mrk_location
		    lappend marker_labels \
			    [list $mrk_location $mrkname $mrk_location 0.0]

# marker_labels structure: name_pos, name, tick_pos, interpolated LOD

		    if {$mrk_location > $max_mrk_location} {
			set max_mrk_location $mrk_location
		    }
		    set mrk_name_len [string length $mrkname]
		    if {$mrk_name_len > $max_mrk_name_len} {
			set max_mrk_name_len $mrk_name_len
		    }
		}
		close $mapfile
	    } else {
		if {"" != $user_map} {
		    error "Error opening map file: $user_map"
		}
	    }
	}
    }

# Open new or existing tclgr session

    if {[catch {tclgr open} errmsg]} {
	if {[string compare $errmsg \
		"tclgr session already opened from this solar session"]} {
	    error $errmsg
	}
    }

# Kill previous set, and graph if not overlaying

    if {$overlay || $reuse_graph} {
	if {!$overlay} {
	    tclgr send kill s$setnum saveall
	    tclgr send kill s0 saveall
	}
	tclgr send focus g$graphnum
    } else {
	tclgr send kill graphs
	tclgr send clear line
	tclgr send clear string
	tclgr send focus g$graphnum
    }

# Calculate scaling for x and y

    set max_x_unscaled $max_locnum
    set max_mrk_location [expr ceil($max_mrk_location)]
    if {$max_mrk_location > $max_x_unscaled} {
	set max_x_unscaled $max_mrk_location
    }
    if {$max_x_unscaled < 5} {set max_x_unscaled 5}


# Apply user-specified range limits

    set min_x_unscaled 0.0
    set restricted_range 0
    if {$user_min > 0} {
	set min_x_unscaled $user_min
	set restricted_range 1
    }
    if {$user_max > 0} {
	set max_x_unscaled $user_max
	set restricted_range 1
    }
	
# min_x and max_x allow extra 5% margin on inside of plot "window" so
# curve is fully exposed

    set delta_x_unscaled [expr $max_x_unscaled - $min_x_unscaled]
    set max_x [expr $max_x_unscaled + (0.05 * $delta_x_unscaled)]
    set min_x [expr $min_x_unscaled - (0.05 * $delta_x_unscaled)]

# Create y expansion factor to permit showing markers at top
# Also to permit showing legend above graph but below markers
#   (Legend is shown only if there are nan's)

    set chars_that_fit_vertically 58.0  ;# using default font size
    set tick_len_in_chars 6.6           ;# This is padded to allow margin
    set legend_height_in_chars 5.0
    set legend_margin 1.5               ;# add below legend only
    if {$if_nans} {
	set allow_for_legend [expr $legend_height_in_chars+$legend_margin]
    } else {
	set allow_for_legend 0.0
    }
    set unavailable_length [expr $max_mrk_name_len + $tick_len_in_chars + \
	    $allow_for_legend]

    set y_expand [expr 1.0/(1.0-($unavailable_length / \
	    $chars_that_fit_vertically))]

    set max_y [expr $max_olod * $y_expand]
    if {$max_y < $min_y_range} {set max_y $min_y_range}

#  Legend y is calculated, but used only if we need it

    set size_of_frame 0.7
    set frame_top 0.85
    set use_tick_len $tick_len_in_chars
    if {$max_mrk_name_len < 1} {
	set use_tick_len [expr $tick_len_in_chars - 1.0]
    }
    set legend_y [expr $frame_top - (($max_mrk_name_len+$use_tick_len+1) \
	    * $size_of_frame / \
	    $chars_that_fit_vertically)]
    
    if {$overlay} {
	set last_max_y [use_global_if_defined Solar_Plot_Last_Max_Y 0]
	if {$max_y < $last_max_y} {set max_y $last_max_y}
    }

# y axis set anew whether reusing graph or not
#   Marker positioning depends on x axis, not y axis

    tclgr send world ymin 0
    tclgr send world ymax $max_y

# Note that neither max_x or max_y are allowed to go to zero
# This is required for later code to work

# Set up basic frame and then read multipoint.gr

    if {!$overlay && !$reuse_graph} {
	tclgr send world xmin $min_x
	tclgr send world xmax $max_x
	if {$user_title != $no_such_title} {
	    tclgr send title \"$user_title\"
	} else {
	    tclgr send title \"Chromosome $chromnum\"
	}
	if {$user_subtitle != $no_such_title} {
	    tclgr send subtitle \"$user_subtitle\"
	} else {
	    tclgr send subtitle \"SOLAR Multipoint Pass # $passnum\"
	}
	if {4.0 < $max_y && $max_y < 10.0} {
	    tclgr send yaxis label place spec
	    tclgr send yaxis label place 0.043,0
	}
	tclgr send yaxis label \"$ylabel\"
	tclgr send xaxis tick op bottom
	tclgr send xaxis ticklabel op bottom
	tclgr send xaxis ticklabel start type spec
	tclgr send xaxis ticklabel start $min_x_unscaled
	global env
	if {[file exists $env(SOLAR_LIB)/multipoint.gr]} {
	    set mpathname [glob $env(SOLAR_LIB)/multipoint.gr]
	    tclgr send read \"$mpathname\"
	}
	if {[file exists ~/lib/multipoint.gr]} {
	    set mpathname [glob ~/lib/multipoint.gr]
	    tclgr send read \"$mpathname\"
	} 
	if {[file exists multipoint.gr]} {
	    tclgr send read \"multipoint.gr\"
	} 
    }
    if {$color != -1} {	
	tclgr send s$setnum color $color
    }

# Sort points if required 

    proc sort_by_first_num {a b} {
	set a0 [lindex $a 0]
	set b0 [lindex $b 0]
	if {$b0 > $a0} {
	    return -1
	} elseif {$b0 < $a0} {
	    return 1
	}
	return 0
    }

    if {$sort_points} {
	set all_points [lsort -command sort_by_first_num $all_points]
    }

# Sort marker ticks and labels too

    if {!$no_markers && $sort_markers} {
	set marker_ticks [lsort $marker_ticks]
	set marker_labels [lsort -command sort_by_first_num $marker_labels]
    }

# Plot points

    foreach point $all_points {
	set x [lindex $point 0]
	set y [lindex $point 1]

	if {$x >= $min_x_unscaled && $x <= $max_x_unscaled} {
	    tclgr send g$graphnum.s$setnum point $x,$y
	    ifverbmax puts "graphing g$graphnum.s$setnum point $x,$y"
	} else {
	    ifdebug puts "Removing point $x,$y because outside range"
	}
    }

# Plot NaN's (if there are any) using set 0 with symbols and linestyle 0
#   (no lines connecting symbols)

# Assign a LOD to each 'new' NaN using interpolation
#   Note: NaN's more than one past last point get last value

    if {0 < $num_nans} {
	set nan_points {}
	set j 0
	set previously_went_beyond_points 0
	set previous_x 0.0
	set previous_y 0.0
	set point_x 0.0
	set point_y 0.0
	for {set i 0} {$i < $num_nans} {incr i} {
	    set found_point_after 0
	    if {!$previously_went_beyond_points} {
		set nan_x [lindex $nans $i]
		set nan_y 0.0
		for {} {$j < $num_points} {incr j} {
		    set point [lindex $all_points $j]
		    set point_x [lindex $point 0]
		    set point_y [lindex $point 1]
		    if {$point_x >= $nan_x} {
			set found_point_after 1
			break
		    }
		    set previous_x $point_x
		    set previous_y $point_y
		}
	    }
	    if {$previously_went_beyond_points} {
		set nan_y $point_y
	    } elseif {!$found_point_after} {
		set previously_went_beyond 1
		set nan_y $point_y
	    } elseif {$point_x == $nan_x} {
		set nan_y $point_y
	    } else {
		set slope [expr ($point_y - $previous_y) / \
			double($point_x - $previous_x)]
		set delta_x [expr $nan_x - $previous_x]
		set nan_y [expr $previous_y + $slope * $delta_x]
	    }
	    if {$nan_y < 0.0} {set nan_y 0.0}
	    lappend nan_points [list $nan_x $nan_y]
	}
	foreach point $nan_points {
	    set x [lindex $point 0]
	    set y [lindex $point 1]
	    
	    tclgr send g$graphnum.s0 point $x,$y
	    ifverbmax puts "graphing NAN g$graphnum.s0 point $x,$y"
	}
    }

# Position legend box for nans
    
    if {$if_nans} {
	tclgr send legend loctype view
	tclgr send legend X1 0.295
	tclgr send legend Y1 $legend_y
	tclgr send legend on
	tclgr send legend box on
    }

# Set up x and y tick marks (major and minor)

    if {$max_x > 499} {
	tclgr send xaxis tick major 100
	tclgr send xaxis tick minor 50
    } elseif {$max_x > 100} {
	tclgr send xaxis tick major 50
	tclgr send xaxis tick minor 10
    } else {
	tclgr send xaxis tick major 10
	tclgr send xaxis tick minor 5
    }

# The design maximum LOD is 2000
#   (highest tick range is 750-1500)
#   (I don't like limits, but this is way beyond anything I can imagine)
# maxy must be 4*10^N or 1*10^N
# the other values are related

    set maxy  1000
    set major 200
    set major_digit 2
    set minor 100

    set maxdiv 2.5
    set majdiv 2
    set mindiv 2

# Since we mess with global variable tcl_precision
# We must catch errors to be sure it gets restored

    global tcl_precision
    set save_tcl_precision $tcl_precision
    set tcl_precision 6
    set not_ok [catch {
    while {1} {
	if {$max_y > $maxy} {
	    tclgr send yaxis tick major $major
	    tclgr send yaxis tick minor $minor
	    break
	}
	set maxy [expr double($maxy) / $maxdiv]
	set major [expr double($major) / $majdiv]
	set minor [expr double($minor) / $mindiv]

	if {$major_digit == 2} {
	    set major_digit 1
	    set maxdiv 2
	    set majdiv 2
	    set mindiv 2
	} elseif {$major_digit == 1}  {
	    set major_digit 5
	    set maxdiv 2
	    set majdiv 2.5
	    set mindiv 2.5
	} else {
	    set major_digit 2
	    set maxdiv 2.5
	    set majdiv 2
	    set mindiv 2
	}
    }
    } caught_error]
    set tcl_precision $save_tcl_precision
    if {$not_ok} {
	error $caught_error
    }
#
#            ***     PLOT MARKERS    ***
#
    if {!$no_markers && !$reuse_graph && !$overlay} {
#
# Remove markers out-of-range
#
	if {$restricted_range} {
	    ifdebug puts "Restricting marker range"
	    set new_marker_labels {}
	    foreach marker $marker_labels {
		set location [lindex $marker 0]
		if {$location < $min_x_unscaled || \
			$location > $max_x_unscaled } {
		    ifdebug puts "    Removing marker at $location"
		    continue
		}
		lappend new_marker_labels $marker
	    }
	    set marker_labels $new_marker_labels
#
# Remove tics out-of-range also (2.1.5)
#
	    set new_marker_ticks ""
	    foreach marker $marker_ticks {
		set location [lindex $marker 0]
		if {$location < $min_x_unscaled || \
			$location > $max_x_unscaled } {
		    ifdebug puts "    Removing marker tick at $location"
		    continue
		}
		lappend new_marker_ticks $marker
	    }
	    set marker_ticks $new_marker_ticks
	}
#
#       puts "[llength $marker_labels] markers within range"
#
# See if we can handle this number of markers in any way
#
	set num_markers [llength $marker_labels]
	set max_markers 42
	if {$modified_xmgr && $num_markers> $max_ticks_modified_xmgr} {
	    set no_markers 1
	    puts "Warning. Cannot display more than $max_ticks_modified_xmgr marker ticks"
	}
    }
    if {!$no_markers && !$reuse_graph && !$overlay} {
#
# Remove marker labels down to maximum we can handle
#
	if {!$no_marker_labels && $num_markers > $max_markers} {
#
# Assign a LOD to each marker using interpolation
#   Note: markers more than one past last point get value 0
#   
	    set new_marker_labels {}
	    set j 0
	    set previously_went_beyond_points 0
	    set previous_x 0.0
	    set previous_y 0.0
	    set point_x 0.0
	    set point_y 0.0
	    for {set i 0} {$i < $num_markers} {incr i} {
		set found_point_after 0
		if {!$previously_went_beyond_points} {
		    set marker [lindex $marker_labels $i]
		    set marker_x [lindex $marker 2]
		    for {} {$j < $num_points} {incr j} {
			set point [lindex $all_points $j]
			set point_x [lindex $point 0]
			set point_y [lindex $point 1]
			if {$point_x >= $marker_x} {
			    set found_point_after 1
			    break
			}
			set previous_x $point_x
			set previous_y $point_y
		    }
		}
		if {$previously_went_beyond_points} {
		    set marker_y 0.0
		} elseif {!$found_point_after} {
		    set previously_went_beyond 1
		    set marker_y $point_y
		} elseif {$point_x == $marker_x} {
		    set marker_y $point_y
		} else {
		    set slope [expr ($point_y - $previous_y) / \
			    double($point_x - $previous_x)]
		    set delta_x [expr $marker_x - $previous_x]
		    set marker_y [expr $previous_y + $slope * $delta_x]
		}
		if {$marker_y < 0.0} {set marker_y 0.0}
		set new_marker [lreplace $marker 3 3 $marker_y]
		lappend new_marker_labels $new_marker
	    }
	    set marker_labels $new_marker_labels
#
# Set zero valued marker labels to negative based on distance to nearest
#   non-zero  (we pass from both directions to do this)
#
	    set distance_to_zero 0
	    for {set i 0} {$i < $num_markers} {incr i} {
		set marker [lindex $marker_labels $i]
		set value [lindex $marker 3]
		if {$value <= 0.0} {
		    if {$distance_to_zero > 0} {
			set marker [lreplace $marker 3 3 \
				[expr 0.0 - $distance_to_zero]]
		      set marker_labels [lreplace $marker_labels $i $i $marker]
		    }
		    incr distance_to_zero
		} else {
		    set distance_to_zero 0
		}
	    }
	    set distance_to_zero 0
	    for {set i [expr $num_markers-1]} {$i >= 0} {set i [expr $i - 1]} {
		set marker [lindex $marker_labels $i]
		set value [lindex $marker 3]
		if {$value <= 0.0} {
		    if {$distance_to_zero >= 0} {
			if {$distance_to_zero < 0.0 - $value} {
			    set marker [lreplace $marker 3 3 \
				    [expr 0.0 - $distance_to_zero]]
			    set marker_labels [lreplace $marker_labels $i $i \
				    $marker]
			}
		    }
		    incr distance_to_zero
		} else {
		    set distance_to_zero 0
		}
	    }

	    while {$num_markers > $max_markers} {
		set lowest_value 10000.0
		set lowest_index 1
		for {set i 1} {$i < $num_markers} {incr i} {
		    set marker [lindex $marker_labels $i]
		    set value [lindex $marker 3]
		    if {$value < $lowest_value} {
			set lowest_value $value
			set lowest_index $i
		    }
		}
		set marker_labels [lreplace $marker_labels \
			$lowest_index $lowest_index]
		set num_markers [expr $num_markers - 1]

		if {$num_markers > $max_markers} {
		    set lowest_value 10000.0
		    set lowest_index [expr $num_markers - 2]
		    for {set i 1} {$i < $num_markers} {incr i} {
			set marker [lindex $marker_labels \
				[expr $num_markers - (1+$i)]]
			set value [lindex $marker 3]
			if {$value < $lowest_value} {
			    set lowest_value $value
			    set lowest_index $i
			}
		    }
		    set lowest_index [expr $num_markers - (1+$lowest_index)]
		    set marker_labels [lreplace $marker_labels $lowest_index \
			    $lowest_index]
		    set num_markers [expr $num_markers - 1]
		}
	    }
	}
	if {!$no_marker_labels} {
#
# Move remaining marker labels around to fit
# This should only take 1.5 passes with current algorithm
#   (old algorithm repeated until no more moves)

	    set min_distance [format %.9f [expr 0.0241 * $delta_x_unscaled]]
	    set markers_moved_left 0
	    set markers_moved_right 0
	    set pass_count 0
	    set max_pass_count 1.5
	    while {$pass_count<1 || \
		    $markers_moved_left>0|| $markers_moved_right>0} {
		incr pass_count
		set markers_moved_left 0
		set markers_moved_right 0

# Scan left to right, moving "next_marker" right if necessary

		for {set i -1} {$i < $num_markers - 1} {incr i} {
		    if {$i == -1} {
			set nloc [format %.10f [expr $min_x_unscaled - $min_distance]]
		    } else {
			set nloc [lindex [lindex $marker_labels $i] 0]
		    }
		    set next_marker [lindex $marker_labels [expr $i + 1]]
		    set next_nloc [lindex $next_marker 0]
		    set distance [format %.10f [expr $next_nloc - $nloc]]
		    if {$distance < $min_distance} {

# Move next marker by 1/2 distance required if this is the first pass
#  (makes offsets more symmetrical)
# Move next marker by entire distance required if this is subsequent pass
# Must maintain rounding to 0.001 precision

			set move_distance [expr $min_distance - $distance]
			if {$pass_count == 1} {
			    set move_distance [expr $move_distance / 2]
			}
			set move_distance [format %.10f $move_distance]
			set new_position [expr $next_nloc + $move_distance]
			set new_position [format %.10f $new_position]
			set j [expr $i + 1]
			set next_marker \
				[lreplace $next_marker 0 0 $new_position]
			set marker_labels \
				[lreplace $marker_labels $j $j $next_marker]
			incr markers_moved_right
		    }
		}
#
# This has been proven to take no more than 1.5 passes
#
		if {$pass_count > $max_pass_count} {
		    break
		}
#
# Scan right to left, moving next_marker right if necessary
#
		for {set i $num_markers} {$i > 0} {set i [expr $i - 1]} {
		    if {$i == $num_markers} {
			set nloc [format %.10f [expr $max_x_unscaled + \
				$min_distance]]
		    } else {
			set nloc [lindex [lindex $marker_labels $i] 0]
		    }
		    set next_marker [lindex $marker_labels [expr $i - 1]]
		    set next_nloc [lindex $next_marker 0]
		    set distance [format %.10f [expr $nloc - $next_nloc]]
		    if {$distance < $min_distance} {
			set move_distance [expr $min_distance - $distance]
			set move_distance [format %.10f $move_distance]
			set new_position [expr $next_nloc - $move_distance]
			set new_position [format %.10f $new_position]
			set j [expr $i - 1]
			set next_marker \
				[lreplace $next_marker 0 0 $new_position]
			set marker_labels [lreplace $marker_labels $j $j \
				$next_marker]
			incr markers_moved_left
		    }
		}
	    }
        }


# Draw marker ticks

	proc view_cx {worldc} {
	    upvar min_x min_x
	    upvar max_x max_x
	    return [expr 0.15 + (0.7 * ($worldc-double($min_x)) / \
		    ($max_x-double($min_x)))]
	}

	set vertical_ticks 0

# Vertical ticks only available for Solaris and Alpha versions
#   requires a recompile of xmgr for linux...not done yet

	if {$modified_xmgr} {
	    set vertical_ticks 1
	    foreach mloc $marker_ticks {
		tclgr send with line
		tclgr send line loctype view
		tclgr send line [view_cx $mloc], 0.85, [view_cx $mloc], 0.8325
		tclgr send line def
	    }
        }

# Write marker labels (when they fit)

	if {!$no_marker_labels} {
	    set last_nloc -100
	    set num_markers [llength $marker_labels]
	    for {set i 0} {$i < $num_markers} {incr i} {
		set marker_label [lindex $marker_labels $i]
		set mname [lindex $marker_label 1]
		set mloc [lindex $marker_label 2]
		set nloc [lindex $marker_label 0]

# setup leader line for label 

		tclgr send with line
		tclgr send line loctype view
		if {$vertical_ticks} {
		   tclgr send line [view_cx $mloc],0.8325,[view_cx $nloc],0.815
		} else {
		   tclgr send line [view_cx $mloc],0.85,[view_cx $nloc],0.815
		}
		tclgr send line def
		
		# setup marker label
		
		tclgr send with string
		tclgr send string on
		tclgr send string loctype view
		tclgr send string g$graphnum
		tclgr send string [view_cx $nloc], 0.8090
		tclgr send string rot 270
		tclgr send string char size 0.60
		tclgr send string def \"$mname\"
		ifdebug puts "Marker $mname at $nloc mapped to [view_cx $nloc]"
	    }
	}
    }

# Done with markers

# DRAW !!!

    tclgr send redraw

# Save certain variables to permit overlays

    global Solar_Plot_Last_Chrom
    global Solar_Plot_Last_Max_Loc 
    global Solar_Plot_Last_Max_Chars
    global Solar_Plot_Sets_In_Use	
    global Solar_Plot_Last_Max_Y
    global Solar_Plot_Last_Graphnum
    global Solar_Plot_If_Nans
    global Solar_Plot_Pass	
    global Solar_Plot_Highest_Chromosome
	
    set Solar_Plot_Last_Chrom $chromnum
    set Solar_Plot_Last_Max_Loc $max_x_unscaled
    set Solar_Plot_Last_Max_Chars $max_mrk_name_len
    set Solar_Plot_Sets_In_Use [lappend sets_in_use $setnum]
    set Solar_Plot_Last_Max_Y $max_y	
    set Solar_Plot_Last_Graphnum $graphnum	
    set Solar_Plot_Nans $if_nans
    set Solar_Plot_Pass $passnum
    set Solar_Plot_Highest_Chromosome $highest_chromosome

    if {$write_flag} {
	plotwrite
    }

    return ""
}


proc tclgrd {args} {
    puts "tclgr $args"
    eval tclgr $args
}

proc plot_all_pass {args} {

# remove -allpass from arglist

    set args [read_arglist $args -allpass {set foo 1} -* foo]

# Do a plot_all for every pass

    for {set i 1} {1} {incr i} {
	if {[file exists [full_filename multipoint$i.out]]} {
	    eval plot_all $args -pass $i
	} else {
	    break
	}
    }
    return ""
}

# solar::miniplot --
#
# Purpose:  Arrange miniature plots on a single page
#
# Usage:    miniplot [-pass <pass>] [-allpass] [-plots <number>] 
#                    [-port] [-land]
#
#             See also "plot -all"
#
#           -pass       Do this pass number (default is 1)
#           -allpass    Do all passes, each on a separate page
#           -plots      Put this many plots on a page
#           -port       Portrait layout
#           -land       Landscape layout
#           -nodisplay  Generate postscript, but don't display
#
#          Output file named passN.out (pass01.out for pass 1) in trait/outdir
#          is created.  The trait or outdir must have been specified previously
#          and the plots must have been created previously (see usage for
#          example).
#
#          The individual chromosome plots should have been created previously
#          using the "plot" command.  In fact, "plot -all" or "plot -allpass"
#          will invoke miniplot automatically.
#
#          This requires that Python (1.5.2 and later works, maybe earlier)
#          be installed.  If you do not have python, use "plot -string"
#          instead.
# -

proc miniplot {args} {

# Read arguments

    set display 1
    set pass 1
    set allpass 0
    set portland ""
    set plots -1
    set lessverbose 0
    set moreargs [read_arglist $args -pass pass -allpass {set allpass 1} \
	    -* foo]
    set badargs [read_arglist $moreargs \
	    -plots plots -port {set portland -P} -land {set portland -L} \
		     -lessverbose {set lessverbose 1} \
	    -nodisplay {set display 0}]
    if {"" != $badargs} {
	error "arrange: invalid arguments: $badargs"
    }

# Translate args to foobar getopt style required by arrangeps.py

    if {$plots == -1} {
	set plots ""
    } else {
	if {![is_integer $plots]} {
	    error "-plots must be followed by integer number of plots per page"
	}
	set plots "-p $plots "
    }
    set foobargs "$plots$portland"

# If -allpass, loop and recurse

    if {$allpass} {
	for {set i 1} {[file exists [full_filename multipoint$i.out]]} \
		{incr i} {
	    eval miniplot -pass $i $moreargs -nodisplay -lessverbose
	}
#	if {$display} {
#	    for {set j [expr $i - 1]} {$j > 0} {incr j -1} {
#		after 2000
#		exec pageview -right [full_filename pass[format %02d $j].ps] &
#	    }
#	}
    } else {
	if {$pass == 1} {
	    set pass 01
	    after 2000
	    set testfilenames [glob [full_filename chr*.ps]]
	    set filenames {}
	    foreach tname $testfilenames {
		if {-1 == [string first pass $tname]} {
		    lappend filenames $tname
		}
	    }
	} else {
	    set pass [format %02d $pass]
	    after 2000
	    set filenames [glob [full_filename chr*.pass$pass.ps]]
	}
	set filenames [lsort $filenames]
	puts "\nChromosome plot filenames for pass $pass are:\n$filenames"
	if {[llength $filenames] < 2} {
	    if {!$lessverbose} {
		puts "Only one chromosome plot is available,"
		puts "  so no page of miniplots will be produced."
	    }
	} else {
	    eval exec arrangeps.py $foobargs -o \
		[full_filename pass$pass.ps] $filenames >/dev/null
#	    if {$display} {
#	        set displayed 0
#
# After all, ghostscript does a useless job of rendering apparently due to
# lousy fonts so this section is currently inactive.
#
#	        catch {
#		    exec gs [full_filename pass$pass.ps] 2>/dev/null &
#		    set displayed 1
#	        }
#	        if {!$displayed} {
#		    puts "gs (ghostscript) not found to display miniplots"
#		    puts "Add gs directory to PATH if available on this system"
#	        }
#	    }
	    puts "\nMiniplots for pass $pass written to file [full_filename pass$pass.ps]"
	}

    }
    if {!$lessverbose} {
	puts "\nNote: plot -all does not display anything on your screen"
	puts "But these postscript files may be sent to printer with lp command"
	puts "An alternative genome plot can be produced with plot -string"
    }
    return ""
}

proc plot_all {args} {

    ifdebug puts "Entering plot_all"

# remove -all from arglist

    set pass 1
    set args [read_arglist $args -all {set foo 1} -pass pass -* foo]

    if {$pass != 1} {
	set args "$args -pass $pass"
    }

# Make list of all chromosomes for this pass

    set chrom_list {}
    set last_chrom {}
    set infilename [full_filename multipoint$pass.out]
    set infile [open $infilename r]
    gets $infile
    gets $infile
    while {-1 != [gets $infile line]} {
	if {2 == [scan $line "%s %s" cid chrom]} {
	    if {[is_integer $chrom]} {   ;# remove leading zero if any
		scan $chrom %d chrom
	    }
	    if {[string compare $chrom $last_chrom]} {
		lappend chrom_list $chrom
		set last_chrom $chrom
	    }
	}
    }
    close $infile

# Loop over all chromosomes, plot, and write files

    foreach chrom $chrom_list {
	ifdebug puts "Plotting chromosome $chrom with $args"
	eval plot $chrom $args
	ifdebug puts "Calling plotwrite"
	plotwrite
    }
    plot -close    ;# Ensure output files have been written
    return ""
}

proc plotwrite {args} {

# Ensure last chrom variable is present; if not, no plot had been done

    ifdebug puts "Starting plotwrite"

    global Solar_Plot_Last_Chrom
    global Solar_Plot_Pass
    if {![if_global_exists Solar_Plot_Last_Chrom]} {
	error "Must create plot first, or use -allplot"
    }

# Set default output filename

    if {[catch {set lchrom [format %02d $Solar_Plot_Last_Chrom]}]} {
	set lchrom $Solar_Plot_Last_Chrom
	if {[is_integer [string range $lchrom 0 0]]} {
	    if {![is_integer [string range $lchrom 1 1]]} {
		set lchrom 0$lchrom
	    }
	}
    }
    set ppass [format %02d $Solar_Plot_Pass]
    set filename [full_filename chr$lchrom.ps]
    if {$ppass != 1} {
	set filename [full_filename chr$lchrom.pass$ppass.ps]
    }

    ifdebug puts "plotwrite filename: $filename"

# Only optional argument allowed is "-filename"

    set badargs [read_arglist $args -write {set write 1} -filename filename]
    if {"" != $badargs} {
	error "plotwrite does not allow arguments: $badargs"
    }

# Send commands to xmgr

    tclgr send print to psmonol
    tclgr send print to file \"$filename\"
    tclgr send hardcopy
    return ""
}

proc plot_lodadj {args} {
    full_filename test_trait_or_outdir

    set color -1
    read_arglist $args -color color -lodadj {set ignore_this 0}
    if {$color != -1} {
	if {$color < 1 || $color > 15} {
	    error "Color must be 1..15"
	}
    }

    if {[catch {tclgr open} errmsg]} {
	if {[string compare $errmsg \
		"tclgr session already opened from this solar session"]} {
	    error $errmsg
	}
    }
    set all_points {}
    set last_point "0 0"
    set num_points 0
    set infile [open [full_filename lodadj.lods]]
    while {-1 != [gets $infile line]} {
	lappend all_points $line
	set last_point $line
	incr num_points
    }
    close $infile

    set last_x [lindex $last_point 0]
    if {$last_x == 0} {
	error "Last point estimated is 0,0...can't plot"
    }

    tclgr send kill graphs
    tclgr send clear line
    tclgr send clear string
    tclgr send focus g0

    tclgr send world ymin 0
    tclgr send world ymax $last_x
    tclgr send world xmin 0
    tclgr send world xmax $last_x

# Get number of reps from lodadj.info

    catch {
	set infile [open [full_filename lodadj.info] r]
	gets $infile line
	set nreps [lindex $line 3]
	close $infile
	tclgr send subtitle \"Repetitions:  $nreps\"
    }

    global env
    if {[file exists $env(SOLAR_LIB)/lodadj.gr]} {
	set mpathname [glob $env(SOLAR_LIB)/lodadj.gr]
	tclgr send read \"$mpathname\"
    }
    if {[file exists ~/lib/lodadj.gr]} {
	set mpathname [glob ~/lib/lodadj.gr]
	tclgr send read \"$mpathname\"
    } 
    if {[file exists lodadj.gr]} {
	tclgr send read \"lodadj.gr\"
    } 
    if {$color != -1} {	
	tclgr send s0 color $color
    }
    foreach point $all_points {
	tclgr send g0.s0 point "[lindex $point 0],[lindex $point 1]"
    }

    tclgr send redraw
}


# solar::plot_liability -- private
#
# Purpose:  Implements "plot -liability"
#
# Usage:    plot_liability [-model <modelname>]
#
#           Default (no arguments) assumes "polygenic" command has just been
#           given; uses poly.mod and poly.out in current trait/outdir.
#
#           -model modelname  Use this model.  Maximization output file
#                             must have been saved as *.out where *.mod
#                             is modelname.
# -

proc plot_liability {args} {

# Get arguments

    set modelname poly
    set badargs [read_arglist $args -model modelname]
    if {"" != $badargs} {
	error "Invalid -liability arguments: $badargs"
    }

# Setup modname and outputname

    set usemodel [append_mod $modelname]
    set useoutput [append_extension [string range $usemodel 0 \
	    [expr [string length $usemodel] - 5]] .out]
    set modname [full_filename $usemodel]
    set outputname [full_filename $useoutput]


# See if model and maximization output files exist

    if {"poly" == $modelname && ![file exists $modname]} {
	error "Missing poly.mod.  Run polygenic command first, or specify -model"
    }
    if {"poly" == $modelname && ![file exists $outputname]} {
	error "Missing poly.out.  Run polygenic command first, or specify -model"
    }

    if {![file exists $modname]} {
	error "Model $modname not found"
    }

    if {![file exists $outputname]} {
	error "Can't find maximization output file $outputname"
    }

# Get maximized parameter values

    set noage 0
    set nosex 0
    set tailname [file tail $modname]
    if {[catch {set bsex [oldmodel $tailname bsex]}]} {
	set bsex 0.0
	incr nosex
    }
    if {[catch {set bage [oldmodel $tailname bage]}]} {
	set bage 0.0
	incr noage
    }
    if {[catch {set bagesex [oldmodel $tailname bage*sex]}]} {
	set bagesex 0.0
	incr noage
	incr nosex
    }
    if {[catch {set bage2 [oldmodel $tailname bage^2]}]} {
	set bage2 0.0
	incr noage
    }
    if {[catch {set bage2sex [oldmodel $tailname bage^2*sex]}]} {
	set bage2sex 0.0
	incr noage
	incr nosex
    }
    set mu [oldmodel $tailname mean]

# Determine whether we found any age or sex covariates

    set found_age 0
    set found_sex 0
    if {$noage < 4} {
	set found_age 1
    }
    if {$nosex < 3} {
	set found_sex 1
    }

# Get mean, min, max age

    if {![catch {set meanage [getvar -mean $useoutput age]}]} {
	set minage [getvar -min $useoutput age]
	set maxage [getvar -max $useoutput age]
    } else {
	if {$found_age} {
	    error "Outdated maximization output file missing age variable"
	}
	set meanage 50.0
	set minage 0.0
	set maxage 100.0
    }

# Determine proper plotting range for age

    set minx [expr round (floor ($minage / 5.0) * 5)]
    set maxx [expr round (ceil ($maxage / 5.0) * 5)]


# Keep min,max for middle range (35% - 65%) so that position of
#   legend may be determined

    set middle_min 1.0
    set middle_max 0.0
    set begin_middle [expr ($maxx - $minx)*0.35 + $minx]
    set end_middle [expr $maxx - ($maxx - $minx)*0.35]

# Create MALE curve (leave out "*sex" terms since male=0)

    set male_curve {}
    for {set x $minx} {$x <= $maxx} {incr x} {
	set z [expr $mu \
		+ $bage * ($x - $meanage) \
		+ $bage2 * (($x - $meanage)*($x - $meanage))]

	set liability [alnorm $z t]


	lappend male_curve [list $x $liability]

	if {$x >= $begin_middle && $x <= $end_middle} {
	    if {$liability > $middle_max} {
		set middle_max $liability
	    }
	    if {$liability < $middle_min} {
		set middle_min $liability
	    }
	}
    }

# Create FEMALE curve (include all terms, female=1)

    if {$found_sex} {
	set female_curve {}
	for {set x $minx} {$x <= $maxx} {incr x} {
	    set z [expr $mu \
		    + $bage * ($x - $meanage) \
		    + $bage2 * (($x - $meanage) * ($x - $meanage)) \
		    + $bsex \
		    + $bagesex * ($x - $meanage) \
		    + $bage2sex * (($x - $meanage) * ($x - $meanage))] 
	    set liability [alnorm $z t]
	    lappend female_curve [list $x $liability]

	    if {$x >= $begin_middle && $x <= $end_middle} {
		if {$liability > $middle_max} {
		    set middle_max $liability
		}
		if {$liability < $middle_min} {
		    set middle_min $liability
		}
	    }
	}
    }

# Open new or existing tclgr session and clear out

    if {[catch {tclgr open} errmsg]} {
	if {[string compare $errmsg \
		"tclgr session already opened from this solar session"]} {
	    error $errmsg
	}
    }
    tclgr send kill graphs
    tclgr send clear line
    tclgr send clear string
    tclgr send focus g0
    tclgr send flush

# Setup default xmgr parameters

    tclgr send world ymin 0
    tclgr send world ymax 1.1

    set graph_x_min [expr $minx - 5]
    set graph_x_max [expr $maxx + 5]
    tclgr send world xmin $graph_x_min
    tclgr send world xmax $graph_x_max

    set captraitname \
      "[string toupper [string range [trait] 0 0]][string range [trait] 1 end]"
    tclgr send title \"$captraitname\"
    tclgr send xaxis tick op bottom

    tclgr send xaxis ticklabel start type spec
    set xstart [expr round (ceil ($minx / 10.0) * 10)]
    tclgr send xaxis ticklabel start $xstart

    set xstop [expr round (floor ($maxx / 10.0) * 10)]
    tclgr send xaxis ticklabel stop type spec
    tclgr send xaxis ticklabel stop $xstop

# Determine whether legend goes at top or bottom

    tclgr send legend loctype view
    if {$middle_min > 0.45} {
	tclgr send legend Y1 0.295
    } else {
	tclgr send legend Y1 0.705
    }

# Load xmgr parameter file liability.gr

    global env
    if {[file exists $env(SOLAR_LIB)/liability.gr]} {
	set mpathname [glob $env(SOLAR_LIB)/liability.gr]
	tclgr send read \"$mpathname\"
    }
    if {[file exists ~/lib/liability.gr]} {
	set mpathname [glob ~/lib/liability.gr]
	tclgr send read \"$mpathname\"
    } 
    if {[file exists liability.gr]} {
	tclgr send read \"liability.gr\"
    } 

# Plot male curve (set 1, so blue is on top of overlap)

    if {$found_sex} {
	set use_set 1
    } else {
	set use_set 2
    }
    foreach point $male_curve {
	set x [lindex $point 0]
	set y [lindex $point 1]
	tclgr send g0.s$use_set point $x,$y
    }

# Plot female curve (set 0)

    if {$found_sex} {
	foreach point $female_curve {
	    set x [lindex $point 0]
	    set y [lindex $point 1]
	    tclgr send g0.s0 point $x,$y
	}

# Turn on legend box

	tclgr send legend on
	tclgr send legend box on
    }

# DRAW !

    tclgr send redraw
}


# solar::plot_power -- private
#
# Purpose:  Implements "plot -power"
#
# Usage:    plot_power [-title <plot_title>]
#
# -

proc plot_power {args} {
    set title "Power"
    read_arglist $args -title title -power {set ignore_this 0}

    if {[catch {tclgr open} errmsg]} {
	if {[string compare $errmsg \
		"tclgr session already opened from this solar session"]} {
	    error $errmsg
	}
    }

    if {[catch {set inf [open power.info r]}]} {
        error "Cannot open power.info"
    }
    gets $inf line
    if {[lindex $line 0] == "data"} {
        gets $inf line
    }
    set nreps [lindex $line 2]
    gets $inf line
    if {[lindex $line 0] == "h2t"} {
        set h2t [lindex $line 2]
        set h2r ""
    } else {
        set h2r [lindex $line 2]
        set h2t ""
    }
    gets $inf line
    set lods [lrange $line 7 [expr [llength $line] - 1]]
    set nlods [llength $lods]
    close $inf

    if {[catch {set inf [open power.out r]}]} {
        error "Cannot open power.out"
    }

    set x {}
    for {set i 0} {$i < $nlods} {incr i} {
        set y($i) {}
    }
    while {-1 != [gets $inf line]} {
        lappend x [lindex $line 0]
        set i1 1
        for {set i 0} {$i < $nlods} {incr i} {
            lappend y($i) [lindex $line $i1]
            incr i1
        }
    }
    close $inf

    set npts [llength $x]
    if {$npts == 0} {
	error "No ELODs in power.out ... can't plot"
    }

    tclgr send kill graphs
    tclgr send clear line
    tclgr send clear string
    tclgr send focus g0

    tclgr send title \"$title\"

    tclgr send world xmin 0
    set max_x [lindex $x [expr [llength $x] - 1]]
    set max_x [expr ceil(10*$max_x)/10]
    tclgr send world xmax $max_x
    tclgr send xaxis tick major 0.1
    tclgr send xaxis tick minor 0.05

    tclgr send world ymin 0
    tclgr send world ymax 1
    tclgr send yaxis tick major 0.1
    tclgr send yaxis tick minor 0.05

    tclgr send legend loctype view
    tclgr send legend X1 0.58
    tclgr send legend Y1 [expr 0.21 + $nlods*0.04]
    tclgr send legend on
    tclgr send legend box on

    global env
    if {[file exists $env(SOLAR_LIB)/power.gr]} {
	set mpathname [glob $env(SOLAR_LIB)/power.gr]
	tclgr send read \"$mpathname\"
    }
    if {[file exists ~/lib/power.gr]} {
	set mpathname [glob ~/lib/power.gr]
	tclgr send read \"$mpathname\"
    } 
    if {[file exists power.gr]} {
	tclgr send read \"power.gr\"
    } 

    for {set i 0} {$i < $npts} {incr i} {
        for {set j 0} {$j < $nlods} {incr j} {
	    tclgr send s$j color [expr $j + 2]
	    tclgr send g0.s$j point "[lindex $x $i],[lindex $y($j) $i]"
            tclgr send legend string $j \"LOD = [lindex $lods $j]\"
        }
    }

    tclgr send redraw
}

proc purge_multipoint_output_directory {} {
    eval delete_files_forcibly \
    [glob -nocomplain [full_filename link*.mod]] \
    [glob -nocomplain [full_filename link*.out]] \
    [glob -nocomplain [full_filename link*.smp]] \
    [full_filename temp.out] \
    [full_filename temp.smp]

    set nullmods [glob -nocomplain [full_filename null*.mod]]
    foreach mod $nullmods {
	if {[string compare $mod [full_filename null0.mod]]} {
	    delete_files_forcibly $mod
	}
    }

    set nullmods [glob -nocomplain [full_filename null*.out]]
    foreach mod $nullmods {
	if {[string compare $mod [full_filename null0.out]]} {
	    delete_files_forcibly $mod
	}
    }
}

proc purge_sporadic_output_directory {} {
    eval delete_files_forcibly [full_filename s0.mod] \
    [full_filename s0.out] \
    [full_filename s0.smp] \
    [glob -nocomplain [full_filename no*.mod]] \
    [glob -nocomplain [full_filename no*.out]] \
    [glob -nocomplain [full_filename no*.smp]] \
    [full_filename spor.mod] \
    [full_filename spor.out] \
    [full_filename spor.smp] \
    [full_filename temp.out] \
    [full_filename temp.smp]
}

proc purge_polygenic_output_directory {} {
    eval delete_files_forcibly [full_filename s0.mod] \
    [full_filename s0.out] \
    [full_filename s0.smp] \
    [full_filename p0.mod] \
    [full_filename p0.out] \
    [full_filename p0.smp] \
    [glob -nocomplain [full_filename no*.mod]] \
    [glob -nocomplain [full_filename no*.out]] \
    [glob -nocomplain [full_filename no*.smp]] \
    [full_filename spor.mod] \
    [full_filename spor.out] \
    [full_filename spor.smp] \
    [full_filename poly.mod] \
    [full_filename poly.out] \
    [full_filename poly.smp] \
    [full_filename temp.out] \
    [full_filename temp.smp]
}

proc bayesavg_purge {args} {
    set testonly 0
    set outname bayesavg
    set prefix c
    read_arglist $args -testonly {set testonly 1} \
	    -outname outname -prefix prefix

    set baylist [concat \
	    [glob -nocomplain [full_filename tmp.*]] \
	    [glob -nocomplain [full_filename $outname.*]] \
	    [glob -nocomplain [full_filename $prefix.spor.*]] \
	    [glob -nocomplain [full_filename $prefix.sat.*]] \
	    [glob -nocomplain [full_filename $prefix.start.*]] \
	    [glob -nocomplain [full_filename $prefix.orig.*]] \
	    [glob -nocomplain [full_filename $prefix.base.*]] \
	    [glob -nocomplain [full_filename $prefix\[0-9\].mod]] \
	    [glob -nocomplain [full_filename $prefix\[0-9\].out]] \
	    [glob -nocomplain [full_filename $prefix\[0-9\].smp]] \
	    [glob -nocomplain [full_filename $prefix\[0-9\]\[0-9\]*.mod]] \
	    [glob -nocomplain [full_filename $prefix\[0-9\]\[0-9\]*.out]] \
	    [glob -nocomplain [full_filename $prefix\[0-9\]\[0-9\]*.smp]]]

    if {!$testonly} {
	eval delete_files_forcibly [glob -nocomplain [full_filename tmp.*]] $baylist
    }
    return $baylist
}

# solar::cleanmodel -- private
#
# Purpose:  Clear parameter values [NOT YET IMPLEMENTED]
#
# Usage:    cleanmodel
#
# Notes:    Use this model to clear out obsolete parameter values but keep the
#           major model settings (trait, covariates, options, etc.)
#
#           This is particularly useful after a convergence failure has put
#           NaN values in some or all parameters.  Such a model cannot be restarted.
#
#           When a parameter has all zero values, starting points and boundaries
#           are setup automatically during maximization.
#-

proc clearmodel {} {
}

# solar::newmod --
#
# Purpose:  Start a new model
#
# Usage:    newmod [<trait>]+
#
#           <trait>  Set the trait(s) to this/these trait(s).  (The trait(s)
#                    can be specified later.  If not specified here, they
#                    become <undefined>.)
# Notes:
#
#    (1)   This combines "model new", "outdir -default", and optionally
#          trait [<trait>]+ .  This is now preferred to using the separate
#          commands, because it is shorter.  For example, the command:
#
#        newmod q1 q2
#
#           takes the place of the commands:
#
#        outdir -default
#        model new
#        trait q1 q2
#
#           Clearly the "newmod" form is superior, it preserves the
#           essential information while reducing redundant keystrokes.
#
#    (2)    Since this clears the outdir, it is adviseable to use this
#           command instead of "model new" to be sure that the outdir
#           is cleared, and not inheirited from some previous script.
#           From now on, the manual advises using "newmod" (and not
#           "model new") for this reason.  However, the behavior of
#           "model new" itself is unchanged, so that existing scripts
#           that operate correctly will continue to operate correctly.
#           When combining previously written scripts that use "model new"
#           instead of "newmod", the user must be careful to update
#           "outdir" status if required.  New scripts using "newmod" will
#           not be subject to the error of incorrectly inheiriting an
#           unwanted outdir setting.
# -

proc newmod {args} {
    outdir -default
    if {{} == $args} {
	return [model new]
    } else {
	model new
	return [eval trait $args]
    }
    return ""
}


# solar::spormod --
#
# Purpose:  Set up a sporadic model with the standard parameters
#
# Usage:    spormod
#
# Notes:    There are no arguments.  You must have previously loaded the
#           phenotypes file, selected the trait, and specified the
#           covariates.
#
#           Household effects are suspended.  If you want a 'household'
#           model, give the spormod command first, then the 'house' command.
#
#           The starting lower bound for e2 is controlled by e2lower.
#
#           Normally you do not use this command directly, but instead use 
#           the "polygenic" command to do a complete polygenic analysis,
#           which maximizes a sporadic model which was set up using this
#           command.  See the tutorial in Chapter 3.
# -

proc spormod {} {

# Suspend household effects
    house -suspend    ;# Suspend household effects, if they had been enabled

# SET UP PARAMETERS FOR UNIVARIATE...
# Since each parameter is inserted at beginning, we add them in the reverse
# from standard order.  Note that if parameter already exists, its position
# is unchanged.

    set ts [trait]
    set nts [llength $ts]
    if {$nts == 1} {
	set multi 0

# e2 and h2r

	global SOLAR_constraint_tol
	parameter -insert h2r = 0 lower -$SOLAR_constraint_tol upper 1
	parameter -insert e2 = 1 lower [e2lower] upper 1.01
	constraint h2r = 0

# Mean and SD

	parameter -insert sd
	parameter -insert mean

# SET UP PARAMETERS FOR BI/MULTIVARIATE...

    } elseif {$nts > 1} {
	set multi 1

# Set up rhoe and rhog parameters for bivariate models
# rhog must be constrained to 0 for sporadic model
    
	if {$nts == 2} {
	    parameter rhoe = 0 lower -0.9 upper 0.9
	    parameter rhog = 0 lower -0.9 upper 0.9
	    constraint rhog = 0
	} else {
            foreach prefix {rhog_ rhoe_} {
		for {set i [expr $nts - 1]} {$i > 0} {incr i -1} {
		    for {set j $nts} {$j > $i} {incr j -1} {
			parameter -insert [catenate $prefix $i $j] = 0 \
			    lower -0.9 upper 0.9
		    }
		}
	    }
	}

# Configure Mean and SD parameters

	for {set i $nts} {$i > 0} {incr i -1} {
	    set tr [lindex $ts [expr $i - 1]]

	    global SOLAR_constraint_tol
    parameter -insert h2r\($tr\) = 0 lower -$SOLAR_constraint_tol upper 1
	    parameter -insert e2\($tr\) = 1 lower [e2lower] upper 1.01
	    parameter -insert sd\($tr\)
	    parameter -insert mean\($tr\)
	    constraint <h2r\($tr\)> = 0
	}
    }

# Remove all h2q's and their constraints and matrices
#   Bivariate version does not support special constraint terms
    set starting_h2qcount [h2qcount]
    if {$starting_h2qcount > 0} {
	for {set i 1} {$i <= $starting_h2qcount} {incr i} {
	    catch {matrix delete mibd$i}
	    foreach tr $ts {
		if {$multi} {
		    set suffix ($tr)
		    constraint <e2$suffix> + <h2r$suffix> = 1
		} else {
		    set suffix ""
		    catch {constraint_remove h2q$i}
		}
		catch {parameter delete h2q$i$suffix}
	    }
	}
    }

# Update omega...
#   Multivariate versions do not support special omega terms
#     They just slap in new omega...
    if {$nts == 2} {

	omega = <sd(ti)>*<sd(tj)>* \
I*sqrt(<e2(ti)>)*sqrt(<e2(tj)>)*(tne*rhoe+teq)

    } elseif {$nts > 2} {

	omega = <sd(ti)>*<sd(tj)>*( \
I*sqrt(<e2(ti)>)*sqrt(<e2(tj)>)*(tne*rhoe_ij+teq) + \
phi2*sqrt(<h2r(ti)>)*sqrt(<h2r(tj)>)*(tne*rhog_ij+teq) )
	
    } else {
	if {"omega = Use_polygenic_to_set_standard_model_parameterization" \
		== [omega]} {
	    omega = pvar*(phi2*h2r + I*e2)
	} else {
	    omega_remove_linkage  ;# Must be done after using h2qcount
	}
    }

# Set up principal variance component constraints

    if {!$multi} {
	constraint e2 + h2r = 1
    } else {
	foreach tr $ts {
	    set suffix \($tr\)
	    constraint <e2$suffix> + <h2r$suffix> = 1
	}
    }

    return ""
}


# Apply to all variance parameters generically

proc vparameter {gname args} {
    if {[catch {trait}]} {
	error "Trait must be specified first"
    }
    set ts [trait]
    if {1 == [llength $ts]} {
	eval parameter $gname $args
    } else {
	foreach t $ts {
	    eval parameter $gname\($t\) $args
	}
    }

# Covariance Matrices Diagonal (requires rhoe start at 0)

    option CMDiagonal 1

    return ""
}

# solar::linkgsd --
# solar::linkqsd0 --
# solar::linkqsd --
#
# Purpose:  Set up linkage model with esd, gsd, qsd parameters (EXPERIMENTAL)
#
# Usage:  linkqsd <path>/<mibdfile>
#         linkqsd0 <path>/<mibdfile>
#
# Example:  model new
#           trait q4
#           covar age sex
#           polygsd
#           maximize
#           gsd2h2r
#           chromosome 9 10
#           interval 5
#           mibddir gaw10mibd
#           multipoint -link linkqsd0 -cparm {esd gsd qsd}
#
# Notes:  Polygenic parameters must already have been set up (use the
#         polygsd command).  Prefereably it should have been maximized
#         also (use the maximize command).
#
#         linkqsd modifieds the model currently in memory.  linkqsd0 assumes
#         the existance of a null0 model in the maximization output directory,
#         loads that, and then adds the linkage element.
#
#         We have not worked out suitable heuristics to force maximization
#         of difficult models, so at the present time this parameterization
#         is not as robust as our standard parameterization.
# -

# linkgsd is an alias
proc linkgsd {mibdfile args} {
    eval linkqsd $mibdfile $args
}

proc linkqsd0 {mibdfile args} {
    load model [full_filename null0]
    eval linkqsd $mibdfile $args
    return ""
}
    
proc linkqsd {mibdfile args} {
    if {![file exists $mibdfile]} {
	error "linkqsd: File $mibdfile not found."
    }
    set ts [trait]
    set ntraits [llength $ts]

    global Solar_qsd_fraction
    if {[if_global_exists Solar_qsd_fraction]} {
	set qsd_fraction $Solar_qsd_fraction
    } else {
	set qsd_fraction 0.02
    }

    if {$ntraits == 1} {
	if {![if_parameter_exists esd] || ![if_parameter_exists gsd]} {
	    error "linkqsd: Must run polygsd first to create esd and gsd"
	}
	set esd [parameter esd =]
	set gsd [parameter gsd =]
	set sd [expr sqrt ($esd*$esd + $gsd*$gsd)]

	set qsd [expr $sd * $qsd_fraction]

	if {$esd > $gsd} {
	    set esd [expr sqrt ($sd*$sd - $gsd*$gsd - $qsd*$qsd)]
	} else {
	    set gsd [expr sqrt ($sd*$sd - $esd*$esd - $qsd*$qsd)]
	}

	set upper_qsd [parameter gsd upper]
	parameter qsd1 = $qsd lower 0 upper $upper_qsd
	parameter esd = $esd
	parameter gsd = $gsd
	matrix load $mibdfile mibd1
	omega = I*esd*esd + phi2*gsd*gsd + mibd1*qsd1*qsd1
    } else {
	foreach tr $ts {
	    if {![if_parameter_exists esd($tr)] || \
		    ![if_parameter_exists gsd($tr)]} {
		error "linkqsd: Must run polygsd first to create esd and gsd"
	    }
	    set esd [parameter esd($tr) =]
	    set gsd [parameter gsd($tr) =]
	    set sd [expr sqrt ($esd*$esd + $gsd*$gsd)]

	    set qsd [expr $sd * $qsd_fraction]

	    if {$esd > $gsd} {
		set esd [expr sqrt ($sd*$sd - $gsd*$gsd - $qsd*$qsd)]
	    } else {
		set gsd [expr sqrt ($sd*$sd - $esd*$esd - $qsd*$qsd)]
	    }

	    set upper_qsd [parameter gsd($tr) upper]
	    parameter qsd1($tr) = $qsd lower 0 upper $upper_qsd
	    parameter esd($tr) = $esd
	    parameter gsd($tr) = $gsd
	}
	matrix load $mibdfile mibd1
	if {$ntraits == 2} {
	    parameter rhoq1 = 0 lower -1 upper 1
	    omega = I*<esd(ti)>*<esd(tj)>*(tne*rhoe+teq) + \
		phi2*<gsd(ti)>*<gsd(tj)>*(tne*rhog+teq) + \
		mibd1*<qsd1(ti)>*<qsd1(tj)>*(tne*rhoq1+teq)
	} else {
	    for {set i [expr $ntraits - 1]} {$i > 0} {incr i -1} {
		for {set j $ntraits} {$j > $i} {incr j -1} {
		    parameter [catenate rhoq1_ $i $j] = 0 \
			lower -1 upper 1
		}
	    }
	    omega = I*<esd(ti)>*<esd(tj)>*(tne*rhoe_ij+teq) + \
		phi2*<gsd(ti)>*<gsd(tj)>*(tne*rhog_ij+teq) + \
		mibd1*<qsd1(ti)>*<qsd1(tj)>*(tne*rhoq1_ij+teq)
	}
    }
    return ""
}


# solar::intraitclass
#
# purpose: set up trait with classwise inormalization
#
# Usage:  intraitclass <traitname> [<class>]+
#
# Example: trait averagefa 0 2 3
#
# Notes: Do model new before using this command.
#
# -

proc intraitclass {traitname args} {
    set def ""
    foreach class $args {
	if {$def == ""} {
	    set def "(class==$class)*inormalc_$class\_$traitname"
	} else {
	    set def "$def + (class==$class)*inormalc_$class\_$traitname"
	}
    }
    define i_$traitname = $def
    trait i_$traitname
}

# solar::sporclass --
# solar::polyclass -- (EXPERIMENTAL)
#
# Purpose:  Set up polygenic model with class specific parameterization
#
# Usage:    polyclass [-g] [-intrait] [-incovar] [<class-start>[-<class-end>]]+
#                     [-comb] [-maxi] [-rincovar] [-maxsnp <snp_name>]
#           sporclass [-g] [-intrait] [-incovar] [<class-start>[-<class-end>]]+
#                     [-comb] [-maxi] [-rincovar] [-maxsnp <snp_name>]
#
#           -g   Use global phenotypic values to set parameter adjustments
#                (otherwise, means are determined for each class)
#
#           -intrait  inormalize trait values on a per-class basis
#           -resmax inormalize residual values in place of traits
#           -incovar  (NOT WORKING IN version 7.1.2) inormalize covar values
#                     on a per-class basis (only used for simple linear
#                     covariates, no interactions or exponents)
#           -comb     all classes combined model
#           -max      after building the model, maximize it
#
#           -maxsnp <snp_name>  Maximize and include  snp_name as covariate
#                        in the model and determine statistics for it: beta,
#                        beta se, chi, p, and variance explained (varexp).  
#                        H2r's are reported for the models with and
#                        without the snp.
#           -append   Append results to existing output file(s) if any
#
# Short Example:
#
#            trait q4
#            covariate age sex
#            polyclass 1-3 9
#            maximize -q
#
# Notes: One phenotypes file must have a field named "class" which defines
#        the class value for each person in the sample.
#
#        Class specific parameters are given names with _c<class> appended.
#
#        User covariates are transformed into class-specific mu addends.
#        All individuals in sample must have all variables specified as
#        covariates.
#
#        After choosing trait and covariates, do either sporclass or
#        polyclass.  You cannot do a second polyclass on a sporclassed model
#        to make it polygenic.
#
#        Unbalanced covariates for multivariate traits are not supported.
#        This is different from ordinary covariate behavior for multivariate
#        traits--which permits covariates to be missing in the sample if they
#        are specific to a missing trait.
#
#        A defined pseudo-covariate named "blank_classes()" restricts the
#        sample to the union of all classes specified.
#
#        The maximized model is asved in the output directory as
#        polyclassmax.mod with output file polyclassmax.out.  Note that if
#        -intrait option is selected, trait name and default output
#        directory will have leading i_ prefix (for the inormalization).
#        If the -resmax option is selected, the trait will be named
#        "residual" or "i_residual" if -intrait is also selected.
#
#-

proc sporclass {args} {
    return [eval polysporclass -s $args]
}

proc polyclass {args} {
    return [eval polysporclass -p $args]
}

proc polysporclass {args} {

    set grand 0
    set sporadic 0
    set intrait 0
    set incovar 0
    set resmax 0
    set comb 0
    set maxi 0
    set nextintrait 0
    set maxsnp ""
    set allterms ""
    set snpappend 0

    if {[lindex $args 0] == "-p"} {
	set args [lrange $args 1 end]
    }
    if {[lindex $args 0] == "-s"} {
	set sporadic 1
	set args [lrange $args 1 end]
    }

    set classargs [read_arglist $args -g {set grand 1} -intrait {set intrait 1}\
		       -maxsnp maxsnp -append {set snpappend 1} \
       -incovar {set incovar 1} -comb {set comb 1} -maxi {set maxi 1} \
		       -max {set maxi 1} -resmax {set resmax 1; set maxi 1}
]

    if {$maxsnp != ""} {
	set maxi 1
	set prefix [string tolower [string range $maxsnp 0 3]]
	if {$prefix != "snp_"} {
	    error "snp names must begin with snp_ prefix"
	}
	set phens [lrange [phenotypes] 1 end]
	if {-1 == [lsearch -exact $phens $maxsnp]} {
	    error "$maxsnp not in phenotypes file (case significant now)"
	}
    }

    if {$resmax} {
	if {$intrait} {
	    set intrait 0
	    set nextintrait 1
	}
    }

    set classlist [expand_ranges $classargs]

    foreach pclass $classlist {
	if {![is_integer $pclass]} {
	    error "$pclass is not a valid phenotype class"
	}
    }
    if {0 == [llength $classlist]} {
	error "polyclass: Classes not specified!"
    }
    puts "full class list is $classlist"

    if {$maxsnp != ""} {
	covariate $maxsnp
    }

    set ts [trait]
    set nt [llength $ts]
    set covs [covariate]
    set oldcovs [covariate]
    set ncov [llength $covs]
    set ntraits [llength $ts]

    set savename [full_filename solar.polyclass]
    save model $savename.start

# if grand mean model, initializations are not class specific so do here    

    if {$grand} {
	model new
	eval trait $ts
	eval covariate $covs
	covariate class()
	polymod
	maximize -initpar
	save model $savename.allc
	set pars [parameter -return]
    }

    set newbetas ""
    set oldbetas ""
    set newmu ""
    set residualmup "\[lindex \$line 1\]"
    set residualmu ""
    set residualmustarted 0
    set newomega ""
    set first 1
    set k -1
    set newtsinit ""
    set newts ""
    set has_sex 0

    foreach ic $classlist {

# if non-grand mean model, we must do class-specific initializations here
# also, accumulate grand definition of all class inormalizations

	if {!$grand} {
	    model new
	    if {!$intrait} {
		set newtsinit $ts

	    } else { ;# FOR -intrait
		set newtsinit ""
		foreach tr $ts {
		    set newt i_$tr
		    lappend newtsinit $newt
		    define $newt = inormalc_$ic\_$tr
		}
	    }
	    if {!$incovar} {
		set newcs $covs
	    } else {
		set newcs ""
		foreach cov $covs {
		    if {-1 != [string first * $cov] || -1 != [string first ^ $cov] \
			    || -1 != [string first \( $cov] || $cov == "sex"} {
			lappend newcs $cov
		    } else {
			set newcov c$ic\_$cov
			define $newcov = inormalc_$ic\_$cov
			lappend newcs $newcov
		    }
		}
	    }

	    eval trait $newtsinit
	    eval covariate $newcs
	    define blank_classes = blank*(class!=$ic)
	    covariate blank_classes()
	    if {$sporadic} {
		spormod
	    } else {
		polymod
	    }
	    set maxstatus [catch {maximize -initpar} errmes]
	    if {$maxstatus != 0} {
		puts $errmes
		error "Error (above) prevents using class $ic"
	    }
	    save model $savename.$ic
	    set pars [parameter -return]
	}
#
# For the first class, grand or not, start the whole model
# This is where we do the real model new and start
#
# We reload the previous model on each pass after the first
#
	if {$first} {
	    model new
	    if {!$intrait} {
		set newts $ts
	    } else {
		set newts ""
		foreach tr $ts {
		    set newt i_$tr
		    lappend newts $newt
		    define $newt = inormalc_$ic\_$tr  ;# temp definition
# we accumulate the string required for the final definition of this trait
		    set intraitdef($newt) "0"
		}
	    }
#	    puts "new traits are $newts"
	    eval trait $newts
	    set defstring "blank"
	    foreach icc $classlist {
		set defstring "$defstring*(class!=$icc)"
	    }
	    eval define blank_classes = $defstring
	    covariate blank_classes()
	    set first 0
	} elseif {!$grand} {
	    load model $savename.temp
	}
	if {$intrait} {
	    set nindex 0
	    foreach newt $newts {
		set oldtr [lindex $ts $nindex]
		incr nindex
		set intraitdef($newt) \
"$intraitdef($newt) + (class==$ic)*inormalc_$ic\_$oldtr"
	    }
	}

# create and initialize parameters for this class

	foreach par $pars {
	    if {$nt == 1 || ([string range $par 0 2] == "rho")} {
		set newpname [lindex $par 0]_c$ic
		set oldpname [lindex $par 0]
	    } else {
		set newp [lindex $par 0]
		set ppos [string first \( $newp]
		set pre [string range $newp 0 [expr $ppos - 1]]
		set post [string range $newp [expr $ppos + 1] end]
		set pp ""
		if {$intrait && $grand} {
		    set pp i_
		}
		set newpname "$pre\_c$ic\($pp$post"
		set oldpname [lindex $par 0]
	    }
	    set newpstring "parameter $newpname = [lrange $par 2 end]"
#	    puts "Evaluating $newpstring"
	    eval $newpstring
	    if {[string index $newpname 0] == "b"} {
		lappend newbetas $newpname
		lappend oldbetas $oldpname
	    }
	}

# create constraints for this class

	if {$sporadic} {
	    if {$nt==1} {
		constraint h2r_c$ic = 0
	    } else {
		foreach tr $newts {
		    constraint <h2r_c$ic\($tr\)> = 0
		}
	    }
	}

	if {$nt==1} {
	    constraint e2_c$ic + h2r_c$ic = 1
	} else {
	    foreach tr $newts {
		constraint <e2_c$ic\($tr\)> + <h2r_c$ic\($tr\)> = 1
	    }
	}



# create omega terms for this class

	set newomega "$newomega + (class_i==$ic)"
	if {$nt==1} {
	    set newomega "$newomega*sd_c$ic*sd_c$ic*(phi2*h2r_c$ic + I*e2_c$ic)"
	} elseif {$nt==2} {
	    set newomega "$newomega*<sd_c$ic\(ti\)>*<sd_c$ic\(tj\)>*( \
I*sqrt(<e2_c$ic\(ti\)>)*sqrt(<e2_c$ic\(tj\)>)*(tne*rhoe_c$ic+teq) + \
phi2*sqrt(<h2r_c$ic\(ti\)>)*sqrt(<h2r_c$ic\(tj\)>)*(tne*rhog_c$ic+teq))"

	} else {
	    set newomega "$newomega*<sd_c$ic\(ti\)>*<sd_c$ic\(tj\)>*( \
I*sqrt(<e2_c$ic\(ti\))*sqrt(<e2_c$ic\(tj\))*(tne*rhoe_c$ic\_ij+teq) + \
phi2*sqrt(<h2r_c$ic\(ti\)>)*sqrt(<h2r_c$ic\(tj\)>)*(tne*rhog_c$ic\_ij+teq))"
	}
#	puts "newomega is $newomega"

# create mu terms for this class and trait

	set tno 0
	foreach trai $newts {
	    incr tno

	    if {$nt==1} {
		set newmu "$newmu + (class==$ic)*(mean_c$ic"
		if {$residualmustarted} {
		    if {$comb} {
			set residualmu "$residualmu - (\[parameter mean = \]"
		    } else {
			set residualmu "$residualmu - (\[lindex \$line 2\]==$ic)*(\[parameter mean_c$ic = \]"
		    }
		} else {
		    if {$comb} {
			set residualmu "$residualmup - (\[parameter mean = \]"
		    } else {
			set residualmu "$residualmup - (\[lindex \$line 2\]==$ic)*(\[parameter mean_c$ic = \]"
		    }
		    set residualmustarted 1
		}
	    } else {
		set newmu "$newmu + t$tno*(class==$ic)*(<mean_c$ic\($trai\)>"
	    }
	    
	    set nbetas [llength $newbetas]

#	    puts "COVS are $covs"
#	    puts "BETAS are $newbetas"

	    for {set i 0} {$i < $ncov} {incr i} {
		set cov [lindex $covs [expr $i]]
		if {[string range $cov end-1 end] == "()"} {
		    covariate $cov
		    continue
		}
		if {$incovar} {
		    if {-1 != [string first * $cov] || -1 != [string first ^ $cov] \
			    || -1 != [string first \( $cov] || $cov == "sex"} {
		    } else {
			set newcov c$ic\_$cov
			set cov $newcov
		    }
		}
		incr k
		set newbeta [lindex $newbetas $k]
		set terms [split $cov *]
		set nexps [llength [split $cov ^]]
		set nterms [llength $terms]

		if {$nt==1&&$nterms==1&&$nexps==1} {
		    set newmu "$newmu + $newbeta"
		} else {
		    set newmu "$newmu + <$newbeta>"
		}
		if {$comb} {
		    set oldbeta [lindex $oldbetas $k]
		    set residualmu "$residualmu + \[parameter $oldbeta =\]"
		} else {
		    set residualmu "$residualmu + \[parameter $newbeta =\]"
		}
		foreach term $terms {
		    set undervex [split $term ^]
		    set exp 1
# check for exponent and split off actual term		    
		    if {[llength $undervex] > 1} {
			set term [lindex $undervex 0]
			set exp [lindex $undervex 1]
		    }
# check if this is sex
		    if {[string tolower $term] == "sex"} {
			set avalue 0
			set term female
			set has_sex 1
		    } else {
# if not sex, add this to terms we need from phen file
# and determine adjustment value
			set allterms [setappend allterms $term]
			if {$term == $maxsnp} {
			    set avalue 1
			} elseif {[getvar -d solar.out $term]} {
			    set avalue [getvar -min solar.out $term]
			} else {
			    set avalue [getvar -mean solar.out $term]
			}
		    }
# set up newmu and residualmu for each term (multiplying for exp's)
		    for {set j 0} {$j < $exp} {incr j} {
			set newmu "$newmu*($term - $avalue)"
			if {$term == "female"} {
			    set residualmu "$residualmu*(\$Female(\[lindex \$line 0\]))"
			} elseif {$term==$maxsnp} {
			    set residualmu "$residualmu*0"
			} else {
			    set residualmu "$residualmu*(\[lindex \$line \
\$Cpos($term)\] - $avalue)"
			}
		    }
		}
	    }
	    set newmu "$newmu\)"
	    set residualmu "$residualmu\)"
	    if {!$grand} {
		save model $savename.temp
	    }
	}
    }

    set newomega [lrange $newomega 1 end]
    set newmu [lrange $newmu 1 end]
    puts "eval> omega = $newomega"
    eval omega = $newomega
    puts "eval> mu = $newmu"
    eval mu = $newmu
    if {$maxsnp != ""} {
	puts "ResidualMu = $residualmu"
    }
#
# make final definition for each trait
#
    if {$intrait} {
	foreach newt $newts {
	    define $newt = $intraitdef($newt)
	}
    }
#
# Now if this is combined-model, start all over with new traits and covars
# and run polymod
#
    if {$comb} {
	model new
	eval trait $newts
	eval covar $covs

	if {$sporadic} {
	    spormod
	} else {
	    polymod
	}
    }

    if {!$grand} {
	catch {file delete [full_filename polclass.tmp.mod]}
    }

    option polyclasses [join $classlist ,]

    if {$maxi} {

	if {$maxsnp != "" && !$comb} {
	    foreach ic $classlist {
		parameter b$maxsnp\_c$ic = 0
		constraint b$maxsnp\_c$ic = 0
	    }
	}
	maximize -o polyclassmax.out
	save model [full_filename polyclassmax.mod]
	if {$maxsnp != ""} {
	    if {$comb} {
		set sd [parameter sd =]
	    } else {
		foreach ic $classlist {
		    set sd$ic [parameter sd_c$ic =]
		}
	    }
#
# compute residuals for maxsnp
#
	        set outresidname [full_filename polyclass.residuals.out]
		set outresid [open $outresidname w]

		if {$has_sex} {
		    set pedfile [tablefile open pedindex.out]
		    tablefile $pedfile start_setup
		    tablefile $pedfile setup id
		    tablefile $pedfile setup sex
		    while {{} != [set line [tablefile $pedfile get]]} {
			set Female([lindex $line 0]) [expr [lindex $line 1]-1]
		    }
		    tablefile $pedfile close
		}

	        covariate class()
		maximize -sampledata
	        set sfilename [full_filename sampledata.out]
		load model [full_filename polyclassmax.mod]

		set sfile [tablefile open $sfilename]
		tablefile $sfile start_setup
		tablefile $sfile setup id
		tablefile $sfile setup trait1
	        puts "class being setup"
		tablefile $sfile setup class
		set residheader id,residual,[trait],class
		set cpos 2
		foreach sterm $allterms {
		    set Cpos($sterm) [incr cpos]
#		    puts "set up position $cpos for $sterm"
		    tablefile $sfile setup $sterm
#		    puts "$sterm set up"
		    set residheader $residheader,$sterm
		}
		if {$has_sex} {
		    set residheader $residheader,female
		}
		puts $outresid $residheader

		while {{} != [set line [tablefile $sfile get]]} {
		    set residual [eval expr $residualmu]
		    set outline [lindex $line 0],$residual,[lindex $line 1],[lindex $line 2]
		    set ucpos 2
		    foreach sterm $allterms {
			set outline $outline,[lindex $line [incr ucpos]]
		    }
		    if {$has_sex} {
			set outline $outline,$Female([lindex $line 0])
		    }
		    puts $outresid $outline
		}
		close $outresid
		tablefile $sfile close

	}
	if {$resmax} {
	    set routfile [full_filename polyclassres.out]
	    if {!$comb} {
		residual polyclassmax.out -out $routfile -class
	    } else {
		residual polyclassmax.out -out $routfile
	    }
	    load phenotypes $routfile
	    model new
	    trait residual
	    set newargs [remove_from_list $args -resmax]
	    puts "executing polyclass $newargs -maxi on residuals"
	    return [eval polyclass $newargs -maxi]
	}
	puts "raw likelihood is [loglike]"
	set all_like [loglike]
	puts ""
	set retline ""
	set first 1
	if {[llength $newts] == 1} {
	    if {$comb} {
		set h2rf [fformat %-12.6y [parameter h2r =]]
		set sef [fformat %-12.6y [parameter h2r se]]
		set retline "h2r = $h2rf se $sef"
		if {$maxsnp != ""} {
		    set snpf [fformat %-12.6y [parameter b$maxsnp =]]
		    set snpse [fformat %-12.6y [parameter b$maxsnp se]]
		    set retline "$retline b$maxsnp = $snpf se $snpse"
		    global SOLAR_constraint_tol
		    parameter b$maxsnp = 0 lower -$SOLAR_constraint_tol upper $SOLAR_constraint_tol
		    constraint b$maxsnp = 0
		    maximize
		    set clike [loglike]
		    set chisq [expr 2 * ($all_like - $clike)]
		    set fchisq [fformat %-12.6y $chisq]
		    set retline "$retline chi $fchisq"
		    if {$chisq < 0} {set chisq 0}
		    set pval [chi -number $chisq 1]
		    set fpval [fformat %-12.6y $pval]
		    set retline "$retline   p $fpval"
		    set csdic [parameter sd =]
		    set varexp [highest 0 [expr 1 - pow($sd/$csdic,2)]]
		    set fvarexp [fformat %-12.6y $varexp]
		    set retline "$retline varexp $fvarexp"
		    set nsh2rf [fformat %-12.6y [parameter h2r =]]
		    set nsse [fformat %-12.6y [parameter h2r se]]
		    set retline "$retline (nosnp)h2r = $nsh2rf"
		    set retline "$retline se $nsse"
		} else {
		    global SOLAR_constraint_tol
		    parameter h2r = 0 lower -$SOLAR_constraint_tol
		    constraint h2r = 0
		    parameter e2 = 1 upper 1.01
		    maximize
		    set clike [loglike]
		    set chisq [expr 2 * ($all_like - $clike)]
		    if {$chisq < 0} {set chisq 0}
		    set pval [chi -number $chisq 1]
		    set apval [expr $pval / 2.0]
		    set fpval [fformat %-12.6y $apval]
		    set retline "$retline p $fpval"
		}
		load model [full_filename polyclassmax]

	    } else { ;# not -comb
		set firstclass 1
		foreach ic $classlist {
		    if {$maxsnp == ""} {
			set se [fformat %-12.6y [parameter h2r\_c$ic se]]
			set h2rf [fformat %-12.6y [parameter h2r\_c$ic =]]
			if {$first} {
			    set retline "h2r_c$ic = $h2rf se $se"
			    set first 0
			} else {
			    set retline "$retline\nh2r_c$ic = $h2rf se $se"
			}
		    } else {
#
# To test non-comb snps:
#   Use the residual created earlier as trait
#   blank unused classes 
#   run mga
#   accumulate mga results
#   restore original phenotypes and model

			set originalmodelname [full_filename polyclassmax]
			set originalphens [phenotype -files]

			load phen $outresidname
			model new
			trait residual
			blank -o class!=$ic
			if {[catch {mga -q}]} {
			    puts "mga for class $ic failed"
			} else {
			    puts " "
			    global SOLAR_mga_last_out
			    global SOLAR_mga_header
			    if {$firstclass} {
				set firstclass 0
				if {$snpappend} {
				    set retline ""
				} else {
				    puts "doing header"
				    set retline "class,$SOLAR_mga_header\n"
				}
			    }
			    puts "adding $ic to results..."
			    set retline "$retline$ic,$SOLAR_mga_last_out\n"
			}
			eval load phen $originalphens
			model new
			load model $originalmodelname
		    }
		}
	    }
	} else {
	    foreach newt $newts {
		if {$comb} {
		    set se [fformat %-12.6y [parameter h2r($newt) se]]
		    set h2rf [fformat %-12.6y [parameter h2r($newt) =]]
		    if {$first} {
			set retline "h2r($newt) = $h2rf se $se"
			set first 0
		    } else {
			set retline "$retline\nh2r($newt) = $h2rf se $se"
		    }
		} else {
		  foreach ic $classlist {
		    set se [fformat %-12.6y [parameter h2r_c$ic\($newt\) se]]
	            set h2rf [fformat %-12.6y [parameter h2r_c$ic\($newt\) =]]
		    if {$first} {
			set retline "h2r_c$ic\($newt\) = $h2rf se $se"
			set first 0
		    } else {
		       set retline "$retline\nh2r_c$ic\($newt\) = $h2rf se $se"
		    }
		  }
		}
            }
        }
	if {$snpappend} {
	    set outfile [open [full_filename polyclass.out] a+]
	} else {
	    set outfile [open [full_filename polyclass.out] w]
	}
	if {!$comb && $maxsnp != ""} {
	    puts $outfile [string range $retline 0 end-1]
	} else {
	    puts $outfile $retline
	}
	close $outfile

	if {$maxsnp != ""} {
	    if {$comb} {
		set combfix .comb
	    } else {
		set combfix ""
	    }
	    set snpname [string range $maxsnp 4 end]
	    set snpfilename [full_filename polyclass$combfix.$snpname.out]
	    if {$snpappend} {
		set snpout [open $snpfilename a+]
	    } else {
		set snpout [open $snpfilename w]
	    }
	    if {$comb} {
		puts $snpout $retline
	    } else {
		puts $snpout [string range $retline 0 end-1]
	    }
	    close $snpout
	}
	return $retline
    }
    return ""
}



# solar::polygsd --
#
# Purpose:  Set up polygenic model esd and gsd parameters (EXPERIMENTAL)
#
# Usage:    polygsd
#
# Note:     "model new" and "trait" commands should be given first.
#           After polygsd, you should use "maximize" command.
#
#           Use the gsd2h2r command to convert resulting esd,gsd parameters
#           to h2r value.
#
#           Use the linkqsd command to add in linkage element afterwards.
#
# Example:  model new
#           trait q4
#           covar age sex
#           polygsd
#           maximize
#           linkqsd gaw10mibd/mibd.9.18.gz  ;# could maximize after this
#           chromosome 9 10
#           interval 5
#           mibddir gaw10mibd
#           multipoint -link linkqsd0 -cparm {esd gsd qsd}
#
#-

proc polygsd {} {
    set ts [trait]
    set ntraits [llength $ts]
    if {$ntraits == 1} {
	omega = I*esd*esd + phi2*gsd*gsd
    } elseif {$ntraits == 2} {
	parameter -insert rhog = 0 lower -1 upper 1
	parameter -insert rhoe = 0 lower -1 upper 1
	omega = I*<esd(ti)>*<esd(tj)>*(tne*rhoe+teq) + \
	    phi2*<gsd(ti)>*<gsd(tj)>*(tne*rhog+teq)
    } else {
	foreach prefix {rhog_ rhoe_} {
	    for {set i [expr $ntraits - 1]} {$i > 0} {incr i -1} {
		for {set j $ntraits} {$j > $i} {incr j -1} {
		    parameter -insert [catenate $prefix $i $j] = 0 \
			lower -1 upper 1
		}
	    }
	}
	omega = I*<esd(ti)>*<esd(tj)>*(tne*rhoe_ij+teq) + \
	    phi2*<gsd(ti)>*<gsd(tj)>*(tne*rhog_ij+teq)
    }

# Multivariate needs phi2.gz

    if {$ntraits > 1 && (-1==[lsearch -exact [matrix] phi2])} {
	load matrix phi2.gz phi2
    }

    set savename [full_filename solar.polygsd.temp]
    for {set i 0} {$i < $ntraits} {incr i} {
	set tr [lindex $ts [expr $ntraits - ($i + 1)]]
#
# Don't use raw stats anymore because defined traits and restricted sample
#	set stats [stats $tr -q -return]

	save model $savename
	set covs [covariate]
	model new
	trait $tr
	eval covariate $covs
	polymod
	maximize -sampledata -q
	if {[catch {set stats [stats trait1 -q -return -file \
				   [full_filename sampledata.out]]}]} {
	    puts "Warning: Unable to initialize parameters for trait $tr"
	    set mean 0
	    set sd 0
	    set mean_upper 0
	    set mean_lower 0
	    
	    set esd_start 0
	    set esd_upper 0
	    set gsd_start 0
	    set gsd_upper 0
	} else {
	    set mean [stats_get $stats mean]
	    set sd [stats_get $stats sd]
	    if {$mean >= 0} {
		set mean_lower [expr (0 - $mean) - 0.5]
		set mean_upper [expr $mean * 3 + 0.5]
	    } else {
		set mean_lower [expr ($mean * 3) - 0.5]
		set mean_upper [expr (0 - $mean) + 0.5]
	    }
	    set esd_upper [expr $sd * 2 + 0.1]
	    set gsd_upper [expr $sd * 2 + 0.1]

	    global Solar_gsd_fraction
	    if {[if_global_exists Solar_gsd_fraction]} {
		set gsd_fraction $Solar_gsd_fraction
	    } else {
		set gsd_fraction 0.2
	    }

	    set gsd_start [expr $sd * $gsd_fraction]
#	    set gsd_start 1
	    set esd_start [expr $sd * (sqrt (1 - $gsd_fraction*$gsd_fraction))]
	}
	load model $savename
	catch {file delete $savename}
	set suffix ""
	if {$ntraits > 1} {
	    set suffix ($tr)
	}
	parameter -insert gsd$suffix = $gsd_start lower 0 upper $gsd_upper
	parameter -insert esd$suffix = $esd_start lower 0 upper $esd_upper
	parameter -insert mean$suffix = $mean lower $mean_lower \
	    upper $mean_upper
    }
    return ""
}

proc polyqsd {mibdfile args} {
    eval polygsd $mibdfile $args
}


# solar::gsd2h2r --
# solar::gsd2sd  --
# solar::gsd2h2q --
#
# Purpose:  Convert esd,gsd,[qsd1] parameters to standard parameters
#
# Usage:                         ;# trait only required for multivariate model
#           gsd2h2r [<trait>]    ;# compute h2r from esd,gsd,[qsd1]
#           gsd2sd  [<trait>]    ;# compute SD from esd,gsd,[qsd1]
#           gsd2h2q [<trait>]    ;# compute h2q1 from esd,gsd,[qsd1]
#
#
# Note:     Use polygsd command to set up model, and maximize to maximize it
#           first, followed by linkgsd for linkage models.
#
#           See the documentation for the polygsd, linkgsd.
# -

proc gsdsuffix {args} {
    set suffix ""
    if {1 < [llength [trait]]} {
	if {"" == $args} {
	    error "gsd2: Must specify trait for multivariate"
	}
	if {-1 == [lsearch [trait] $args]} {
	    error "gsd2: Invalid trait specified"
	}
	set suffix "($args)"
    }
    return $suffix
}

proc gsd2h2r {args} {
    set suffix [gsdsuffix $args]
    set esd [parameter esd$suffix =]
    set gsd [parameter gsd$suffix =]
    set sum [expr $esd*$esd + $gsd*$gsd]
    if {[if_parameter_exists qsd1$suffix]} {
	set qsd [parameter qsd1$suffix =]
	set sum [expr $sum + $qsd*$qsd]
    }
    return [expr ($gsd*$gsd)/$sum]
}

proc gsd2sd {args} {
    set suffix [gsdsuffix $args]
    set esd [parameter esd$suffix =]
    set gsd [parameter gsd$suffix =]
    set sum [expr $esd*$esd + $gsd*$gsd]
    if {[if_parameter_exists qsd1$suffix]} {
	set qsd [parameter qsd1$suffix =]
	set sum [expr $sum + $qsd*$qsd]
    }
    return [expr sqrt ($sum)]
}

proc gsd2h2q {args} {
    set suffix [gsdsuffix $args]
    if {![if_parameter_exists qsd1$suffix]} {
	error "gsd2h2q: No qsd1 pmarameter"
    }
    set esd [parameter esd$suffix =]
    set gsd [parameter gsd$suffix =]
    set sum [expr $esd*$esd + $gsd*$gsd]
    set qsd [parameter qsd1$suffix =]
    set sum [expr $sum + $qsd*$qsd]
    return [expr ($qsd*$qsd)/$sum]
}

proc qsd2h2q {args} {
    return [gsd2h2q $args]
}

proc qsd2h2r {args} {
    return [gsd2h2r $args]
}

proc qsd2sd {args} {
    return [gsd2sd $args]
}


# solar::blank --
#
# Purpose:  Blank individuals according to variable data conditions
#
# Usage:    blank [-o] [-q] [-n] [<conditional expression>]
#
# <conditional expression> can be any solar variable expression (as allowed
# by the define command for covariates) that adds up to zero or non-zero.
# If it adds to non-zero for a person, that person is removed from the sample.
#
#           [-q]  Go about blanking quietly.
#
#           [-o]  Force overwrite of existing definition having same name
#                 (see below for example of definition naming).
#
#           [-n]  Make new definition name if this would otherwise
#                 conflict with existing definition name
#
#           With no arguments, blank shows blanking definitions currently in
#           effect.  To see all the definitions available, use define command.
#
# Examples:
#
#           blank class!=1           ;# include only class=1 in sample
#
#           blank age<<55 + sex==1   ;# blank all but old guys
#
#           blank age>=55 * sex==2   ;# blank only old guys
#
# Notes:
# 
# 1.  blank creates a definition and a null covariate to achieve the
#     desired blanking.  It shows you what it does, and then suggests
#     how this blanking may be deleted:
#
#     solar> blank age<<55 + sex==1
#
#     define blank_age = blank * (0!= (age<<55 + sex==1)
#     covariate blank_age()
#     To delete: covariate delete blank_age()
#
#     solar>
#
# 2.  blanking is cumulative through the effect of all blanking covariates
#     that remain in effect.  If you choose a condition which would create
#     the same name as used by a previous condition  (see example above) it
#     will raise an error.  You can force overwrite with -o.
#
# 3.  To restrict sample based on available of some variable, use a regular
#     null covariate for that variable, as documented for the covariate
#     command, for example:
#
#     covariate age()
#
#     null covariates (having following empty parentheses) are not included
#     in likelihood estimation, but are used to delimit the available sample,
#     just as blanking covariates are.
#
# 4.  You may also create covariate definitions just like blank does.  But
#     be careful because it is easy to do it wrong.
# -

proc blank {args} {

    set quiet 0
    set overwrite 0
    set new 0

    if {$args == ""} {
	set covs [covariates]
	set blanks ""
	foreach cov $covs {
	    if {[string range $cov 0 5] == "blank_"} {
		catch {set blanks "$blanks\n[define [string range $cov 0 end-2]]"}
	    }
	}
	return [string range $blanks 1 end]
    }

    set trying 1
    while {$trying} {
	set first [lindex $args 0]
	if {$first == "-q"} {
	    set quiet 1
	    set args [lrange $args 1 end]

	} elseif {$first == "-o"} {
	    set overwrite 1
	    set args [lrange $args 1 end]

	} elseif {$first == "-n"} {
	    set new 1
	    set args [lrange $args 1 end]
	    
	} else {
	    set trying 0
	}
    }

    set elength [string length $args]
    set estart -1
    set namestring ""

    for {set i 0} {$i < $elength} {incr i} {
	set ch [string index $args $i]
#	puts "testing $ch"
	if {[is_alpha $ch]} {
	    set estart $i
	    break
	}
    }
    if {$estart>=0} {
	for {set i [expr $estart+1]} {$i < $elength} {incr i} {
	    set ch [string index $args $i]
#	    puts "testing $ch"
	    if {![is_alnum $ch]} {
		set namestring [string range $args $estart [expr $i - 1]]
		break
	    }
	}
    }
    if {$namestring == ""} {
	error "conditions contain no variable name"
    }
    set extend ""
#   puts "checking this string $args"
    if {-1 != [string first << $args]} {
	set extend _lt
    } elseif {-1 != [string first >> $args]} {
	set extend _gt
    } elseif {-1 != [string first == $args]} {
	set extend _eq
    } elseif {-1 != [string first >= $args]} {
	set extend _ge
    } elseif {-1 != [string first <= $args]} {
	set extend _le
    }
    set namestring blank_$namestring$extend

    if {!$overwrite && -1!=[lsearch [define names] $namestring]} {
	if {$new} {
	    set counter 2
	    set newnamestring $namestring\_$counter
	    while {-1 != [lsearch [define names] $newnamestring]} {
		incr counter
		set newnamestring $namestring\_$counter
	    }
	    set namestring $newnamestring
	} else {
	    error "existing $namestring: use -o to overwrite or -n for newname"
	}
    }

# do the work

    set definition "define $namestring = blank * (0 != \($args\))"
    set covariate "covariate $namestring\(\)"
    eval $definition
    eval $covariate

# output description of work

    if {!$quiet} {
	puts ""
	puts "$definition"
	puts "$covariate"
	puts "To delete: covariate delete $covariate"
	puts ""
    }

    return ""
}

# solar::polymod --
#
# Purpose:  Set up polygenic model with the standard parameters
#
# Usage:    polymod [-d]
#
# IMPORTANT::  Phenotypes, trait, and covariate commands must be
#              given beforehand.
#
#           -d  Check for discrete trait(s) and make necessary changes.
#               In most cases, this option is not necessary because
#               "maximize" later checks for discrete traits and can also
#               make these changes: constraining SD to 1 and making
#               sure phi2 matrix is loaded, for each discrete trait.
#               However, use of -d option can make the constraint or matrix
#               order inside complex models easier to deal with.
#
# Notes:    The starting lower bound for e2 is controlled by e2lower.
#
#           Normally you do not use this command directly, but instead use 
#           the "polygenic" command to do a complete polygenic analysis,
#           which maximizes a polygenic model which was set up using this
#           command.  See the tutorial in Chapter 3.
#
#           polymod will modify an existing sporadic or linkage model
#           to change it to polygenic.  Use spormod to set up a
#           sporadic model, and linkmod to set up a linkage model.
#           None of these commands maximize models, they just set up
#           or modify the parameters and omega as required.
#           
#           This command removes a house parameter (if present) from
#           the omega, since a "polygenic" model is distinct from a
#           "household polygenic" model.  If you want the latter, call
#           polymod first, then house.  Or call house, THEN polygenic,
#           since the polygenic command will check for and handle household
#           effect properly.
# -

proc polymod {args} {

    set check_discrete 0
    set norhoij 0

# Suspend household effects
    house -suspend    ;# Suspend household effects, if they had been enabled

    if {[llength $args]} {
	if {$args == "-d"} {
	    set check_discrete 1
	} elseif {$args == "-norhoij"} {
	    set norhoij 1
	} else {
	    error "invalid polymod argument $args"
	}
    }


    set multi 0        ;# bivariate and above
    set trivariate 0

    set ts [trait]
    set ntraits [llength $ts]
    if {$ntraits > 1} {
	set multi 1
	if {$ntraits > 2} {
	    set trivariate 1
	}
    }

# Since each parameter is inserted at beginning, we add them in the reverse
# from standard order.  Note that if parameter already exists, its position
# is unchanged.

# Rho's (for multivariate models)

    foreach ty {g e} {
	if {$ntraits == 2 || ($ntraits > 2 && $norhoij)} {
	    set rholow -0.9
	    set rholowers lower
	    set rhoup 0.9
	    set rhouppers upper
	    if {[if_parameter_exists rho$ty]} {
		if {"" != [parameter rho$ty fixlower]} {
		    set rholow [parameter rho$ty fixlower]
		    set rholowers fixlower
		}
		if {"" != [parameter rho$ty fixupper]} {
		    set rhoup [parameter rho$ty fixupper]
		    set rhouppers fixupper
		}
	    }
	    parameter -insert rho$ty = 0 $rholowers $rholow $rhouppers $rhoup

	} elseif {$ntraits > 2} {

	    set trivariate 1
	    set prefix rho$ty\_

	    for {set i [expr $ntraits - 1]} {$i > 0} {incr i -1} {
		for {set j $ntraits} {$j > $i} {incr j -1} {
		    set pname [catenate $prefix $i $j]
		    set rholow -0.9
		    set rholowers lower
		    set rhoup 0.9
		    set rhouppers upper
		    if {[if_parameter_exists $pname]} {
			if {"" != [parameter $pname fixlower]} {
			    set rholow [parameter $pname fixlower]
			    set rholowers fixlower
			}
			if {"" != [parameter $pname fixupper]} {
			    set rhoup [parameter $pname fixupper]
			    set rhouppers fixupper
			}
		    }
		    parameter -insert $pname = 0 $rholowers $rholow \
			$rhouppers $rhoup
		    set qindex 1
		    while {[if_parameter_exists rhoq$qindex\_$i$j]} {
			parameter delete rhoq$qindex\_$i$j
			incr qindex
		    }
		}
	    }
	}
    }

    for {set it [expr $ntraits - 1]} {$it > -1} {incr it -1} {

	set tr [lindex $ts $it]

	set suf ""
	if {$ntraits > 1} {
	    set suf \($tr\)
	}

# e2 and h2r

	set h2rlow 0
	set h2rlowers lower

	set h2rup 1
	set h2ruppers upper

	set e2low [e2lower]
	set e2lowers lower

	set e2up 1
	set e2uppers upper

	if {[if_parameter_exists h2r$suf]} {
	    if {"" != [parameter h2r$suf fixlower]} {
		set h2rlow [parameter h2r$suf fixlower]
		set h2rlowers fixlower
	    }
	    if {"" != [parameter h2r$suf fixupper]} {
		set h2rup [parameter h2r$suf fixupper]
		set h2ruppers fixupper
	    }
	}
	if {[if_parameter_exists e2$suf]} {
	    if {"" != [parameter e2$suf fixlower]} {
		set e2low [parameter e2$suf fixlower]
		set e2lowers fixlower
	    }
	    if {"" != [parameter e2$suf fixupper]} {
		set e2up [parameter e2$suf fixupper]
		set e2uppers fixupper
	    }
	}

	parameter -insert h2r$suf $h2rlowers $h2rlow $h2ruppers $h2rup
	parameter -insert e2$suf $e2lowers $e2low $e2uppers $e2up
	catch {constraint delete h2r$suf}

# Mean and SD (initialized now in c++)

	parameter -insert sd$suf
	parameter -insert mean$suf

# Remove all h2q's and their constraints and matrices
	set starting_h2qcount [h2qcount]
	if {$starting_h2qcount > 0} {
	    for {set i 1} {$i <= $starting_h2qcount} {incr i} {
		catch {matrix delete mibd$i}
		catch {constraint_remove h2q$i$suf}
		catch {parameter delete h2q$i$suf}
	    }
	}

# set starting values for e2 and h2r
# Prior to 7.1.0, this was always forced to 0.9/0.1 to start
#   Now, under certain circumstances, existing values are left alone.
#   These conditions must be satisfied:
#     a) e2 and h2r add up to 1.0
#     b) e2 is not 1.0
#   Otherwise, if 0.9/0.1 fit into boundaries, they are used
#     Otherwise error

	set sum [expr [parameter e2$suf =] + [parameter h2r$suf =]]
	set eps 1e-6
	if {([parameter e2$suf =] == 1) || \
		($sum + $eps) < 1.0 || ($sum - $eps) > 1.0} {

	    if {[parameter e2$suf upper] >= 0.9} {
		parameter e2$suf = 0.9
	    } else {
		error "h2r$suf and e2$suf do not sum to 1.0"
	    }
	    if {[parameter h2r$suf lower] <= 0.1} {
		parameter h2r$suf = 0.1
	    } else {
		error "h2r$suf and e2$suf do not sum to 1.0"
	    }
	}

    } ;# end setting parameters for each trait

# Bivariate needs phi2.gz

    if {$multi && (-1==[lsearch -exact [matrix] phi2])} {
	load matrix phi2.gz phi2
    }


# Update omega...
#   Bi/Multivariate version does not support special omega terms

    if {$trivariate && !$norhoij} {

# Multivariate (> 2)

	omega = <sd(ti)>*<sd(tj)>*( \
I*sqrt(<e2(ti)>)*sqrt(<e2(tj)>)*(tne*rhoe_ij+teq) + \
phi2*sqrt(<h2r(ti)>)*sqrt(<h2r(tj)>)*(tne*rhog_ij+teq) )

    } elseif {$multi} {

# Bivariate

	omega = <sd(ti)>*<sd(tj)>*( \
I*sqrt(<e2(ti)>)*sqrt(<e2(tj)>)*(tne*rhoe+teq) + \
phi2*sqrt(<h2r(ti)>)*sqrt(<h2r(tj)>)*(tne*rhog+teq) )

    } else {
	if {"omega = Use_polygenic_to_set_standard_model_parameterization" \
		== [omega]} {
	    omega = pvar*(phi2*h2r + I*e2)
	} else {
	    omega_remove_linkage  ;# Must be done after using h2qcount
	}
    }

# Set up constraints

    if {!$multi} {
	constraint e2 + h2r = 1
    } else {
	if {[if_parameter_exists rhog]} {
	    	catch {constraint delete rhog}
	}
	foreach tr $ts {
	    set suffix \($tr\)
	    constraint <e2$suffix> + <h2r$suffix> = 1
	}
	set qindex 1
	while {[if_parameter_exists rhoq$qindex]} {
	    parameter delete rhoq$qindex
	    incr qindex
	}
    }

# Covariance matrices aren't diagonal now
    option cmdiagonal 0

# Check for discrete if requested

    if {$check_discrete} {
	setup_discrete
    }

    return ""
}

#
# Discrete setup...
# check if trait is discrete, and, if so, constrain SD to 1 and ensure
# phi2 is loaded
#
proc setup_discrete {} {
    set traits [trait]
    set ntraits [llength $traits]
    set found 0
    foreach tr $traits {
	set rstats [stats $tr -q -return]
	set discrete [stats_get $rstats discrete]
	if {$discrete} {
	    set found 1
	    if {$ntraits == 1} {
		constraint sd = 1
		parameter sd = 1 lower 0 upper 2
	    } else {
		constraint sd($tr) = 1
		parameter sd($tr) = 1 lower 0 upper 2
	    }
	}
    }
    if {$found && 0 > [lsearch [string tolower [matrix]] phi2]} {
	matrix load phi2.gz phi2
    }
}

    

	    
	



#
# Constraint editing procedures
#   SOLAR procedures should use these procedures to "edit" constraints as
#   needed rather than just deleting and replacing all prior constraints
#
# constraint_include ensures vcomp is included in a constraint which
#   sums to a nonzero value (this would be the e2 + ... = 1 constraint)
#   the base term defaults to "e2" but also may be caller specified to
#   allow multivariate terms such as e2(trait)

proc constraint_include {vcomp {basevc e2}} {

# If component names contain (), must wrap with <>

    if {-1 != [string first \( $vcomp]} {
	set vcomp <$vcomp>
	set basevc <$basevc>
    }

# convert to lower case for comparisons

    set lvcomp [string tolower $vcomp]
    set lbasevc [string tolower $basevc]

#   puts "names are $lvcomp and $lbasevc"

    set cons [constraint command]

# If currently constrained to zero, delete those constraints

    while {0 != [set zero_constraint [is_constrained_to_zero $lvcomp]]} {
#	puts "deleting constraint"
	constraint delete $zero_constraint
    }

# See if currently constrained to non-zero value
#   If so, we're finished

    if {0 != [is_constrained_to_nonzero $lvcomp]} {
	return ""
    }

# Find constraint(s) with basevc, add vcomp to end

    set cons [constraint command]
    set cnum 0
    set done 0
    foreach con $cons {
	set con [string tolower $con]
	incr cnum
	if {-1 != [string first $lbasevc $con]} {
	    regsub = $con "+ $vcomp =" new_const
#	    puts "deleting constraint $cnum"
	    constraint delete $cnum
#	    puts "making constraint $new_const"
	    eval $new_const
	    set done 1
	    break
#	} else {
#	    puts "constraint did not match: $con"
	}
    }

# No constraint with e2, so make one

    if {!$done} {
	if {0==[string compare $basevc $vcomp]} {
	    constraint $basevc = 1
	} else {
#	    puts "making new constraint $basevc + $vcomp = 1"
	    constraint $basevc + $vcomp = 1
	}
    }
    return ""
}

# solar::fix
#
# Purpose:  Constrain a parameter to its current value
#
# Usage:    fix <name>    ; Name is the name of the parameter you want to fix
#
# Example:  fix h2r

proc fix {pname} {
constraint $pname = [parameter $pname =]
}


# This is unnecessary and depricated now
# The constraint command itself now does this, and more...
proc constrain_to_zero {param} {
    if {![is_constrained_to_zero $param]} {
	constraint $param = 0
    }
    return ""
}

# find_simple_constraint returns the value a parameter is constrained to
# in a "simple" constraint (containing no other terms, e.g. h2r = 1)
# If there is no such constraint, an error is raised, so this is normally
# included in a "catch" statement.

proc find_simple_constraint {param} {
    set cons [constraint command]
    foreach con $cons {
	set con [remove_whitespace $con]
	set con [string range $con [string length constraint] end]
	set eindex [string first = $con]
	set lexp [string range $con 0 [expr $eindex - 1]]
# Remove <> if found in constraint specification but not parameter spec
	if {[string_imatch [string index $lexp 0] < ] && \
		[string_imatch [string range $lexp end end] > ] && \
		![string_imatch [string index $param 0] < ]} {
	    set lexp [string range $lexp 1 [expr [string length $lexp] - 2]]
	}
	if {[string_imatch $param $lexp]} {
	    return [string range $con [expr $eindex + 1] end]
	}
    }
    error "Parameter $param has no simple constraint"
}

#
# is_constrained procs return 0 if not constrained to zero/nonzero, or
#   constraint number (1..N) of the first constraint to zero/nonzero
#     Note: both could be true (either constraint could have other terms)
#
# WARNING!  These scripts, based on fairly simple text comparison, have
# some pretty terrible bugs.  For example, if <bage*sex> is constrained,
# it will appear that bage is constrained also.  It is recommended that
# these be phased out; current usage with standard variance components is
# probably OK.
#

proc is_constrained_to_zero {param} {
    return [is_constrained $param 0]
}

proc is_constrained_to_nonzero {param} {
    return [is_constrained $param 1]
}

proc is_constrained {param nonzero} {
# Case insensitivity
    set param [string tolower $param]
    set cons [string tolower [constraint command]]

    set found_constraint 0
    set cnum 0
    foreach con $cons {
	incr cnum
	if {-1<[string first $param $con]} {
	    set eindex [string first = $con]
	    if {$eindex>-1} {
		set rexp [string range $con [expr $eindex + 1] end]
		if {1==[scan $rexp %f rval]} {
		    if {$nonzero} {
			if {$rval != 0.0} {
			    set found_constraint $cnum
			    break
			}
		    } else {
			if {$rval == 0.0} {
			    set found_constraint $cnum
			    break
			}
		    }
		}
	    }
	}
    }
    return $found_constraint
}

# Remove constraint references to a particular parameter
#   Constraints with JUST that parameter are deleted
#   Constraints with multiple parameters are edited

proc constraint_remove {vcomp} {

#   puts "ENTERING constraint_remove"

    set hit 1
# Each time we delete a constraint, we must restart since they are reordered
    while {$hit} {
	set hit 0
	set cons [constraint command]
	set cnum 0
	foreach con $cons {
	    incr cnum
# Remove leading word "constraint"
	    set con [lrange $con 1 end]
# See if our vcomp is used
	    if {-1<[string first $vcomp $con]} {
		set iplus [string first + $con]
		if {$iplus==-1} {
# This constraint has only one term--our vcomp.  Delete it.
		    constraint delete $cnum
		    set hit 1
		    break
		} else {
# Our vcomp is being added to something else.  Excise it.

# Check if this vc contains parens, if so, adapt for pattern usage
# because parens are a metacharacter

		    catch {
		    if {-1 != [string first \( $vcomp]} {
			regsub "\\(" $vcomp "\\\\(" vcomp2
			regsub "\\)" $vcomp2 "\\\\)" vcomp3
#			puts "vcomp2 is $vcomp2"
#			puts "vcomp3 is $vcomp3"
			set vcomp $vcomp3
		    }
		    }

#		    set target "( )*\\+( )*$vcomp"
#		    puts "target is $target"
		    if {0<[regsub -nocase "( )*\\+( )*$vcomp" $con "" \
			    newcon]} {
			constraint delete $cnum
			eval constraint $newcon
			set hit 1
			break
# If our vcomp comes first, a slightly different regexp is required
		    } elseif {0<[regsub -nocase "( )*$vcomp\( )*\\+( )*" $con \
			    "" newcon]} {
			constraint delete $cnum
			eval constraint $newcon
			set hit 1
			break
		    } else {
			error "Can't cope with constraint $con"
		    }
		}
	    }
	}
    }
    return ""
}


#
# suspend2constrain changes suspended covariates to constrained betas
#   covariate suspension is an optimization, but users prefer to
#   see constrained betas.  Eventually, constraining will automatically
#   become suspension internally, explicit suspension will be phased out,
#   so this shouldn't be needed.
#
proc suspend2constrain {} {
    set covarlist [covariates -applicable]
    set betalist [covariates -betanames]
    for {set i 0} {$i < [llength $covarlist]} {incr i} {
	set tcovar [lindex $covarlist $i]
	set tbeta [lindex $betalist $i]
	if {-1 != [string first "Suspended\[" $tcovar]} {
	    set covarname [string range $tcovar 10 \
		    [expr [string length $tcovar] - 2]]
	    covariate restore $covarname
	    parameter $tbeta = 0
	    if {-1 != [string first * $tbeta] || \
		    -1 != [string first ^ $tbeta] || \
		    -1 != [string first - $tbeta] || \
		    -1 != [string first "(" $tbeta]} {
		constrain <$tbeta> = 0
	    } else {
		constrain $tbeta = 0
	    }
	    global SOLAR_constraint_tol
	    if {[parameter $tbeta lower] > 0} {
		parameter $tbeta lower -$SOLAR_constraint_tol
	    }
	    if {[parameter $tbeta upper] < 0} {
		parameter $tbeta upper $SOLAR_constraint_tol
	    }
	}
    }
}


#
# Omega editing functions
#   SOLAR procedures should use these procedures to "edit" the omega
#   equation rather than just setting it to some new expression.
#


proc omega_remove_linkage {} {
    return [omega_remove h2q H2q h2Q H2Q]
}

proc omega_add {vcterm} {
    set current_omega [omega]
    if {")" == [string range $current_omega end end]} {
	set last [expr [string length $current_omega] - 2]
	set new_omega [string range $current_omega 0 $last]
	set new_omega "$new_omega + $vcterm)"
    } else {
	set new_omega "$current_omega + $vcterm"
    }
    eval $new_omega
    return $new_omega
}

proc omega_remove {args} {
    set omega_list [split [omega] ""]
    set new_omega ""
    set operator " "
    set term ""
    set after_first_paren 0
    set after_first_term 0
    foreach char $omega_list {

# Copy everything until the first left paren
	if {!$after_first_paren} {
	    set new_omega "$new_omega$char"
	    if {"(" == $char} {
		set after_first_paren 1
	    }
	    continue
	}

# variance components are separated by + (no other +'s allowed)

	if {$char == "+"} {
	    set remove_term 0
	    foreach arg $args {
		if {-1 != [string first $arg $term]} {
		    set remove_term 1
		    break
		}
	    }
	    if {!$remove_term} {
		set new_omega "$new_omega$term"
		set after_first_term 1
	    }
	    if {$after_first_term} {
		set term "+"
	    } else {
		set term ""
	    }
	} else {
	    set term "$term$char"
	}
    }

# decide what to do with last term

    if {"" != $term} {
	set remove_term 0
	foreach arg $args {
	    if {-1 != [string first $arg $term]} {
		set remove_term 1
		break
	    }
	}
	if {!$remove_term} {
	    set new_omega "$new_omega$term"
	}
    }

# Add final parentheses

    set lparens [char_count $new_omega "("]
    set rparens [char_count $new_omega ")"]
    if {$lparens > $rparens} {
	if {" " == [string range $new_omega end end]} {
	    set last [expr [string length $new_omega] - 2]
	    set new_omega [string range $new_omega 0 $last]
	}
	while {$lparens > $rparens} {
	    set new_omega "$new_omega)"
	    incr rparens
	}
    }
    eval $new_omega
    return $new_omega
}

# solar::epistasis
#
# Purpose:  Use command:  multipoint -epistasis <N>
#
# Usage:    multipoint -epistasis <N>  (<N> is the mibd index of interest)
#
# The 'epistasis' command itself is reserved for future use.
# -

proc epistasis {} {
    return [helpscript epistasis]
}


#
# Purpose: Set up analysis of epistasis
#
# Usage:  epistasis          ;# Set up epistasis for first 2 currently loaded
#                            ;#   matrices
#         epistasis -delete  ;# Delete epistasis parameters
#
# Note:   The matrices may loaded using the 'linkmod' or 'multipoint' commands.
#-


proc epistasis-future {args} {
    error "This is not yet supported"
    if {2 > [h2qcount]} {
	error "Two ibd/mibd matrices must already have been loaded"
    }
    if {[if_parameter_exists]} {
    }
}


# solar::h2power --
#
# Purpose:  Perform heritability power calculations
#
# Usage:    h2power [-prev] [-grid {<from> <to> <incr>}] [-data <fieldname>]
#                   [-nreps <nreps>] [-seed <seed>] [-overwrite] [-plot]
#                   [-nosmooth]
#
#           h2power -restart [-grid {<from> <to> <incr>}] [-nreps <nreps>]
#                   [-plot] [-nosmooth]
#
#
#           This command performs a power calculation for the currently
#           loaded pedigree, with the following default assumptions:
#
#               (1) the trait to be studied is either quantitative or
#                   dichotomous (e.g. affected/unaffected)
#
#               (2) the trait to be studied is influenced by additive
#                   genetics
#
#               (3) all pedigree members will be phenotyped for the trait
#                   to be studied (unless the -data option is used to
#                   exclude those individuals who will not have phenotypic
#                   data; see the description of this option below)
#
#           Simulation is used to estimate the frequency with which one
#           would expect to obtain a significantly non-zero estimate of
#           heritability given that a specified fraction of the phenotypic
#           variance is due to additive genetics.  Twice the difference in
#           the loglikelihoods of the polygenic and sporadic models is
#           asymptotically distributed as a 1/2:1/2 mixture of a chi-square
#           random variate with one degree of freedom and a point mass at 0.
#           A result is considered significant if the probability of
#           obtaining the observed chi-square value, in the absence of a
#           genetic effect, is less than or equal to .05.
#
#           The default is to perform 10 replicates of the simulation for
#           each heritability in the range .01, .02, .03, ..., .99.  For
#           each replicate, a polygenic model is fitted to the simulated
#           data, and the resulting heritability estimate and chi-square
#           statistic are recorded.  The observed chi-squares are converted
#           to power, i.e. the power to detect the corresponding observed
#           heritability at a significance level of .05.
#
#           The following options give the user some control over the power
#           calculation procedure:
#
#               -prev     If the trait to be studied is dichotomous, SOLAR
#                         will assume the existence of an unobserved liability
#                         distribution. Individuals with liabilities above
#                         some threshold value will be "affected", i.e. they
#                         will have the larger of the two trait values (for
#                         example, a 1 for a 0/1 trait.) The -prev option
#                         is used to specify the "disease" prevalence, or
#                         fraction of individuals who are "affected", which
#                         in turn determines the liability threshold.
#
#               -grid     Specify the set of heritabilities for which power
#                         will be computed. At each grid point, trait data
#                         having that expected heritability are simulated,
#                         sporadic and polygenic models are fitted to the
#                         data, and the loglikelihoods of the models are
#                         compared. The observed chi-square test statistics
#                         are averaged to obtain the expected chi-square
#                         value for that heritability. The grid is given by
#                         a set of three numbers enclosed in curly braces:
#
#                             {<from> <to> <incr>}
#
#                         where <from> is the starting heritability, <to>
#                         is the last heritability considered, and <incr>
#                         is the interval between grid points.  If the
#                         desired grid consists of a single effect size,
#                         the three-number list can be replaced by that
#                         single number and curly braces are not required.
#
#               -data     Exclude individuals from the power calculation
#                         who are missing data for phenotype <fieldname>.
#
#               -nreps    Perform <nreps> simulations at each grid point.
#                         The default number of replicates is 100.
#
#               -seed     Set the random number generator seed.  The default
#                         is to set the seed based on the date and time.
#
#               -plot     At the end of the power calculations, display a
#                         plot of power versus QTL heritability.  To display
#                         this plot for a previously completed calculation,
#                         use the command "plot -h2power".
#
#               -nosmooth By default, the power curve is smoothed by fitting
#                         a line through the observed chi-square values as
#                         a function of the heritability squared prior to
#                         converting the chi-square values to power.  This
#                         option turns the smoothing off.
#
#               -overwrite (or -ov)  Overwrite the results of a previous
#                                    power calculation.
#
#               -restart (or -r)     Restart a power calculation.
#
#
# Notes:    It is possible to change the grid of heritabilities and the number
#           of replicates when restarting a calculation.  The calculation
#           will not be restarted if a grid is chosen that does not include
#           all the points in the previously specified grid unless the
#           -overwrite option is included, in which case the simulation
#           replicates for any extra grid points are discarded.  Similarly,
#           the -overwrite option is necessary if fewer replicates are
#           requested than were done previously, in which case any extra
#           replicates are discarded.
#
#           The following files are created:
#
#               h2power.out   A space-delimited file containing a line for
#                             each grid point in the format X Y, which is
#                             suitable for input to plotting packages such
#                             as xmgr.  The first (or X) column contains the
#                             heritability.  The second (or Y) column contains
#                             the power.
#
#               h2power.info  Stores the various options selected along with
#                             the chi-square statistic, averaged over the
#                             replicates, at each grid point.
#
#               h2power.chis  Stores the results of the simulation replicates
#                             run at each grid point.  This file, along with
#                             h2power.info, is used to restart an interrupted
#                             power calculation.
#
#            During a power calculation, various files named "simqtl.*" are
#            created along with a trait directory named "simqt". These will
#            be removed at the end of the run.
#

proc h2power {args} {

    set eps 1e-7

    set data ""
    set grid ""
    set h2_from 0
    set h2_to 0
    set h2_incr 0.01
    set prev ""
    set nreps ""
    set seed ""
    set overwrite 0
    set restart 0
    set plot 0
    set nosmooth 0
    set noavg 0

    set badargs [read_arglist $args -grid grid -data data -nreps nreps \
                   -seed seed -restart {set restart 1} -r {set restart 1} \
                   -overwrite {set overwrite 1} -ov {set overwrite 1} \
                   -plot {set plot 1 } -nosmooth {set nosmooth 1} -prev prev]

    if {$badargs != ""} {
        error "Invalid h2power command: bad arguments: $badargs"
    }

    if {$restart} {
        if {[catch {set rstf [open h2power.info r]}]} {
            error \
            "Can't restart power calculations: file h2power.info not found"
        }

        if {$data != ""} {
            puts "The -data option is ignored on a restart."
            set data ""
        }

        set record [gets $rstf]
        if {[lindex $record 0] == "data"} {
            set data [lindex $record 2]
            set phenfile_name [lindex $record 8]
            set vnum [verbosity -number]
            verbosity min
            phenotypes load $phenfile_name
            verbosity $vnum
            set record [gets $rstf]
        }

        if {[lindex $record 0] == "nreps"} {
            set onreps [lindex $record 2]
            if {$nreps == ""} {
                set nreps $onreps
            }
        } else {
            error \
            "Can't restart power calculations: file h2power.info is corrupted"
        }

        set record [gets $rstf]
        if {[lindex $record 0] == "grid"} {
            set oh2_from [lindex $record 2]
            set oh2_to [lindex $record 3]
            set oh2_incr [lindex $record 4]
            if {$oh2_from == 0 && $oh2_to == 0} {
                set oh2_from .01
                set oh2_to .99
                set oh2_incr .01
                set noavg 1
            }
            if {$grid == ""} {
                set grid "$oh2_from $oh2_to $oh2_incr"
            }
            if {[llength $record] == 8 && [lindex $record 5] == "prev"} {
                set prev [lindex $record 7]
            }
        } else {
            error \
            "Can't restart power calculations: file h2power.info is corrupted"
        }
        close $rstf

    } else {
        if {[file exists h2power.info] && !$overwrite} {
            error \
            "Power calculations have already been run. Use -overwrite option."
        }
    }

    if {$grid != ""} {
        if {[llength $grid] != 1 && [llength $grid] != 3} {
            error \
    "You must specify the h2 grid with a list {<from> <to> <incr>} or a single constant value."
        }

        set h2_from [lindex $grid 0]
        if {[scan $h2_from "%f" tmp] != 1 || $h2_from < 0 || $h2_from > 1} {
            error "Starting h2 in grid must be between 0 and 1."
        }

        if {[llength $grid] == 1} {
            set h2_to $h2_from
            set h2_incr 1

        } else {
            set h2_to [lindex $grid 1]
            if {[scan $h2_to "%f" tmp] != 1 || $h2_to < 0 || $h2_to > 1} {
                error "Ending h2 in grid must be between 0 and 1."
            }
            if {$h2_to < $h2_from} {
                error "Ending h2 in grid cannot be less than the starting h2."
            }

            set h2_incr [lindex $grid 2]
            if {[scan $h2_incr "%f" tmp] != 1 || $h2_incr <= 0} {
                error "Increment in h2 grid must be greater than 0."
            }
        }

        if {$restart && !$overwrite} {
            set h2 $h2_from
            set oh2 $oh2_from
            set done 0
            while {!$done} {
                if {$oh2 < [expr $h2 - $eps]} {
                    error \
"The requested grid does not include all of the existing grid \{$oh2_from $oh2_to $oh2_incr\}.
Add -overwrite if you really want to discard the extra grid points."
                } elseif {$oh2 > [expr $h2_to + $eps]} {
                    error \
"The requested grid does not include all of the existing grid \{$oh2_from $oh2_to $oh2_incr\}.
Add -overwrite if you really want to discard the extra grid points."
                } else {
                    set done 1
                    if {$oh2 > [expr $h2 - $eps] && $oh2 < [expr $h2 + $eps]} {
                        set oh2 [expr $oh2 + $oh2_incr]
                        set done 0
                    }
                    if {$h2 < [expr $h2_to - $eps]} {
                        set h2 [expr $h2 + $h2_incr]
                        set done 0
                    }
                    if {$oh2 > [expr $oh2_to + $eps]} {
                        set done 1
                    }
                }
            }
        }
    }

    if {$h2_from == 0 && $h2_to == 0} {
        set h2_from .01
        set h2_to .99
        set h2_incr .01
        set noavg 1
        if {$nreps == ""} {
            set nreps 10
        }
    }

    if {$prev != ""} {
        if {[scan $prev "%f" tmp] != 1 || $prev <= 0 || $prev >= 1} {
            error "Prevalence must be greater than 0 and less than 1."
        }
    }

    if {$nreps == ""} {
        set nreps 100
    }

    if {[scan $nreps "%d" tmp] != 1 || $nreps <= 0} {
        error "Number of replicates must be a positive integer."
    }

    if {$seed == ""} {
        set seed 0
    }

    if {$restart && $nreps < $onreps && !$overwrite} {
        error \
"The requested number of replicates is less than the existing number ($onreps).
Add -overwrite if you really want to discard the extra replicates."
    }

    if {[scan $seed "%d" tmp] != 1 || $seed < 0} {
        error "Random number seed must be a non-negative integer."
    }

    set phenfile_name ""
    if {[catch {set infof [open phenotypes.info r]}]} {
        if {$data != ""} {
            error "A phenotypes file has not been loaded."
        }

    } else {
        gets $infof phenfile_name
        close $infof
        if {$data != ""} {
            if {$phenfile_name == "simqtl.phn"} {
                error \
"The currently loaded phenotypes file is named \"simqtl.phn\", which is a
filename reserved for use by the power command. You will have to rename
the phenotypes file and then reload it."
            }
            if {[catch {set phenfile [tablefile open $phenfile_name]}]} {
                error "Can't find phenotypes file $phenfile_name"
            }
            if {![tablefile $phenfile test_name $data]} {
                tablefile $phenfile close
                error "The phenotypes file does not contain a field named $data."
            }
            tablefile $phenfile start_setup
            tablefile $phenfile setup $data
            set navail 0
            while {"" != [set record [tablefile $phenfile get]]} {
                if {$record != "{}"} {
                    incr navail
                }
            }
            tablefile $phenfile close
        }
    }

    set outf [open h2power.info w]
    if {$data != ""} {
        puts $outf "data = $data  navail = $navail  phenf = $phenfile_name"
    }
    puts $outf "nreps = $nreps  seed = $seed"
    if {$noavg} {
        puts -nonewline $outf "grid = 0 0 0.01"
    } else {
        puts -nonewline $outf "grid = $h2_from $h2_to $h2_incr"
    }
    if {$prev != ""} {
        puts -nonewline $outf "  prev = $prev"
    }
    puts $outf ""
    flush $outf

    drand $seed

    if {$restart} {
        exec mv h2power.chis h2power.chis.tmp
        set ochisf [open h2power.chis.tmp r]
        set oh2 $oh2_from
    }

    set chisf [open h2power.chis w]
    set h2 $h2_from
    set nh2 0
    set schi 0
    set sh2 0
    set rep1 0

    while {$h2 <= [expr $h2_to + $eps]} {

        if {$restart && $oh2 < [expr $h2 - $eps]} {
            while {$rep1 < $onreps && {} != [set chirec [gets $ochisf]]} {
                incr rep1
            }
            set rep1 0
            set oh2 [expr $oh2 + $oh2_incr]
            continue

        } elseif {$restart && $oh2 > [expr $h2 - $eps] && \
                  $oh2 < [expr $h2 + $eps]} {
            while {$rep1 < $onreps && {} != [set chirec [gets $ochisf]]} {
                if {$rep1 < $nreps} {
                    set chi [lindex $chirec 1]
                    if {$noavg} {
                        set chis($nh2) [lindex $chirec 1]
                        set o_h2($nh2) [lindex $chirec 2]
                        puts $chisf [format "%5d%12.6f%12.6f" [expr $nh2 + 1] \
                                     [lindex $chirec 1] [lindex $chirec 2]]
                        incr nh2
                    } else {
                        set schi [expr $schi + $chi]
                        set sh2 [expr $sh2 + [lindex $chirec 2]]
                        puts $chisf [format "%5d%12.6f%12.6f" [expr $rep1 + 1] \
                                     [lindex $chirec 1] [lindex $chirec 2]]
                    }
                }
                incr rep1
            }
            set th2 $h2
            set nexth2 [expr $h2 + $h2_incr]
            set nextoh2 [expr $oh2 + $oh2_incr]

        } else {
            set th2 $h2
            set nexth2 [expr $h2 + $h2_incr]
            if {$restart} {
                set nextoh2 $oh2
            }
        }

        if {$data != ""} {
            simqtl -freq .5 -mean {100 100 100} -sdev 10 -h2r $th2 -cov $data
        } else {
            simqtl -freq .5 -mean {100 100 100} -sdev 10 -h2r $th2
        }

        for {set i $rep1} {$i < $nreps} {incr i} {
            if {$phenfile_name != ""} {
                phenotypes load $phenfile_name
            }
            simqtl
            if {$prev != ""} {
                mkdisc $prev
            }
            phenotypes load simqtl.phn
            model new
            trait simqt

            spormod
            option standerr 0
            set errmsg [maximize_quietly last]
            if {$errmsg != ""} {
                if {[string compare [verbosity] "verbosity min"] != 0} {
                    puts "    *** Error maximizing, rep $i"
                }
                set i [expr $i - 1]
                continue
            }
            set slike [loglike]

            polymod
            option standerr 0
            set errmsg [maximize_quietly last]
            if {$errmsg != ""} {
                if {[string compare [verbosity] "verbosity min"] != 0} {
                    puts "    *** Error maximizing, rep $i"
                }
                set i [expr $i - 1]
                continue
            }

            set chi [expr 2*([loglike] - $slike)]
            if {[string compare [verbosity] "verbosity min"] != 0} {
                if {$noavg} {
                    puts [format "%5d%12.6f%12.6f" [expr $nh2 + 1] $chi \
                          [parameter h2r =]]
                } else {
                    puts [format "%5d%12.6f%12.6f" [expr $i + 1] $chi \
                          [parameter h2r =]]
                }
            }
            if {$noavg} {
                puts $chisf [format "%5d%12.6f%12.6f" [expr $nh2 + 1] $chi \
                             [parameter h2r =]]
            } else {
                puts $chisf [format "%5d%12.6f%12.6f" [expr $i + 1] $chi \
                             [parameter h2r =]]
            }
            flush $chisf

            if {$noavg} {
                set chis($nh2) $chi
                set o_h2($nh2) [parameter h2r =]
                incr nh2
            } else {
                set schi [expr $schi + $chi]
                set sh2 [expr $sh2 + [parameter h2r =]]
            }
        }

        if {!$noavg} {
            set chis($nh2) [expr $schi/$nreps]
            puts [format "h2 = %g  EChi2 = %.6g" $th2 $chis($nh2)]
            puts $outf [format "h2 = %g  EChi2 = %.6g" $th2 $chis($nh2)]
            flush $outf
            incr nh2
            set schi 0
            set sh2 0
        }

        if {$restart} {
            set oh2 $nextoh2
        }
        set h2 $nexth2
        set rep1 0
    }

    if {$restart} {
        close $ochisf
        file delete h2power.chis.tmp
    }

    close $chisf
    close $outf

    if {$nosmooth} {
        set outf [open "|[usort] -nu > h2power.out" w]
    } else {
        set xx 0
        set xy 0
    }

    set h2 $h2_from
    set nh2 0
    set last_h2 [expr $h2_to + $eps]
    while {$h2 <= $last_h2} {
        if {$noavg} {
            for {set i 0} {$i < $nreps} {incr i} {
                set chi $chis($nh2)
                if {$chi < 0} {
                    set chi 0
                }
                if {$nosmooth} {
                    puts $outf "[format %.4g $o_h2($nh2)] \
                                 [expr 1 - [chinc 2.705542 1 $chi]]"
                } elseif {$chi < 13.6796} {
                    set xx [expr $xx + pow($o_h2($nh2),4)]
                    set xy [expr $xy + pow($o_h2($nh2),2)*$chi]
                }
                incr nh2
            }
        } else {
            set chi $chis($nh2)
            if {$chi < 0} {
                set chi 0
            }
            if {$nosmooth} {
                puts $outf "[format %.4g $h2] \
                             [expr 1 - [chinc 2.705542 1 $chi]]"
            } elseif {$chi < 13.6796} {
                set xx [expr $xx + pow($h2,4)]
                set xy [expr $xy + pow($h2,2)*$chi]
            }
            incr nh2
        }
        set h2 [expr $h2 + $h2_incr]
    }

    if {!$nosmooth} {
        set outf [open h2power.out w]
        set last [expr int(100*$h2_to)]
        for {set i 0} {$i < $last} {incr i} {
            puts $outf [format "%.4g %.6g" [expr $i*.01] \
                [expr 1 - [chinc 2.705542 1 [expr pow($i*.01,2)*$xy/$xx]]]]
        }
    }
    close $outf

    if {$plot} {
        plot_h2power
    }

    exec rm -rf simqt
    delete_files_forcibly simqtl.qtl simqtl.par simqtl.dat

    if {$phenfile_name != ""} {
        delete_files_forcibly simqtl.phn
        set vnum [verbosity -number]
        verbosity min
        catch {phenotypes load $phenfile_name}
        verbosity $vnum
    }
}


proc mkdisc {prev} {
    set ifp [tablefile open simqtl.phn]
    tablefile $ifp start_setup
    set qtfld [tablefile $ifp setup simqt]
    set n 0
    set qts {}
    while {{} != [set rec [tablefile $ifp get]]} {
        if {[lindex $rec 0] != ""} {
            lappend qts [lindex $rec 0]
            incr n
        }
    }
    tablefile $ifp close
    set qts [lsort -real -decreasing $qts]
    set thresh [lindex $qts [expr int($prev*$n)]]

    set ofp [open simqtl.tmp w]
    set ifp [tablefile open simqtl.phn]
    tablefile $ifp start_setup
    set flds [tablefile $ifp names]
    tablefile $ifp setup [lindex $flds 0]
    puts -nonewline $ofp [lindex $flds 0]
    for {set i 1} {$i < [llength $flds]} {incr i} {
        tablefile $ifp setup [lindex $flds $i]
        puts -nonewline $ofp ",[lindex $flds $i]"
    }
    puts $ofp ""

    while {{} != [set rec [tablefile $ifp get]]} {
        if {[lindex $rec $qtfld] == ""} {
            continue
        }
        puts -nonewline $ofp [lindex $rec 0]
        for {set i 1} {$i < [llength $rec]} {incr i} {
            if {$i == $qtfld} {
                if {[lindex $rec $i] > $thresh} {
                    puts -nonewline $ofp ",1"
                } else {
                    puts -nonewline $ofp ",0"
                }
            } else {
                puts -nonewline $ofp ",[lindex $rec $i]"
            }
        }
        puts $ofp ""
    }
    tablefile $ifp close
    close $ofp
    exec mv simqtl.tmp simqtl.phn
}


# solar::plot_h2power -- private
#
# Purpose:  Implements "plot -h2power"
#
# Usage:    plot_h2power [-title <plot_title>]
#
# -

proc plot_h2power {args} {
    set title "Power"
    read_arglist $args -title title -h2power {set ignore_this 0}

    if {[catch {tclgr open} errmsg]} {
	if {[string compare $errmsg \
		"tclgr session already opened from this solar session"]} {
	    error $errmsg
	}
    }

    if {[catch {set inf [open h2power.out r]}]} {
        error "Cannot open h2power.out"
    }

    set x {}
    set y {}
    while {-1 != [gets $inf line]} {
        lappend x [lindex $line 0]
        lappend y [lindex $line 1]
    }
    close $inf

    set npts [llength $x]
    if {$npts == 0} {
	error "No data in h2power.out ... can't plot"
    }

    tclgr send kill graphs
    tclgr send clear line
    tclgr send clear string
    tclgr send focus g0

    tclgr send title \"$title\"

    tclgr send world xmin 0
    set max_x [lindex $x [expr [llength $x] - 1]]
    set max_x [expr ceil(10*$max_x)/10]
    tclgr send world xmax $max_x
    tclgr send xaxis tick major 0.1
    tclgr send xaxis tick minor 0.05

    tclgr send world ymin 0
    tclgr send world ymax 1
    tclgr send yaxis tick major 0.1
    tclgr send yaxis tick minor 0.05

    global env
    if {[file exists $env(SOLAR_LIB)/h2power.gr]} {
	set mpathname [glob $env(SOLAR_LIB)/h2power.gr]
	tclgr send read \"$mpathname\"
    }
    if {[file exists ~/lib/h2power.gr]} {
	set mpathname [glob ~/lib/h2power.gr]
	tclgr send read \"$mpathname\"
    } 
    if {[file exists h2power.gr]} {
	tclgr send read \"h2power.gr\"
    } 

    for {set i 0} {$i < $npts} {incr i} {
	tclgr send s0 color 2
	tclgr send g0.s0 point "[lindex $x $i],[lindex $y $i]"
    }

    tclgr send redraw
}


# solar::house --
#
# Purpose:  Enable analysis of household effects
#
# Usage:    house            ; enable analysis of household effects
#           house -suspend   ; temporarily suspend household effects
#                            ; (the c2 parameter is constrained to zero)
#           house -delete    ; delete household effects (c2 is deleted)
#
# Examples:
#
#  ** new model ** Note you must give house command after trait or model new
#
#         solar> model new
#         solar> trait weight
#         solar> covar age^1,2#sex
#         solar> house                       ; activates h. effects
#         solar> polygenic -screen           ; polygenic analysis w. screening
#
# ** old model ** 
#
#         solar> load model poly
#         solar> house
#         solar> polygenic
#
# Notes:  This may be used for any common environmental effects (not
#         necessarily "household" effects).  The house command changes the
#         current model as follows:
#
#             1) A parameter c2 (c stands for "common") is created
#             2) The house matrix is loaded
#             3) A c2*house term is added to the omega
#             4) The c2 parameter is added to the e2 + ... = 1 constraint
#             5) The starting value of c2 is carved away from the value of
#                e2 so that the constraint remains satisfied
#
#         The pedigree file must contain a HHID field.  If so, the
#         'load pedigree' command produces a matrix named house.gz.
#         That matrix will be used.  If house.gz is not present, this
#         command will fail, although you can map HHID to any particular
#         field in your pedigree file using the "field" command.
#
#         WARNING!  If you load a pedigree without a HHID field (or a field
#         mapped to it with the field command) a pre-existing house.gz, now
#         assumed to be obsolete, will be deleted.  This is to prevent you
#         from making the mistake of using an obsolete house.gz.
#
#         HHID can be a number or an alphanumeric name (with no internal
#         spaces or tabs) but the number 0 (zero) has a special meaning.
#         Zero indicates singleton households--each individual with HHID
#         of zero is a separate household, not associated with other
#         individuals having HHID of zero.  Blank or null has the same
#         effect as zero.
#
#         The 'house' command should be specified after such commands as
#         automodel, trait, polymod, or spormod (which initialize polygenic
#         or sporadic models) and/or just before commands which maximize
#         models such as 'polygenic,' 'maximize,' or 'multipoint.'  This
#         is because "polygenic" or "sporadic" models, by definition,
#         do not have household effects.  But the polygenic command will
#         do the household "analysis" if it detects the presence of a c2
#         parameter which is not constrained to zero.
#
#         We define the following model classes:
#
#         sporadic  (covariates only...and e2)
#           household (covariates and c2)
#           polygenic (covariates and h2r)
#             household polygenic (covariates, c2, and h2r)
#
#         To create a pure "household" model with no active genetic component,
#         give the commands "spormod" and "house" in that order after setting
#         up the trait(s) and covariate(s).
#
#         By default, if a household element is in the model, pedigrees will
#         be merged whenever individuals in separate pedigrees share the same
#         household.  The resulting groups are called "pedigree-household"
#         groups.  This may significantly increase memory requirements.
#         Pedigree merging is controlled by two SOLAR options (see the
#         option command).  The default of 1 for MergeHousePeds means that
#         pedigree merging, as just described, will be done.  This feature
#         may be changed by setting MergeHousePeds to zero prior to the
#         polygenic or maximize commands:
#
#         solar> option mergehousepeds 0
#         solar> polygenic -screen
#
#         The MergeAllPeds option combines all the pedigrees into one large
#         group if set to 1.  This is an alternative simpler method of
#         merging, but it may increase memory requirements even more.
#         
#-

proc house {args} {

# Handle arguments

    if {"" != $args} {
	if {"-suspend" == $args} {
	    return [outhouse]
	} elseif {"-delete" == $args} {
	    return [nohouse]
	}
	error "house: invalid argument"
    }

# Multiple traits?

    set ts [trait]
    set ntraits [llength $ts]

# If no model yet, make this sporadic (sporadic+house = household model)
# (Note: spormod calls "house -suspend", so house actually gets called back)

    if {$ntraits > 1} {
	set firste2 e2\([lindex $ts 0]\)
	if {![if_parameter_exists $firste2]} {
	    spormod
	}
    } else {
	if {![if_parameter_exists e2]} {
	    spormod
	}
    }

# Add house matrix (make sure only once)

    if {"" == [housematrix]} {
	catch {matrix delete house}
	matrix load house.gz house
    }

# Setup omega
#   If bivariate, also setup rhoc (required for omega)


    if {$ntraits > 2} {
	for {set i 1} {$i < $ntraits} {incr i} {
	    for {set j [expr $i + 1]} {$j <= $ntraits} {incr j} {
		set rname [catenate rhoc_ $i $j]
		parameter $rname = 0 lower -1 upper 1
		catch {constraint_remove $rname}
	    }
	}
	if {-1 == [string first \
		       house*sqrt(<c2(ti)>)*sqrt(<c2(tj)>)*(tne*rhoc_ij+teq) \
		       [omega]]} {
	    omega_add house*sqrt(<c2(ti)>)*sqrt(<c2(tj)>)*(tne*rhoc_ij+teq)
	}
    } elseif {$ntraits == 2} {
	parameter rhoc = 0 lower -1 upper 1
	catch {constraint_remove rhoc}
	if {-1 == [string first \
		house*sqrt(<c2(ti)>)*sqrt(<c2(tj)>)*(tne*rhoc+teq) \
		[omega]]} {
	    omega_add house*sqrt(<c2(ti)>)*sqrt(<c2(tj)>)*(tne*rhoc+teq)
	}
    } else {
	if {-1 == [string first c2*house [omega]]} {
	    omega_add c2*house
	}
    }
	
# For each trait, create c2 parameter, add to vc constraint, carve new value

    foreach tr $ts {

# Add c2 parameter with starting value of 0.01
# Put in variance components constraint and omega

	set suffix ""
	if {$ntraits > 1} {
	    set suffix ($tr)
	}

	parameter c2$suffix lower 0 upper 1

	constraint_include c2$suffix e2$suffix  ;# This also purges zero constraints
	if {0.0==[parameter c2$suffix =]} {
	    carve_new_value c2$suffix 0.01
	}
	if {1<[parameter e2$suffix upper]} {
	    parameter e2$suffix upper 1
	}
    }

    option cmdiagonal 0
    return ""
}


# Remove household effect from model
# Remove completely (even if partially removed before)
# Do not crash if household effect is partially or not at all present

proc nohouse {} {
    catch {matrix delete house}

    set ts [trait]
    set ntraits [llength $ts]

# Delete Rho's and remove house term from omega

    if {$ntraits > 2} {
	for {set i 1} {$i < $ntraits} {incr i} {
	    for {set j [expr $i + 1]} {$j <= $ntraits} {incr j} {
		set rname [catenate rhoc_ $i $j]
		catch {parameter $rname delete}
	    }
	}
	catch {
	    set newomega [stringsub [omega] \
		"+ house*sqrt(<c2(ti)>)*sqrt(<c2(tj)>)*(tne*rhoc_ij+teq)" ""]
	    eval $newomega
	}
    } elseif {$ntraits == 2} {
	catch {parameter rhoc delete}
	catch {
	    set newomega [stringsub [omega] \
		    "+ house*sqrt(<c2(ti)>)*sqrt(<c2(tj)>)*(tne*rhoc+teq)" ""]
	    eval $newomega
	}
    } else {
	catch {omega_remove c2}
    }

# Move variance back to e2's and delete c2's
# Edit c2 from constraints

    foreach tr $ts {
	set suffix ""
	set cname c2
	if {$ntraits > 1} {
	    set suffix \($tr\)
	    set cname <c2$suffix>
	}
	if {[if_parameter_exists c2$suffix]} {
	    parameter e2$suffix = [expr [parameter e2$suffix =] + \
		    [parameter c2$suffix =]]
	    catch {constraint_remove $cname}
	    parameter c2$suffix delete
	    make_bounds_ok_for_value e2$suffix 0 1
	}
    }
    return ""
}


# house -suspend
# IMPORTANT: DO NOT ASSUME HOUSE IS ACTIVE!
# This is called w/o seeing if model has house in the first place
proc outhouse {} {
    set ts [trait]
    set ntraits [llength $ts]

# Constrain c2's

    foreach tr $ts {
	set suffix ""
	set cname c2
	if {$ntraits > 1} {
	    set suffix \($tr\)
	    set cname <c2$suffix>
	}
	if {[if_parameter_exists c2$suffix]} {
	    parameter e2$suffix = [expr [parameter e2$suffix =] + \
		    [parameter c2$suffix =]]
	    make_bounds_ok_for_value e2$suffix 0 1
	    catch {constraint_remove $cname}
	    constraint $cname = 0
	    parameter c2$suffix = 0
	}
    }

# Edit omega and constrain rho's (if applicable)

    if {$ntraits > 2} {
	for {set i 1} {$i < $ntraits} {incr i} {
	    for {set j [expr $i + 1]} {$j <= $ntraits} {incr j} {
		set rname [catenate rhoc_ $i $j]
		if {[if_parameter_exists $rname]} {
		    parameter $rname = 0
		    constraint $rname = 0
		}
	    }
	}
	set newomega [stringsub [omega] \
	  "+ house*sqrt(<c2(ti)>)*sqrt(<c2(tj)>)*(tne*rhoc_ij+teq)" ""]
	eval $newomega

    } elseif {$ntraits == 2} {
	if {[if_parameter_exists rhoc]} {
	    parameter rhoc = 0
	    constraint rhoc = 0
	}
	set newomega [stringsub [omega] \
          "+ house*sqrt(<c2(ti)>)*sqrt(<c2(tj)>)*(tne*rhoc+teq)" ""]
	eval $newomega
    } else {
	catch {omega_remove c2}
    }

    return ""
}

#
# check_house and check_epistasis
#   return 1 if C2 parameter is active, 0 otherwise
#
proc check_house {} {
    set ts [trait]
    set ntraits [llength $ts]

    if {-1 < [string first c2 [omega]]} {
	foreach tr $ts {
	    set test c2
	    set testc c2
	    if {$ntraits > 1} {
		set test [catenate c2 ( $tr ) ]
		set testc <$test>
	    }
	    if {[if_parameter_exists $test]} {
		if {![is_constrained_to_zero $testc]} {
		    return 1
		}
	    }
	}
    }
    return 0
}


proc check_epistasis {} {
    if {[if_parameter_exists h2qe1]} {
	if {-1<[string first h2qe1 [omega]]} {
	    if {![is_constrained_to_zero h2qe1]} {
		return 1
	    }
	}
    }
    return 0
}

#
# Procedures parameter_roll_into and carve_new_value make it easier
# to add or adjust variance components which need to add up to 1.0
#
proc parameter_roll_into {old new} {
    if {0.0 != [parameter $old =]} {
	parameter $new = [expr [parameter $new =] + [parameter $old =]]
	if {[parameter $new upper] < [parameter $new =]} {
	    parameter $new upper [expr [parameter $new =] + 0.01]
	}
	parameter $old = 0.0
	parameter $old lower -0.01
    }
}

# solar::carve_new_value -- private
#
# Purpose: carve starting value for new variance component (see notes).
#
# Usage:   carve_new_value vcomp newvalue [tryfirst]
#
# Notes:
#
# carve_new_value "carves" a starting value for a new variance
# component from other (standard) variance components.  Custom
# variance components are not supported, and carve_new_value should
# not be called for custom variance components.  However, it is NOT
# assumed that all variance components are "known" or that known
# components add up to 1.0, only that when a new one is added, others
# (known ones, what else?) must be decreased by a corresponding amount.  
# (That is what "carving" means.)
#
# carve_new_value is now extended to handle bivariate:
#  target vcomp should be specified with required trait suffix
#  HOWEVER, tryfirst should be specified generically WITH NO SUFFIX
#    (suffix will be copied from vcomp)
#  
# The current value is vcomp is NOT assumed to be zero.  However it
# probably works better if it IS zero, or only a very tiny change is
# being made, or if the "tryfirst" option is likely to succeed.
#
# The entire difference is taken from the first variance component
# having sufficient distance from the natural boundaries of 0 and 1
# and which is not already constrained.  This simple algorithm is
# subject to change in the future.  perturb and linkmod use somewhat
# more sophisticated algorithms which may distribute the required
# deltas and may subtract the differences from existing parameters
# in a different order.
#
# Boundaries of changed variance components are made compatible if
# necessary, except for the new variance component.  It is assumed
# that the caller has plans to set boundaries for the new variance
# component, or leave that to solarmain, which simply defaults them
# to 0 and 1.
#
# Actual setting of the value for the new variance component must
# be done here, since allowances for a previous value are made.
# (Strictly speaking, the "new" variance component need not actually
# be new.)
# -
 
proc carve_new_value {vcomp newvalue {tryfirst e2}} {

#   puts "Entering carve_new_value"

    set s ""
    set multi 0
    set firstp [string first \( $vcomp]
    if {-1 != $firstp} {
	set lastp [string first \) $vcomp]
	set s [string range $vcomp $firstp $lastp]
#	puts "suffix is $s"
	set multi 1
    }

    set amount [expr $newvalue - [parameter $vcomp =]]

    set clist [list $tryfirst$s e2$s h2r$s h2q1$s c2$s h2qe1$s]
    for {set i 2} {$i <= [h2qcount]} {incr i} {
	lappend clist h2q$i$s
    }
    set done 0

    foreach comp $clist {
	if {$comp == $vcomp} {
	    continue
	}
#	puts "testing parameter $comp"
#
# Cannot carve from parameter that has "simple" constraint, or is
# within 0.03 of ultimate boundaries
#
	if {[if_parameter_exists $comp]} {
#	    puts "Parameter $comp exists"
	    if {[catch {find_simple_constraint $comp}]} {
#		puts "No constraint found for $comp"
		set current [parameter $comp =]
		set maxval 0.97
		if {"" != [parameter $comp fixupper]} {
		    set maxval [parameter $comp fixupper]
		}
		if {($amount > 0 && $current - $amount > 0.03) || \
		    ($amount < 0 && $current - $amount < $maxval)} {
		    set carve_value [expr $current - $amount]
#
# OK, carve value from this parameter
#
		    parameter $comp = $carve_value
		    make_bounds_ok_for_value $comp 0 1
		    set done 1
		    break
		}
	    }
	}
    }
    if {!$done} {
	error "Couldn't carve value for $vcomp from other variance components"
    }
#
# Now set target parameter
#
    parameter $vcomp = $newvalue
#
# Caller assumes responsibility for bounds.
#   Not suitable for generic solution.
#   (usually this is done automatically by SOLARMAIN)
#   parameter $vcomp lower 0 upper 1
#   make_bounds_ok_for_value $vcomp 0 1

    return ""
}

# solar::make_bounds_ok_for_value -- private
#
# Purpose: Make upper and lower bounds compatible with current value
#
# Usage:   make_bounds_ok_for_value par <lowest-possible> <highest-possible>
#
# Notes:   Boundaries >= 0.01 away from current value are unchanged.
#
#          If boundary is moved, it is moved 0.01 away from current value,
#          if possible.
#
#          Upper boundary is moved no higher than <highest-possible>.
#          Lower boundary is moved no lower than <lowest-possible>.
# -

proc make_bounds_ok_for_value {comp nlower nupper} {
    set olower [parameter $comp lower]
    set oupper [parameter $comp upper]
    set value [parameter  $comp =]
    if {$value < $olower + 0.01} {
	if {"" == [parameter $comp FixLower]} {
	    parameter $comp lower [highest $nlower [expr $value - 0.01]]
	} else {
	    puts "Warning.  Unable to lower fixed lower boundary for $comp"
	}
    }
    if {$value > $oupper - 0.01} {
	if {"" == [parameter $comp FixUpper]} {
	    parameter $comp upper [lowest $nupper [expr $value + 0.01]]
	} else {
	    puts "Warning.  Unable to raise fixed upper boundary for $comp"
	}
    }
    return ""
}


# solar::linkmod --
#
# Purpose:  Set up parameters and constraints for multipoint linkage model
#
# Usage:    linkmod [-add [-epistasis]] [-zerostart] [-2p] <ibdfile>
#                   [-cparm] [-se]
#
#                    -add means add this to previous linkage elements
#                       (otherwise, it supercedes the previous one)
#                    -zerostart is used for score analysis (see below)
#                    -2p  twopoint (ibd not mibd)
#                    -epistasis sets up epistasis parameter between new
#                         element and previous linkage element.  Use with
#                         "-add".  Not supported for bivariate.
#                    -se     Turn on standard error calculation option.
#                            The default is to turn it off.
#                    -cparm  "Custom Parameterization"  Simply replace old
#                            matrix with new matrix.  Parameters, constraints,
#                            and omega are unchanged.  A "prototype" model
#                            with suitable matrix, parameters, omega, and
#                            constraints must already be loaded.  See
#                            final note below for more information.
#                            Note: if -cparm is specified, standard errors
#                            are NOT turned off, but left in whatever state
#                            they were in when linkmod was called.
#
# Notes:    Use the -2p option for twopoint models.  linkmod2p is now
#           obsolescent (linkmod -2p is invoked when you give the linkmod2p
#           command).
#
#           A polygenic or linkage model should already have been
#           created and maximized first.  Boundaries are set around existing
#           values assuming this has been done.
#
#           Multiple linkage terms will be included if Solar_Fixed_Loci
#           is defined.  The script multipoint does this.
#
#           By default, standard error is turned off.  You may turn it on
#           again by giving the command 'option standerr 1' after running
#           linkage and before running maximize.
#
#           The -zerostart option starts the new linkage component at 0.
#           (The linkage component MUST NOT HAVE BEEN ALREADY CREATED!)
#           This is used for score test analysis.
#
#           The -cparm option requires that a prototype linkage model
#           with all required matrices, parameters, omega terms, and
#           constraints be already loaded.  Other than that, however, it
#           ATTEMPTS to be as general as possible.  However, it is
#           necessary to make one assumption regarding the name of
#           the first matrix.  If the -2p option is specified, the
#           relevant matrix that will be modified must be named
#           ibd or ibd1, ibd2, etc.  Otherwise, the relevant matrix
#           must be named mibd or mibd1, mibd2, etc.  It is the
#           ibd or mibd matrix with the highest number, if any,
#           which will be replaced.  If a second matrix column such 
#           as d7 or delta7 is included, it will be assumed to be
#           included in the replacement matrix as well.  This option
#           is used by "multipoint -cparm" and "twopoint -cparm".
# -

proc linkmod {args} {

    set add 0
    set zs 0
    set previous_h2q_value none
    set epistasis 0
    set twopoint 0
    set noparm 0
    set secalc 0

    set ibdfilename [read_arglist $args -zerostart {set zs 1} \
	    -epistasis epistasis \
	    -2p {set twopoint 1} \
	    -cparm {set noparm 1} \
	    -noparm {set noparm 1} \
      	    -se {set secalc 1} \
	    -lasth2q previous_h2q_value -add {set add 1}]
    if {1!=[llength $ibdfilename]} {
	error "Invalid or missing arguments to linkmod"
    }

# Edit options

    option cmdiagonal 0

    if {$secalc} {
	option standerr 1
    } else {
	option standerr 0
    }

# Noparm option, quick and easy

    if {$noparm} {
	set matrices [string tolower [matrix]]
	set second_matrix ""
	set index ""
	set prefix mibd
	if {$twopoint} {
	    set prefix ibd
	}
	for {set tindex 1} {1} {incr tindex} {
	    if {-1 != [lsearch $matrices $prefix$tindex]} {
		set index $tindex
	    } else {
		break
	    }
	}
	set position [lsearch $matrices $prefix$index]
	if {-1 == $position} {
	    error "$prefix$index matrix must already be loaded in prototype model"
	}
	set second [lindex $matrices [expr $position + 1]]
	if {"matrix" == $second} {
	    set second ""
	}
	eval matrix load $ibdfilename $prefix$index $second
	return ""
    }

# Normal parameter tracking and such (this gets complicated!)

    if {$add} {
	set new_h2q_index [expr 1 + [h2qcount]]
    } else {
	set new_h2q_index [expr 1 + [use_global_if_defined Solar_Fixed_Loci 0]]
    }

    set ts [trait]
    if {[llength $ts] == 1} {
	set multi 0
	set suffix ""
    } else {
	set multi [llength $ts]

# Adjust boundaries for rhoe and rhog

	if {$multi == 2} {
	    set rhog [parameter rhog =]
	    parameter rhog lower [highest -1 [expr $rhog - 0.1]]
	    parameter rhog upper [lowest 1 [expr $rhog + 0.1]]

	    set rhoe [parameter rhoe =]
	    parameter rhoe lower [highest -1 [expr $rhoe - 0.05]]
	    parameter rhoe upper [lowest 1 [expr $rhoe + 0.05]]
	} else {
	    foreach par [parameter -names] {
		if {0 == [string compare "rho" [string range $par 0 2]]} {
		    set tolerance 0.1
		    if {0 == [string compare "e" [string index $par 3]]} {
			set tolerance 0.05
		    }
		    set rho [parameter $par =]
		    parameter $par lower [highest -1 [expr $rho - $tolerance]]
		    parameter $par upper [lowest 1 [expr $rho + $tolerance]]
		}
	    }
	}
    }

# Track household parameters similarly to h2q's

    if {[check_house] && ![boundarywide]} {
	foreach tr $ts {
	    if {!$multi} {
		set pname c2
	    } else {
		set pname c2($tr)
	    }
	    if {[if_parameter_exists $pname]} {
		set c2upper [parameter $pname upper]
		set c2upper [lowest 1 [expr [parameter $pname =] + \
						  [h2q_float]]]
		parameter $pname upper $c2upper
	    }
	}
    }
	    
# Create new rhoq's If they do not already exist
    
    set RHO rhoq$new_h2q_index

    if {$multi} {
	if {$multi == 2} {
	    if {![if_parameter_exists $RHO]} {
		parameter $RHO = 0 lower -1 upper 1
	    }
	} else {
	    if {![if_parameter_exists [catenate $RHO _12]]} {
		set nts [llength $ts]
		for {set i 1} {$i < $nts} {incr i} {
		    for {set j [expr $i + 1]} {$j <= $nts} {incr j} {
			set rhoname [catenate $RHO _$i$j]
			if {![if_parameter_exists $rhoname]} {
			    parameter $rhoname = 0 \
				lower -1 upper 1
			}
		    }
		}
	    }
	}
    }

    foreach tr $ts {
	if {$multi} {
	    set suffix ($tr)
	}

# Get starting variance parameter values; create h2q1 if it doesn't exist


	if {[catch {set e2start [parameter e2$suffix =]}]} {
	    error \
	    "Parameter e2$suffix does not exist.  Start with polygenic model."
	}
	if {[catch {set h2rstart [parameter h2r$suffix =]}]} {
	    error \
            "Parameter h2r$suffix does not exist.  Start with polygenic model."
	}
	parameter h2q1$suffix
	set h2q1start [parameter h2q1$suffix =]


# I could check for summing to 1 here, but it's pretty complicated because
# the latest $h2q may or may not exist.  Errors will be caught anyway.

# Set e2 boundaries
# This is done conservatively (adjusting as little as necessary)
# But using boundary heuristics as needed

	set e2low [parameter e2$suffix lower]
	set e2upp [parameter e2$suffix upper]
	if {$e2upp == 0 || $e2low > $e2start || $e2upp < $e2start} {
	    error \
	 "Parameter e2$suffix improperly bounded.  Start with polygenic model."
	} elseif {$e2start < [e2lower]} {

# Assume it was maximized to zero or nearly so (could be bad user setup too)
# Let "perturb" (later) take care of "starting on boundary condition", if
#   applicable, by adjusting starting value.

	} elseif {$e2start == 1} {

# Previous model was sporadic or had zero heredity
# Set lower bound heuristically no lower than current setting

	    soft_lower_bound e2$suffix [highest $e2low [e2lower]]
	} else {

# E2 somewhere in between [e2lower] and 1
# Set lower bound using e2squeeze heuristic, e2lower, and "soft" bounding

	    soft_lower_bound e2$suffix \
		[highest [e2lower] [expr $e2start - [e2squeeze]]]
	}

# Set E2 upper bound no higher than 1, current bound, or bracketed range

	parameter \
	    e2$suffix  upper [lowest 1 $e2upp [expr $e2start + [e2squeeze]]]

# setup h2r bounds and starting point

	parameter h2r$suffix lower 0
	set h2rupper 1
	set maxh2rupper [expr 1 - [parameter e2$suffix lower]]
	if {[h2rf] > 0 && ![catch {set null0h2r [nulln 0 h2r$suffix]}]} {
	    set h2rupper [expr [h2rf] * $null0h2r]
	}
	if {$h2rupper > $maxh2rupper} {
	    set h2rupper $maxh2rupper
	}
	parameter h2r$suffix upper $h2rupper
	constraint_include h2r$suffix e2$suffix

	set H2Q h2q$new_h2q_index
	set K2 K2_$new_h2q_index
	if {$twopoint} {
	    set MIBD ibd$new_h2q_index
	} else {
	    set MIBD mibd$new_h2q_index
	}

	set constrain_new_to_zero 0
	if {$epistasis} {

# If new chromosome and locus are the same as epistasis base, we
#   contrain the new h2q to zero

	    set matlist [matrix]
	    set mfile ""
	
	    for {set i 0} {"" != [set line [line_index $matlist $i]]} \
		{incr i} {
		    set matname [lindex $line 3]
		    if {![string compare mibd$epistasis $matname]} {
			set mfile [lindex $line 2]
			break
		    }
		}
	    if {"" == $mfile} {
		error "Can't find matrix for epistasis"
	    }
	    set echr [get_chromosome $mfile]
	    set eloc [get_locus $mfile]
	    set tchr [get_chromosome $ibdfilename]
	    set tloc [get_locus $ibdfilename]
	    if {$echr == $tchr && $eloc == $tloc} {
		set constrain_new_to_zero 1
	    }

# Setup epistasis interaction term

	    if {![if_parameter_exists h2qe1]} {
		parameter h2qe1 lower 0.0 upper 1.0
		carve_new_value h2qe1 0.01
	    } else {
		parameter h2qe1 lower 0.0
	    }
	    constraint_include h2qe1
	    omega_remove h2qe1
	    omega_add mibd$epistasis*$MIBD*h2qe1
	}

# Find good bounds for h2q unless -zerostart

	if {$zs} {
	    parameter $H2Q$suffix start 0.0 lower -0.00001 upper 1.0
	} else {

# If previous $H2Q value specified or found, base h2q upper bound on that

	    if {"none" != $previous_h2q_value} {
		set h2qupper [lowest 1.0 [expr [h2q_float] + $previous_h2q_value]]
# If no linkage analysis has been done, upper bound will be 1.0 and value
# will be close to zero (less than 0.01) if not exactly zero
	    } elseif {![catch {set h2q_upper [parameter $H2Q$suffix upper]}] && \
			  ($h2q_upper != 1.0 || [parameter $H2Q$suffix =] > 0.01)} {
		set h2qupper [lowest 1.0 \
				  [expr [h2q_float] + [parameter $H2Q$suffix start]]]
	    } else {

# Use h2qsupper ("boundary start upper") to set upper bound for new h2q*

		set h2qupper [h2qsupper -get $new_h2q_index]
	    }


# if $H2Q doesn't exist, create it
# or, if it does exist, but its value is 0, set to 0.01

	    if {[catch {set h2q_start [parameter $H2Q$suffix start]}] || \
		    $h2q_start <= 0} {
		
		parameter $H2Q$suffix start 0.01 lower 0 upper $h2qupper

# Find a place to carve new $H2Q from:
#   Subtract 0.01 from h2r for initial h2q, or
#   subtract 0.01 from e2 for initial h2q, or
#   if new_h2q_index==1, start e2 and h2r at 0.9 and 0.01, or
#   subtract 0.01 from first H2qN larger than 0.02

	    set min_e2_test [expr .02 + [parameter e2$suffix lower]]
	    
	    if {0.02 < [parameter h2r$suffix =]} {
		parameter h2r$suffix = [expr -0.01 + [parameter h2r$suffix =]]
	    } elseif {$min_e2_test < [parameter e2$suffix =]} {
		parameter e2$suffix = [expr -0.01 + [parameter e2$suffix =]]
	    } elseif {$new_h2q_index == 1} {
		parameter e2$suffix = 0.9
		parameter h2r$suffix = 0.09
	    } else {
		set done 0
		puts "    *** Attempting to carve $H2Q$suffix from other H2Qs"
		for {set i 1} {$i < $new_h2q_index} {incr i} {
		    if {0.02 < [parameter h2q$i$suffix =]} {
			parameter h2q$i$suffix = \
				[expr -0.01 + [parameter h2q$i$suffix =]]
			set done 1
			break
		    }
		}
		if {!$done} {
	       error "Unable to carve $H2Q$suffix starting value from anywhere"
		}
	    }
	    } else {
		parameter $H2Q$suffix lower 0.0 upper $h2qupper
	    }
	    perturb
	}

	if {$twopoint} {
	    if {-1 != [string first "D7" [omega]] || \
		    -1 != [string first "d7" [omega]]} {
		matrix load $ibdfilename $MIBD d7
	    } else {
		matrix load $ibdfilename $MIBD
	    }
	} else {
	    if {[ifneedk2]} {
		eval [use_global_if_defined Solar_Linkage_Matrix \
			  {matrix load $ibdfilename $MIBD $K2}]
	    } else {
		eval [use_global_if_defined Solar_Linkage_Matrix \
			  {matrix load $ibdfilename $MIBD}]
	    }
	}

# Edit constraints

	constraint_include $H2Q$suffix e2$suffix

# If this must be constrained to zero for epistasis, do so now

	if {$constrain_new_to_zero} {
	    parameter_roll_into $H2Q$suffix h2r$suffix
	    constrain_to_zero $H2Q$suffix
	    parameter $H2Q$suffix lower -0.01
	} else {

# Otherwise, make sure lower bound is 0, unless zerostart

	    if {0 > [parameter $H2Q$suffix lower] && !$zs} {
		parameter $H2Q$suffix lower 0.0
	    }
	}
    }

# Edit omega

    if {$multi==2} {
	if {-1 == [string first $H2Q\(ti\) [omega]]} { 
	   omega_add sqrt(<$H2Q\(ti\)>)*sqrt(<$H2Q\(tj\)>)*$MIBD*(tne*$RHO+teq)
	}
    } elseif {$multi > 2} {
	if {-1 == [string first $H2Q\(ti\) [omega]]} { 
	    omega_add sqrt(<$H2Q\(ti\)>)*sqrt(<$H2Q\(tj\)>)*$MIBD*(tne*[catenate $RHO _ij]+teq)
	}
    } else {
	omega_remove $H2Q
	omega_add $MIBD*$H2Q
    }

# Now, apply special boundary heuristics
#  they must be applied after setting up omega since h2qcount is based
#  on omega.

    foreach tr $ts {
	if {"none" == $previous_h2q_value} {
	    boundarywide -apply   ;# Apply current boundarywide status
	}
	boundarynull -apply
    }

    return ""
}

# solar::linkmod2p -- private
#
# Purpose:  Use linkmod -2p instead.  linkmod2p is now obsolescent.
#-

# Set up parameters and constraints for twopoint linkage 
#           models
#
# Usage:    linkmod2p <ibdfile>
#
# Notes:    A polygenic or linkage model should already have been
#           created and maximized.  Boundaries are set around existing
#           values.
#
#           By default, standard error is turned off.  You may turn it on
#           again by giving the command 'option standerr 1' after running
#           linkmod and before running maximize.
#
#           To save memory, the d7 (dominance) values from the ibd matrix
#           will not be loaded unless d7 is found in the omega.
# -

proc linkmod2p {args} {
    return [eval linkmod -2p $args]
}

proc linkmod2p-obsolete {ibdfilename} {

    set new_h2q_index 1

# Catch missing basic parameters...instruct the dummies on what to do

    if {[catch {set e2start [parameter e2 start]}]} {
	error "Parameter e2 does not exist.  Start with polygenic model."
    }
    if {[catch {set h2rstart [parameter h2r start]}]} {
	error "Parameter h2r does not exist.  Start with polygenic model."
    }
    if {[catch {set h2q1start [parameter h2q1 start]}]} {
	error "Parameter h2q1 does not exist.  Start with polygenic model."
    }

# I could check for summing to 1 here, but it's pretty complicated because
# the latest $h2q may or may not exist.  Errors will be caught anyway.

# Set e2 boundaries
# This is done conservatively (adjusting as little as necessary)
# But using boundary heuristics as needed

    set e2low [parameter e2 lower]
    set e2upp [parameter e2 upper]
    if {$e2upp == 0 || $e2low > $e2start || $e2upp < $e2start} {
	error "Parameter e2 improperly bounded.  Start with polygenic model."
    } elseif {$e2start < [e2lower]} {

# Assume it was maximized to zero or nearly so (could be bad user setup too)
# Let "perturb" (later) take care of "starting on boundary condition", if
#   applicable, by adjusting starting value.

    } elseif {$e2start == 1} {

# Previous model was sporadic or had zero heredity
# Set lower bound heuristically no lower than current setting
	soft_lower_bound e2 [highest $e2low [e2lower]]
    } else {

# E2 somewhere in between [e2lower] and 1
# Set lower bound using e2squeeze heuristic, e2lower, and "soft" bounding
	soft_lower_bound e2 [highest [e2lower] [expr $e2start - [e2squeeze]]]
    }
# Set E2 upper bound no higher than 1, current bound, or bracketed range
    parameter e2 upper [lowest 1 $e2upp [expr $e2start + [e2squeeze]]]

# setup h2r bounds and starting point

    parameter h2r lower 0
    set h2rupper 1
    set maxh2rupper [expr 1 - [parameter e2 lower]]
    if {[h2rf] > 0 && ![catch {set null0h2r [nulln 0 h2r]}]} {
	set h2rupper [expr [h2rf] * $null0h2r]
    }
    if {$h2rupper > $maxh2rupper} {
	set h2rupper $maxh2rupper
    }
    parameter h2r upper $h2rupper
    constraint_include h2r

# Find good bounds for h2q
	
    set h2qupper [h2qsupper -get $new_h2q_index]

    set H2Q h2q1
    set IBD ibd1
    set D7 d7

# if $H2Q doesn't exist, create it
# or, if it does exist, but its value is 0, set to 0.01

    if {[catch {set h2q_start [parameter $H2Q start]}] || \
	$h2q_start <= 0} {
		
	parameter $H2Q start 0.01 lower 0 upper $h2qupper

# Find a place to carve new $H2Q from:
#   Subtract 0.01 from h2r for initial h2q, or
#   subtract 0.01 from e2 for initial h2q, or
#   if new_h2q_index==1, start e2 and h2r at 0.9 and 0.01, or
#   subtract 0.01 from first H2qN larger than 0.02

	set min_e2_test [expr .02 + [parameter e2 lower]]

	if {0.02 < [parameter h2r start]} {
	    parameter h2r start [expr -0.01 + [parameter h2r start]]
	} elseif {$min_e2_test < [parameter e2 start]} {
	    parameter e2 start [expr -0.01 + [parameter e2 start]]
	} elseif {$new_h2q_index == 1} {
	    parameter e2 start 0.9
	    parameter h2r start 0.09
	} else {
	    error "Unable to carve $H2Q starting value from anywhere"
	}
    } else {
	set h2qupper [expr [h2q_float] + $h2q_start]
	parameter $H2Q lower 0.0 upper $h2qupper
    }

    perturb

# include d7 only if found in omega
    if {-1 != [string first "D7" [omega]] || \
	    -1 != [string first "d7" [omega]]} {
	matrix load $ibdfilename $IBD $D7
    } else {
	matrix load $ibdfilename $IBD
    }

# old
#    eval [use_global_if_defined Solar_2P_Linkage_Matrix \
#	    {matrix load $ibdfilename $IBD $D7}]

# Edit constraints and omega equation

    constraint_include $H2Q
    omega_remove $H2Q
    omega_add $IBD*$H2Q

    option cmdiagonal 0
    option standerr 0

    boundarywide -apply   ;# Apply current boundarywide status
    boundarynull -apply
    return ""
}

# solar::tdist
#
# Purpose:  Set up t option for robust estimation of mean and variance
#
# Usage:    tdist        set up t option
#           tdist -off   turn off t option
#
# Notes:    tdist creates a parameter t_param and sets tdist option
#           tdist -off deletes t_param and unsets tdist option
#
#-
proc tdist {args} {
    if {"" == $args || "-on" == $args || "on" == $args} {
	set on 1
    } elseif {"-off" == $args || "off" == $args} {
	set on 0
    } else {
	error "Invalid argument to tdist"
    }

    if {$on} {
	if {![if_parameter_exists t_param]} {
	    parameter t_param start 10 lower 0.5 upper 500
	}
	option tdist 1
    } else {
	parameter delete t_param
	option tdist 0
    }
}

#
# check_artificial_boundaries and adjust_boundaries go together
# check_artificial_boundaries will not include fixed boundaries in the list
#   of problems, so adjust_boundries does not have to check the fix status
#
proc adjust_boundaries {} {
    set boundary_problems [check_artificial_boundaries]
    if {{} == $boundary_problems} {
	error "No variance component boundary conditions detected"
    }
    set adjustment [boundary_change]
    foreach problem $boundary_problems {
	set component [lindex $problem 0]
	set direction [lindex $problem 1]
	set value [parameter $component =]
	set delta [expr [parameter $component upper] - \
		       [parameter $component lower]]
	if {"lower" == $direction} {
	    set min_lower 0
	    if {"rho" == [string tolower [string range $component 0 2]]} {
		set min_lower -1
	    }
	    set newlower [expr $value - (0.998 * $delta)]
	    set newupper [expr $value + (0.001 * $delta)]
	    if {$newlower < $min_lower} {set newlower $min_lower}
	    if {"e2" == [string tolower [string range $component 0 1]]} {
		if {[trace_boundaries]} {
		   puts "Using soft bounds for $component"
		}
		soft_lower_bound $component -push $newlower
		parameter $component upper $newupper
	    } else {
		parameter $component lower $newlower upper $newupper
	    }
	} else {
	    set newlower [expr $value - (0.001 * $delta)]
	    set newupper [expr $value + (0.998 * $delta)]
	    if {$newupper > 1} {set newupper 1}
	    parameter $component lower $newlower upper $newupper
	}
    }
    return $boundary_problems
}


# do_boundries crunches boundaries after convergence error
# Fix boundaries if they need fixing (done by c++ now)
# Otherwise crunch boundaries

proc do_boundaries {args} {

# The minimum boundary delta is established in preopt.f
    set rmin 1e-4

    set crunch [bcrunch]
    if {"" != $args} {
	set crunch $args
	ensure_float $args
    }
    set checklist [get_vc_list]

# Check for invalid boundaries

    set invalid_boundaries 0
    foreach check $checklist {
	set value [parameter $check =]
	set lower [parameter $check lower]
	set upper [parameter $check upper]
	set delta [expr $upper - $lower]
	if {$delta < $rmin || $upper < $value || $value < $lower} {
	    set invalid_boundaries 1
	}
    }	

# Either crunch all boundaries, or fix invalid ones

    set crunched {}
    foreach check $checklist {
	set value [parameter $check =]
	set lower [parameter $check lower]
	set upper [parameter $check upper]
	set delta [expr $upper - $lower]

	if {[trace_boundaries]} {
	    set rdelta [format %.4g $delta]
	    puts "Old boundary range for $check is $rdelta"
	}	

	set crunchthis $crunch
	if {"rhoe" == $check} {
	    set crunchthis [expr 0.3 * $crunch]
	}


	if {$crunchthis < $rmin} {
	    set crunchthis $rmin
	}

# If there are invalid boundaries, fix them, but leave others alone
# Otherwise, if valid boundaries are tighter than new setting, leave them alone

	if {$delta > $rmin && $upper - $value > $rmin && $value - $lower > $rmin} {
	    if {$invalid_boundaries} {
#
# Invalid boundaries are now fixed in C++
# so we just re-run model
#
		continue
	    }

# Don't crunch rhoqX until second level

	    if {"rhoq" == [string tolower [string range $check 0 3]]} {
		if {$crunchthis >= 0.09} {
		    continue
		} else {
		    set crunchthis [expr 5 * $crunchthis]
		}
	    }
	    if {$delta < $crunchthis*2} {
		if {[trace_boundaries]} {
		    puts "Boundaries for $check not changed"
		}
		continue
	    }
	}

	set bottom 0
	if {"e2" == [string tolower [string range $check 0 1]] || \
		"h2r" == [string tolower [string range $check 0 2]]} {
	    set bottom 0.01
	    if {$value <= $bottom + 0.001} {
		set bottom 0
	    }
	} elseif {"rho" == [string tolower [string range $check 0 2]]} {
	    set bottom -1
	}

	if {"" == [parameter $check fixlower]} {
	    parameter $check lower [highest $bottom \
					[expr $value - $crunchthis]]
	}
	if {"" == [parameter $check fixupper]} {
	    parameter $check upper  [lowest 1.0 [expr $value + $crunchthis]]
	}

	set newdelta [expr [parameter $check upper] - [parameter $check lower]]
	if {[trace_boundaries]} {
	    set rdelta [format %.4g $newdelta]
	    puts "New boundary range for $check is $rdelta"
	}
	lappend crunched $check
    }

    if {$invalid_boundaries} {
	error "   *** Fixed bad boundaries: $crunched"
    }
    return $crunched
}


# get_vc_list
# Get a list of active variance components and rho's

proc get_vc_list {} {
    set checklist {}
    set h2qi [h2qcount]

# For multiple traits
    set nt [llength [trait]]
    if {1 < $nt} {
	foreach tname [trait] {
	    lappend checklist e2\($tname\)
	    if {[if_parameter_exists h2r\($tname\)]} {
		lappend checklist h2r\($tname\)
	    }
	    for {set i 1} {$i <= $h2qi} {incr i} {
		lappend checklist h2q$i\($tname\)
	    }
	}
	foreach par [parameter -names] {
	    if {0 == [string compare "rho" [string range $par 0 2]]} {
		lappend checklist $par
	    }
	}
	if {[check_house]} {
	    foreach tname [trait] {
		if {[if_parameter_exists c2($tname)]} {
		    lappend checklist c2($tname)
		}
	    }
	}

# For single trait
    } else {
	lappend checklist e2
	if {[if_parameter_exists h2r]} {
	    lappend checklist h2r
	}
	for {set i 1} {$i <= $h2qi} {incr i} {
	    lappend checklist h2q$i
	}
	for {set i 1} {$i <= $h2qi} {incr i} {
	    lappend checklist h2q$i
	}
	if {[check_house]} {
		lappend checklist c2
	}

# Epistasis boundary control only supported for univariate

	if {[check_epistasis]} {
	    lappend checklist h2qe1
	}
    }

    if {[solardebug]} {
	puts "solardebug: Recognized variance components are: $checklist"
    }
    return $checklist
}    


proc check_artificial_boundaries {} {
    set blist {}
    set checklist [get_vc_list]
    foreach check $checklist {
	set value [parameter $check =]
	set upper [parameter $check upper]
	set lower [parameter $check lower]
	set fixupper [parameter $check fixupper]
	set fixlower [parameter $check fixlower]
	if {"rho" == [string tolower [string range $check 0 2]]} {
	    set min_lower -1
	    set max_upper 1
	} else {
	    set min_lower 0
	    set max_upper 1
	}
	if {$value <= $lower && $lower > $min_lower && $fixlower == ""} {
	    lappend blist [list $check lower]
	}
	if {$value >= $upper && $upper < $max_upper && $fixupper == ""} {
	    lappend blist [list $check upper]
	}
    }
    return $blist
}

proc check_real_upper_boundaries {} {
    set blist {}
    set checklist [get_vc_list]
    foreach check $checklist {
# RhoqX hitting 1 is normal
	if {"rhoq" == [string tolower [string range $check 0 3]]} {
	    continue
	}
#
# check that we didn't just hit upper bound because of constraint
#
	if {[is_constrained_to_nonzero $check]} {
	    continue
	}
	if {0 != [string first e2 $check]} {
	    set value [parameter $check =]
	    if {$value >= 1.0} {
		lappend blist [list $check upper]
	    }
	}
    }
    return $blist
}



# solar::perturb
#
# Purpose:  Perturb starting values for E2, H2r, and H2q's at bounds
#
# Usage:    perturb
#
# Notes:   perturb is specially tailored to the standard parameterization
#          of e2, h2r, h2q1, etc.  perturb does nothing silently if
#          parameters e2 and h2r are not present.
#
#          It is no longer necessary or possible to specify h2qindex as an
#          unqualified argument as in earlier versions.  If an unqualified
#          argument is specified, it is ignored.
#
#          This is used automatically by the 'linkmod' script, and therefore
#          also by multipoint and twopoint.
#
#          perdelta is the quantity used in the adjustment (this may be set
#          with the perdelta command).  It defaults to 0.001
#
#          Accumulated deltas are distributed only to other parameters whose
#          values are 3*perdelta away from the relevant bound, and then only
#          in the perdelta quantity.
#
#          This does not handle conditions where parameters exceed boundaries
#          by more than a very small amount.  (Of course, they shouldn't
#          exceed the boundaries at all, but sometimes they do by very small
#          amounts.  Recent changes to the maximization routines ought to
#          eliminate that.)
# -

proc perturb {args} {

    set give_warning 0
    ifverbplus set give_warning 1
    ifdebug set give_warning 1
    set give_first_warning $give_warning

    set adj [perdelta]
    set adj2 [expr $adj * 2]
    set adj3 [expr $adj * 3]
#
# We deal with the variance components for each trait in a separate pass
#
    set tnames [trait]
    foreach tname $tnames {
	if {1 == [llength $tnames]} {
	    set parvector [get_variance_components]
	} else {
	    set parvector [get_variance_components $tname]
	}

	if {{} == $parvector} continue

# Adjust parameters on boundaries

	set accum 0
	set parvals {}
	set unchanged $parvector
	set count [llength $parvector]
	set constraints_found 0
	foreach par $parvector {
	    set val [parameter $par start]
	    lappend parvals $val
	    if {![catch {find_simple_constraint $par}]} {
		incr constraints_found
		continue
	    }
	    if {$val <= [parameter $par lower] && $val + $adj3 < 1} {
		if {$give_first_warning} {puts "    *** Perturbing parameters"}
		parameter $par start [expr $val + $adj]
		set accum [expr $accum - 1]
		set give_first_warning 0
		set unchanged [remove_from_list $unchanged $par]
		ifdebug puts "    *** Raising $par from lower bound"
		if {[parameter $par =] >= [parameter $par upper]} {
		    parameter $par upper [expr [parameter $par =] + $adj]
		    ifdebug puts "perturb:  raising $par upper bound"
		}
	    } elseif {$val >= [parameter $par upper] && $val - $adj3 > 0} {
		if {$give_first_warning} {puts "    *** Perturbing parameters"}
		parameter $par start [expr $val - $adj]
		set accum [expr $accum + 1]
		set give_first_warning 0
		set unchanged [remove_from_list $unchanged $par]
		ifdebug puts "    *** Lowering $par from upper bound"
		if {[parameter $par =] <= [parameter $par lower]} {
		    parameter $par lower [expr [parameter $par =] - $adj]
		    ifdebug puts "perturb:  lowering $par lower bound"
		}
	    }
	}

# Actually, if there are N-1 constraints for N parameters, we can't
# perturb, so we skip to the restore part

	if {$count - 1 > $constraints_found} {

# Now, find places to "dump" the deltas we've accumulated
# Allow 3*NPAR passes, if we can't do it by then, give up

	if {{} == $unchanged} {
	    set unchanged $parvector
	}

	set limit [expr 3 * [llength $parvector]]
	for {set i 0} {$i < $limit} {incr i} {
	    if {$accum == 0} break
	    foreach par $unchanged {
		if {![catch {find_simple_constraint $par}]} continue
		if {$accum == 0} break
		set val [parameter $par start]
		if {$accum > 0} {
		    if {[parameter $par upper] - $val > $adj3} {
			parameter $par start [expr $val + $adj]
			set accum [expr $accum - 1]
			ifdebug puts "perturb:  raising $par to compensate"
		    }
		} else {
		    if {$val - [parameter $par lower] > $adj3} {
			parameter $par start [expr $val - $adj]
			set accum [expr $accum + 1]
			ifdebug puts "perturb:  lowering $par to compensate"
		    }
		}
	    }
	}
#
# Didn't find place to dump deltas.  That could be because of narrow artificial
# boundaries.  Try again, but this time permit the moving of artificial
# boundaries to within $adj of 0 or 1 and allowing $adj between boundary and
# value.
#
	if {$accum != 0} {
	    ifdebug puts "    *** Second phase of perturb required"
	    for {set i 0} {$i < $limit} {incr i} {
		if {$accum == 0} break
		foreach par $unchanged {
		    if {![catch {find_simple_constraint $par}]} continue
		    if {$accum == 0} break
		    set val [parameter $par =]
		    if {$accum > 0} {
			if {[parameter $par =] + $adj3 <= 1} {
			    parameter $par = [expr $val + $adj]
			    parameter $par upper [expr $val + $adj2]
			    set accum [expr $accum - 1]
			    ifdebug puts "perturb:  raising $par to compensate"
			}
		    } else {
			if {[parameter $par =] -  $adj3 >= 0} {
			    parameter $par = [expr $val - $adj]
			    parameter $par lower [expr $val - $adj2]
			    set accum [expr $accum + 1]
			    ifdebug puts "perturb:  lowering $par to compensate"
			}
		    }
		}
	    }
	}
#
# If failed, restore original parameters for this trait and quit
#
        }
	if {$accum != 0} {
	    if {$count - 1 > $constraints_found} {
		puts "    *** Warning: Unable to perturb variance parameters\n"
	    } elseif {$give_warning} {
		puts "    *** Can't perturb because $constraints_found constraints for $count parameters"
	    }
	    for {set i 0} {$i < $count} {incr i} {
		parameter [lindex $parvector $i] = [lindex $parvals $i]
		if {$give_warning} {
		    puts "    *** Restoring parameter [lindex $parvector $i] to [lindex $parvals $i]"
		}
	    }
	}
    }
    return ""
}


# solar::allcovar --
#
# Purpose:  Set up all non-trait variables as covariates
#
# Usage:    allcovar
#
# Notes:    Phenotypes and trait commands must already have been given.
#           If there is a variable named "age," it will be set up
#           as "age^1,2#sex."  If undesired variables are made into
#           covariates, they should be removed with the covariate
#           delete command.
#
#           allcovar will not include names mapped to any of the standard
#           field variables (see 'help field').  Be sure to set up field
#           mappings (if required) first to ensure you don't get extra
#           covariates for the likes of ID, FAMID, etc.
#
#           allcovar will also not include any names added to the 'exclude'
#           list.  Use the 'exclude' command to add names to the exclude
#           list, or to display the exclude list.  By default, the exclude
#           list includes some standard PEDSYS mnemonics
# -

proc allcovar {} {
    if {0 == [string length [trait]]} {error "Trait not specified"}
    set ltrait [string tolower [trait]]
#
# Get phenotypes list and clean it from multiple phenotype names
#
    set rawlist [lrange [phenotypes] 1 end]
    set covarlist {}

    foreach cov $rawlist {
	if {[string compare ":" [string range $cov end end]]} {
	    lappend covarlist $cov
	}
    }

    set covarlist [string tolower $covarlist]
    set covarlist [remove_from_list $covarlist $ltrait]
    set covarlist [remove_from_list $covarlist ibdid]
#
# Setup covariates
#
    covariate sex
    foreach cov $covarlist {
	if {![string compare age $cov]} {
	    covariate age^1,2#sex
	} elseif {![string compare [string tolower [field ID]] $cov]} {
	} elseif {![string compare [string tolower [field FA]] $cov]} {
	} elseif {![string compare [string tolower [field MO]] $cov]} {
	} elseif {![string compare [string tolower [field SEX]] $cov]} {
	} elseif {![string compare [string tolower EGO] $cov]} {
	} elseif {![string compare [string tolower SIRE] $cov]} {
	} elseif {![string compare [string tolower DAM] $cov]} {
	} elseif {![string compare [string tolower GENDER] $cov]} {
	} elseif {![string compare [string tolower [field PROBND]] $cov]} {
	} elseif {![string compare [string tolower [field MZTWIN]] $cov]} {
	} elseif {![string compare [string tolower [field FAMID]] $cov]} {
	} elseif {![string compare [string tolower [field SEX]] $cov]} {
	} elseif {-1 < [lsearch [exclude] $cov]} {
	} else {
	    covariate $cov
	}
    }
}

# solar::pedlod --
#
# Purpose:  Calculate pedigree-specific LOD scores
#
# Usage:    pedlod [<test-model> [<null-model>]]
#
# Notes:    If no model is specified, the model currently in memory
#           is used as the test-model (useful if you have just run
#           multipoint or twopoint), and its null-model (having
#           one less linkage element) is used as the null model.
#
#           If only one model is specified, the null model is taken
#           from the outdir after the specified model is loaded.
#
#           The pedigree numbers used are based on SOLAR's pedigree
#           index "pedindex.out" created by the "load pedigree"
#           command.  These do not necessarily correspond to user
#           pedigree numbers (and there is not necessarily even
#           a one-to-one correspondence).  Refer to pedindex.out
#           to associate your ID's with the pedigrees associated
#           by SOLAR.  (Note: pedindex.out has a code file pedindex.cde
#           and is best read using PEDSYS, but may also be read fairly
#           well as a text file if PEDSYS is not available.)
#
#           Note that the LOD score calculation may be affected by the
#           number trait(s), and the lodp options.  See the documentation
#           for the "lodp" command for further details.  When applicable,
#           SOLAR converts 2df bivariate LODs to "1df effective" LODs.
# -

proc pedlod {args} {

    full_filename foo   ;# test trait/outdir

    delete_files_forcibly [full_filename pedlod.out]

# Check inputs

    set test_model ""
    set null_model ""

    if {2 < [llength $args]} {
	error "Only two models may be specified"
    } elseif {2 == [llength $args]} {
	set test_model [lindex $args 0]
	set null_model [lindex $args 1]
    } elseif {1 == [llength $args]} {
	set test_model [lindex $args 0]
    }

# Save starting model

    set start_model ""
    if {![catch {trait}]} {
	set start_model [full_filename pedlod.start]
	save model $start_model
    }

# Check for existence of models
# null_model and test_model are assigned if not already
# test_model is loaded if not already

    if {"" != $test_model} {
	if {![file exists $test_model] && ![file exists $test_model.mod]} {
	    error "Can't find $test_model; did you specify full path?"
	}
	load model $test_model  ;# Needed for h2qcount below
    } else {
	set test_model $start_model
    }

    if {"" == $null_model} {
	set null_index [expr [h2qcount] - 1]
	if {$null_index < 0} {
	    error "Test model has no linkage elements"
	}
	set null_model [full_filename null$null_index.mod]
	if {![file exists $null_model]} {
	    error \
            "Can't find implied null model [full_filename null$null_index.mod]"
	}
    } else {
	if {![file exists $null_model] && ![file exists $null_model.mod]} {
	    error "Can't find $null_model; did you specify full path?"
	}
    }

# Do pedlike on test_model; save as pedlod.test.out

    puts "\nComputing per-pedigree loglikehoods for test model...\n"

    pedlike $test_model
    file rename -force [full_filename pedlike.out] [full_filename pedlod.test.out]

# Do pedlike on null_model; save as pedlod.null.out

    puts "\nComputing per-pedigree loglikehoods for null model...\n"

    pedlike $null_model
    file rename -force [full_filename pedlike.out] [full_filename pedlod.null.out]

# Read both pedlike files and write LOD scores to output file

    set outfile [open [full_filename pedlod.out] w]
    set testfile [tablefile open [full_filename pedlod.test.out]]
    set nullfile [tablefile open [full_filename pedlod.null.out]]

    tablefile $testfile start_setup
    tablefile $nullfile start_setup

    set need_famid 0
    tablefile $testfile setup Loglikelihood
    tablefile $testfile setup PEDNO

    tablefile $nullfile setup Loglikelihood
    tablefile $nullfile setup PEDNO

    puts "\nComputing per-pedigree LOD scores...\n"
    putstee $outfile "PEDNO, LOD"

    set after_sum 0.0
    while {1} {
	set testrec [tablefile $testfile get]
	set nullrec [tablefile $nullfile get]

# Check for EOF

	if {{} == $testrec && {} == $nullrec} {
	    break
	} elseif {{} == $testrec} {
	    tablefile $testfile close
	    tablefile $nullfile close
	    close $outfile
	    error "Premature end of testfile"
	} elseif {{} == $nullrec} {
	    tablefile $testfile close
	    tablefile $nullfile close
	    close $outfile
	    error "Premature end of nullfile"
	}
	    
# Compare PEDNO and FirstID for consistency (and FAMID, if present)

	if {[string compare [lindex $testrec 1] [lindex $nullrec 1]]} {
	    tablefile $testfile close
	    tablefile $nullfile close
	    close $outfile
	    error "PEDNO out of sequence"
	}

# Get loglikelihoods and compute LOD
# If either file is missing likelihood, use error message instead

	set testlike [lindex $testrec 0]
	set nulllike [lindex $nullrec 0]
	if {![is_float $testlike]} {
	    set lodscore $testlike
	} elseif {![is_float $nulllike]} {
	    set lodscore $nulllike
	} else {
	    set lodscore [lod $testlike $nulllike]
	    set lodscore [format %.5f $lodscore]
	    set after_sum [expr $after_sum + $lodscore]
	}

# Write record to output file and screen

	putstee $outfile [format "%5s,  %s" \
		    [lindex $testrec 1] $lodscore]
    }

# Close files

    tablefile $testfile close
    tablefile $nullfile close
    close $outfile

# Reload test_model (or original model)

    if {"" != $start_model} {
	load model $start_model
    }

    set pout [open [full_filename pedlod.info] w]
    close $pout
    puts " "
    set pinfo [open [full_filename pedlike.info]]
    while {-1 != [gets $pinfo record]} {
	if {-1 != [lsearch $record pedigrees]} {
	    putsout pedlod.info "$record"
	}
    }
    close $pinfo
    putsout pedlod.info "LOD sum (of rounded scores) is [format %.4f $after_sum]"
    putsout pedlod.info "Results have also been written to [full_filename pedlod.out] and [full_filename pedlod.info]"
}


# solar::pedlike --
#
# Purpose:  Calculate pedigree-specific loglikelihoods
#
# Usage:    pedlike [-q] [<model>]
#
#           -q    (quiet) Supress output to terminal
#
# Notes:    Default model will be current model, if current model has
#           been maximized.  If changes have been made to current model
#           since the last maximization, results may not be predictable.
#
#           If current model has not been maximized, default model is
#           the null0 model in current outdir.
#
#           Results are written to "pedlike.out" in the outdir
#           and also shown on terminal with some additional summary info.
#
#           The pedigree numbers used are based on SOLAR's pedigree
#           index "pedindex.out" created by the "load pedigree"
#           command.  These do not necessarily correspond to user
#           pedigree numbers (and there is not necessarily even
#           a one-to-one correspondence).  Refer to pedindex.out
#           to associate your ID's with the pedigrees associated
#           by SOLAR.  (Note: pedindex.out has a code file pedindex.cde
#           and is best read using PEDSYS.)
#
#     
# -

proc pedlike {args} {

    file delete [full_filename pedlike.info]
    file delete [full_filename pedlike.out]

    set output 1
    set q ""
    if {"" != $args && "-q" == [lindex $args 0]} {
	set output 0
	set q "-q"
	set args [lrange $args 1 end]
    }

# Determine what model to use, save current model if necessary

    set start_model ""
    if {![catch {trait}]} {
	set start_model [full_filename pedlike.start]
	save model $start_model
    }

    set model_message ""
    set modelname $args
    if {[llength $args] > 1} {
	error "specific: Invalid arguments"
    } elseif {[llength $args] == 0} {
	if {![catch {loglike}]} {
	    set model_message "\nResults for model currently loaded"
	    set modelname ""
	} else {
	    set modelname [full_filename null0]
	}
    }
    if {"" != $modelname} {
	set model_message "\nResults for model $modelname"
	load model $modelname
    }

    if {$output} {puts "Scanning pedigrees..."}

# Read pedindex.out to get highest pedno
    set last_pedno 0
    set pin [tablefile open pedindex.out]
    tablefile $pin start_setup
    tablefile $pin setup PEDNO
    while {{} != [set record [tablefile $pin get]]} {
	set last_pedno [lindex $record 0]
    }
    tablefile $pin close

# Get and write loglikelihoods for each pedigree

    if {$output} {puts "Estimating likelihoods for each pedigree..."}
    option maxiter 1
    option standerr 0
    option pedlike 1
    if {[catch {set errmsg [maximize_quietly pedlikemod.out]} cmessage]} {
	set errmsg "Unknown error"
    }
    if {![string compare [string range $errmsg 0 0] "\n"]} {
	set errmsg [string range $errmsg 1 end]
    }
    if {"" != $errmsg} {
	error "pedlike maximization failed: $errmsg"
    }

# Read pedexclude.dat for excluded pedigrees
    set pex {}
    if {[file exists [full_filename pedexclude.dat]]} {
	set pxf [open [full_filename pedexclude.dat]]
	gets $pxf  ;# skip header
	while {-1 != [gets $pxf record]} {
	    lappend pex $record
	}
	close $pxf
    }

# Read pedlike.dat and write pedlike.out

    set pex_save $pex
    set pin [open [full_filename pedlike.dat]]
    gets $pin ;# skip over header
    set oname [full_filename pedlike.out]
    set pout [open $oname w]
    close $pout
    eval putsout $q pedlike.out \"  PEDNO,    Loglikelihood\"
    set pedno 0
    set pinc 0
    set pexc 0
    set plast 0
    set lsum 0.0
    while {-1 != [gets $pin loglike]} {
#
# Skip over deleted pedigrees
#
	incr pedno
	while {[llength $pex] && [lindex $pex 0] == $pedno} {
	    incr pedno
	    incr pexc
	    set pex [lrange $pex 1 end]
	}
	eval putsout $q pedlike.out \"[format %7d $pedno], [format %14.6f $loglike]\"
	set lsum [expr $lsum + $loglike]
	set plast $pedno
	incr pinc
    }
    close $pin
    ifdebug puts "pexc: $pexc  last_pedno: $last_pedno  plast: $plast"
    set pexc [expr $pexc + ($last_pedno - $plast)]
#
# Print summary information
#
    eval putsout $q pedlike.info $model_message
    eval putsout $q pedlike.info  \"Results written to [full_filename pedlike.out] and [full_filename pedlike.info]\"
    eval putsout $q pedlike.info \"$pinc pedigrees included, $pexc pedigrees excluded\"
    eval putsout $q pedlike.info \"Loglikelihood sum is [format %14.6f $lsum]\"
    if {$output} {puts ""}
#
# Restore original model status
#
    if {"" != $start_model} {
	load model $start_model
	file delete $start_model
    } else {
	model new
    }
    return ""
}


# solar::hlod
#
# Purpose:  Heterogeneity test for linkage
#
# Usage:    hlod [-step <stepsize>]
#
#           <stepsize> is size of increment in h2q1 and h2r.  Default is
#             0.1, 0.05, 0.02, or 0.01 chosen to be about 1/10 of total
#             heritability in starting model (with a minimum of
#             8 test points plus maximized h2q1 and total heretability).
#
# Notes:    Linkage test model must be loaded at beginning.  Corresponding
#           null model will be found in current output directory.
#
#           Complete results are written to hlod.out in maximization output
#           directory.
#
#           Linkage model may only have 1 linkage element; null model
#           is "polygenic" (null0) model in current output directory.
#
#           H0 vs. H1 test is only considered significant if p < 0.0001
#           (LOD 3).  If this is not significant, there will be a warning
#           that the H1 vs H2 test makes no sense.
#
#           hlod uses "homo" program written by Harald Goring for which
#           documentation is provided in Appendix 5 of the documentation
#           (use "doc" command to browse, click on Table of Contents).
#
#-

proc hlod {args} {

# Delete old output and intermediate files

    file delete [full_filename hlod.out]
    file delete [full_filename homo.in]
    
# Check for linkage model

    if {"" == [matrix]} {
	error "hlod: You must load linkage model first"
    }
    if {[h2qcount] != 1} {
	error "hlod: Only works with 1 QTL linkage model"
    }

# Examine and save current model

    save model [full_filename hlod.start]
    set h2t [expr [parameter h2q1 =] + [parameter h2r =]]
    set h2q1_max [parameter h2q1 =]
    puts "    *** Maximized h2q1 is $h2q1_max"
    if {$h2q1_max==0} {
	putsout hlod.out "    *** Warning: current model has h2q1 = 0\n"
    }
    putsout hlod.out "    *** Total heritability is [format %.10g $h2t]"

# Process arguments

    set step ""

    set badargs [read_arglist $args "-step" step]

    if {{} != $badargs} {
	error "Invalid arguments to hlod: $badargs"
    }

# Set and report step size

    if {"" == $step} {
	if {$h2t > 0.8} {
	    set step 0.1
	} elseif {$h2t > 0.4} {
	    set step 0.05
	} elseif {$h2t > 0.16} {
	    set step 0.02
	} else {
	    set step 0.01
	}
	putsout hlod.out "    *** Step size of $step will be used"
    } else {
	ensure_float $step
	putsout hlod.out "    *** Step size of $step chosen"
    }

# Save previous pedlike results

    save_pedlike hlod

# Load best polygenic model

    puts "    *** Getting per-pedigree likelihoods for polygenic model"
    pedlike -q [full_filename null0.mod]
    set h2r_0 [hlod_storepedlike]
    
# Setup vector and header for intermediate file

    set header "pedno,H0(0.0)"
    set vector ""
    set h2q1_max_output 0
    for {set h2q1 0.0} {$h2q1 < $h2t} \
	{set h2q1 [expr $h2q1 + $step]} {
	if {$h2q1 > $h2q1_max && $h2q1 != $h2q1_max && !$h2q1_max_output} {
	    set h2q1f [format %.10g $h2q1_max]
	    set header "$header,$h2q1f"
	    set vector "$vector$h2q1f "  ;# space required at end
            set h2q1_max_output 1
	}
	set h2q1f [format %.10g $h2q1]
	set header "$header,$h2q1f"
	set vector "$vector$h2q1f "    ;# space required at end
    }
    set h2q1f [format %.10g $h2t]
    set header "$header,$h2q1f"
    set vector "$vector$h2q1f"

# Step through possible values for h2q1

    set i -1
    foreach h2q1 $vector {

	incr i
	set last_i $i

        if {$h2q1 >= $h2t} {
	    set h2r 0
        } else {
	    set h2r [expr $h2t - $h2q1]
	}

	load model [full_filename hlod.start]

# Setup parameters and contraints

	parameter h2q1 = $h2q1
	constraint h2q1 = $h2q1
	if {$h2q1 == 0.0} {
	    parameter h2q1 lower -0.01
	} else {
	    parameter h2q1 lower 0.0
	}
	if {$h2q1 == 1.0} {
	    parameter h2q1 upper 1.01
	} else {
	    parameter h2q1 upper 1.0
	}

	set h2r $h2r
	constraint h2r = $h2r
	if {$h2r == 0.0} {
	    parameter h2r lower -0.01
	} else {
	    parameter h2r lower 0.0
	}
	if {$h2r == 1.0} {
	    parameter h2r upper 1.01
	} else {
	    parameter h2r upper 1.0
	}

	puts "    *** Getting per-pedigree likelihoods for null1.mod at h2q1=$h2q1"

	option MaxIter 1
	maximize -q -o hlod.test.out

	save model [full_filename hlod.test]
	pedlike -q [full_filename hlod.test]
	set h2q1_$i [hlod_storepedlike]
    }
    set npeds [llength $h2q1_1]

# Now, write results to intermediate file

    set ofile [open [full_filename homo.in] w]
    puts $ofile $header
    for {set pedno 1} {$pedno <= $npeds} {incr pedno} {
# Buildup output line
	set record "$pedno,[lindex $h2r_0 [expr $pedno - 1]]"
	for {set i 0} {$i <= $last_i} {incr i} {
	    set record "$record,[lindex [eval set foo \$h2q1_$i] \
                                [expr $pedno - 1]]"
	}
	puts $ofile $record
    }
    close $ofile

# Run the homo program

    load model [full_filename hlod.start]
    set pwd [pwd]
    cd [full_filename ""]
    if {[catch {exec homo -c} errmsg]} {
	cd $pwd
	hlod_cleanup
	error "Program homo returned error: $errmsg"
    }
    cd $pwd

# Read through homo.out, copying and echoing as desired

    set hf [open [full_filename homo.out]]
    set terminal 0
    set file 0
    while {-1 != [gets $hf record]} {

# Look for guideposts

	if {0 == [string compare "description of hypotheses:" $record]} {
	    set file 1
	    putsout -q hlod.out ""
	} elseif {0 == [string compare "test results:" $record]} {
	    set terminal -1
	} elseif {-1 != [string first "pedigree-specific" $record]} {
	    set terminal 0
	}

# copy and/or display as required

	if {$terminal == 1} {
	    putsout hlod.out $record
	} else {
	    if {$terminal == -1} {
		set terminal 1
	    }
	    if {$file} {
		putsout -q hlod.out $record
	    }
	}
    }
    close $hf

# Cleanup files and restore user pedlike files, if applicable

    hlod_cleanup
    restore_pedlike hlod

    return "Complete results have been written to [full_filename hlod.out] (use more to read)"
}

proc save_pedlike {prefix} {
    set saved ""
    if {[file exists [full_filename pedlike.info]]} {
	file rename -force [full_filename pedlike.info] [full_filename $prefix.pedlike.info]
	lappend saved pedlike.info
    }
    if {[file exists [full_filename pedlike.out]]} {
	file rename -force [full_filename pedlike.out] [full_filename $prefix.pedlike.out]
	lappend saved pedlike.out
    }
    if {[file exists [full_filename pedlike.dat]]} {
	file rename -force [full_filename pedlike.dat] [full_filename $prefix.pedlike.dat]
	lappend saved pedlike.dat
    }
    return $saved
}

proc restore_pedlike {prefix} {
    set files [glob -nocomplain [full_filename $prefix.pedlike.*]]
#   puts "Restoring files $files"
    foreach file $files {
	if {".info" == [file extension $file]} {
	    file rename -force $file [full_filename pedlike.info]
	} elseif {".out" == [file extension $file]} {
	    file rename -force $file [full_filename pedlike.out]
	} elseif {".dat" == [file extension $file]} {
	    file rename -force $file [full_filename pedlike.dat]
	} else {
#	    puts "file $file didn't match any patterns"
	}
    }
}

	

proc hlod_cleanup {} {
    eval file delete [full_filename pedlike.info pedlike.out pedlike.dat pedlikemod.out homo.out pedlike.start.mod]
}

proc hlod_storepedlike {} {
    set ulist ""
    set pfile [tablefile open [full_filename pedlike.out]]
    tablefile $pfile start_setup
    tablefile $pfile setup Loglikelihood
    while {"" != [set record [tablefile $pfile get]]} {
	lappend ulist [lindex $record 0]
    }
    tablefile $pfile close
    return $ulist
}


# solar::lodadj --
#
# Purpose:  Use or calculate an empirical LOD adjustment
# 
# Usage:    lodadj [-calc] [-off] [-null <N>] [-nreps <#replicates>] [-restart]
#                  [-restorenull0] [-query] [-summary]
#
#              lodadj   If no arguments are given, this turns ON the
#                       previously calculated empirical LOD adjustment
#                       for the current trait/outdir.  This value is
#                       stored in a file named lodadj.info if currently
#                       ON or lodadj.off if currently OFF.
#                       It is an error if the null0 model has a later
#                       timestamp than the lodadj.info file.  (You can
#                       update the timestamp of the lodadj.info file with
#                       the Unix "touch" command if you are sure it is OK.)
#     
#             -off      Turn OFF empirical LOD adjustment.
#
#             -query    Return the LOD adjustment currently in effect
#                       (1.0 if none).
#
#             -calc     Calculate and use a new empirical LOD adjustment.
#                       (This requires an existing null0.mod file from the
#                       polygenic command.)  The adjustment is turned ON.
#
#             -null     Use an existing nullN model instead of null0.
#
#             -nreps    Number of replicates.  In each replicate, a
#                       fully-informative marker, unlinked to the trait,
#                       is simulated, IBDs are calculated for this marker,
#                       and a LOD is computed for linkage of the trait
#                       to this marker.  The default number is 10000.
#
#             -restart  (or -r) Perform additional replicates, adding the
#                       LODs to the set of previously computed LODS, until
#                       the total number of replicates (old and new) reaches
#                       the number specified by the -nreps argument.  The
#                       same null model is used as in the previous replicates;
#                       the -null argument is ignored if present.
#
#             -cutoff   Specify the fraction of the highest observed LODs
#                       that will not be used to compute the empirical LOD
#                       adjustment.  For example, if the cutoff is .01, then
#                       the largest 1% of the observed LODs will be ignored
#                       when the LOD adjustment is calculated.  The default
#                       cutoff is .05.
#
#             -overwrite       (or -ov) Recalculate the empirical LOD
#                              adjustment.  Existing LOD adjustment output
#                              files in the trait/outdir will be overwritten.
#
#             -restorenull0    Restore the null0 model in effect at the time
#                              the last empirical LOD adjustment was
#                              calculated.  This will overwrite a later
#                              null0 model.
#
#             -summary  Display a summary of the LOD adjustment calculation.
#                       The summary shows the distribution of the original
#                       and adjusted LOD scores, the number of replicates
#                       performed, and the name of the null model.
#
# Notes:     The -calc option produces output files in the trait/outdir:
#            lodadj.out, lodadj.lods, and lodadj.info.  lodadj.out contains
#            summary information, lodadj.lods contains the raw actual vs.
#            theoretical LODs, and lodadj.info contains state information
#            including the null model currently in effect.
#
#            The lodadj value and state (on or off) is saved in each
#            trait/outdir (by the lodadj.info or lodadj.off file).  This
#            is now preserved when restarting SOLAR.
#
#            lodadj is now supported for bivariate lods.  Since the
#            correction is always computed with one additional degree of
#            freedom, the lodadj adjustment is applied AFTER the
#            lod correction to one degree of freedom, and the user is
#            advised not to disable the one degree of freedom correction
#            with the lodp command.
#-

# Additional private arguments:
#
#             -inform  <outfile>  Write message to <outfile>

proc lodadj {args} {

    full_filename test  ;# ensure trait/outdir specified

    global Solar_LOD_Adjustment

    set null 0
    set nreps -1
    set calc 0
    set restart 0
    set overwrite 0
    set cutoff -1
    set off 0
    set restorenull0 0
    set query 0
    set informfile 0
    set summary 0

    set badargs [read_arglist $args \
	    -null null \
	    -calc {set calc 1} \
	    -nreps nreps \
	    -restart {set restart 1} -r {set restart 1} \
	    -overwrite {set overwrite 1} -ov {set overwrite 1} \
	    -cutoff cutoff \
	    -off {set off 1} \
	    -restorenull0 {set restorenull0 1} \
	    -verify {set verify 1} \
	    -query {set query 1} \
	    -inform informfile \
	    -summary {set summary 1} \
	    ]

    set use_default_cutoff 0
    if {$cutoff < 0} {
        if {$cutoff == -1} {
            set cutoff .05
            set use_default_cutoff 1
        } else {
            error "Cutoff must be between 0 and 1"
        }
    } elseif {$cutoff > 1} {
        error "Cutoff must be between 0 and 1"
    }

    if {$summary} {
        if {$use_default_cutoff} {
	    lodadj -restart -nreps 0
        } else {
	    lodadj -restart -nreps 0 -cutoff $cutoff
        }
	return ""
    }

    if {$restart} {
        set calc 1
    }

    if {![is_integer $nreps]} {
	error "Invalid nreps value; must be integer"
    }
    if {$nreps != -1} {
	set calc 1
    } else {
	set nreps 10000
    }

    if {![is_integer $null] || $null < 0} {
	error "Invalid null value; must be a non-negative integer"
    }

    if {$off} {
	set Solar_LOD_Adjustment -1.0
	catch {
	    file rename -force [full_filename lodadj.info] [full_filename lodadj.off]
	}
	return ""
    }

    if {$restorenull0 == 1} {
	if {![file exists [full_filename lodadj.info]]} {
	    error "Can't find lodadj.info from previous lodadj -calc"
	}
	set ifile [open [full_filename lodadj.info] r]
	gets $ifile irecord
	set null 0
	if {[llength $irecord] == 6 || [llength $irecord] == 8}  {
	    set null [lindex $irecord 5]
	}
	set ofile [open [full_filename null$null.mod] w]
	while {-1 != [gets $ifile irecord]} {
	    puts $ofile $irecord
	}
	close $ifile
	close $ofile
	load model [full_filename null$null.mod]
	exec touch [full_filename lodadj.info]
	return [lodadj]
    }


#  *** QUERY ***

    if {$query} {

# If lodadj.info doesn't exist in current trait/outdir, lodadj is -off

	if {![file exists [full_filename lodadj.info]]} {
	    set Solar_LOD_Adjustment -1.0
	    set lodadj_value 1.0
	} else {

# Otherwise, read lodadj from lodadj.info

	    set lodadj_value [lodadj]

	}

# Output message to "informfile" (e.g. multipoint.out) if required

	if {$lodadj_value != 1.0} {
	    if {[string compare $informfile 0]} {
		if {"stderr" == $informfile || "stdout" == $informfile} {
		    set ifile $informfile
		    set mustclose 0
		} else {
		    set ifile [open [full_filename $informfile] a]
		    set mustclose 1
		}
		puts $ifile \
		 "\n    *** Using LOD Adjustment:  [format %.5f $lodadj_value]"
		if {$mustclose} {close $ifile}
	    }
	}

# Return lodadj value and QUERY is done

	return $lodadj_value
    }

# Now if not calculating, must be turning LOD adjustment on
#   (Or, knowing it's on, reading its value)

    if {!$calc} {
	if {![file exists [full_filename lodadj.info]]} {
	    if {[file exists [full_filename lodadj.off]]} {
		file rename -force [full_filename lodadj.off] [full_filename lodadj.info]
	    } else {
		error \
"No stored LOD adjustment found for this trait (lodadj.info|off not found)"
            }
        }
	set lodadj_time [file mtime [full_filename lodadj.info]]
	set ifile [open [full_filename lodadj.info] r]
	set lastring [gets $ifile]
        set ntraits 0
	close $ifile
	set null 0
	if {[llength $lastring] == 6 || [llength $lastring] == 8}  {
	    set null [lindex $lastring 5]
	    if {![is_integer $null]} {
	        purge_global Solar_LOD_Adjustment
	        error "Invalid lodadj.info file"
	    }
	}
	set null_time [file mtime [full_filename null$null.mod]]
	if {$lodadj_time < $null_time} {
	    error "The null$null model was changed after last lodadj -calc"
	}
	set Solar_LOD_Adjustment [lindex $lastring 1]
	if {![is_float $Solar_LOD_Adjustment]} {
	    purge_global Solar_LOD_Adjustment
	    error "Invalid lodadj.info file"
	}
	return $Solar_LOD_Adjustment
    }

# Calculate LOD adjustment...
    
    set outfile [full_filename lodadj.lods]

    if {![file exists [full_filename null$null.mod]]} {
	if {!$null} {
	    error "Model [full_filename null$null] not found.\
\nThis can be created with polygenic command."
	} else {
	    error "Model [full_filename null$null] not found."
	}
    }

    set soutf [full_filename siminf.out]
    set sibdf [full_filename ibd.siminf]

    set quiet 1
    if {[verbosity -number] > 0} {
        set quiet 0
    }

    set rep1 1
    set llods {}

    if {[file exists $outfile]} {
        if {$restart} {
            if {[file exists [full_filename lodadj.off]]} {

# If LOD adjustment is currently off, turn it on to restore lodadj.info

                lodadj
            }
            if {[file exists [full_filename lodadj.info]]} {

# If LOD adjustment is now on, get null model index from lodadj.info;
#   otherwise, lodadj.info was not created in previous run, so null
#   model index will come from -null argument or default to 0

                set ifile [open [full_filename lodadj.info] r]
                set lastring [gets $ifile]
                close $ifile
                set null 0
                if {[llength $lastring] == 6 || [llength $lastring] == 8}  {
                    set null [lindex $lastring 5]
                }
                if {$use_default_cutoff} {
                    if {[llength $lastring] == 8}  {
                        set cutoff [lindex $lastring 7]
                    }
                }
            }
            set outfd [open $outfile r]
            while {-1 < [gets $outfd line]} {
                if {1 == [scan $line "%f" tlod]} {
                    lappend llods $tlod
                    incr rep1
                } else {
                    error "Error reading [full_filename lodadj.lods]"
                }
            }
            close $outfd
            if {$nreps < $rep1} {
                set nreps [expr $rep1 - 1]
            }
            set outfd [open $outfile a]
        } else {
            if {!$overwrite} {
	        error "LOD adjustment output files exist. Use -overwrite option."
            }
            set outfd [open $outfile w]
        }
    } else {
        set outfd [open $outfile w]
    }

# Remove lodadj state file (if one exists) so that any stored LOD
#   adjustment information is completely gone before computing new
#   or additional lodadj reps

    delete_files_forcibly [full_filename lodadj.info]
    delete_files_forcibly [full_filename lodadj.off]

#    catch {
#	load model [full_filename null$null]
#        set ntraits [llength [trait]]
#        if {1 < $ntraits} {	
#	    puts "\n    Warning!  lodadj is not supported for Bivariate models!"
#	    puts "    You can calculate adjustment, but it may be inaccurate"
#	    puts "    and it will not be used.\n"
#	}
#    }

    for {set rep $rep1} {$rep <= $nreps} {incr rep} {
        if {!$quiet} {
            puts -nonewline "Computing LOD adjustment, replicate $rep "
            flush stdout
        }
        load model [full_filename null$null.mod]
        siminf -out $soutf -ibd $sibdf
        linkmod -add $sibdf
        set errmsg [maximize_quietly last]
        if {$errmsg != ""} {
            puts "    *** Error maximizing, rep $rep"
            set rep [expr $rep - 1]
            continue
        }

        lappend llods [lodn $null]
        puts $outfd [lodn $null]
        flush $outfd

        if {!$quiet} {
            puts -nonewline "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
            puts -nonewline "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
            flush stdout
        }
    }

    close $outfd
    set outfd [open $outfile w]

    set sllods [lsort -real $llods]
    set sxx 0
    set sxy 0

    for {set rep 0} {$rep < $nreps} {incr rep} {
        set x [lindex $sllods $rep]
        set t [expr double($rep) / double($nreps) ]
        if { $t <= 0.5} {
            set y 0
        } else {
            set y [chi -inverse [expr 2 * (1 - $t)] 1]
            set y [expr $y / 4.60517]
        }
        puts $outfd "$x $y"
        if {$rep < [expr (1 - $cutoff) * $nreps]} {
            set sxx [expr $sxx + $x * $x]
            set sxy [expr $sxy + $x * $y]
        }
    }

    close $outfd

    global Solar_LOD_Adjustment
    purge_global Solar_LOD_Adjustment
    set Solar_LOD_Adjustment 1
    if {$sxx} {
        set Solar_LOD_Adjustment [expr $sxy / $sxx]
    }

    set o5 0
    set o9 0
    set o95 0
    set o975 0
    set o99 0
    set o999 0
    set o9999 0
    set a5 0
    set a9 0
    set a95 0
    set a975 0
    set a99 0
    set a999 0
    set a9999 0

    for {set rep 1} {$rep <= $nreps} {incr rep} {
        set y [lindex $sllods [expr $rep - 1]]
        if {$y < 3.003382} {
            set o9999 $rep
            if {$y < 2.073655} {
                set o999 $rep
                if {$y < 1.175178} {
                    set o99 $rep
                    if {$y < 0.834162} {
                        set o975 $rep
                        if {$y < 0.587501} {
                            set o95 $rep
                            if {$y < 0.356637} {
                                set o9 $rep
                                if {$y < 0.0000001} {
                                    set o5 $rep
                                }
                            }
                        }
                    }
                }
            }
        }

        set ay [expr $y * $Solar_LOD_Adjustment]
        if {$ay < 3.003382} {
            set a9999 $rep
            if {$ay < 2.073655} {
                set a999 $rep
                if {$ay < 1.175178} {
                    set a99 $rep
                    if {$ay < 0.834162} {
                        set a975 $rep
                        if {$ay < 0.587501} {
                            set a95 $rep
                            if {$ay < 0.356636} {
                                set a9 $rep
                                if {$ay < 0.0000001} {
                                    set a5 $rep
                                }
                            }
                        }
                    }
                }
            }
        }
    }

    set outfile [full_filename lodadj.out]
    set outfd [open $outfile w]

    puts "                                                     "
    puts $outfd "                                                     "
    puts "    LOD Correction Constant = [format %10.6f $Solar_LOD_Adjustment]"
    puts $outfd "    LOD Correction Constant = [format %10.6f $Solar_LOD_Adjustment]"
    puts ""
    puts $outfd ""
    puts "    LOD          % Original    % Adjusted    % Normal"
    puts $outfd "    LOD          % Original    % Adjusted    % Normal"
    puts "    ---------    ----------    ----------    --------"
    puts $outfd "    ---------    ----------    ----------    --------"

    set opct [expr double($o5) / $nreps]
    set apct [expr double($a5) / $nreps]
    puts \
"    =  0          [format %8.6f $opct]      [format %8.6f $apct]      0.5000"
    puts $outfd \
"    =  0          [format %8.6f $opct]      [format %8.6f $apct]      0.5000"

    set opct [expr 1 - (double($o9) / $nreps)]
    set apct [expr 1 - (double($a9) / $nreps)]
    puts \
"    >= 0.357      [format %8.6f $opct]      [format %8.6f $apct]      0.1000"
    puts $outfd \
"    >= 0.357      [format %8.6f $opct]      [format %8.6f $apct]      0.1000"

    set opct [expr 1 - (double($o95) / $nreps)]
    set apct [expr 1 - (double($a95) / $nreps)]
    puts \
"    >= 0.588      [format %8.6f $opct]      [format %8.6f $apct]      0.0500"
    puts $outfd \
"    >= 0.588      [format %8.6f $opct]      [format %8.6f $apct]      0.0500"

    set opct [expr 1 - (double($o975) / $nreps)]
    set apct [expr 1 - (double($a975) / $nreps)]
    puts \
"    >= 0.834      [format %8.6f $opct]      [format %8.6f $apct]      0.0250"
    puts $outfd \
"    >= 0.834      [format %8.6f $opct]      [format %8.6f $apct]      0.0250"

    set opct [expr 1 - (double($o99) / $nreps)]
    set apct [expr 1 - (double($a99) / $nreps)]
    puts \
"    >= 1.175      [format %8.6f $opct]      [format %8.6f $apct]      0.0100"
    puts $outfd \
"    >= 1.175      [format %8.6f $opct]      [format %8.6f $apct]      0.0100"

    set opct [expr 1 - (double($o999) / $nreps)]
    set apct [expr 1 - (double($a999) / $nreps)]
    puts \
"    >= 2.074      [format %8.6f $opct]      [format %8.6f $apct]      0.0010"
    puts $outfd \
"    >= 2.074      [format %8.6f $opct]      [format %8.6f $apct]      0.0010"

    set opct [expr 1 - (double($o9999) / $nreps)]
    set apct [expr 1 - (double($a9999) / $nreps)]
    puts \
"    >= 3          [format %8.6f $opct]      [format %8.6f $apct]      0.0001"
    puts $outfd \
"    >= 3          [format %8.6f $opct]      [format %8.6f $apct]      0.0001"
    puts ""
    puts $outfd ""

    puts "    #Reps: $nreps    Cutoff: $cutoff    Null Model: [full_filename null$null.mod]"
    puts $outfd "    #Reps: $nreps    Cutoff: $cutoff    Null Model: [full_filename null$null.mod]"
    puts ""
    puts $outfd ""

    close $outfd

    delete_files_forcibly $soutf $sibdf.gz
    eval exec echo "#lodadj: $Solar_LOD_Adjustment  nreps: $nreps  null: $null  cutoff: $cutoff" \
         >[full_filename lodadj.info]
    eval exec cat [full_filename null$null.mod] >>[full_filename lodadj.info]
}


# solar::empp --
#
# Purpose:  Calculate an empirical p-value from lodadj results
# 
# Usage:    empp <lod>
#-

proc empp {args} {

    if {[catch {trait}]} {
	error "Trait must be specified first"
    }

    set lod [read_arglist $args ]
    if {$lod == ""} {
        error "A LOD must be specified"
    }
    if {[scan $lod "%f" tmp] != 1 || $lod <= 0} {
        error "A LOD greater than 0 must be specified"
    }

    if {![file exists [full_filename lodadj.lods]]} {
        error "File [full_filename lodadj.lods] not found. Have you run lodadj?"
    }

    set lodfile [open [full_filename lodadj.lods] r]
    set n 0
    set p 0
    while {-1 != [gets $lodfile record]} {
        incr n
        if {[scan $record "%f" olod] != 1} {
            error "Error reading [full_filename lodadj.lods], line $n"
        }
        if {$olod < $lod} {
            incr p
        }
    }

    if {$p < $n} {
        puts "p = [format "%.7g" [expr 1 - $p/double($n)]]"
    } else {
        puts "p < [format "%.7g" [expr 1/double($n)]]"
    }

    if {$n < 10000} {
        puts \
"Warning: Only $n observations in [full_filename lodadj.lods]; p-value may not be reliable."
    }
}

# solar::loadkin --
#
# Purpose:  Load a matrix named phi2.gz containing phi2 and delta7
#
# Usage:    loadkin
#
# Notes:    If the file phi2.gz does not exist, this command will be
#           silently ignored.  This command is mainly for scripts.  You
#           can perform the same action with a known matrix file with:
#
#           matrix load phi2.gz phi2 delta7
# -

proc loadkin {} {
    if {[file exists "phi2.gz"]} {
	matrix load phi2.gz phi2 delta7
    }
}

#
# Write to file AND terminal
#
proc putstee {args} {
    if {0==[string compare [lindex $args 0] -nonewline]} {
	puts -nonewline [lindex $args 2]
	puts -nonewline [lindex $args 1] [lindex $args 2]
    } else {
	puts [lindex $args 1]
	puts [lindex $args 0] [lindex $args 1]
    }
}

#
# Write (append) to filename AND terminal
#
proc putsteer {args} {
    if {0==[string compare [lindex $args 0] -nonewline]} {
	puts -nonewline [lindex $args 2]
	set ofile [open [lindex $args 1] a]
	puts -nonewline $ofile [lindex $args 2]
	close $ofile
    } else {
	puts [lindex $args 1]
	set ofile [open [lindex $args 0] a]
	puts $ofile [lindex $args 1]
	close $ofile
    }
}


# solar::putsout
#
# Purpose:  Write message to terminal and/or file [obsolescent]
#
# NOTE: This is inefficient and no longer recommended.  New code should
#       use putsa to append to a file and putsat to append to file
#       and write to terminal.  See help putso.
#
# Usage:    putsout [-q] [-d.] [-nonewline] <filename> <message>
#
#           -q            No output to terminal
#           -d.           Write file to current directory
#           -nonewline    As with puts command (note: may delay output)
#           <filename>    *name* of file in current output directory (outdir)
#           <message>     string
#
# Simple Example:  putsout mine.out "The result was $result"
#
#
# Advanced Example: (Beginners ignore!)
#
#            set q ""
#            ifverbmax set q "-q"
#            eval putsout $q \"Iteration: $i   Value: $value\"
#
# Note: If using a variable for -q which could be "", be sure to use
#       eval, otherwise "" would be considered filename argument, then
#       remember to \ the quotes or they disappear during the eval.
# -


proc inout {args} {
    return $args
}

proc putsout {args} {
    set terminal 1
    if {"-q" == [lindex $args 0]} {
	set terminal 0
	set args [lrange $args 1 end]
    }
    upvar putsout_quiet putsout_quiet
    if {![catch {set foo $putsout_quiet}] && \
	    $putsout_quiet} {
	set terminal 0
    }

    set outfunc full_filename
    if {"-d." == [lindex $args 0]} {
	set outfunc inout
	set args [lrange $args 1 end]
    }
    if {0==[string compare [lindex $args 0] -nonewline]} {
	if {$terminal} {puts -nonewline [lindex $args 2]}
	if {![catch {set ofile [open [$outfunc [lindex $args 1]] a]}]} {
	    puts -nonewline $ofile [lindex $args 2]
	    close $ofile
	}
    } else {
	if {$terminal} {puts [lindex $args 1]}
	if {![catch {set ofile [open [$outfunc [lindex $args 0]] a]}]} {
	    puts $ofile [lindex $args 1]
	    close $ofile
	}
    }
}

# solar::putsnew --
# solar::putsat --
# solar::putsa --
#
# Purpose: Write to file and/or terminal without having to open and close
#
# Usage: putsnew <filename>            ;# create new file or truncate old file
#        putsa <filename> <string>     ;# append to file
#        putsat <filename> <string>    ;# append to file and write to terminal
#
# Notes:
#
# 1.  putsnew creates the file if not yet created, or truncates existing
#     file to zero length.  This is generally needed before starting to
#     append to file using putsa unless you know the file already exists.
#
# 2.  These procedures are NOT as efficient as Tcl's built-in operations
#     for writing to a file (open, puts, close).  If efficiency is
#     paramount, and you have a tight writing loop not subject to
#     exceptions or with fully handled exceptions, it is preferable to
#     use the Tcl operations.  However these procedures are believed to
#     be as efficient as shell append piping (>>).  These procedures are
#     considerably more efficient than the now obsolescent "putsout".
#
# 3.  putsa writes (appends) to the existing file
#
# 4.  putsat writes (appends) to the existing file, and to the terminal
#
# 5.  If an absolute filename is not specified, the file path is relative to
#     the current working directory.  If you need to write to a file in the
#     maximization output directory, use the procedure full_filename first
#     to produce the required pathname.  Beware that the pathname produced
#     by full_filename is also relative to the current working directory, so
#     that if you change the current working directory, or the trait, or the
#     outdir, you should run full_filename again, if you want to continue
#     writing to a file in the current maximization directory (though it
#     will then be a different directory and a different file).
#
#  Example:
#
#       # writing to file in maximization output directory
#       set myfullname [full_filename myname.out]
#       putsnew $myfullname
#       putsat $myfullname "Writing to $myfullname"
#       putsa $myfullname "writing more, but not to terminal this time"
#       trait newtrait
#       set myfullname [full_filename myname.out]
#       putsnew $myfullname
#       putsat $myfullname "Now writing to $myfullname"
# -

proc putsnew {filename} {
    set outfile [open $filename w]
#    puts -nonewline $outfile ""
    close $outfile
}

proc putsa {filename string} {
    set outfile [open $filename a]
    puts $outfile $string
    close $outfile
}

proc putsat {filename string} {
    set outfile [open $filename a]
    puts $outfile $string
    close $outfile
    puts $string
}

# solar::full_filename --
#
# Purpose:  Prepend the maximization output directory name to filename(s)
#
# Usage:    full_filename [<filename>]+
#
# Note:     See "help outdir".  full_filename is intended for scripts.
# -

proc full_filename {args} {
    if {[if_global_exists Solar_Out_Dir]} {
	global Solar_Out_Dir
	set pathname $Solar_Out_Dir
    } else {
	if {[catch {set pathname [join [trait] .]}]} {
	    error "Trait, outdir, or model must be specified first"
	}
    }
    if {"Must_give_command_model_new" == $pathname} {
	error "Invalid trait change; must give command 'model new' first"
    }
    if {![file exists $pathname]} {file mkdir $pathname}
    set outputnames {}
    foreach filename $args {
	lappend outputnames "$pathname/$filename"
    }
    return $outputnames
}
 
#
# Procedures to get information from model files
#

# solar::lod_or_slod_n -- private
#
# Purpose:  Calculate current lod or slod relative to nullN
#
# Usage: current_lod_or_slod_n ifslod n  ;# ifslod is 0 or 1
# -
proc lod_or_slod_n {ifslod n} {
    if {$ifslod==1} {
	set newn [expr $n + 1]
	set rlod [slod h2q$newn]
    } else {
	set rlod [lodn $n]
    }
    return $rlod
}

# solar::oldlod -- private
#
# Purpose:  Calculate LOD score for current model (relative to nullX.mod)
#
# Usage:    lod
#
# Notes:    The current model must have been maximized, either through the
#           "twopoint" or "multipoint" command, or directly with the
#           "maximize" command.
#
#           The null model should be saved as nullX.mod (for example, null0.mod
#           or null1.mod) where X is the number of active linkage elements,
#           which is assumed to be one less than the current model for this
#           command.  Linkage parameters must be specified as h2q1, h2q2,
#           etc.  The null model should be saved in the maximization output
#           directory.  Then, the loglikelihood of the current model is
#           compared with the loglikelihood of the model in the applicable
#           null model to get the LOD.
#
#           SOLAR LODs are intended to be easy to interpret in the usual
#           cases.  By default, SOLAR applies empirical LOD adjustment, if
#           currently in effect (see lodadj command), and also converts the
#           LOD to be equivalent to a 1dF univariate LOD.  If you need to
#           change these defaults, use the lodp command, which sets LOD
#           calculation preferences.  LODs are actually calculated by
#           the clod command, which lets you specify the likelihood values
#           and options directly.  The "lod" command is merely a simplified
#           interface to clod.
#
#           If you are attempting to do some special statistical test,
#           for example, involving the constraint of rho parameters, it
#           may be preferable to use the raw loglikelihood values, which are
#           reported in models and multipoint1.out files, and accessible
#           for the current model with the loglike command.  That way any
#           default conversions built in to the LOD calculation are bypassed.
#
#           See also: lodp, clod, lodn, lodadj, loglike, read_model
# -

# solar::lodn --
#
# Purpose:  Calculate LOD score for current model relative to nullX
#
# Usage:    lodn X <options>
#
#                X   Number indicating index of relevant null model (for
#                    example, 0 for null0, the model having no linkage
#                    elements).
#                <options>  See "help lod".
#
# Notes:    In many cases you can more easily use the "lod" command, which
#           determines the applicable null model automatically, or, you can
#           specify the loglikelihoods.  "lodn" may be useful if you
#           are not sure whether the current model contains h2q parameters,
#           for example, if it includes a custom parameterization.
#
#           The current model must have been maximized, either through the
#           "twopoint" or "multipoint" command, or directly with the
#           "maximize" command.
#
#           The null model should be saved as nullX.mod (for example, null0.mod
#           or null1.mod) where X is the number of active linkage elements.
#
#           There are many special options for LOD calculation.  See
#           "help lodp" for more information.  The primary LOD calculating
#           procedure in SOLAR is "lod" which lets you specify the
#           loglikelihood values and option(s) directly.
#
#           See also lod, lodp.
# -

proc lodn {N args} {return [eval lod [loglike] [nulln $N loglike] $args]}

# solar::lodp
#
# Purpose:  Change LOD preferences (such as conversion to 1df)
#
# Usage:    lodp [-auto | -off | [-traits <N>] [-rhoq <N>]]
#                [-1t|-2t|-3t|-4t|-t1|-t2|-t3|-t4]
#
#           (If no argument is given, current preferences are displayed.)
#
#           -auto    Convert LODs to 1 degree of freedom (1dF) effective LODs 
#                    automatically based on traits and rhoq constraints
#                    in current model (default).
#
#           -off     Do not perform LOD conversion to 1 df equivalence
#
#           -traits <N>      Convert assuming there are/were <N> traits
#           -1t or -t1 etc.  Shortcuts for 1 trait, 2 traits, etc. up to 4
#           -rhoq <N>        Convert assuming <N> rhoq's are constrained
#
# Notes:    If -traits is specified without -rhoq, -rhoq is assumed to be 0.
#           If -rhoq is specified without -traits, trait count is determined
#           automatically (and might be determined to be 1, in which case
#           rhoq specification is irrelevant).  If you need to set both
#           -traits and -rhoq, you must give both in the same lodp command.
#
#           This should not be confused with lodadj (see).  The lodp command
#           sets global preferences for "degrees of freedom" issues which
#           arise with multivariate models.  The default "-auto" will
#           apply conversion based on the number of traits in the current
#           model and the number of relevant rhoq's (defined below) which
#           are constrained.  LODs will be converted to 1 degree of freedom
#           effective LODs (for which the traditional cutoff in statistical
#           genetics for a genome-wide linkage scan is 3).
#
#           Relevant rhoq's are parameters prefixed rhoq which correspond
#           to the highest numbered linkage element.  For example, in a
#           bivariate linkage model with one linkage element, the relevant
#           rhoq whould be "rhoq1", but with two linkage elements, it would
#           be "rhoq2".  For a trivariate model with one linkage element,
#           the relevant rhoq's would be: rhoq1_12, rhoq1_13, rhoq1_23.
#
#           The preferences set by this command will apply to all LOD scores
#           subsequently calculated by SOLAR, including those reported by
#           the twopoint and multipoint commands, and the lod and lodn
#           commands.  The lod command, which is what ultimately
#           calculates all LOD scores, has options which are similar to
#           lodp.
#
#           Changes to lodp preferences apply only within the current
#           SOLAR session, so the command must be re-entered each time
#           SOLAR is started or at the beginning of SOLAR scripts when
#           you need to change the defaults.
#
#           For more discussion of the how the conversion is performed,
#           which rhoq constraints are relevant, etc., see help for
#           the lod command.
#           
#           See also lod, lodn, and lodadj.
#-

proc loddf {args} {
    error "loddf has been replaced by the lodp command; see help lodp"
}

proc lodp {args} {

# If no arguments, return current state from globals
# If all globals are blank, return default state -auto

    global SOLAR_LODP
    global SOLAR_LODP_RHOQ
    if {{} == $args} {
	set lodp ""
	set rho ""
	if {[if_global_exists SOLAR_LODP]} {
	    set lodp $SOLAR_LODP
	    if {"Error!" == $lodp} {
		error "Error in last lodp specification!"
	    }
	}
	if {[if_global_exists SOLAR_LODP_RHOQ]} {
	    set rho $SOLAR_LODP_RHOQ
	}
	set state -auto
	if {"" != $lodp} {
	    set state $lodp
	    if {"" != $rho} {
		set state "$lodp $rho"
	    }
	} elseif {"" != $rho} {
	    set state $rho
	}
	return $state
    }

# If attempt to change preferences fails, we fall into an error state

    set SOLAR_LODP Error!
    set SOLAR_LODP_RHOQ ""

# Handle -off

    if {-1 != [lsearch -exact $args "-off"]} {
	if {"-off" != $args} {
	    error "lodp: -off is incompatible with other arguments"
	}
	set SOLAR_LODP -off
	set SOLAR_LODP_RHOQ ""
	return ""
    }

# Handle -auto

    if {-1 != [lsearch -exact $args "-auto"]} {
	if {"-auto" != $args} {
	    error "lodp: -auto is incompatible with other arguments"
	}
	set SOLAR_LODP -auto
	set SOLAR_LODP_RHOQ ""
	return ""
    }

# Handle -trait and -rhoq

    set traits ""
    set rhoq ""

    set badargs [read_arglist $args \
		     -traits traits \
		     -rhoq rhoq \
		     -1t {set traits 1} \
		     -2t {set traits 2} \
		     -3t {set traits 3} \
		     -4t {set traits 4} \
		     -t1 {set traits 1} \
		     -t2 {set traits 2} \
		     -t3 {set traits 3} \
		     -t4 {set traits 4} \
		    ]

    if {"" != $badargs} {
	error "lodp: Invalid arguments: $badargs"
    }

    if {"" != $traits} {
	ensure_integer $traits
    }

    if {"" != $rhoq} {
	ensure_integer $rhoq
    }


# At this point, no more errors are possible

    set SOLAR_LODP ""
    set SOLAR_LODP_RHOQ ""

    if {"" != $traits} {
	set SOLAR_LODP "-traits $traits"
    }

    if {"" != $rhoq} {
	set SOLAR_LODP_RHOQ "-rhoq $rhoq"
    }
    
    return ""
}


# solar::clod --
# solar::lod --
#
# Purpose:  Calculate LOD score
#
# Usage:    lod [<test-loglike> <null-loglike>] [<options>]
#           options := [-auto|-off|-raw] [-trait <N>] [-rhoq <N>] [-v]
#                      [-1t|-2t|-3t|-4t|-t1|-t2|-t3|-t4] [-nolodadj]
#
#           If no likelihoods are specified, the likelihoods of the
#           "current" model and the applicable "null" model are used.
#
#           -auto        Convert multivariate LOD to 1df effective LODs based
#                          on number of traits in current model and constraint
#                          of relevant rhoq's (default)
#           -off         Do not convert LODs to 1df effective
#           -raw         Do not perform LOD conversion or lodadj
#           -traits <N>  Convert multivariate LOD to 1dF assuming <N> traits
#           -1t or -t1   Assume 1 trait (same as "-traits 1")
#           -2t or -t2   Assume 2 traits (same as "-traits 2")
#           -3t or -t3   Assume 3 traits (same as "-traits 3")
#           -4t or -t4   Assume 4 traits (same as "-traits 4")
#           -rhoq <N>    Convert multivariate LOD to 1df assuming <N>
#                          constraints of relevant rhoq's
#           -nolodadj    Do not perform lod adjustment (lodadj)
#           -v           verbose: Show adjustment and conversion steps
#
# Examples: outdir test1
#           load model test1/null1
#           lod
#           lod -v
#           lod -2000.51 -2030.87
#           lod -trait 3 -rhoq 1 -v -2000 -2030
#           lod -raw -2000 -2030
#
# Notes:    If no likelihoods are specified, the current model must have
#           been maximized through a command such as "maximize," "twopoint",
#           or "multipoint", and the applicable null model should be saved as
#           nullX.mod (e.g. null0.mod, null1.mod) where X is the number
#           of active linkage elements, which is assumed to be one less
#           linkage element than in the current model.  Linkage elements are
#           parameters named h2q1, h2q2, etc.  The null model must have
#           been saved in the maximization output directory, either named
#           after the trait or set by the outdir command.
#
#           By default, SOLAR provides easily interpreted "1 df effective" LODs
#           which are equivalent to those in univariate models.
#           However, you can also have complete control over the LOD
#           conversion performed either using arguments here or
#           preferences set globally with the lodp command.  Options
#           specified here override the defaults and lodp preferences.
#
#           The correction of 2 trait LODs to 1dF effective LODs is based
#           on this formula: the LOD is converted to chi square with
#           1/2 1df, 1/4 3df, and 1/4 point mass at zero.  If rhoq is
#           constrained, the formula is 1/2 1df, 1/4 2df, and 1/4
#           point mass at zero.  This is then converted to a 1/2 1df
#           chi square of equivalent p-value, which is divided by
#           2ln10 to get the 1df effective lod score.
#
#           The correction of 3 trait LODs to 1dF effective LODs is based
#           on the formula: the LOD is converted to chi square with
#           3/8 1df, 3/8 3df, 1/8 6df, and 1/8 point mass at zero.
#           For each rhoq constrained, the 6df is changed downward
#           by 1df.
#
#           The conversion of higher multivariate LODs follows a similar
#           expanding sum.  If you wish to see the weights used, use the
#           lod command with the -v option.
#
#           Empirical LOD adjustment, if any, is automatically applied (see
#           the lodadj command) unless the -raw option is used.  Unless you
#           specify -raw, SOLAR will need to search the output directory for
#           a lodadj.info file, which means that a trait or outdir must
#           have been selected.
#
#           Empirical LOD adjustment is not yet supported for bivariate
#           models.  The lodadj value is ignored when bivariate LODs are
#           computed, and, in the cases where the lodadj value would be
#           shown (such as in the multipoint.out file, or if lod is called
#           from the command prompt) a warning message is shown instead.
#
#           In SOLAR version 3.0.2, the "clod" and "lod" commands were
#           combined into a new "lod" command.  The options allowed
#           have changed compared with the earlier "clod" ; the original
#           "lod" command did not allow any arguments.
#
#           Use the "lodn" command if you the current model may not use
#           the "h2q1" linkage parameter and you are not specifying
#           loglikelihoods explicitly.
#           
#           See also lodn, lodp, lodadj.
# -

proc clod {args} {
    return [eval lod $args]
}

proc lod {args} {

# Read and check arguments...note they changed in 3.0.2

    set auto_arg ""
    set traits_arg ""
    set rhoq_arg ""
    set cverb 0
    set nolodadj 0

    set nonoptions [read_arglist $args \
			-auto {set auto_arg -auto} \
			-off {set auto_arg -off} \
			-raw {set auto_arg -raw} \
			-traits traits_arg \
			-1t {set traits_arg 1} \
			-2t {set traits_arg 2} \
			-3t {set traits_arg 3} \
			-4t {set traits_arg 4} \
			-t1 {set traits_arg 1} \
			-t2 {set traits_arg 2} \
			-t3 {set traits_arg 3} \
			-t4 {set traits_arg 4} \
			-rhoq rhoq_arg \
			-nolodadj {set nolodadj 1} \
			-v {set cverb 1} \
			-* returned
		       ]

# If no likelihoods specified, get then from current and null models

    if {[llength $nonoptions] == 0} {
	set x [expr [h2qcount] - 1]
	if {$x < 0} {
	    error \
		    "lod: Unable to determine required null model\n\
Current model does not have a h2q1 parameter\n\
Note: Specify loglikelihoods or use lodn command for custom parameterizations"
	}
	return [eval lodn $x $args]
    }
    if {[llength $nonoptions] == 1} {
	error "lod: Invalid option or only one loglikelihood specified"
    }

    set invalid_options [lrange $nonoptions 2 end]

    set logl [lindex $nonoptions 0]
    set null_logl [lindex $nonoptions 1]

    if {![is_float $logl]} {
	lappend invalid_options $logl
    }
    if {![is_float $null_logl]} {
	lappend invalid_options $null_log
    }
    if {{} != $invalid_options} {
	error "lod: Invalid options:  $invalid_options\n\
Note: Options have changed!  See help for lod."
    }

    if {"" != $traits_arg} {
	ensure_integer $traits_arg
    }

    if {"" != $rhoq_arg} {
	ensure_integer $rhoq_arg
    }

# Initialize default preferences

    set auto_mode -auto
    set assume_traits ""
    set assume_rhoq ""

# Get preferences from lodp

    set lodp [lodp]
    if {$cverb} {
	puts "Preferences from lodp: $lodp"
    }
    if {[string_imatch $lodp -auto]} {
	set auto_mode -auto
    } elseif {[string_imatch $lodp -off]} {
	set auto_mode -off
    } else {
	read_arglist $lodp \
	    -traits assume_traits \
	    -rhoq assume_rhoq
	set auto_mode ""
    }

# Let arguments supercede preferences

    if {"" != $auto_arg} {
	set auto_mode $auto_arg
	set assume_traits ""
	set assume_rhoq ""
    }

    if {"" != $traits_arg} {
	set assume_traits $traits_arg
	set assume_rhoq ""
	set auto_mode ""
    }

    if {"" != $rhoq_arg} {
	set assume_rhoq $rhoq_arg
	set auto_mode ""
    }

# Assume 0 constraints if traits specified but contraints not specified

    if {"" != $assume_traits && "" == $assume_rhoq} {
	set assume_rhoq 0
    }

# assume we need to convert lod if mode defaulted or specified auto
#   and 1 trait NOT assumed...then reset if we find only 1 trait

    set convert_lod 0

    if {"-auto" == $auto_mode || \
	    "" == $auto_mode && ![string_imatch 1 $assume_traits]} {
	set convert_lod 1
    }

# Now, get information from current model IF we need it

    set use_traits $assume_traits
    set c_rhoqs $assume_rhoq

# See how many traits...if 1, then no conversion anyway

    if {$convert_lod} {

	if {"" == $assume_traits} {
	    if {[catch {set use_traits [llength [trait]]}]} {
		error "lod: No current model...need to know how many traits"
	    } elseif {$cverb} {
		puts "Found $use_traits traits"
	    }
	}
	if {$use_traits == "1"} {
	    set convert_lod 0
	}
    }

# Search for constrained rhoqs at the current h2q level

    if {$convert_lod && "" == $assume_rhoq} {

	set finished 0
	catch {
	    set c_rhoqs 0
	    set index [h2qcount]
	    set vclist [get_vc_list]
	    set vcend [expr 3 + [string length $index]]
	    foreach vc $vclist {
		if {[string_imatch rhoq$index [string range $vc 0 $vcend]]} {
		    set lcindex [expr $vcend + 1]
		    set endchar [string range $vc $lcindex $lcindex]
		    if {![string compare "" $endchar] || \
			    ![string compare _ $endchar]} {
			if {![catch {find_simple_constraint $vc}]} {
			    incr c_rhoqs
			}
		    }
		}
	    }
	    set finished 1
	}
	if {!$finished} {
	    error "Unable to determine if any rhoq parameters are constrained."
	}
	if {$cverb} {
	    puts "Found $c_rhoqs constrained rhoq's"
	}
    }

    if {"" == $c_rhoqs} {
	set c_rhoqs 0
    }

# Compute LOD, return here if raw mode

    set rawlod [expr ($logl - $null_logl) / log (10)]
    if {$cverb} {
	puts "Raw LOD = ($logl - $null_logl) / log(10) = $rawlod"
    }
    if {"-raw" == $auto_mode} {
	return $rawlod
    }

# See if lod correction to 1df is required

    if {!$convert_lod} {
	if {$cverb} {
	    puts \
"Only one trait so correction to 1df is not required"
	}
	set lod1 $rawlod
    } else {

# Convert bivariate or trivariate LOD to 1dF

    if {$cverb} {
        puts \
           "Correcting LOD to 1df from $use_traits traits and $c_rhoqs constrained rhoq's"
    }

    set chisq [expr 2*log(10)*$rawlod]

    set negate 0
    if {$chisq < 0} {
	set chisq [expr 0.0 - $chisq]
	set negate 1
    }

    if {$cverb} {puts "Chisq = 2*log(10)*abs(LOD) =  $chisq"}

    set p 0.0
    set n $use_traits
    for {set i 1} {$i <= $n} {incr i} {
	set weight [expr pow(0.5,$n) * [factorial $n] / \
			([factorial $i]*[factorial [expr $n - $i]])]
	set cdf [expr $i*($i+1)/2]
	
	if {$i == $n && $c_rhoqs} {
	    set cdf [expr $cdf - $c_rhoqs]
	}
	
	set thisp [expr $weight*[chi -number $chisq $cdf]]
	
	if {$cverb} {
	    puts "Weight: $weight  df: $cdf  w*p: $thisp"
	}
	set p [expr $p + $thisp]
    }

    set usep [expr 2.0*$p]
    if {$cverb} {
	puts "2*sum(w*p's) = $usep"
    }
    if {$usep < 1.0} {
	set newchisq [chi -inverse $usep 1]
	if {$cverb} {
	    puts "Corrected chisq = \[chi -inverse $usep 1\] = $newchisq"
	}
	set lodc1 [expr $newchisq / (2.0*log (10))]
	if {$cverb} {
	    puts "abs(LOD) = $newchisq / (2*log(10)) = $lodc1"
	}
	if {$negate} {
	    set lodc1 [expr 0.0 - $lodc1]
	    if {$cverb} {
		puts "Negated."
	    }
	}
    } else {
	set lodc1 0.0
    }
    set lod1 $lodc1
    }

# Apply lodadj, if applicable

    if {!$nolodadj} {
	if {[catch {set loda [lodadj -query]}]} {
	    error "lod: trait or outdir not specified to find lodadj.info"
	}
	if {$cverb || 1 == [info level]} {
	    lodadj -query -inform stdout
	}
	set lod1 [expr $loda * $lod1]
    }

   return $lod1
}

# solar::factorial --
#
# Purpose:  Compute factorial
#
# Usage:    factorial N
#
# Example:  set big [factorial 10]
#
# Notes:    A double precision value is returned, since double precision can
#           represent larger numbers exactly than integers.
#
#           Non-integral N is rounded to the nearest integer first, then
#             the factorial is computed for that integer.
#
#           For large enough N, the value returned might not be exact.
#              (Currently this happens for N > 18.)
#
#           Negative N (after rounding) raises a range error
#
#           This may be, but need not be, used in an "expr".
# -

proc factorial {n} {
    if {[catch {set n [expr round($n)]}]} {
	error "factorial: Syntax error: $n is not a number"
    }

    if {$n < 0} {
	error "factorial: $n is out of range"
    } elseif {$n < 2} {
	set fact 1.0
    } else {
	set fact 1.0
	for {set i 2} {$i <= $n} {incr i} {
	    set fact [expr $fact * $i]
	}
    }
    return $fact
}
	

# solar::slod --
#
# Purpose:  Calculate slod (score-based LOD equivalent) on current model
#
# Usage:    slod newparam
#
# Example:  slod h2q1
#-

proc slod {np} {
    set pscore [parameter $np score]
    if {$pscore <= 0.0} {
	return 0.0
    }
    set se [parameter $np se]
    if {0.0 == $se} {
	error "Unable to evaluate stest for parameter $np: missing S.E."
    }
    set stest [expr $pscore*$pscore*$se*$se]
    set slod [expr $stest/(2.0*log(10))]
    return $slod
}

# solar::null --
#
# Purpose:  Return an optimized parameter from null.mod
#
# Usage:   null loglike
#          null h2q
#
# Notes:   see also nulln
# -

proc null {parameter_name} {
    return [oldmodel "null" $parameter_name]
}

# solar::nulln --
#
# Purpose:  Return an optimized parameter from nullX.mod
#            (e.g. null0.mod,  null1.mod, etc.).
#
# Usage:   nulln 0 loglike
#          nulln 1 loglike
#          nulln 2 h2q1
# -

proc nulln {N parameter_name} {
    return [oldmodel "null$N" $parameter_name]
}

# solar::Solar_Model_Startup -- private
#
# Purpose: To run model initialization file .solar_model_new
#
# Note: Since this is invoked by "model new", it must not itself run model new
#       or any other command, such as "load model" which does.  Hence the only
#       allowed commands are: 
#
#       option trait parameter covariate constraint omega define
#
# 
# -

proc Solar_Model_Startup {} {
    if {[file exists .solar_model_new]} {
	set allowed {option trait parameter covariate constraint omega define}
	set commands [listfile .solar_model_new]
	foreach command $commands {
	    if {[llength $command] && [string index $command 0] != "\#"} {
		set commandname [lindex $command 0]
		if {-1 == [lsearch $allowed $commandname]} {
		    error "\nForbidden command \"$commandname\" in .solar_model_new\nPermitted commands: $allowed\n"
		}
		if {[catch {eval $command}]} {
		    error "\nError executing command in .solar_model_new\n  $command\n"
		}
	    }
	}
	if {[info level] == 1} {
	    puts "executed commands in .solar_model_new"
	}
    }
    return ""
}


# solar::read_model --
#
# Purpose:  Read a parameter value or likelihood from any saved model
#
# Usage:    read_model <model-name> loglike             ; returns loglikelihood
#           read_model <model-name> <parameter>         ; returns mle value
#           read_model <model-name> <parameter> -se     ; standard error
#           read_model <model-name> <parameter> -lower  ; lower bound
#           read_model <model-name> <parameter> -upper  ; upper bound
#           read_model <model-name> <parameter> -score  ; score
#
# Model is read from current maximization output directory (see
# help outdir).
#
# Example:
#
#            trait q4
#            read_model null0 h2r
# -

proc read_model {model_name parameter_name {getfield -value}} {
    return [oldmodel $model_name $parameter_name $getfield]
}

proc oldmodel {model_name parameter_name {getfield -value}} {

# loglike handled by proc modloglike

    if {0 == [string compare $parameter_name loglike]} {
	return [modloglike $model_name]
    }

# open file and scan for parameter data

    set f [open [full_filename [append_mod $model_name]] r]
    set parameter_seen 0
    while {-1 < [gets $f line]} {

# See if this is a matching parameter statement

	if {1 == [scan $line "parameter %s" pname] && \
		[string_imatch $parameter_name $pname]} {
	    set parameter_seen 1

# See if info requested is found

	    for {set i 2} {$i < [llength $line]-1} {incr i 2} {
		set token [lindex $line $i]
		set value [lindex $line [expr $i + 1]]
		if {"-value" == $getfield} {
		    if {[string_imatch = $token] || \
			    [string_imatch start $token]} {
			close $f
			return $value
		    }
		} elseif {"-lower" == $getfield} {
		    if {[string_imatch lower $token]} {
			close $f
			return $value
		    }
		} elseif {"-upper" == $getfield} {
		    if {[string_imatch upper $token]} {
			close $f
			return $value
		    }
		} elseif {"-se" == $getfield} {
		    if {[string_imatch se $token]} {
			close $f
			return $value
		    }
		} elseif {"-score" == $getfield} {
		    if {[string_imatch score $token]} {
			close $f
			return $value
		    }
		} else {
		    close $f
		    error "oldmodel: Invalid field requested: $getfield"
		}
	    }
	}
    }
    close $f

# Try old parameter name convention (used : instead of * in bage*sex)

    if {-1 != [string first * $parameter_name]} {
	set new_parameter_name [substar $parameter_name :]
#	puts "retrying parameter $parameter_name as $new_parameter_name"
	if {0 == [catch {oldmodel $model_name $new_parameter_name $getfield} \
		rv]} {
	    return $rv
	}
    }

# If standard error or score weren't found, but parameter was, return 0

    if {"-se" == $getfield || "-score" == $getfield} {
	if {$parameter_seen} {
	    return 0
	}
    }

# Error: data not found

    set field_name [string range $getfield 1 end]
    error "$field_name for parameter $parameter_name not found in $model_name"
    return ""
}

	
# solar::find_string -- private
#
# Purpose:  Return the first line containing any string from a file.
#           If the string is not found, the empty string is returned.
#           (Note: comparison is case insensitive)
#
# Example: find_string poly.out "The analysis sample size is"
# -

proc find_string {filename keystring} {
    set f [open $filename r]
    set ikeystring [string tolower $keystring]
    while {-1 < [gets $f line]} {
	set iline [string tolower $line]
	if {0 <= [string first $ikeystring $iline]} {
	    close $f
	    return $line
	}
    }
    close $f
    return ""
}

proc outfile_sample_size {outfile_name} {
    set sstring [find_string [full_filename $outfile_name] \
	    "analysis sample size is"]
    if {"" != $sstring} {
	set ll [llength $sstring]
	set last_elem [lindex $sstring [expr $ll - 1]]
	set size [expr round($last_elem)]
	return $size
    }
    error "Can't find analysis sample size in output file $outfile_name"
}

proc oldsporadic {model_name} {
    set sstring [find_string [full_filename [append_mod $model_name]] \
	    "constraint h2r = 0"]
    if {"" == $sstring} {
	return 0
    }
    return 1
}

	
proc modloglike {model_name} {
    set f [open [full_filename [append_mod $model_name]] r]
    while {-1 < [gets $f line]} {
	if {1 == [scan $line "loglike set %s" number]} {
	    close $f
	    return $number
	}
    }
    close $f
    error "Loglikelihood not saved"
}


# solar::usage
#
# Purpose:  Print short "usage" message about a command
#
# Usage:    usage <command>
#
# Example:  usage multipoint    ;# shows usage of multipoint command
#
# Notes:  Since this is printed directly to terminal, it will stay visible for
#         next command.
#
#         If help message contains no "Usage" section, the first 15 lines will
#         be printed.
#
# -

proc usage {command} {
    help-on-command -usage $command
    return ""
}

# solar::help-list-of-commands -- private
#
# Purpose:  Get a list of documented commands
#
# Usage:    help-list-of-commands [-describe <doclist>] [-user]
#         
#           <doclist> becomes an array of one-line descriptions for each
#            command
#
#           -user   specifies search only for user commands (all *.tcl
#                   files in . and ~/lib execpt solar.tcl)
#
#                   (otherwise, search active solar.tcl and all commands
#                    included in $SOLAR_LIB are included)
#-

proc help-list-of-commands {args} {
    global env
    set descriptions ""
    set user 0
    set carray(0) 0
    read_arglist $args -describe descriptions -user {set user 1}
    if {"" != $descriptions} {
	upvar $descriptions command_array
    }

    set command_list {}
    if {$user} {
	set homelib $env(HOME)/lib
	set user_paths ". $homelib"
	set sourcefilenames ""
	foreach d $user_paths {
	    set candidates [glob -nocomplain $d/*.tcl]
	    foreach candidate $candidates {
		if {"solar.tcl" != [file tail $candidate]} {
		    lappend sourcefilenames $candidate
		}
	    }
	}
    } else {
	set script_path [find_script_path solar.tcl]
	set sourcefilenames "$script_path/solar.tcl"
	set libfilenames [glob -nocomplain $env(SOLAR_LIB)/*.tcl]
	foreach libfilename $libfilenames {
	    if {"solar.tcl" != [file tail $libfilename]} {
		lappend sourcefilenames $libfilename
	    }
	}
    }
    ifdebug puts "searching files $sourcefilenames"
    foreach sourcefilename $sourcefilenames {
	set sourcefile [open $sourcefilename r]
	set packagename [file tail $sourcefilename]
	set endchar [expr [string first . $packagename] - 1]
	set packagename [string range $packagename 0 $endchar]
	set targetname "\# [catenate $packagename ::]"
	set coffset [expr [string length $targetname] - 0]
#	ifdebug puts "packagename: $packagename  coffset: $coffset"
    while {-1 != [gets $sourcefile source_line]} {
	set subs [string range $source_line 0 [expr $coffset - 1]]
	if {0 == [string compare $subs $targetname]} {
	    set sbase [string range $source_line $coffset end]
	    if {1 == [scan $sbase "%s" command_name]} {
		if {-1 != [lsearch -exact $source_line private]} {
		    continue
		}
		if {$user && \
		    ![catch {set aaa $carray($command_name)}]} {
		    continue
		} elseif {$user} {
		    set carray($command_name) 1
		}
		lappend command_list $command_name
#
# Get 'Purpose' statement and put into associative array
#
		if {"" == $descriptions} {continue}
		set last_pos [tell $sourcefile]
		set got_purpose 0
		while {-1 != [gets $sourcefile source_line]} {
		    if {0 == [string compare "Purpose" \
			    [string range $source_line 2 8]]} {
			set purpose [string range $source_line 11 end]
			set command_array($command_name) $purpose
			set got_purpose 1
			break
		    }
		    set piece [string range $source_line 0 8]
		    if {0 == [string compare $piece "# solar::"]} {
#			break
		    }
		}
		if {!$got_purpose} {
		    set command_array($command_name) " "
		}
		seek $sourcefile $last_pos
	    }
	}
    }
    close $sourcefile
    }
    set command_list [lsort $command_list]
    return $command_list
}

# solar::updatedoc -- private
#
# Purpose:  Generate auto-generated documentation
#
# Usage:    updatedoc
#
# Notes: index.html, 91.appendix_1_text.html 94.appendix_4.html and
#        README.news are created
#
# Part of the work is actually done by updatechanges.
#
# -

proc updatedoc {args} {

    set indexfile_name "index.html"
    set outfile_name "91.appendix_1_text.html"

    set command_list [help-list-of-commands -describe command_descriptions]

# Write the index file

    puts "Writing index file $indexfile_name"
    set ifile [open $indexfile_name w]

    puts $ifile <html>
    puts $ifile <head>
    puts $ifile "<title>SOLAR Manual Appendix 1</title>"
    puts $ifile </head>
    puts $ifile ""
    puts $ifile "<body bgcolor=ffffff text=#000000>"
    puts $ifile ""
    puts $ifile "<font size=+1><a href=00.contents.html>Go to full SOLAR Manual (Table of Contents)</a></font>"
    puts $ifile "<br><a href=03.chapter.html>Go to SOLAR tutorial</a>"
    puts $ifile "<br><a href=http://www.tcl.tk/man/tcl8.0/TclCmd/contents.htm>Manual for Tcl Commands (External Link)</a>"
    puts $ifile ""
    puts $ifile <h1><center>
    puts $ifile "Appendix 1"
    puts $ifile </center></h1>
    puts $ifile ""
    puts $ifile <h1><center>
    puts $ifile "SOLAR Command Descriptions"
    puts $ifile </center></h1>
    puts $ifile ""
    puts $ifile ""

# Write command names in a quick-access table

    puts $ifile <p><table>
    set number_of_commands [llength $command_list]
    set command_lines_required [expr ($number_of_commands / 6) + \
	    (($number_of_commands%6)?1:0)]
    for {set i 0} {$i < $command_lines_required} {incr i} {
	set command1 [lindex $command_list $i]
	set command2 [lindex $command_list [expr $command_lines_required+$i]]
	set command3 [lindex $command_list [expr 2*$command_lines_required+$i]]
	set command4 [lindex $command_list [expr 3*$command_lines_required+$i]]
	set command5 [lindex $command_list [expr 4*$command_lines_required+$i]]
	set command6 [lindex $command_list [expr 5*$command_lines_required+$i]]

	puts $ifile <tr>
	puts $ifile "<td><a href=91.appendix_1_text.html#$command1>"
	puts $ifile $command1
	puts $ifile </a></td>

	if {"" != $command2} {
	    puts $ifile "<td><a href=91.appendix_1_text.html#$command2>"
	    puts $ifile $command2
	    puts $ifile </a></td>
	}
	if {"" != $command3} {
	    puts $ifile "<td><a href=91.appendix_1_text.html#$command3>"
	    puts $ifile $command3
	    puts $ifile </a></td>
	}
	if {"" != $command4} {
	    puts $ifile "<td><a href=91.appendix_1_text.html#$command4>"
	    puts $ifile $command4
	    puts $ifile </a></td>
	}
	if {"" != $command5} {
	    puts $ifile "<td><a href=91.appendix_1_text.html#$command5>"
	    puts $ifile $command5
	    puts $ifile </a></td>
	}
	if {"" != $command6} {
	    puts $ifile "<td><a href=91.appendix_1_text.html#$command6>"
	    puts $ifile $command6
	    puts $ifile </a></td>
	}
	puts $ifile </tr>
    }
    puts $ifile </table></p>
    puts $ifile ""

# Now write 1-line command descriptions in a second table

    puts $ifile <p><table>
    foreach command $command_list {
	puts $ifile <tr>
	set description $command_descriptions($command)
	puts $ifile "<td><a href=91.appendix_1_text.html#$command>"
	puts $ifile $command
	puts $ifile </a></td>
	puts $ifile <td>
	puts $ifile $description
	puts $ifile </td>
	puts $ifile </tr>
    }
    puts $ifile </table></p>
    puts $ifile ""

    puts $ifile </body>
    puts $ifile </html>
    close $ifile

# Write text file

    puts "Writing text file $outfile_name"

    set outfile [open $outfile_name w]
    puts $outfile "<html>"
    puts $outfile "<head>"
    puts $outfile "<title>SOLAR Manual Appendix 1</title>"
    puts $outfile "</head>"
    puts $outfile ""
    puts $outfile "<body bgcolor=ffffff text=#000000>"
    puts $outfile ""
    puts $outfile "<h1><center>"
    puts $outfile "Appendix 1"
    puts $outfile "</center></h1>"
    puts $outfile ""
    puts $outfile "<h1><center>"
    puts $outfile "SOLAR Command Descriptions"
    puts $outfile "</center></h1>"
    puts $outfile ""
    puts $outfile ""
    puts $outfile "<p>"
    puts $outfile " \
NOTICE: <b>SOLAR</b> is an evolving work of software and is subject to change.\n\
Commands, arguments, features, performance, and availability are\n\
all subject to change.  There is no committment to support scripts\n\
written using the current commands in future releases.\n"
    puts $outfile "</p>"
    puts $outfile "<a href=index.html>Return to command index</a>"
    close $outfile

    set i 0
    foreach command $command_list {
	incr i
	puts "Documenting $command..."

	set outfile [open $outfile_name a]
	puts $outfile "<h2>"
	puts $outfile "<a name=$command>"
	puts $outfile "A1.$i  $command"
	puts $outfile "</a>"
	puts $outfile "</h2>"
	puts $outfile ""
	puts $outfile "<p>"
	puts $outfile "<pre><font size=-1>"

	set command_helpfile [help-on-command $command]
	set hf [open $command_helpfile]
	while {-1 != [gets $hf line]} {
	    puts $outfile [html-fix $line]
	}
	close $hf
	delete_files_forcibly $command_helpfile

	puts $outfile "</font></pre>"
	puts $outfile "</p>"
	puts $outfile "<a href=index.html>Return to command index</a>"
	puts $outfile ""
	puts $outfile ""
	close $outfile
    }
    set outfile [open $outfile_name a]
    puts $outfile "</body>"
    puts $outfile "</html>"
    close $outfile
    updatechanges
    puts "new files: index.html 91.appendix_1_text.html 94.appendix_4.html README.news"
}    

proc html-fix {text} {
    regsub -all < $text {\&lt;} text1
    regsub -all > $text1 {\&gt;} text2
    return $text2
}


# solar::help-all-commands -- private
#
# Purpose:  Display help message listing all commands
#
# Usage:  help-all-commands
#- 

proc help-all-commands {args} {
    set user 0

    if {"-user" == $args} {
	set user 1
	set command_list [help-list-of-commands -describe command_array -user]
	if {{} == $command_list} {
	    error "No user commands are documented"
	}
    } else {
	set command_list [help-list-of-commands -describe command_array]
    }
    set ofilename /tmp/commands.[pid]
    set ofile [open $ofilename w]
    if {$user} {
	puts $ofile \
"The following user-defined commands are available:\n"
    } else {
	puts $ofile \
      "The following solar commands are available (and may be abbreviated):\n"
    }
    set number_of_commands [llength $command_list]
    set command_lines_required [expr ($number_of_commands / 4) + \
	    (($number_of_commands%4)?1:0)]
    for {set i 0} {$i < $command_lines_required} {incr i} {
	puts -nonewline $ofile "[format %-20s [lindex $command_list $i]] "
	puts -nonewline $ofile \
      "[format %-20s [lindex $command_list [expr $command_lines_required+$i]]]"
	puts -nonewline $ofile \
    "[format %-20s [lindex $command_list [expr 2*$command_lines_required+$i]]]"
	puts $ofile \
    "[format %-19s [lindex $command_list [expr 3*$command_lines_required+$i]]]"
    }
    puts $ofile "\n"
    if {!$user} {
    puts $ofile \
{You can also use any TCL or Unix command (but wildcards require [glob]).}
    puts $ofile \
"For more information about a particular command, use 'help <command>'"
    }
    puts $ofile \
"Brief description of each command follows:"
    puts $ofile ""
    foreach command $command_list {
	puts $ofile \
  "[format %-19s $command] [string range $command_array($command) 0 57]"
    }
    close $ofile
    return $ofilename
}


proc helpscript {command} {
    set outfile [help-on-command $command]
    exec more $outfile >&@stdout
    delete_files_forcibly $outfile
}


# Help for a particular command

proc help-on-command {args} {

    if {"-user" == $args} {
	return [help-all-commands -user]
    }	

    set use_more 1
    set usage_only 0
    set name [read_arglist $args -nomore {set use_more 0} \
	    -usage {set usage_only 1; set use_more 0}]
    set nlen [string length $name]

# First, find if one or more script name matches given name
#   Look first at solar.tcl currently in use.  Then look at all other
#   *.tcl files in tcl search paths: . ~/lib $SOLAR_LIB
#
#   Header line for help message is "# <packagename>::<commandname>" where
#   packagename is the first dotted segment of filename.  For example,
#   if filename is solar.john.tcl, packagename is "solar" but if filename
#   is john.solar.tcl, packagename is "john".
#
# Remove duplicate paths, auto_path sometimes has duplicates
# Current solar.tcl is put at front of list...it has "priority"
#
    set script_path [find_script_path solar.tcl]
    ifdebug puts "script path is $script_path"
    global auto_path
    set sourcefilenames ""
    set checked_path ""
    foreach d $auto_path {
	if {-1==[lsearch $checked_path $d]} {
	    lappend checked_path $d
	    catch {set sourcefilenames "$sourcefilenames [glob $d/*.tcl]"}
	}
    }
    set sourcefilenames \
	[remove_from_list $sourcefilenames $script_path/solar.tcl]
    set sourcefilenames "$script_path/solar.tcl $sourcefilenames"
    ifdebug puts "searching files $sourcefilenames"
    set matched 0
    set exact_matched 0
    set mfilename ""
    set mtargetname ""
    set mcoffset 0
    set matchlist ""
    while {$matched == 0 && 0 != [llength $sourcefilenames]} {
	set sourcefilename [lindex $sourcefilenames 0]
	set sourcefilenames [lrange $sourcefilenames 1 end]
	set sourcefile [open $sourcefilename r]
	set packagename [file tail $sourcefilename]
	set endchar [expr [string first . $packagename] - 1]
	set packagename [string range $packagename 0 $endchar]
	set targetname "\# [catenate $packagename ::]"
	set coffset [expr [string length $targetname] - 0]
	ifdebug puts "searching $sourcefilename for $targetname"
	while {-1 != [gets $sourcefile source_line]} {
	    set subs [string range $source_line 0 [expr $coffset - 1]]
	    if {0 == [string compare $subs $targetname]} {
		set sbase [string range $source_line $coffset end]
		if {1 == [scan $sbase "%s" sall]} {
		    set sshort [string range $sall 0 [expr $nlen - 1]]
#		    puts "sall is $sall, name is $name"
		    if {0 == [string compare $sshort $name]} {
			set mfilename $sourcefilename
			set mtargetname $targetname
			set mcoffset $coffset
			set matched [expr $matched + 1]
			lappend matchlist $sall
			set target $sall
			if {0 == [string compare $sall $name]} {
			    set exact_matched 1
			    break
			}
		    }
		}
	    }
	}
	close $sourcefile
    }	
#
# Check for ambiguity or non-presence
#
    if {$matched > 1 && $exact_matched == 0} {
	error "Ambiguous command name $name:\n  $matchlist"
    }
    if {$matched == 0} {
	error "Help not found for $name"
    }
#
# Output help file to /tmp
#
    set ofilename /tmp/$name.[pid]
    if {$use_more} {
	set ofile [open $ofilename w]
    }
    set sourcefile [open $mfilename r]
    while {-1 != [gets $sourcefile source_line]} {
	set subs [string range $source_line 0 [expr $mcoffset - 1]]
	if {0 == [string compare $subs $mtargetname]} {
	    set sbase [string range $source_line $mcoffset end]
	    if {1 == [scan $sbase "%s" sall] && \
		    0 == [string compare $sall $target]} {
		set just_starting 1
		set line_count 0
		set found_usage 0
		while {-1 != [gets $sourcefile source_line]} {
		    incr line_count
		    if {$just_starting && 4>[string length $source_line]} {
			continue
		    }
		    set just_starting 0
		    if {0 == [string compare $source_line "# -"]} break
		    if {[string range $source_line 0 0] != "#"} break
		    if {$use_more} {
			puts $ofile [string range $source_line 2 end]
		    } else {
			puts [string range $source_line 2 end]
		    }
		    if {$usage_only} {
			if {-1 != [string first "Usage:" $source_line]} {
			    set found_usage 1
			}
			if {$found_usage && 5 > [string length $source_line]} {
			    break
			}
			if {$line_count >= 15} break
		    }
		}
		break
	    }
	}
    }
    if {![catch {shortcut $sall}]} {
	if {$use_more} {
	    puts $ofile "\n[shortcut $sall]\n"
	    flush $ofile
	} else {
	    puts "\n[shortcut $sall]\n"
	}
    }
    close $sourcefile
    if {$use_more} {
	close $ofile
	return $ofilename
    }
    return ""
}

proc moreout {name} {
    eval exec more $name
}

proc catout {name} {
    eval exec cat $name
}

proc find_script_path {scriptname} {
    global auto_path
    foreach d $auto_path {
	if {[file exists "$d/$scriptname"]} {
	    return $d
	}
    }
    puts "Directory not found"
    error "Directory not found"
}

# Column ruler:
#12345678|234567890123456789012345678901234567890123456789012345678901234567890

# solar::ifverbplus -- private
#
# Purpose:  Execute a tcl command if verbosity level is "plus" or higher
#
# Usage:    ifverbplus <command>
#-

proc ifverbplus {args} {
    set vlevel [verbosity -number]
    if {$vlevel >= 0x3ff} {
	return [uplevel $args]
    }
    return ""
}

proc ifverbmax {args} {
    set vlevel [verbosity]
    if {"verbosity max" == $vlevel} {
	return [uplevel $args]
    }
    return ""
}

# solar::evdout --
# solar::evdmat --
# solar::evdinx --
# solar::evdiny --
# solar::evdinz --
# solar::evdinev --
#
# Purpose: I/O for EVD data
#
# Usage:   trait ...
#          covar ...
#
#          evdout [<-evectors>[<-all>]] ;# write evddata.out,evectors if asked
#
#          evdinx  [<evdfile>] ;# return X* matrix from evddata.out file
#          evdiny  [<evdfile>] ;# return Y* matrix from evddata.out file
#          evdinz  [<evdfile>] ;# return Z* matrix from evddata.out file
#
#          evdinev [<-all>]  ;# load eigenvector matrix(es) from file(s) saved
#                            ;# by evdout -evectors
#
# Note: You must select trait, covariates, and anything else that would
#       restrict the sample size before invoking evdout.  You
#       do not need a maximized model, just trait and covariates.
#
# evdout writes out evd transformed variables and the eigenvalues
#       (which are called lambda) to a file named evddata.out in the
#       maximization output directory but without actually doing a
#       model maximization.  The trait value, which is not demeaned,
#       is written to variable <traitname>_evd.  The covariate values,
#       which are demeaned or scaled appropriately, are written to
#       variables named evd2_<varname>_evd.  Ignore other fields.
#
#       Additionally, you can write out the eigenvectors, either in
#       normal per-pedigree mode (one matrix for each family) or in
#       entire-sample mode if you select the -all option.  Filenames are
#       evectors.family<n>.mat.csv for per-family matrices and
#       evectors.mat.csv for entire sample (-all) matrix.  Matrix files
#       are written to the outdir.
#
#       Current methods rely on the EVD transformation of variables
#       and do not require the Eigenvectors to be output, so generally
#       speaking you should not use the -evectors option unless you
#       know you need it.
#
# evdinx returns the X* matrix corresponding to current evddata.out.
#       The X* matrix has from left to right: (1) a column of 1's, (2+) one or
#       more columns of EVD transformed covariates which have been scaled to
#       mean of zero, in the order in which covariates occur in the model.
#       evdinx has a -method2 option needed for the -method2 option of fphi.
#
# evdiny returns the Y* matrix, which is a one vertical column matrix
#       (aka vector) of EVD transformed trait values.
#
# evdinz returns the Z matrix, which has has a column of 1's and a column
#        of lambda's.
#
# evdinev returns a list of eigenvector matrices, one for each family,
#       or if the -all option is specified, just one matrix for the entire
#       sample.  In order to use the -all option, you must have previously
#       done evdout with the -evectors and -all options.
#
# Example:
#
#       foreach covar $testvars {
#           model new
#           trait q4
#           covar $covar
#           evdout
#           set X [evdinx]
#           set Y [evdiny]
#           solve $X $Y
#       }
#-

# Junk documentation for features not supported
#          evdmatx [<-last>] ;# return X* matrix from current model or last EVD
#          evdmaty [<-last>] ;# return Y* matrix from current model or last EVD
#          evdmatxy    ;# return X* and Y* matrix from current model as list
#          evdmatev [<-all>] ;# return Eigenvector matrix or matrices
# -



proc evdinev {args} {
    if {$args == "-all"} {
	return [load matrix [full_filename evectors.mat.csv]]
    } elseif {$args != ""} {
	error "Invalid argument $args to evdin"
    }
    set ms [glob [full_filename evectors.family*.mat.csv]]
    set allms ""
    foreach mms $ms {
	lappend allms [load matrix $mms]
    }
    return $allms
}

proc evdoldjunk {} {
# figure out cols from current covariates
    set covs [covariates]
    foreach cov $covs {
	set newcov ""
	for {set i 0} {$i < [string length $cov]} {incr i} {
	    set ch [string index $cov $i]
	    if {$ch == "*"} {
		set ch X
	    } elseif {$ch == "^"} {
		set ch "up"
	    }
	    set newcov "$newcov$ch"
	}
	lappend covcols evd2_$newcov\_evd
    }
}

proc evdinx {{evdfile ""} args} {
    set method 1
    if {$args == "-method2"} {
	set method 2
    } elseif {$args != ""} {
	error "Invalid argument $args to evdinx"
    }
    
    if {$method==2} {
	set covcols {{1}}
    } else {
	set covcols ""
    }

    if {$evdfile == ""} {
	set evdfile [full_filename evddata.out]
    }
    set inevd [open $evdfile]
    gets $inevd line
    close $inevd

    set linelist [split $line ,]
    set length [llength $linelist]
    for {set i 8} {$i < $length} {incr i} {
	set test [lindex $linelist $i]
	if {[string range $test 0 4] == "evd2_"} {
	    if {$method!=2} {
		set covcols [concat $covcols $test]
	    }
	} else {
	    break
	}
    }
    if {$method==2} {
	for {} {$i < $length} {incr i} {
	    set test [lindex $linelist $i]
	    set covcols [concat $covcols $test]
	}
    }

    if {[info level]==0} {
	puts "evdinx columns: $covcols"
    }
    set X [load matrix -cols $covcols [full_filename evddata.out]]
    return $X
}

proc evdiny {{evdfile ""}} {
    if {$evdfile == ""} {
	set evdfile [full_filename evddata.out]
    }
    set Y [load matrix -cols [trait]_evd $evdfile]
    return $Y
}

proc evdinz {{evdfile ""}} {
    if {$evdfile == ""} {
	set evdfile [full_filename evddata.out]
    }
    set zcols {{1} lambda}
    set Z [load matrix -cols $zcols [full_filename evddata.out]]
    return $Z
}


proc evdoutev {args} {
    set evdmode 1
    if {$args == "-all"} {
	set evdmode 2
    } elseif {$args != ""} {
	error "invalid argument: $args"
    }
    return [evdmat $evdmode]
}

# Internally used options
# option eigenvectors: 0=do not output eigenvectors
#                      1=write eigenvector(s) for each ped (or all)
#                      2=make eigenvector matrix for each ped (or all)
#                      (each or all determined by evdmat)
#
# option evdmat: (for X Y matrixes)
# 4 means make matrix for all peds
# 3 means make matrix for each ped (default for evdmat)
# 2 means write file for all peds
# 1 means write files for each ped (default for evdout)
#
# option evdcovars: count of covariates in evd model
#
# evdout is the main procedure which does evdout or evdmat operations
# evdmatx,y are front ends
#

proc evdmat {args} {
    error "evdmat obsolete.  Use evdout then evdinx, evdiny, and/or evdinev"
}

proc evdmatxNotWorking {args} {
    global SOLAR_evdmatx
    global SOLAR_evdmaty

    if {$args == "-last"} {
	if {[if_global_exists SOLAR_evdmatx]} {
	    return $SOLAR_evdmatx
	} else {
	    error "No previous evdmatx or evdmaty"
	}
    } elseif {$args != ""} {
	error "invalid argument to evdmatx"
    }
    evdout -mat
    set SOLAR_evdmaty [mathmatrix lastid]
    set pentid [string range $SOLAR_evdmaty 4 end]
    set SOLAR_evdmatx .mm.[expr $pentid - 1]
    return $SOLAR_evdmatx
}

proc evdmatyNotWorking {args} {
    global SOLAR_evdmatx
    global SOLAR_evdmaty

    if {$args == "-last"} {
	if {[if_global_exists SOLAR_evdmatx]} {
	    return $SOLAR_evdmaty
	} else {
	    error "No previous evdmatx or evdmaty"
	}
    } elseif {$args != ""} {
	error "invalid argument to evdmaty"
    }
    evdout -mat
    set SOLAR_evdmaty [mathmatrix lastid]
    set pentid [string range $SOLAR_evdmaty 4 end]
    set SOLAR_evdmatx .mm.$pentid

    return $SOLAR_evdmaty
}

proc evdmatxyNotWorking {} {
    global SOLAR_evdmatx
    global SOLAR_evdmaty

    evdout -mat 
    set SOLAR_evdmaty [mathmatrix lastid]
    set pentid [string range $SOLAR_evdmaty 4 end]
    set SOLAR_evdmatx .mm.[expr $pentid - 1]
    return {$SOLAR_evdmatx $SOLAR_evdmaty}
}

proc evdmatevNotWorking {args} {
    set all ""
    set lastid [mathmatrix lastid]
    set outlist {}
    if {$args != ""} {
	if {$args == "-all"} {
	    set all "-all"
	} else {
	    error "Invalid argument $args to evdmatev"
	}
    }
    eval evdout $all -evectors -mat
    set startid [string range $lastid 4 end]
    set endid [string range [mathmatrix lastid] 4 end]
    for {set i $startid+1} {$i <= $endid} {incr i} {
	lappend outlist .mm.$i
    }
    return $outlist
}

proc evdout {args} {
    set eigenvectors 0
    set all 0
    set matonly 0
    set bad [read_arglist $args \
		 -evectors {set eigenvectors 1} \
		 -all {set all 1} \
	         -mat {set matonly 1}]

    if {$bad != ""} {
	error "evdout: invalid argument(s) $bad"
    }

    set savedname [full_filename evdout.startmodel]
    save model $savedname
    if {-1 != [string first "to_set_standard_model" [omega]]} {
	
    }
    polymod
	constraint delete_all
    option modeltype evd2
    option evdphase 1
    option eigenvectors $eigenvectors
    option evdcovs [llength [covariates]]
    if {$matonly} {
	if {$all} {
	    option evdmat 4
	} else {
	    option evdmat 3
	}
    } else {
	if {$all} {
	    option evdmat 2
	} else {
	    option evdmat 1
	}
    }
    if {$all} {
	option mergeallpeds 1
    }
    set code [catch {maximize -q} rets]
    load model $savedname
    if {$code} {
	return -code $code $rets
    }
    set retval ""
}


proc evdmatOLD {args} {
    set evdmode 3 ;# default to one matrix
    if {$args == "-all"} {
	set evdmode 4
    } elseif {$args == 1 || $args == 2} {
	set evdmode $args
    } elseif {$args != ""} {
	error "Invalid argument $args to evdmat or evdout"
    }

    set startid 0
    if {$evdmode == 3} {
	if {![catch {mathmatrix lastid}]} {
	    set startid [string range [mathmatrix lastid] 4 end]
	}
    }

    set savedname [full_filename evdout.startmodel]
    save model $savedname
    option modeltype evd2
    option eigenvectors $evdmode
    if {1 - ($evdmode % 2)} {
	option mergeallpeds 1        ;# one big matrix produced
    }
    option evdphase 1
    if {-1 != [string first "to_set_standard_model" [omega]]} {
	polymod
    }
    if {$evdmode & 0x8} {
	error "maximize would be next"
    }
    set code [catch {maximize -q} rets]
    load model $savedname
    if {$code} {
	return -code $code $rets
    }
    set retval ""
    if {$evdmode == 3} {
	set endid [string range [mathmatrix lastid] 4 end]
	for {set id [expr $startid + 1]} {$id < $endid} {incr id} {
	    lappend retval .mm.$id
	}
    } elseif {$evdmode == 4} {
	set retval [mathmatrix lastid]
    }
    return $retval
}




# solar::maximize --
#
# Purpose:  Find the maximum loglikelihood of a model by adjusting
#           parameter values within specified constraints.
# 
# Usage:    maximize [-quiet] [-out <filename>]
#
#               -quiet  (or -q) Use minimum verbosity while maximizing
#               -out (or -o)    Write results to this filename.  The default
#                               is 'solar.out' in a subdirectory named after
#                               the current trait, or the current 'outdir.'
#                               If a filename is specified without any /
#                               characters, it will also be written in the
#                               default location.  If the filename contains
#                               / characters, it is used as a full pathname.
#
#               -noquad         Do not test quadratic
#
#               -who            Do not maximize, but list who would be
#                               included in analysis.  File "who.out" is
#                               written to trait/outdir containing list
#                               of pedindex.out sequential ID's.  This
#                               option is used by the "relatives" command
#
#                -runwho        Maximize, AND produce who.out file as above.
#
#                -sampledata    Do not maximize, but write out the data that
#                               would be included in the analysis to a file
#                               named "sampledata.out" in the maximization
#                               output directory.  [WARNING!  The fields in
#                               this file are preliminary and subject to
#                               change!]  The trait data (fields trait1,
#                               trait2, etc.) might be from a phenotype or
#                               an expression created by the "define" command.
#
# Notes:    This is the key command of solar.  It is used by polygenic,
#           twopoint, multipoint, bayesavg, and other scripts to find
#           the model parameter values which produce the maximum
#           loglikelihood.
#
#           The final values are not saved to a model file.  To do that,
#           issue a 'save model' command after maximization.
#
#           Multiple solar processes should not be writing to the same
#           directory.  Use the outdir command to specify different output
#           directories.
#
#           Advanced SOLAR users sometimes use the raw "cmaximize" command
#           which bypasses many of the retry mechanisms (and their implicit
#           assumptions) now built-in to SOLAR.  This is not recommended for
#           most users.
#-


# This was a layer around the old Tcl maximize, now called "tmaximize"
# However, currently it does nothing extra since the reduction of conv is
# not currently deemed necessary, so maximize and tmaximize are identical.
#
# Some scripts (which could do their own conv reduction) call tmaximize
# directly.

proc maximize {args} {
    return [eval tmaximize $args]
}

#
# This is the additional code in the pre-6.4.0 version of maximize
# long since made obsolete, which did conv reduction, now completely
# removed for better efficiency.
proc pre640_maximize {args} {
    if {0} {
    set rets ""
    set mfile [full_filename last_first]
    save model $mfile
    set tries 0
    set max_tries 2

    set quiet 0
    catch {read_arglist $args -q {set quiet 1} -quiet {set quiet 1}}
    ifdebug set quiet 0

    global errorInfo errorCode
    set code [catch {eval tmaximize $args} rets]
    if {!$code} {
	return $rets
    } else {
	if {-1<[string first "CONVERGENCE FAILURE" $rets]} {
	    if {!$quiet} {puts "\n    *** Retrying with reduced conv"}
	    load model $mfile
	    option conv 1e-4
# default for discrete anyway
#	    option conv(discrete) 1e-4   
	    set code [catch {eval tmaximize $args} rets]
	    if {!$code} {
		return $rets
	    }
	}
    }
    return -code $code -errorinfo $errorInfo -errorcode $errorCode $rets
    }
}
	    
# tmaximize:
# This is the main maximize procedure which checks for convergence and
# normalized quadratic unity and does retries after boundary modification
# which is intended to be applicable to all models with standard
# parameterization.  It calls cmaximize, which handles the trap system
# introduced in version 6.

proc tmaximize {args} {

    ifdebug puts "Entering tmaximize"

# Process arguments and options

    set quiet 0
    set crunch_count 0
    set fix_count 0
    set outfile solar.out
    set noquad 0
    set opts {}
    set badargs [read_arglist $args \
	    -output outfile -out outfile -o outfile \
	    -noquad {set noquad 1} \
	    -who {lappend opts -who} \
	    -runwho {lappend opts -runwho} \
	    -sampledata {lappend opts -sampledata} \
	    -initpar {lappend opts -initpar} \
	    -quiet {set quiet 1} -q {set quiet 1}]

    if {"" != $badargs} {
	error "Unexpected maximize argument(s): $badargs"
    }

    if {-1 == [string first "/" $outfile]} {
	set outfile [full_filename $outfile]
    }

# Check for multivariate

    set ts [trait]
    if {[llength $ts] == 1} {
	set multi 0
    } else {
	set multi 1
    }

# Check for arbitrary parameterization

    set aparama 0
    if {!$multi} {
	if {![if_parameter_exists e2] || ![if_parameter_exists h2r]} {
	    ifdebug puts "This model has arbitary parameterization...cannot check boundaries"
	    set aparama 1
	}
    } else {
	foreach t $ts {
	    if {![if_parameter_exists e2\($t\)] || \
		    ![if_parameter_exists h2r\($t\)]} {
		ifdebug puts "This model has arbitary parameterization...cannot check boundaries"
		set aparama 1
	    }
	}
    }
    if {$aparama == 0} {
	ifdebug puts "This model has checkable boundaries"
    }


# Initialize retry variables

    set tried_perturb_last 0
    set no_previous_messages 1
    set quadratic_retries 0
    set smaller_ll_retries 0
    set code 0
    set first_time 1

# We loop over retries required to get it right
#   exiting on success, failure, or exhaustion

    while {1} {

# Perturb before all retries

	if {!$aparama && !$first_time} {
	    perturb
	}
	set first_time 0

# Boundary tracing

	global errorInfo errorCode
	set savelevel [verbosity -number]
	if {$quiet!=0} {verbosity min}
	if {!$aparama && 1==[llength [trait]] && \
	  ([trace_boundaries] ||[string_imatch "verbosity max" [verbosity]])} {
	    set h2qc [h2qcount]
	    puts -nonewline "    *** Parameters    e2   h2r"
	    for {set i 1} {$i <= $h2qc} {incr i} {
		puts -nonewline "   h2q$i"
	    }
	    puts ""

	    puts -nonewline "    ***       Values  [parameter e2 start]"
	    catch {puts -nonewline " [parameter h2r start]"}
	    for {set i 1} {$i <= $h2qc} {incr i} {
		puts -nonewline " [parameter h2q$i start]"
	    }
	    puts ""

	    puts -nonewline "    *** Upper Bounds  [parameter e2 upper]"
	    catch {puts -nonewline " [parameter h2r upper]"}
	    for {set i 1} {$i <= $h2qc} {incr i} {
		puts -nonewline " [parameter h2q$i upper]"
	    }
	    puts ""

	    puts -nonewline "    *** Lower Bounds  [parameter e2 lower]"
	    catch {puts -nonewline " [parameter h2r lower]"}
	    for {set i 1} {$i <= $h2qc} {incr i} {
		puts -nonewline " [parameter h2q$i lower]"
	    }
	    puts ""
	}

# Count number of maximizations...see also "countmax"

	set showmax 0
	if {[if_global_exists SOLAR_Maximizes]} {
	    global SOLAR_Maximizes
	    incr SOLAR_Maximizes
	    puts "    *** Beginning maximization number $SOLAR_Maximizes"
	    set showmax 1
	}
	ifdebug set showmax 1

	set rets ""
	set code [catch {eval cmaximize $outfile $opts} rets]
# Note: EVD Phase 2 is trapped and handled by cmaximize.  We simply exit here.
	if {-1 != [string first "Trap EVD Phase 2" $rets]} {
	    return $rets
	}
	verbosity $savelevel

# break out for evdout
	if {[option evdmat] > 0} {
	    return ""
	}


# -who and -sampledata options don't actually maximize, so no error check

	if {-1<[string first "-who" $opts]} {
	    return -code $code $rets
	}
	if {-1<[string first "-sampledata" $opts]} {
	    return -code $code $rets
	}
	if {-1<[string first "-initpar" $opts]} {
	    return -code $code $rets
	}

# If option maxiter 1, convergence and quadratic are irrelevant
# So exit now

	if {[option maxiter] == 1} {
	    return -code $code $rets
	}

# Report error message if applicable

# If hard convergence failure, try crunching

	if {!$aparama && -1<[string first "Convergence failure" $rets]} {
	    if {"" != $rets && (!$quiet || [trace_boundaries] || $showmax)} {
		puts "$rets\n"
	    }
	    set ready_to_retry 0
	    set said_useable 0
	    while {$crunch_count < [maxcrunch] && $fix_count < 2} {
		set quadratic_retries 0
		if {![restart-if-can] || \
			-1==[string first "estartable" $rets]} {
		    if {!$said_useable && \
			    !$quiet || [trace_boundaries] || $showmax} {
			puts "    *** Unable to use current model: Reloading last"
			set said_useable 1
		    }
		    load model [full_filename last.mod]
		} else {
		    if {!$said_useable && \
			    !$quiet || [trace_boundaries] || $showmax} {
			puts "    *** Current model useable"
			set said_useable 1
		    }
		}
		set margin [expr (1.0/pow(5.0,$crunch_count))*[bcrunch]]
		if {[catch {set progress [do_boundaries $margin]} errorstring]} {
# Fixed boundaries, didn't crunch them
		    if {!$quiet || [trace_boundaries] || $showmax} {
			set rmargin [format %.5g $margin]
			puts $errorstring
		    }
		    set ready_to_retry 1
		    incr fix_count
		    break
		}
		incr crunch_count
		if {{} != $progress} {
		    if {!$quiet || [trace_boundaries] || $showmax} {
			set rmargin [format %.5g $margin]
			puts "    *** Crunched boundaries to $rmargin: $progress\n"
		    }
		    set ready_to_retry 1
		    break
		}
	    }
	    if {!$ready_to_retry} {
		error "CONVERGENCE FAILURE"
	    }
	    continue
	}


	if {$crunch_count} {
	    save model [full_filename converged.mod]
	}
	set crunch_count 0
	set fix_count 0

# If last likelihood inferior, try retries

	if {-1<[string first "Last likelihood significantly smaller" $rets]} {
	    if {"" != $rets && (!$quiet || [trace_boundaries] || $showmax)} {
		puts "$rets\n"
	    }
	    if {$smaller_ll_retries <= 5} {
		incr smaller_ll_retries
		continue
	    } else {
		error "\nConvergence failure (restartable): $rets"
	    }
		
	    
	}
	set smaller_ll_retries 0

# Return with error here if unfixable error

	if {$code == 1} {
	    return -code $code -errorinfo $errorInfo -errorcode $errorCode \
		    $rets
	}

# If arbitrary parameterization, exit here (don't check boundaries or quad)

	if {$aparama} {
	    ifdebug puts "Unusual parameterization; unable to check boundaries"
	    break
	}

# Check for artificial boundary problems (if found, expand and retry)

	ifdebug puts "Checking for boundary conditions"
	set real_boundary_conditions [check_real_upper_boundaries]
	set artificial_boundary_conditions [check_artificial_boundaries]
	if {{} != $artificial_boundary_conditions} {
	    set tried_perturb_last 0
	    set quadratic_retries 0
	    if {!$quiet || [trace_boundaries] || $showmax} {
		if {$no_previous_messages} {
		    set no_previous_messages 0
		    puts " "
		}
		puts -nonewline "    *** Retry with moved boundary for:"
		foreach boundary $artificial_boundary_conditions {
		    puts -nonewline " [lindex $boundary 0]"
		}
		puts "\n"
	    }
	    adjust_boundaries

# If we hit real boundaries (and no artificial ones) perturb and try again

	} elseif {{} != $real_boundary_conditions && !$tried_perturb_last} {
	    set tried_perturb_last 1
	    set quadratic_retries 0
	    if {!$quiet || [trace_boundaries] || $showmax} {
		if {$no_previous_messages} {
		    set no_previous_messages 0
		    puts " "
		}
		puts -nonewline \
		       "    *** Will perturb because hit real upper bound for:"
		foreach boundary $real_boundary_conditions {
		    puts -nonewline " [lindex $boundary 0]"
		}
		puts ""
	    }	    
	    perturb

# Check quadratic

	} elseif {[option evdphase] == 3} {
	    break
	} elseif {!$noquad && 0==[option tdist] && \
		[catch {find_simple_constraint SD}] && \
		[option maxiter] != 1 && \
		([quadratic] > [expr 1.0 + [boundary_quadratic]] || \
		 [quadratic] < [expr 1.0 - [boundary_quadratic]])} {
	    if {$multi} {
#		putsout $outfile "Warning: Quadratic is [quadratic]"
		break
	    } elseif {$quadratic_retries == 0} {
		set last_ll [loglike]
	    } else {
		if {$last_ll >= [loglike]} {
		    load model [full_filename last.mod]
		    error "\nConvergence failure (restartable)"
		}
	    }
	    incr quadratic_retries
	    if {$quadratic_retries > 10} {
		error \
			"\nConvergence failure (restartable) after 10 retries"
	    }
	    continue

# No problems found, so break out of loop

	} else {
	    break
	}
    }
    return -code $code $rets
}

# New in version 4.4.0, cmaximize is Tcl, ccmaximize is C++
# cmaximize handles trap for zscore

proc cmaximize {args} {
    set debug0 0

    if {[option modeltype] == "evd2"} {
	set EVD2_dirname [full_filename ""]
	set outfilename [full_filename evd2]
	set userout [lindex $args 0]
	set descout [lindex $args 0].desc
	set args [concat $descout [lrange $args 1 end]]
    }
    set badargs [read_arglist $args -output outfile -*]
#
# If evd2 model, save original model here and
# change covariates to definitions which allows the required scaling
# for each internal term.  Variables must be pre-scaled because EVD is
# computed immediately after sample delineation, and normally scaling is
# done only later during actual maximization interations.
#
    if {[option modeltype] == "evd2"} {
	save model $outfilename.evdphase1
	option modeltype Default
	option evdphase 0
	maximize -who -q -o $outfilename.getmeans.out
	load model $outfilename.evdphase1
	option evdphase 1

	set covars [covariates]
	set traits [trait]
	set oldomega [omega]
	set matrices [matrix -return]
	set parameters [parameter -return]
	set ntraits [llength $traits]

	foreach cov $covars {
	    ifdebug0 puts "processing $cov"

# suspended covariates used w/o change

	    if {"Suspended\[" == [string range $cov 0 9] ||
		"\(\)" == [string range $cov end-1 end]} {


	    } else {
#
# This is an active covariate. Make a definition name, a definition,
#   and a covariate which simply uses that definition
#     first, definition (and covariate) name, free of special cov characters:
#
		set dname "evd2_"
		for {set i 0} {$i < [string length $cov]} {incr i} {
		    set ch [string index $cov $i]
		    if {$ch == "*"} {
			set ch X
		    } elseif {$ch == "^"} {
			set ch "up"
		    }
		    set dname "$dname$ch"
		}
#
# now make definition
#   remove parenthesized trait specifier if present from name and expr

		set tsub ""
		set dname_nos $dname
		if {-1 != [set pos [string first \( $dname]]} {
		    set dname_nos [string range $dname 0 [expr $pos - 1]]
		    set tsub [string range $dname $pos end]
		    set tssub [string range $dname $pos+1 end-1]
		}
		set expr_nos [string range $cov 0 end]
		if {-1 != [set pos [string first \( $expr_nos]]} {
		    set expr_nos [string range $expr_nos 0 $pos-1]
		}

# Now we need to scale every term properly
#
		set slist [split $expr_nos *]
		ifdebug0 puts "slist is $slist"
		set newexp ""
		set define_names [define names]
		set noscale 0
		set vsums 0
		foreach sterm $slist {
		    set adjval 0
		    set newterm $sterm
		    set sulist [split $sterm ^]
		    set sel [lindex $sulist 0]
		    set s2 [lindex $sulist 1]
		    if {[string tolower $sel] == "sex"} {
			set newterm \(sex-1\)
		    } else {
#
# accumulate sum of squares for each term range
#
			set termexp 1
			if {[llength $sulist] > 1} {
			    set termexp $s2
			}
			set tmin [getvar -min $EVD2_dirname/evd2.getmeans.out \
				      $sel]
			set tmax [getvar -max $EVD2_dirname/evd2.getmeans.out \
				      $sel]
			set trange [expr $tmax - $tmin]
			ifdebug0 puts "range for $sel is $trange"
			set vterm $trange
			if {$trange != 0} {
			    for {set iexp 2} {$iexp <= $termexp} {incr iexp} {
				set vterm [expr $vterm * $trange]
			    }
			    set vsums [expr $vsums + $vterm * $vterm]
			}
# check for definition
			if {-1 != [lsearch $define_names $sel]} {

# this term is a definition...
# no current capability to have definitions within definitions
# if this is an interaction covariate, give an error
# if this is a scalar covariate, give a warning and abort scaling

			    if {-1 != [string first * $expr_nos] || -1 != \
				    [string first ^ $expr_nos]} {
				error \
"EVD2 does not yet support defined terms in interaction covariates\n\
Make a definition for $cov including required interactions and scaling"
			    } else {
				puts \
"Warning!  EVD2 cannot yet scale defined covariates.\n\
  You must scale them within their own definitions."
			    }
			    set noscale 1
			    break
			}
			set isd [getvar -d evd2.getmeans.out $sel]
			if {$isd} {
			    set adjval [getvar -min evd2.getmeans.out $sel]
			} else {
			    set adjval [getvar -mean evd2.getmeans.out $sel]
			}
			set newterm \($sel-$adjval\)
		    }
		    if {$s2 != ""} {
			set newterm "$newterm^$s2"
		    }
		    if {$newexp == ""} {
			set newexp $newterm
		    } else {
			set newexp "$newexp*$newterm"
		    }
		}

		if {!$noscale} {
		    ifdebug0 puts "define $dname_nos = $newexp"
		    eval define $dname_nos = $newexp
#
# Get original start, lower, and upper values for beta parameter(s)
#   there is one beta parameter for each trait if not qualified
#   then delete covariate, add new covariate, and transfer values
#
# Much easier for univariate
#
		    if {$ntraits==1 || $tsub != ""} {
			set start [parameter b$cov =]
			set upper [parameter b$cov upper]
			set lower [parameter b$cov lower]
			set fixupper [parameter b$cov fixupper]
			set fixlower [parameter b$cov fixlower]
			covariate delete $cov
			covariate $dname
		      parameter b$dname$tsub = $start lower $lower upper $upper
			set p1names(b$dname$tsub) b$cov
			if {"" != $fixupper} {
			    parameter b$dname$tsub fixupper $fixupper
			}
			if {"" != $fixlower} {
			    parameter b$dname$tsub fixlower $fixlower
			}
#
# If not already set,
# compute beta bounds following same algorithm as in covariate.cc
#
			ifdebug0 puts "sum of squares for $cov is $vsums"
			set divisor [expr sqrt ($vsums)]
			if {$divisor == 0} {set divisor 1}
			ifdebug0 puts "divisor is $divisor"
			set thistraitmax [getvar -max $EVD2_dirname/evd2.getmeans.out $traits]
			set thistraitmin [getvar -min $EVD2_dirname/evd2.getmeans.out $traits]
			set deltaT [expr $thistraitmax - $thistraitmin]
			set spreadfactor [option autocovarbound]
			set mulT [expr $deltaT * $spreadfactor]
			ifdebug0 puts "mulT is $mulT"
			set maxbound [expr $deltaT * $spreadfactor / $divisor]
			if {$maxbound < 2.0e-4} {set maxbound 2.0e-4}
			if {$lower == 0 && \
			    "" == [parameter b$dname$tsub fixlower] } {
			    parameter b$dname$tsub lower -$maxbound
			    ifdebug0 puts "setting lower bound to -$maxbound"
			}
			if {$upper == 0 && \
				"" == [parameter b$dname$tsub fixupper]} {
			    parameter b$dname$tsub upper $maxbound
			    ifdebug0 puts "setting upper bound to $maxbound"
			}
		    } else {
			foreach tr $traits {
			    set start($tr) [parameter b$cov\($tr\) =]
			    set upper($tr) [parameter b$cov\($tr\) upper]
			    set lower($tr) [parameter b$cov\($tr\) lower]
			    set fixupper($tr) [parameter b$cov\($tr\) fixupper]
			    set fixlower($tr) [parameter b$cov\($tr\) fixlower]
			}
			covariate delete $cov
			covariate $dname
			foreach tr $traits {
			    set pbname b$dname\($tr\)
			    parameter $pbname = $start($tr) upper $upper($tr)\
				lower $lower($tr)
			    set p1names($pbname) b$cov\($tr\)
			    if {"" != $fixupper($tr)} {
				parameter $pbname fixupper $fixupper($tr)
			    }
			    if {"" != $fixlower($tr)} {
				parameter $pbname fixlower $fixlower($tr)
			    }
			}
#
# If not already set,
# compute beta bounds following same algorithm as in covariate.cc
#
			ifdebug0 puts "sum of squares for $cov is $vsums"
			set divisor [expr sqrt ($vsums)]
			if {$divisor == 0} {set divisor 1}
			ifdebug0 puts "divisor is $divisor"
			foreach tr $traits {
			    set tsub \($tr\)
			    set thistraitmax [getvar -max $EVD2_dirname/evd2.getmeans.out $tr]
			    set thistraitmin [getvar -min $EVD2_dirname/evd2.getmeans.out $tr]
			    set deltaT [expr $thistraitmax - $thistraitmin]
			    set spreadfactor [option autocovarbound]
			    set mulT [expr $deltaT * $spreadfactor]
			    ifdebug0 puts "mulT is $mulT"
			    set maxbound [expr $deltaT * $spreadfactor / $divisor]
			    if {$maxbound < 2.0e-4} {set maxbound 2.0e-4}
			    if {$lower($tr) == 0 && \
			         "" == [parameter b$dname$tsub fixlower]} {
				parameter b$dname$tsub lower -$maxbound
				ifdebug0 puts "setting lower bound to -$maxbound"
			    }
			    if {$upper($tr) == 0 && \
			         "" == [parameter b$dname$tsub fixupper]} {
				parameter b$dname$tsub upper $maxbound
				ifdebug0 puts "setting upper bound to $maxbound"
			    }
			}
		    }
		}
	    }
	} ;# end foreach covariate
    } ;# we have converted evd2 covariates to phase 1

    zscorexp -reset
    for {set i 0} {$i < 2} {incr i} {
	set rets ""

#*******************************************************************
#   Actual maximize is done here with errors caught
#     If this is EVD2, we simply run this to get evd's calculated then
#     real maximize is done later with evd transformed phenotypic data
#*******************************************************************

	set code [catch {eval ccmaximize $args} rets]
	ifdebug0 puts "covariates converted"

	ifdebug0 puts "\nString maximize returned is $rets; code is $code\n"

	if {-1 != [string first "Trap EVD Phase 2" $rets]} {

	    if {[option evdmat] > 0} {
#		puts "Eigenvectors Made, returning blank"
		return ""
	    }
	    option evdphase 2
	    set maxibd [lindex $rets end]
	    ifdebug0 puts "Max IBDID is $maxibd"
#
# Use info file rather than globals because info file
# remains available in future session should user forget
#
	    set EVD_phenname [phenotypes -files]
	    set outfile [open .evd2_info w]
	    puts $outfile "EVD2_dirname=$EVD2_dirname"
	    puts $outfile "EVD2_phenname=$EVD_phenname"
	    close $outfile

	    exec mv pedindex.out [full_filename evd2.pedindex.out]
	    exec mv pedindex.cde [full_filename evd2.pedindex.cde]
	    exec mv phi2.gz [full_filename evd2.phi2.gz]
	    makefakepedindex $maxibd
	    makefakephi2 $maxibd
#
# translate model to EVD2 Phase 2 parameterization
#
	    set ntraits [llength [traits]]

	    load phenotypes [full_filename evddata.out]
	    set covars1 [covariates]
	    set parameters [parameter -return]

# ************************* Phase Two Model ****************************

	    model new
	    ifdebug0 puts "STARTING NEW EVD2 phase 2 model"
	    set newtraits {}
	    set newcovars {}
#
# Traits each have _evd appended
#
	    foreach trait $traits {
		lappend newtraits $trait\_evd
	    }
	    eval trait $newtraits
#
# Covariates have _evd appended at the end of each name
#   use new cov names as in phase 1 model
#
	    foreach cov $covars1 {
		ifdebug0 puts "analyzing cov $cov"
#
# transformed data is already sample restricted, so we may ignore suspended
# covariates in the phase 2 model, and we may also ignore null-trait covariates
#
		if {"Suspended\[" != [string range $cov 0 9] &&
		    "\(\)" != [string range $cov end-1 end]} {

# No need for special handling interactions cause they were all converted
#   prior to phase one (see above).
# However, all covariates need evd appended prior to ()
		    if {-1 == [set pos [string first \( $cov]]} {
			set cov $cov\_evd
		    } else {
			set cov "[string range $cov 0 $pos-1]_evd[string range $cov $pos end]"
		    }
		    eval covariate $cov
		    noscale $cov
		}
	    }

#	    global pnames
	    covariate tmean
	    noscale tmean

# Replay matrix load commands

	    foreach mat $matrices {
		ifdebug0 puts "evaulating $mat"
		if {[lindex $mat 2] != "phi2.gz"} {
		    eval $mat
		}
	    }

# Replay old parameter values, adjusting as needed
# first do mean, btmean, and sd parameters

	    ifdebug0 puts "REPLAYING PARAMETER VALUES"

	    set vparameters {} ;# do variance components last seems to help
	    foreach par $parameters {
		set line "parameter $par"
		ifdebug0 puts "line is $line"
		set pname [lindex $line 1]
		set newpname $pname
		set pname3 [string range $pname 0 2]
		set pname5 [string range $pname 0 4]
#
# parameter mean gets played into btmean
# actual parameter "mean" is a dummy constrained to 0
# setup btmean with correct start, upper, and lower bounds for true mean
# setup pnames array with back translations
#
		if {($ntraits==1 && $pname == "mean") || \
			($ntraits>1 && $pname5 == "mean\(")} {
		    if {$ntraits==1} {
# univariate
			parameter mean = 0 lower 0 upper 1

			set pname btmean
			set pnames(btmean) mean
			set pnames(mean) *
			set ttmean 0
			if {[lindex $line 2] == "="} {
			    set ttmean [lindex $line 3]
			}
			if {$ttmean != 0} {
			    ifdebug0 puts \
				"setting btmean to $ttmean from user"
			    parameter btmean = $ttmean
			} else {
			    set ttmean [getvar -mean \
				    $EVD2_dirname/evd2.getmeans.out $traits]
			    ifdebug0 puts \
				"setting btmean to $ttmean from stats"
			    parameter btmean = $ttmean
			}
			set ttlower 0
			if {[lindex $line 4] == "lower"} {
			    set ttlower [lindex $line 5]
			}
			if {$ttlower != 0} {
			    parameter btmean lower $ttlower
			    ifdebug0 puts \
				"setting btmean lower to $ttlower from user"
			} else {
			    set ttlower [getvar -min \
				       $EVD2_dirname/evd2.getmeans.out $traits]
			    parameter btmean lower $ttlower
			    ifdebug0 puts \
				"setting btmean lower to $ttlower from stats"
			}
			set ttupper 0
			if {[lindex $line 6] == "upper"} {
			    set ttupper [lindex $line 7]
			}
			if {$ttupper != 0} {
			    parameter btmean upper $ttupper
			    ifdebug0 puts \
				"setting btmean upper to $ttupper from user"
			} else {
			    set ttupper [getvar -max \
				       $EVD2_dirname/evd2.getmeans.out $traits]
			    parameter btmean upper $ttupper
			    ifdebug0 puts \
				"setting btmean upper to $ttupper from stats"
			}
		    } else {
# multivariate mean
			ifdebug0 puts "translate multivariate mean parameter"

			set meanname "[string range $pname 0 end-1]_evd\)"
			set pnames(bt$meanname) $pname
			set pnames($meanname) *

			parameter $meanname = 0 lower 0 ;# dummy "mean" param

			set newline "parameter bt$meanname [lrange $line 2 end]"
			ifdebug0 puts "eval $newline"
			eval $newline
			set firstc [string first \( $pname]
			set newtname [string range $pname [expr $firstc+1] end-1]
			if {[parameter bt$meanname =] == 0} {
			    set newtmean [getvar -mean \
					      $EVD2_dirname/evd2.getmeans.out \
					      $newtname]
			    ifdebug0 puts "setting bt$meanname to $newtmean"
			    parameter bt$meanname = $newtmean
			}
			if {[parameter bt$meanname lower] == 0} {
			    set newtmin [getvar -min \
			       $EVD2_dirname/evd2.getmeans.out $newtname]
			  ifdebug0 puts "setting bt$meanname lower to $newtmin"
			    parameter bt$meanname lower $newtmin
			}
			if {[parameter bt$meanname upper] == 0} {
			    set newtmax [getvar -max \
				     $EVD2_dirname/evd2.getmeans.out $newtname]
			  ifdebug0 puts "setting bt$meanname upper to $newtmax"
			    parameter bt$meanname upper $newtmax
			}
		    }
		} elseif {($ntraits==1 && $pname == "sd") || \
			      ($ntraits>1 && $pname3 == "sd\(")} {

# unlike mean, for sd there is no ignored base parameter and bt parameter

		    if {$ntraits==1} {
			set pnames($pname) $pname
			ifdebug0 puts "evaluating $line"
			eval $line
			if {[parameter sd =] == 0} {
			    set tsd [getvar -std \
				    $EVD2_dirname/evd2.getmeans.out $traits]
			    ifdebug0 puts "setting sd to $tsd using stats"
			    parameter sd = $tsd
			}
			if {[parameter sd upper] == 0} {
			    parameter sd upper [expr [parameter sd =] * 5]
			}
# lower defaults to zero unless already set otherwise
		    } else {
			set newpname "[string range $pname 0 end-1]_evd\)"
			set pnames($newpname) $pname
			set line "parameter $newpname [lrange $line 2 end]"
			ifdebug0 puts "evaluating $line"
			eval $line
			if {[parameter $newpname  =] == 0} {
			    set firstc [string first \( $pname]
			    set newtname [string range $pname \
					      [expr $firstc+1] end-1]
			    set tsd [getvar -std \
                                $EVD2_dirname/evd2.getmeans.out $newtname]
			    parameter $newpname = $tsd
			    ifdebug0 puts "setting $newpname to $tsd"
			}
			if {[parameter $newpname upper] == 0} {
			  parameter $newpname upper [expr [parameter $newpname \
							       = ] * 5]
			}
# sd lower would simply be set to zero if not already zero                 
		    }
		} else {
		    ifdebug0 puts "  appending to vparameters"
		    lappend vparameters $line
		}
	    }


#  *** for non-mean non-sd parameters ***
	    ifdebug0 puts "VPARAMTERS!"
	    set last_pname ""
	    foreach line $vparameters {
		ifdebug0 puts "line is $line"
		set pname [lindex $line 1]
		if {$pname == $last_pname} {
		    set domeans 0
		} else {
		    set domeans 1
		}
		set last_pname $pname

		if {[string index $pname 0] == "b"} {
#
# If this is a beta parameter, evd is appended prior to (trait)...if any
#   and each variable included must have _evd appended also
#
# Boundary setting was done prior to EVD-calculating "maximize" so is not
# needed here, we just copy the existing boundaries.
#
		    set oldindex 0
		    set arrow 0  ;# set after arrow since exponent no _evd
		    set newpname ""
		    set thistrait $traits
		    set thisterm ""
		    set vsums 0
		    while {"" != [set nc [string index $pname $oldindex]]} {
			if {$nc == "*" || $nc == "^" || $nc == "("} {
			    if {!$arrow} {
				set newpname "$newpname\_evd$nc"
			    } else {
				set newpname "$newpname$nc"
			    }
			    if {$nc == "^"} {
				set arrow 1
			    } else {
				set arrow 0
			    }
			    if {$nc == "("} {
			       set thistrait [string range $pname \
						  [expr $oldindex+1] \
						  end-1]
			    }
			    set thisterm ""
			} else {
			    set newpname "$newpname$nc"
			    if {$oldindex > 0 && !$arrow} {
				set thisterm "$thisterm$nc"
			    }
			}
			incr oldindex
		    }
		    if {$ntraits==1 && !$arrow} {
			set newpname "$newpname\_evd"
		    }
		    
		} else {

# for non-beta parameters, change nothing now

		    set newpname $pname
		}

# Now, for each multivariate parameter having (tr), including betas,
#  suffix the tr with _evd

		if {$ntraits > 1} {
		    if {"()" != [string range $newpname end-1 end]} {
			if {-1 != [set cpos [string first \( $newpname]]} {
			    set newpname \
				"[string range $newpname 0 end-1]_evd\)"
			}
		    }
		}

# Now we have created new parameter name, associate it with original

		set pnames($newpname) $pname

# Substitute back into line and do the command

		set line "parameter $newpname [lrange $line 2 end]"
		ifdebug0 puts "evaluating: $line"
		eval $line
	    }

# Replay old constrants
#   and options

	    set inmodel [open $outfilename.evdphase1.mod]
	    gets $inmodel line
	    while {"" != $line} {
		if {[lindex $line 0] == "constraint"} {
		    if {$ntraits == 1} {
			ifdebug0 puts "evaluating $line"
			eval $line
		    } else {
			set newline ""
			while {-1 != [set ppos [string first ")" $line]]} {
			    set newline "$newline[string range $line 0 [expr $ppos-1]]_evd\)"
			    set line [string range $line [expr $ppos+1] end]
			}
			set newline $newline$line
			ifdebug0 puts "evaluating $newline"
			eval $newline
		    }
		} elseif {[lindex $line 0] == "option"} {
		    set optionname [lindex $line 1]
		    set ifevd [string range [lindex $line 2] 0 2]
		    ifdebug0 puts "ifevd is $ifevd"
		    if {$ifevd != "evd" && $optionname != "MergeAllPeds"} {
			ifdebug0 puts "evaluating $line"
			eval $line
		    }
		}
		set linelen [gets $inmodel line]
		ifdebug0 puts "got line $line with len $linelen"
		if {$linelen <= 0} {
		    ifdebug0 puts "breaking"
		    break
		}
	    }
	    ifdebug0 puts "closing model file"
	    close $inmodel

# special new constraints for mean

	    foreach tr $newtraits {
		if {$ntraits > 1} {
		    ifdebug0 puts "doing constraint mean($tr) = 0"
		    constraint mean($tr) = 0
		} else {
		    constraint mean = 0
		}
	    }

# do user omega if provided

	    if {[if_global_exists SOLAR_EVD2_omega]} {
		global SOLAR_EVD2_omega
		if {[catch {eval $SOLAR_EVD2_omega} errmes]} {
		    evd2_restore_phen
		    error "user omega returned $errmes"
		}
	    } else {		


	    if {$ntraits  == 1} {
		omega = pvar*(h2r*lambda_i + e2)
	    } elseif {$ntraits == 2} {
                omega = <sd(ti)>*<sd(tj)>*(teq*(<h2r(ti)>*lambda_i + <e2(ti)>) + tne*(sqrt(abs(<h2r(ti)>))*sqrt(abs(<h2r(tj)>))*lambda_i*rhog + sqrt(abs(<e2(ti)>))*sqrt(abs(<e2(tj)>))*rhoe))
	    } else {
		ifdebug0 puts "doing trivariate omega"
                omega = <sd(ti)>*<sd(tj)>*(teq*(<h2r(ti)>*lambda_i + <e2(ti)>) + tne*(sqrt(<h2r(ti)>)*sqrt(<h2r(tj)>)*lambda_i*rhog_ij + sqrt(<e2(ti)>)*sqrt(<e2(tj)>)*rhoe_ij))
	    }

	}
	    option evdoptimizations 1
	    save model $outfilename.evdphase2

	    ifdebug0 puts "begging phase 2 maximization"
	    set code2 [catch {maximize -o $userout} rets2]
	    ifdebug0 puts "Phase 2 maximize returned $code2 $rets2"

	    set loglike [loglike]
	    set maxpars [parameter -return]

	    save model $outfilename.evdphase2m
	    set thisout $userout

	    if {[string range $userout end-3 end] != ".out"} {
		set thisout $userout.out
	    }
	    exec cat $descout.out $thisout >$userout.tmp
	    exec mv $userout.tmp $thisout
#
# Restore original pedigree and phenotypes
#
	    evd2_restore_phen
#
# (since we just saved this model, it is mostly assumed to be "correct")
#
# Restore original model, then load phenotype values from phase 2 model into it
#
	    set imean 0
	    load model $outfilename.evdphase1

# Read parameter values from phase2 model

	    foreach mp $maxpars {
		ifdebug0 puts "Processing: $mp"
		set pname2 [lindex $mp 0]
		if {[catch {set pname $pnames($pname2)}]} {
		    puts "these are available: [array names pnames]"
		    error "Don't recognize parameter $pname2"
		}
		ifdebug0 puts "Parameter name is now $pname"

		if {[catch {set pname $p1names($pname)}]} {}

		if {$pname == "*"} {continue} ;# skip mean parameter
		set line "parameter $pname [lrange $mp 1 end]"
		ifdebug0 puts "   Evaluating:  $line"
		eval $line
	    }

	    loglike set $loglike
	    option modeltype evd2
	    option evdphase 3
	    return -code $code2 $rets2 ;# don't return evd2 trap
	    }

	if {[lindex $rets 0] != "Trap"} {

# TRAP is used when previous maximize sets up global env so next maximize
# can run.  It is now used for zscore in expressions, but could be extended
# to inormal, etc.
	    break
	}
    }
    return -code $code $rets
}

proc copyparameter {name values} {
    parameter $name = [parameter $values =]
    parameter $name se [parameter $values se]
    parameter $name lower [parameter $values lower]
    parameter $name upper [parameter $values upper]
    parameter $name score 0
    return ""
}

# solar::evd2 -- private
# solar::evd -- private
#
# EVD1 and EVD2
#
# EVD1 is our first EVD method, only works for univariate quantitative traits.
# EVD2 is our second EVD method, intended for all standard models, including
#   multivariate.
#
# To nudge a model to be an EVD1 model you give the command:
#
# option modeltype evd
#
# and to nudge a model to be an EVD2 model, you give the command:
#
# option modeltype evd2
#
# Then you set the trait, covariates, etc., just as for a regular model.
# Then you can run "maximize", "polygenic", or other model maximizing
# command as with original models.
#
# EVD2 maximization is faster because it eliminates the need for matrix
# inversions, and it is no more complicated for the user than normal
# maximization.  However, the maximization process is fairly complicated.
# During a phase 1 maximization the sample is determined as usual and
# transformed data files are output.  Covariates are converted into definitions
# so that each term can be scaled correctly.  A special model is created based
# on the user model that uses these transformed data, and maximized.
# Then the model is translated back into the usual form and the original
# phenotypes and pedigree files are enabled without having to be reloaded.
#
# Because of this complexity, it is possible that EVD2 maximization will
# fail and leave the state of pedigree and phenotypes loaded as invalid..  If
# this happens, it is recommended to reload the pedigree file, reload the
# phenotypes file, and give the command "model new" before creating
# additional models.  If the failure has just happened, you may also be
# able to use the shortcut command:
#
#   evd2_restore_phen
#
# to restore the pedigree and phenotypes to a valid state.  Note that the
# transformed phenotypes and special model files are stored in the current
# maximization output directory.  Note that EVD maximization of both types
# suppresses the usual iteration output for greater efficiency.
#
#-


# SOLAR::evd2_restore_phen --
#
# Purpose: restore the pedigree and phenotypes files used prior to EVD2 phase 2
#
# Usage: evd2_restore_phen
#
# Note: saves fake pedigree files in phase two output directory
# -

proc evd2_restore_phen {} {

    if {![file exists .evd2_info]} {
	return "Did not run EVD2 phase 1 to completion, or phen already restored"
    }
    set infile [open .evd2_info]
    gets $infile line
    set EVD2_dirname [lindex [split $line =] 1]
    gets $infile line
    set EVD2_phenname [lindex [split $line =] 1]
    close $infile

    exec mv pedindex.out [full_filename evd2.pedindex.out].phase2
    exec mv pedindex.cde [full_filename evd2.pedindex.cde].phase2
    exec mv phi2.gz [full_filename evd2.phi2.gz].phase2

    exec mv $EVD2_dirname\evd2.pedindex.out pedindex.out
    exec mv $EVD2_dirname\evd2.pedindex.cde pedindex.cde
    exec mv $EVD2_dirname\evd2.phi2.gz phi2.gz
    eval load phen $EVD2_phenname
    file delete .evd2_info
    return OK
}

#
# SOLAR::makefakepedindex -- private
#
# Purpose: generate an all-founders pedindex for EVD2
#
# Usage: makefakepedindex maxibdid
#-

proc makefakepedindex {maxibdid} {
#
# copy new pedindex.cde from example
#
    set ofile [open pedindex.out w]
    set idlen [string length $maxibdid]
    for {set i 1} {$i <= $maxibdid} {incr i} {
	set id [format %5s $i]
	set idl [format %$idlen\s $i]
	puts $ofile \
	    "$id     0     0 1   0 $id     0 $idl"
    }
    close $ofile
    makefakepedindexcde $idlen
}

proc makefakepedindexcde {idlen} {
    set ofile [open pedindex.cde w]
    puts $ofile "pedindex.out                                          "
    puts $ofile " 5 IBDID                 IBDID                       I"
    puts $ofile " 1 BLANK                 BLANK                       C"
    puts $ofile " 5 FATHER'S IBDID        FIBDID                      I"
    puts $ofile " 1 BLANK                 BLANK                       C"
    puts $ofile " 5 MOTHER'S IBDID        MIBDID                      I"
    puts $ofile " 1 BLANK                 BLANK                       C"
    puts $ofile " 1 SEX                   SEX                         I"
    puts $ofile " 1 BLANK                 BLANK                       C"
    puts $ofile " 3 MZTWIN                MZTWIN                      I"
    puts $ofile " 1 BLANK                 BLANK                       C"
    puts $ofile " 5 PEDIGREE NUMBER       PEDNO                       I"
    puts $ofile " 1 BLANK                 BLANK                       C"
    puts $ofile " 5 GENERATION NUMBER     GEN                         I"
    puts $ofile " 1 BLANK                 BLANK                       C"
    puts $ofile " $idlen ID                    ID                          C"
    close $ofile
}

proc makefakephi2 {maxibdid} {
    set ofile [open phi2 w]
    for {set i 1} {$i < $maxibdid} {incr i} {
	set id [format %5s $i]
	puts $ofile "$id $id  1.0000000  1.0000000"
    }
    close $ofile
    exec gzip phi2
}


# SOLAR::restart-if-can -- private
#
# Purpose: Block SOLAR crunching retries (default is not blocked)
#
# Usage:  restart-if-can 1    ; allow retries
#         restart-if-can 0    ; block retries
#         restart-if-can      ; show current status
#
# Note:   this is used inside the maximize procedure
#-

proc restart-if-can {args} {
    global Solar_Restart_If_Can
    if {[llength $args]} {
	set Solar_Restart_If_Can $args
	return
    }
    return [use_global_if_defined Solar_Restart_If_Can 1]
}


# solar::inormal -- 
#
# Purpose:  Save inverse normal transformation to a file (see also define)
#
# IMPORTANT: To create a model using an inverse normal transformation,
#            it is more convenient to use the "define" command, and NOT
#            the inormal command.  The "inormal" command itself is for
#            those rare situations where you need to save the inverse
#            normal transformation to a file for some other purpose.
#
# Usage:    define <defname> = inormal_<phenotype>
#           trait <defname>
#
#           inormal -trait <trait> [-file <filename>] -out <filename>
#                   [-phenfile] [-nofamid] [-class <class>]
#
#           -class <class> only include ID's when their class variable equals this value
#
#           (See notes below for obscure forms of the inormal command not
#            recommended for most users.)
#
# Notes:    For the "define" command, the <defname> can be any name you
#           can make up.  The inormal_ prefix may be abbreviated down to inor_ .
#           The <phenotype> is any phenotypic variable in the currently
#           loaded phenotypes file.
#
#           For the "inormal" command itself, you must use one of the
#           arguments "-phenfile" or "-file <filename>".  The "-phenfile"
#           argument is a shorthand way of specifying the currently loaded
#           phenotypes file.  The "-file <filename>" argument is used to
#           specify any file.  In either case, the file must be in the form
#           of a phenotypes file, with fields for <trait> and ID (and FAMID
#           if required to make the ID's unique).  BE SURE TO SPECIFY THE
#           "-out" ARGUMENT FOR THE OUTPUT FILE.
#
#           The inverse normal transformation of a dataset is performed
#           by the following procedure:
#
#             The trait values are sorted, and for any value V found
#             at position I in the sorted list, a quantile is computed
#             for it by the formula I/(N+1).  The inverse normal
#             cumulative density function (see "normal") is computed for
#             each quantile and stored in an array keyed by ID, and 
#             FAMID if applicable.  When the value V occurs multiple times,
#             the inverse normal is computed for each applicable quantile,
#             averaged, then the average is what is stored for each ID.
#             These values are accessed when the ID is provided.  The
#             array for each trait is deleted by the -free option.
#           
#           See also the "normal" command, which computes normal distribution
#           functions.  inormal uses a "normal -inverse" command.
#
#           OBSCURE FORMS OF THE INORMAL COMMAND
#
#           Largely for internal purposes, such as the implementation of
#           the define command, there are additional obscure forms of the
#           inormal command which save the inverse normal results in
#           a tcl variable for access on an individual ID basis:
#
#           inormal -trait <trait> [-file <filename>] -tclvar
#                   [-phenfile] [-nofamid] 
#           inormal -trait <trait> -id <id> [-famid <famid>]
#           inormal -free <trait>
#           inormal -reset
#
#           The first form above is like the standard form, except that the
#           -out argument is replaced with a -tclvar argument, signifying
#           that the inverse normal results are to be saved to a Tcl variable
#           associated with the trait name.  In the second form, a result is
#           obtained from previously stored data for each ID.  In the third
#           form, stored data for a particular trait is freed.  In the
#           fourth form, all stored data is freed.
#
#           The -out and -tclvar arguments cannot be used at the same time.
#           If the -out argument is used, inverse normals are simply written
#           to a file and nothing is stored, so the second form cannot be
#           used.
#
#           FAMID should only be specified if required.
#           The rules regarding FAMID are almost identical with
#           those used during maximization, so that in general you don't
#           have to think about them.  If FAMID field is found in both
#           pedigree and phenotypes files, or if pedigree file isn't loaded
#           (which wouldn't be allowed during maximization) and FAMID is
#           found (only) in phenotypes file,  FAMID is automatically required,
#           unless the -nofamid argument is used.  If FAMID is found in
#           only one of the two files (and both are loaded), a test for
#           ID uniqueness is performed, then if ID's are unique without FAMID,
#           it is not required, otherwise FAMID is required and if not present,
#           it is an error.  FAMID can be mapped to any other field name using
#           the field command.
#
#           When using these obscure forms of the inormal command, it is
#           recommended to load the data and then use it in short order,
#           even though the inormal command doesn't intrinsically require
#           this.  Internal "inormal" data is not saved from one SOLAR
#           session to the next.
#
#           BEWARE that "maximize" or any SOLAR command that performs
#           maximization, such as "polygenic" or "multipoint", may clear
#           out inverse normal data stored using -tclvar.  Also, if
#           different layers of procedures get inormals on traits
#           with the same name from different files, and their inormal
#           operations overlap, there
#           could be problems.
#
#           When the -class option is used, the traitname
#           is qualified with a suffix like .SOLARclass.1 (where 1 is the class
#           number).  To free such a classed trait, the fully suffixed name must
#           be used.  For example, for trait q4 and class 1, the command would be:
#
#               inormal -free q4.SOLARclass.1
# -

# Globals:
# SOLAR_Inormal_Traits:  List of trait names currently stored
# SOLAR_Inormal_Results_<i>(<id>)
#   Arrays of results for each <i> where <i> is trait index
#   and <id> is formatted ID.  ID is formatted like this:
#     <id>.famid.<famid>
#   <famid> can be null if famid not present or -nofamid arg given
#   Why this ugly name to combine id and famid?  Because it's unambiguous
#   and ugly enough that no one would be likely to preformat their ID's
#   this way.
#
#   Examples:
#       299.famid.       ID=299  no famid
#       299.famid.10     ID=299  FAMID=10
#-


proc inormal {args} {

    global SOLAR_Inormal_Traits   ;# Stored traits, indexes into other lists

    set LOCAL_DEBUG 0

    set phen_filename ""
    set traitname ""
    set free_traitname ""
    set get_id ""
    set famid ""
    set nofamid 0
    set phenfile 0
    set reset 0
    set outfilename ""
    set tclvar 0
    set class ""
    set classpos 2

    set badargs [read_arglist $args \
		     -file phen_filename \
                     -out outfilename \
		     -free free_traitname \
		     -trait traitname \
		     -id get_id \
		     -nofamid {set nofamid 1} \
		     -famid famid \
		     -debug {set LOCAL_DEBUG 1} \
		     -phenfile {set phenfile 1} \
		     -reset {set reset 1} \
		     -tclvar {set tclvar 1} \
		     -class class
		     ]
    if {"" != $badargs} {
	error "inormal: Invalid arguments: $badargs"
    }
#
# Check for valid options
#
    if {$reset && 1 != [llength $args]} {
	error "inormal: -reset must not be used with other arguments"
    }
    if {"" != $outfilename && 0 == $phenfile && "" == $phen_filename} {
	error "inormal: -out option requires -file or -phenfile option"
    }
#
# If class option, add qualifier to trait name
#
    set traitclassname $traitname
    if {$class != ""} {
	set traitclassname $traitname.SOLARclass.$class
    }
#
# Load new trait from new datafile *******************************************
#
    if {"" != $phen_filename || $phenfile} {
	if {"" == $traitname} {
	    error "inormal: -trait required with -file"
	}
	if {"" != $phen_filename && $phenfile} {
	    error "inormal: Either specify -phenfile or -file <filename>, not both"
	}
	if {"" != $outfilename} {
	    catch {file delete $outfilename}
	    if {[file exists $outfilename]} {
		error "File $outfilename is protected from deletion"
	    }
	}

#
# Check for previous data
#
	if {[if_global_exists SOLAR_Inormal_Traits] && \
		-1 < [lsearch $SOLAR_Inormal_Traits $traitclassname]} {
	    error "inormal: Must free previous data:  inormal -free $traitclassname"
	}
#
# Get Ranking Identifier (rid) to identify this ranking information
#
	if {![if_global_exists SOLAR_Inormal_Traits]} {
	    set SOLAR_Inormal_Traits [list $traitclassname]
	} else {
	    lappend SOLAR_Inormal_Traits $traitclassname
	}
	set rid [expr [llength $SOLAR_Inormal_Traits] - 1]
	if {$LOCAL_DEBUG} {puts "Using RID $rid"}
#
# Find trait in phenotypes file(s)
#
	if {$phenfile} {
	    set files [phenotypes -files]
	    foreach file $files {
		if {[file exists $file]} {
		    set sid [solarfile open $file]
		    if {[solarfile $sid test_name $traitname]} {
			solarfile $sid close
			set phen_filename $file
			break
		    }
		    solarfile $sid close
		}
	    }
	    if {"" == $phen_filename} {
		error "inormal: trait not found in phenotypes file(s)"
	    }
	}

#
# Open datafile
#
	if {![file exists $phen_filename]} {
	    catch {inormal -free $traitclassname}
	    error "inormal: Data file $phen_filename not found"
	}
	set datafile [solarfile open $phen_filename]

# set up id, trait, [famid]

	set inormal_data {}

	solarfile $datafile start_setup
	if {[catch {solarfile $datafile setup id}]} {
	    solarfile $datafile close
	    catch {inormal -free $traitclassname}
	    error "inormal: ID field not found in $phen_filename"
	}
	if {[catch {solarfile $datafile setup $traitname}]} {
	    solarfile $datafile close
	    catch {inormal -free $traitclassname}
	    error "inormal: Variable $traitname not found in $phen_filename"
	}
	set need_famid 0
	global SOLAR_Inormal_Famid
	if {!$nofamid} {
	    if {[catch {set need_famid [check_phenotypes $phen_filename]} errmes]} {
		if {-1 == [string first uplicate $errmes]} {
		    error "inormal: error reported in phenotypes file:\n$errmes"
		}
		set need_famid 1
	    }
	}
	if {$need_famid} {
	    lappend SOLAR_Inormal_Famid $traitclassname
	} else {
	    if {[if_global_exists SOLAR_Inormal_Famid]} {
		set SOLAR_Inormal_Famid [remove_from_list $SOLAR_Inormal_Famid $traitclassname]
	    }
	}
	if {$need_famid} {
	    incr classpos
	    solarfile $datafile setup famid
	}
	if {$class != ""} {
	    solarfile $datafile setup class
	}
	if {"" != $outfilename} {
	    if {$need_famid} {
		putsout -q -d. $outfilename "id,famid,inormal_$traitname"
	    } else {
		putsout -q -d. $outfilename "id,inormal_$traitname"
	    }
	}
#
# Read data, inserting data and key values in list
# to be sorted later
#
	set Count 0
	set check_keys {}
	while {{} != [set line [solarfile $datafile get]]} {
	    set id [lindex $line 0]
	    set data [lindex $line 1]
	    if {{} == $data} {continue}  ;# Missing data?
	    if {$class != ""} {
		set classfound [lindex $line $classpos]
		if {$classfound != $class} {continue}
	    }
	    set famid ""
	    if {$need_famid} {
		set famid [lindex $line 2]
	    }
	    set key "$id.famid.$famid"
#	    puts "key is $key"
#	    puts "check_keys is $check_keys"
	    if {-1 != [lsearch $check_keys $key]} {
		solarfile $datafile close
		catch {inormal -free $traitclassname}
		error "inormal: ID $id repeated in $phen_filename; FAMID required"
	    }
	    lappend check_keys $key
#
# Append new data to list
#
	    lappend inormal_data [list $key $data]
	    incr Count
	}
	solarfile $datafile close
#
# Now, sort list and create array with results
#
	set inormal_data [lsort -index end -real $inormal_data]
	global SOLAR_Inormal_Results_$rid

	set last_data {}
	set last_position -1

	set data_length [llength $inormal_data]
	set current_position 0
	set force_last 0
	foreach element $inormal_data {
	    incr current_position
	    set key [lindex $element 0]
	    set data [lindex $element 1]

 	    set pass 1
	    while {$pass || $force_last} {
		set pass 0
		if {$LOCAL_DEBUG} {puts "current_position: $current_position"}

		if {{} == $last_data} {
		    set last_data $data
		    set last_position $current_position
		} else {
		    if {$data > $last_data || $force_last} {
#
# New different data value
# So process previous same data values
#
			set matches 0
			set sum 0.0
			for {set i $last_position} {$i < $current_position} {incr i} {
			    set pct [expr double($i) / double($Count + 1.0)]
			    set z [normal -inverse $pct]
			    set sum [expr $sum + double($z)]
			    incr matches
			}
			set zavg [expr $sum / $matches]
			for {set i $last_position} {$i < $current_position} {incr i} {
			    set ele [lindex $inormal_data [expr $i - 1]]
			    set key [lindex $ele 0]
			    if {$LOCAL_DEBUG} {puts "Assigning $zavg to $key"}
			    eval set [catenate SOLAR_Inormal_Results_$rid (\$key)] \$zavg
			    if {"" != $outfilename} {
				set endpos [string first .famid. $key]
				set id [string range $key 0 [expr $endpos - 1]]
				set famid [string range $key [expr $endpos + 7] end]
				if {$need_famid} {
				    putsout -q -d. $outfilename "$id,$famid,$zavg"
				} else {
				    putsout -q -d. $outfilename "$id,$zavg"
				}
			    }
			}
			
#
# Now store new data value
#
			set last_data $data
			set last_position $current_position
			if {$force_last} {
			    break
			}
		    }
#
# Force one last iteration to store last element(s)
#
		    if {$current_position == $data_length} {
			set force_last 1
			incr current_position
		    }
		}
	    }
	}
	set save_count $Count

	if {"" != $outfilename} {
	    inormal -free $traitclassname
	}

	return $save_count
    }
#
# Get Percentile *************************************************************
#
    if {"" != $get_id} {
	if {"" == $traitname} {
	    error "inormal: -id option requires -trait"
	}
	if {![if_global_exists SOLAR_Inormal_Traits]} {
	    error "inormal: no traits have been loaded for ranking"
	}

# build key, adding famid only if required, but requiring it if it is required

	global SOLAR_Inormal_Famid
	if {"" == $famid} {
	    if {[if_global_exists SOLAR_Inormal_Famid]} {
		if {-1 != [lsearch $SOLAR_Inormal_Famid $traitclassname]} {
		    error "inormal: -famid required for trait $traitname"
		}
	    }
	    set get_id [catenate $get_id .famid.]
	} else {
	    if {![if_global_exists SOLAR_Inormal_Famid] || \
		    -1 == [lsearch $SOLAR_Inormal_Famid $traitclassname]} {
		set get_id [catenate $get_id .famid.]
	    } else {
		set get_id [catenate $get_id .famid. $famid]
	    }
	}

	set rid [lsearch $SOLAR_Inormal_Traits $traitclassname]
	if {-1 == $rid} {
	    if {$class == ""} {
		error "inormal: trait $traitname is not loaded"
	    } else {
		error "inormal: trait $traitname with class $class not loaded"
	    }
	}

	global SOLAR_Inormal_Results_$rid

	set inormal {}
	catch {
	   eval set inormal [catenate \$SOLAR_Inormal_Results_ $rid (\$get_id)]
	}
#
# Return data found or null
#
	return $inormal
    }
#
# "-free" option
#
    if {"" != $free_traitname} {
	if {"" != $class} {
	    set free_traitname $free_traitname.SOLARclass.$class
	}
	if {[if_global_exists SOLAR_Inormal_Traits]} {
	    if {-1 != [set rid [lsearch $SOLAR_Inormal_Traits $free_traitname]]} {
		global SOLAR_Inormal_Results_$rid
		catch {unset SOLAR_Inormal_Results_$rid}
		set SOLAR_Inormal_Traits [remove_from_list $SOLAR_Inormal_Traits $free_traitname]
		if {[if_global_exists SOLAR_Inormal_Famid]} {
		    global SOLAR_Inormal_Famid
		    set SOLAR_Inormal_Famid [remove_from_list $SOLAR_Inormal_Famid $free_traitname]
		}
		return ""
	    }
	    error "inormal:  No data saved for trait $free_traitname"
	}
	error "inormal:  No data saved"
    }
#
# -reset option
#
    if {"" != $reset} {
	if {[if_global_exists SOLAR_Inormal_Traits]} {
	    global SOLAR_Inormal_Traits
	    set rid -1
	    foreach trait $SOLAR_Inormal_Traits {
		incr rid
		global SOLAR_Inormal_Results_$rid
		catch {unset SOLAR_Inormal_Results_$rid}
	    }
	    unset SOLAR_Inormal_Traits
	}
	if {[if_global_exists SOLAR_Inormal_Famid]} {
	    global SOLAR_Inormal_Famid
	    unset SOLAR_Inormal_Famid
	}
	return ""
    }
	
    error "inormal: invalid command form (combination of arguments)"
}


# solar::normal --
#
# Purpose:  Normal distribution functions
#
# Usage:    normal -i[nverse] <p>
#
# Notes:    Currently, the only supported function is the "inverse normal
#           cumulative density function", which maps the open range
#           0,1 to the whole real line.  (The values for 0 and 1 are
#           out of range because they would be negative and positive
#           infinity.)
#
#           This normal function is used by the inormal procedure to
#           perform an inverse normal transformation on a dataset.
#           For further information, see the help for "inormal".
#           In turn, the inormal procedure is part of the mechanism
#           behind the "inormal_" prefix which may be applied to
#           phenotypes in the define command.
#
#           We will add additional normal distribution functions here as
#           we need them.
#
#           Our implementation is indirectly based on:
#
#             Cody, W.D. (1993). "ALGORITHM 715: SPECFUN - A Portabel FORTRAN
#             Package of Special Function Routines and Test Drivers"
#             ACM Transactions on Mathematical Software. 19, 22-32.
# -


# solar::stats --
#
# Purpose:  Get and/or show statistics for any variable in a file
#
# Usage:    stats [<variable>+ | -all [-file <filename>]] [-q] [-return]
#                 [-out <outfile>] -sample
#
#           -all     show stats for all variables in phenotypes file
#           -return  do not write output file, return list of stats;
#                      use stats_get to parse return list
#           -q       do not display to terminal
#           -out     specify alternate output filename; also returns list
#                    of stats
#           -sample  Use only the sample of the current model (there must be
#                    a current model, or at least a defined trait, and 
#                    statistics can only be computed for the trait(s) and
#                    covariate(s) in that model)
#
#           The default variable is the current trait, and the default
#           filename is the currently loaded phenotypes file.  You may also
#           specify one or more variables.
#
#           Results are written to stats.out in the CURRENT WORKING DIRECTORY.
#           (Not the usual output directory, since the trait need not be set.)
#           They are also displayed on the terminal.
#
#           The statistics computed are mean, minimum, maximum, standard
#           deviation, skewness, and kurtosis.  (Note: We define kurtosis
#           as 0 for a standard normal distribution; 3 has already been
#           subtracted from the normalized 4th central moment.)
#
#           See also the zscore command, which uses these stats to
#           zscore the current trait during maximization.  The zscore
#           procedure uses stats with the -out option.
#
#           If there are multiple phenotypes files, joinfiles will be
#           used to create a joined file in the working directory named
#           joinfiles.stats.[pid].[uname -n].out.  Non-unique fieldnames
#           will be renamed following the rules of joinfiles.  Under most
#           circumstances, this joined file will be deleted before
#           stats returns.  To run through the entire contents (and names)
#           in the joined file, use the "stats -all" command.
#
#           Variables created by a "define" command will work with stats
#           beginning with SOLAR version 4.2.2.  Note that such variables
#           are not evaluated by the command "stats -all".
# -

proc stats {args} {

    set variable ""
    set pf ""
    set outfile ""
    set outr ""
    set allvars 0
    set quiet 0
    set return 0
    set joinedfile 0
    set deletefile ""
    set fortrand 0
    set use_sample 0
    set add_vars ""

    if {{} != $args} {
	set variable [read_arglist $args -file pf -out outfile \
			  -all {set allvars 1} -q {set quiet 1} \
			  -sample {set use_sample 1} \
			  -return {set return 1}]
    }

    if {$use_sample} {
	if {[catch {trait}]} {
	    error "stats: trait must be defined for -sample"
	}
    }

    if {"" == $outfile} {
	set outfile stats.out
    } else {
	set return 1
    }

    if {$quiet} {
	set putsout_quiet 1
    } else {
	set putsout_quiet 0
    }
    
    set pfs $pf
    if {"" == $pf} {
	set pf [set pfs [phenotypes -files]]
	if {[llength $pfs] > 1} {
	    set deletefile joinfiles.stats.[pid].[exec uname -n].out
	    set pf $deletefile
	    eval joinfiles $pfs -o $pf
	    set joinedfile 1
	}
    }

    if {$allvars} {
	if {$use_sample} {
	    set testvariable [concat [trait] [covar]]
	    set variable ""
	    foreach var $testvariable {
		set subvar [split $var "*^"]
		set firstvar [lindex $subvar 0]
		if {"sex" != $firstvar} {
		    setappend variable $firstvar
		}
	    }
	} else {
	set phen [tablefile open $pf]
	set variable [tablefile $phen names]
	tablefile $phen close
	set variable [remove_from_list_if_found $variable id]
	set variable [remove_from_list_if_found $variable fa]
	set variable [remove_from_list_if_found $variable mo]
	set variable [remove_from_list_if_found $variable ego]
	set variable [remove_from_list_if_found $variable sire]
	set variable [remove_from_list_if_found $variable dam]
	}

    } elseif {"" == $variable} {
	if {[catch {set variable [trait]} errmsg]} {
	    if {$joinedfile} {
		file delete $deletefile
	    }
	    error $errmsg
	}
    } 

    notquiet puts ""
    putsout -d. $outfile "Phenotypes File: $pfs"
    set trno 0
    set dnames [define names]
    set got_sample 0
    foreach tr $variable {
	incr trno
	if {$use_sample} {
	    if {!$got_sample} {
		set savemodelname stats.presave.[pid]
		save model $savemodelname
		if {-1 != [string first Use_polygenic_to [omega]]} {
		    spormod
		}
		maximize -q -sampledata
		load model $savemodelname
		file delete $savemodelname.mod
		set got_sample 1
	    }
	    set phen [tablefile open [full_filename sampledata.out]]
	    if {-1 != [lsearch [trait] $tr]} {
		set tindex [expr 1 + [lsearch [trait] $tr]]
		set tabname trait$tindex
	    } else {
		set tabname $tr
	    }
	} elseif {-1 != [lsearch $dnames $tr]} {
	    set savemodelname stats.presave.[pid]
	    save model $savemodelname
	    model new
	    trait $tr
	    spormod
	    notquiet puts "\nEvaluating Definition: [define $tr]"
	    maximize -q -sampledata
	    set phen [tablefile open [full_filename sampledata.out]]
	    set tabname trait1
	    load model $savemodelname
	    file delete $savemodelname.mod
	} else {
	    set phen [tablefile open $pf]
	    set tabname $tr
	}
	tablefile $phen start_setup
	if {[catch {tablefile $phen setup $tabname} errmsg]} {
	    tablefile $phen close
	    if {1 == [llength $pfs]} {
		if {$use_sample} {
		    error "Variable $tabname not included in model"
		} else {
		    error $errmsg
		}
	    } else {
		if {$joinedfile} {
		    file delete $deletefile
		}
		if {$use_sample} {
		    error "Variable $tabname not included in model"
		} else {
		    error "$errmsg\n  $tr is not found or unique in files specified"
		}
	    }
	}
#
# Read records to compute mean and check if discrete
#
	set mean 0
	set min 0
	set max 0
	set sd NaN
	set skew NaN
	set kurt NaN
	set discrete 1
	set alpha 0

	set count 0
	set sum 0.0
	set missing 0

	while {{} != [set record [tablefile $phen get]]} {
	    set value [lindex $record 0]
	    if {{} == $value} {
		incr missing
	    } else {
		incr count
		if {$count == 1} {
		    set min $value
		}
		if {!$alpha} {
		    if {![is_float $value]} {
			set trialfloat [regsub D $value E]
			if {[is_float $trialfloat]} {
			    set value $trialfloat
			    set fortrand 1
			} else {
			    set alpha 1
			    set max $value
			    set discrete 0
			}
		    }
		}
		if {!$alpha} {
		    set sum [expr $sum + $value]
		    if {$count == 1} {
			set max $value
			set value1 $value
			set values_seen 1
		    } else {
			if {$min > $value} {set min $value}
			if {$max < $value} {set max $value}
			if {$values_seen == 1} {
			    if {$value != $value1} {
				set value2 $value
				set values_seen 2
			    }
			} elseif {$values_seen == 2} {
			    if {$value != $value1 && $value != $value2} {
				set values_seen 3
				set discrete 0
			    }
			}
		    }
		}
	    }
	}
	
	if {!$alpha && $count > 0} {
	    set mean [expr double ($sum) / double ($count)]
	}
	set dc [expr double ($count)]
#
# Reread records to compute SD
#
	if {!$alpha} {
	    if {$count > 1} {
		tablefile $phen rewind
		set sumsd 0.0
		while {{} != [set record [tablefile $phen get]]} {
		    set value [lindex $record 0]
		    if {{} != $value} {
			if {$fortrand} {
			    set value [regsub D $value E]
			}
			set dev [expr double($value) - $mean]
			set sdev [expr $dev * $dev]
			set sumsd [expr $sumsd + $sdev]
		    }
		}
		set var [expr $sumsd / double ($count - 1)]
		set sd [expr sqrt ($var)]
	    }
	
#
# Reread records to compute skewness and kurtosis
#
	    if {$count > 2 && $sd != 0} {
		tablefile $phen rewind
		set d3 0.0
		set d4 0.0
		while {{} != [set record [tablefile $phen get]]} {
		    set value [lindex $record 0]
		    if {{} != $value} {
			if {$fortrand} {
			    set value [regsub D $value E]
			}
			set dev [expr double($value) - $mean]
			set devd [expr $dev / $sd]
			set d3 [expr $d3 + ($devd * $devd * $devd)]
			set d4 [expr $d4 + ($devd * $devd * $devd * $devd)]
		    }
		}
		set skew [expr $d3 * $dc / (($dc - 1.0) * ($dc - 2.0))]
		catch {
		    set kurt [expr ($d4 * ($dc*($dc+1.0)) / \
					(($dc-1.0)*($dc-2.0)*($dc-3.0))) - \
				  ((3.0*($dc-1.0)*($dc-1.0)) / \
				       (($dc-2.0)*($dc-3.0)))]
		}
	    }
	}
#
# Report and save statistics
#
	tablefile $phen close
	if {$use_sample} {
	    putsout -d. $outfile "\nVariable:  $tr  Sample Size: $count"
	} else {
	    putsout -d. $outfile "\nVariable:  $tr  Sample Size: $count  Missing: $missing"
	}
	if {$alpha} {
	    putsout -d. $outfile   "First:                 $min"
	    putsout -d. $outfile   "Last:                  $max"
	    putsout -d. $outfile   "This variable contains alphanumeric entries and cannot be used as a phenotype."
	} else {
	    putsout -d. $outfile "\nMean:                  [format %.9g $mean]"
	    putsout -d. $outfile   "Minimum:               [format %.9g $min]"
	    putsout -d. $outfile   "Maximum:               [format %.9g $max]"
	    putsout -d. $outfile   "Standard Deviation:    [format %.9g $sd]"
	    putsout -d. $outfile   "Skewness:              [format %.7g $skew]"
	    putsout -d. $outfile   "Kurtosis:              [format %.7g $kurt]"
	}
#
# Check for discrete
#
	if {!$alpha && $discrete} {
	    if {($min==$max) || $max-$min>0.99 && $max-$min<1.01} {
		putsout -d. $outfile "This variable is discrete."
	    } else {
		putsout -d. $outfile "This variable seems discrete, but is not coded properly;\nSee \"help discrete-notes\""
		set discrete -1
	    }
	}
#
# Add to return list for -out option
#
	set outr "$outr variable: $tr"
	set outr "$outr count: $count"
	set outr "$outr missing: $missing"
	set outr "$outr mean: $mean"
	set outr "$outr min: $min"
	set outr "$outr max: $max"
	set outr "$outr sd: $sd"
	set outr "$outr skewness: $skew"
	set outr "$outr kurtosis: $kurt"
	set outr "$outr discrete: $discrete"
	set outr "$outr alpha: $alpha"
#
# Repeat for all variables
#
    }
    notquiet puts ""
    if {$return} {
	return $outr
    }
    if {$joinedfile} {
	file delete $deletefile
    }
    return ""
}

# solar::stats_get --
#
# Purpose:  Retrieve statistics from list returned by stats
#
# Usage:    stats_get <stats> <statistic> [<variable>]
#
#           <stats>      list returned by stats procedure
#           <statistic>  name of statistic desired (see below for list)
#           <variable>   select this variable (default: <first>)
#
# Example:  set stat [stats -q -return q1]
#           set kurt [stats_get $stat kurtosis]
#           set skew [stats_get $stat skewness]
#
# Notes:    The following statistics are available:
#
#           variable     name of variable
#           count        number of individuals having this variable (sample)
#           missing      number of individuals missing this variable
#           mean         mean
#           min          minimum value
#           max          maximum value
#           sd           standard deviation
#           skewness     skewness
#           kurtosis     kurtosis
#           discrete     0 if quantitative, 1 if discrete, -1 if not coded
#                          properly
#           alpha        0 if valid numbers; 1 if alphanumeric
#
#           Of course, if a variable is selected, that variable must have
#           been included in the stats list.  When running the stats command
#           you may select any number of variables or use the -all option.
#           See the stats command for further information.
#-

proc stats_get {stats field {variable ""}} {

    if {"" != $variable} {
	set target "variable: $variable"
	set index [string first $target $stats]
	if {-1 == $index} {
	    error "stats_get: variable $variable not found in stats list"
	}
	set index [expr [string length $target] + 1 + $index]
	set stats [string range $stats $index end]
    }

    set index [lsearch $stats [catenate $field :]]
    if {-1 == $index} {
	error "stats_get: field $field not found"
    }
    set value [lindex $stats [expr 1 + $index]]

    return $value
}


# solar::zscore --
#
# Purpose:  Zscore current trait(s) or covariate(s)
#
# Usage:    define defname = zscore_phenotype 
#           trait defname
#           OR
#           covariate defname
#
#           (defname is any user defined name, phenotype is any phenotype name)
#
# Notes:    zscore_ is a prefix that may be used in the define command,
#           similar to the inormal_ prefix.  Once a definition has been
#           created, it may be used in either the trait or covariate commands.
#           For further information, see "help define".
#
#           The Mean and SD are obtained from the current maximization sample,
#           not the entire phenotypes file.
#
#           In versions of SOLAR prior to 4.4.0, zscore was a command that
#           could be only used to zscore the current trait.  That command
#           is still available as before, but was considered obsolescent.
#           It was difficult and problemantical.  For information about that
#           command, for understanding previous uses, see "help old_zscore".
#
#-

# solar::old_zscore --
# solar::zs
#
# Purpose:  The old zscore command to zscore current trait
#          
# Old Usage:    zscore [-off] [-q]
#               zs     [-off]       ;# Perform zscore quietly
#
#               -off                Turn off zscore
#               -q                  Perform zscore quietly
#
# Notes:    The "Mean" and "SD" values used by zscore are computed only
#           once, at the time the zscore command is given.  Thus they do
#           not reflect later changes to the phenotypes file, or to the
#           sample, which might be restricted due to individuals missing
#           covariates added later.  Generally, for this reason the
#           zscore command should be given after the covariates command
#           and immediately before a model maximizing command such as
#           polygenic.
#
#           Starting with SOLAR Version 4.0.9, the trait mean and SD
#           are computed from the actual sample that would be included
#           in an analysis (at the time the zscore command is given).
#
#           As described in the notes below, you can adjust the Mean
#           and SD by using "option zmean1" and "option zsd1" to set
#           the values actually used.  These values are applied to
#           the trait values during maximization.
#
#           If the trait is changed without giving the "model new"
#           command, the new trait will be zscored automatically.
#           This feature is obsolescent.  In a future update, zscore
#           will be turned off when the trait is changed.
#
#           An alternative to zscore is to define the trait as the
#           inverse normal transformation of a variable.  See
#           "help inormal" and "help define" for further details.
#           
#           zscore will also calculate a number of statistics
#           for the trait: mean, minimum, maximum, standard deviation,
#           skewness, and kurtosis.  These will be written to the file
#           zscore.out in the current output directory.  As of version
#           4.0.9, these statistics are no longer written to the terminal.
#           Instead, a single line is displayed with the trait name,
#           mean, and SD.  Even that line is not shown if zscore is
#           invoked from a script or the zs abbreviation of the command
#           is used.
#
#           To calculate these statistics for any phenotypic variable without
#           zscoring and without necessarily making it the trait, use the
#           "stats" command instead.
#
#           A trait must already have been selected with the trait command
#           or loaded model.  Also the phenotypes file must have been loaded.
#
#           When a maximization is performed, trait values are replaced with
#           their zscored values.  The formula is:
#
#           zscored = (value - Mean) / SD
#
#           zscore is a model dependent option controlled by "option zscore".
#           It remains in effect until another model is loaded or the
#           "model new" command is given.  When models maximized with zscore
#           are reloaded, zscore is again activated.
#
#           "option zscore" is set to 1 ("on") by this command, and the
#           related options zmean1 and zsd1 (mean and standard deviation
#           for the first trait) and zmean2 and zsd2 (mean and standard
#           deviation for the second trait) are set as required.  You can
#           adjust these options directly to fine tune the mean and standard
#           deviation values used, but be sure that zscore is not set to 1
#           until the mean and (non-zero !) standard deviation values are
#           set for all traits in the model.
#
#           In a multivariate model, zscore will only be applied to the
#           first two traits.
#
#           Whenever zscore is activated or deactivated, parameters mean
#           and SD are reset to zero to force setting new boundaries and
#           starting point during the next maximization.
#
#           If a new phenotypes file is loaded, the zscore command should be
#           repeated to reflect the new file.
#-

proc zscorexp {args} {
    set outlist ""
    global SOLAR_Zscore_Phenotypes
    if {![if_global_exists SOLAR_Zscore_Phenotypes]} {
	set SOLAR_Zscore_Phenotypes ""
    }
    if {$args == "-reset"} {
	set SOLAR_Zscore_Phenotypes ""
	return ""
    }
    set opname [lindex $args 0]
    set pname [lindex $args 1]
    if {$opname == "set"} {
	global SOLAR_Zmean_$pname
	global SOLAR_ZSD_$pname
	set SOLAR_Zmean_$pname [lindex $args 2]
	set SOLAR_ZSD_$pname [lindex $args 3]
	setappend SOLAR_Zscore_Phenotypes $pname
	return ""
    } elseif {$opname == "get"} {
	if {-1 != [lsearch $SOLAR_Zscore_Phenotypes $pname]} {
	    set type [lindex $args 2]
	    if {$type == "mean"} {
		if {[if_global_exists SOLAR_Zmean_$pname]} {
		    global SOLAR_Zmean_$pname
		    eval return \$SOLAR_Zmean_$pname
		}
	    } elseif {$type == "sd"} {
		if {[if_global_exists SOLAR_ZSD_$pname]} {
		    global SOLAR_ZSD_$pname
		    eval return \$SOLAR_ZSD_$pname
		}
	    }
	}
    }
    return "not found"
}

proc zs {args} {
    return [eval zscore -q $args]
}

proc zscore {args} {

    set zscored 0
    set zfile [full_filename zscore.out]
    set ztemp [full_filename zscore.temp.out]
    set samplefile [full_filename sampledata.out]
    set savename [full_filename solar.zscore.orig.mod]
    file delete $zfile

    set off 0
    set quiet 0
    set badargs [read_arglist $args -off {set off 1} -q {set quiet 1}]
    if {{} != $badargs} {
	error "zscore: Invalid argument: $badargs"
    }

    set ts [trait]
    if {[llength $ts] == 1} {
	set multi 0
    } else {
	set multi 1
    }
#
# zscore always turned off now to prevent recursive zscore
#
    option zscore 0

    if {$off} {
	option zmean1 0.0
	option zsd1 0.0
	option zmean2 0.0
	option zsd2 0.0
	if {!$multi} {
	    if {[if_parameter_exists mean]} {
		parameter mean = 0 lower 0 upper 0
	    }
	    if {[if_parameter_exists sd]} {
		parameter sd = 0 lower 0 upper 0
	    }
	} else {
	    foreach tr $ts {
		if {[if_parameter_exists mean($tr)]} {
		    parameter mean($tr) = 0 lower 0 upper 0
		}
		if {[if_parameter_exists sd($tr)]} {
		    parameter sd($tr) = 0 lower 0 upper 0
		}
	    }
	}
	return ""
    }
#
# Save initial model
# Add default omega if not already defined
#
    set definitions [string tolower [define names]]
    save model $savename
    if {"omega = Use_polygenic_to_set_standard_model_parameterization" \
	    == [omega]} {
	polymod
    }
#
# Adjust parameters and set options
#
    maximize -sampledata -q
    set tindex 0
    set suffix ""
    foreach tr $ts {
	incr tindex
	if {$tindex > 2} {
	    puts "Warning.  Only first 2 traits are zscored."
	    break
	}
	if {-1 != [lsearch -exact $definitions [string tolower $tr]]} {
	    load model $savename
	    zscore -off
	    error \
		"Error!  Trait $ts is a definition.  Zscore has been deactivated."
	}
	set meansd [stats trait$tindex -file $samplefile -q \
			-out $ztemp]
	exec cat $ztemp >>$zfile
	file delete $ztemp
	set mean [stats_get $meansd mean]
	set sd [stats_get $meansd sd]
	set discrete [stats_get $meansd discrete]
	if {$discrete && [option enablediscrete]} {
	    load model $savename
	    zscore -off
	    error \
		"Error!  Trait $ts is discrete.  Zscore has been deactivated.\n"
	}
	if {[llength $ts] > 1} {
	    set suffix "($tr)"
	}
	if {[if_parameter_exists mean$suffix]} {
	    parameter mean$suffix = 0 lower 0 upper 0
	}
	if {[if_parameter_exists sd$suffix]} {
	    parameter sd$suffix = 0 lower 0 upper 0
	}
	load model $savename
	option zmean$tindex $mean
	option zsd$tindex $sd
	set zscored 1
	save model $savename
	if {!$quiet && [info level] < 2} {
	    puts "Trait: $tr   Mean: [format %.8g $mean]   SD: [format %.8g $sd]"
	}
    }

#    if {![if_global_exists SOLAR_zscore_warning] && [info level] < 2 && !$quiet} {
#	global SOLAR_zscore_warning
#	set SOLAR_zscore_warning 1
#	puts "Warning.  mean and sd set only when zscore command is given."
#   }

    if {$zscored} {
	option zscore 1
    }
    file delete $samplefile
    file delete $savename
    file delete [full_filename solar.out]
    file delete [full_filename last.mod]
    return ""
}

# solar::screencov --
#
# Purpose:  Perform polygenic analysis with covariate screening
#             Same as 'polygenic -screen'
#
# solar::sporadic --
# solar::polygenic --
#
# Purpose:  Perform polygenic, sporadic, and/or household analysis
#             Calculate H2r, significance of H2r, and proportion of variance
#               contributed by covariates.
#             Optionally performs covariate screening (determine significance
#               level of each covariate).
#
# Usage:   polygenic [-screen] [-all] [-p | -prob <p>] [-fix <covar>]
#                    [-testcovar <covar>] [-testrhoe] [-testrhog] [-testrhoc]
#                    [-sporadic] [-keephouse] [-testrhop] [-rhopse]
#
#          (screencov is an alias for 'polygenic -screen')
#          (sporadic is an alias for 'polygenic -sporadic')
#
#          Typically before giving this command, you will give trait,
#          covariate, and house (if applicable) commands.  You will also load
#          pedigree and phenotypes files if they have not already been loaded.
#
#              solar> load pedigree ped
#              solar> load phenotypes phen
#              solar> trait hbp
#              solar> covariate age sex age*sex smoke
#              solar> polygenic -screen
#
#          Alternatively, you may use the "automodel" command first to
#          include all available phenotypes as covariates.  See note 2
#          below and "help automodel".
#
#          -screen   (or -s)  Perform covariate screening:
#                    Calculate significance level for each covariate, and run
#                    only the significant covariates in the final analysis.
#                    An inclusive significance threshold of 0.1 is used,
#                    but may be changed with the -prob option.  Covariates
#                    may be locked in regardless of significance with the
#                    -fix or -all options.
#
#          (An alternative method of covariate analysis using bayesian
#           model averaging is available with the command:
#               bayesavg -covariates)
#
#          -p        (or -prob)  p is the probability level for keeping
#                    covariates as "significant."  The default is 0.1.
#                    It is set to be generous so that covariates are not
#                    removed unnecessarily.  (The probability levels for
#                    H2r and C2 are fixed at 0.05, however, H2r is never
#                    removed from the final model even if it judged to
#                    be not significant, and C2 is only removed from the
#                    model if it is zero in the final model and therefore
#                    has no effect at all.)
#
#          -fix      (or -f) "fix" (lock in) this particular covariate
#                    regardless of significance level.  NOTE: a -fix or -f
#                    qualifier is required for each covariate to be fixed,
#                    for example:  -f age -f sex
#
#          -all      (or -a) Keep all covariates in final anaysis regardless
#                    of significance level.
#
#          -testcovar <covar>  Test the probability of this covariate only.
#                     All other covariates are fixed and unscreened.  This
#                     argument is incompatible with -screen (screen all
#                     covariates).  The tested covariate is not removed from
#                     final model regardless of probability.  For -testcovar,
#                     the default probability level for declared
#                     "significance" is 0.05 (which can be changed with -p
#                     option).  Also, the reported proportion of variance
#                     is for the tested covariate only.
#
#          -testrhoe  (Bivariate only)  Test significance of rhoe difference
#                     from 0 by running model where rhoe is constrained to 0.
#                     The p value is shown in the same line as the RhoE value.
#
#          -testrhog  (Bivariate only)  Test significance of rhog differences
#                     from zero and from 1 (if positive) or -1 (if negative).
#                     Because there may be two p values, they are shown
#                     in line(s) below the RhoG result and standard error.
#
#          -testrhoc  (Bivariate Household only) Test significance of rhoc
#                     differences from zero and 1 (if positive) and -1 (if
#                     negative).  Because there may be two p values, they are
#                     shown in line(s) below the RhoC result and std. error.
#
#          -testrhop  (Bivariate polygenic only) Test significance of derived
#                     estimate of phenotypic correlation differences
#                     (difference from 0).
#
#          -rhopse     (-testrhop must be specified also) Get standard error
#                      of rhop, saved in model file rhop.mod and variable
#                      SOLAR_RhoP_SE
#
#          -sporadic  Only evaluate sporadic models, not polygenic.
#
#          -keephouse Keep "household effect" C2 parameter in final model
#                     even if it maximizes to zero in the best polygenic
#                     (or sporadic) model.
#
#          -residinor After maximizing final sporadic model (after covariate
#                     testing, if that is done), residualize the final model
#                     and inormalize the residual trait.  (Warning!  The
#                     phenotypes file loaded at the end of analysis will be
#                     the residual phenotypes file.)
#
# Notes:    (1) Output is written to directory selected by 'outdir' command,
#           or, if none is selected, to a directory named by the trait.  This
#           is called the "maximization output directory."  Polygenic results
#           are in file named polygenic.out.  Important loglikelihoods and
#           statistical computations are recorded in polygenic.out.logs.  If
#           the -sporadic option is selected, the files are sporadic.out and
#           sporadic.out.logs.  For univariate models, the residuals are
#           computed and written to a file named polygenic.residuals (or
#           sporadic.residuals), then the statistics of those residuals
#           are written to a file named polygenic.residuals.stats (or
#           sporadic.residuals.stats).  If the residual kurtosis is
#           above 0.8, you get a special warning (see note 5 below).  You
#           also get a special warning if the trait standard deviation is
#           below 0.5, which is undesireable for numerical reasons.
#
#           (2) Prior to running polygenic, you should set up the trait and
#           covariates.  You may use the trait and covariate commands, or
#           use the "automodel" command. "automodel" selects all variables
#           otherwise unaccounted for in the phenotypes file as candidate
#           covariates, and also sex and the standard interactions with
#           sex and age.  (If you are unfamiliar with "automodel" it would
#           be a good idea to examine the covariates afterwards with the
#           covariates command...)
#
#           (3) If household effect (see "house") is in effect when the
#           polygenic command is given, it will be included in the analysis.
#           If the household parameter C2 is 0 in the household polygenic
#           model, it will be removed from the final model regardless of
#           whether "covariate screening" is performed, unless -keephouse
#           is specified.  The p value for C2 will be computed (if C2 is
#           nonzero), but the p value will not cause C2 to be removed from
#           the final model. The p value of the C2 parameters is not
#           computed for bivariate models.
#
#           (4) If any covariates have been constrained by the user,
#           certain tests are not allowed: the determination of total
#           variance due to covariates, or the Leibler-Kullback R
#           squared (done for discrete traits).  Also, such covariates
#           are not included in the "screening" if the screening option
#           is selected.
#
#           (5) If you get the message about Residual Kurtosis being too high
#           because it is above 0.8, there is danger of LOD scores  being
#           estimated too high in a subsequent linkage analysis.  You should
#           start over using either tdist or lodadj or inormal (see 
#           documentation) to protect against this.  If you are already
#           using tdist or lodadj, you may ignore this warning, but it would
#           be fair to report both the Residual Kurtosis and the method
#           you are using to deal with it.  We most strongly recommend
#           inormal, which in conjunction with the define command creates
#           an inverse normalized transformation of your trait(s).
#
#           If there are no covariates, the Kurtosis is computed from the
#           trait itself, and no "residuals" are computed.  The same warning
#           threshold applies.  We define Kurtosis as 0 for a standard
#           normal distribution; 3 has already been subtracted from the
#           normalized 4th central moment.
#
#           (6) The polygenic command only supports our "standard"
#           parameterizations.  If you would like to use the esd,gsd,qsd
#           parameterization, use the polygsd command (see "help polygsd"
#           for more information) instead.
#
#           (7) For bivariate polygenic models only, a derived estimate of
#           RhoP, the phenotypic correlation, is displayed on terminal
#           and written to polygenic.out.  This estimate is computed from the
#           h2r's, rhog, and rhoe according to the following formula:
#
#               sqrt(h2r(ti))*sqrt(h2r(tj))*rhog + 
#                   sqrt(1-h2r(ti))*sqrt(1-h2r(tj))*rhoe
#
#           To determine the significance of RhoP by comparing models with
#           a rhop parameter and a rhop parameter constrained to zero, use
#           the -testrhop option.  Additional models rhop.mod and rhop0.mod
#           are written to the output directory.
#
#           (8) The polygenic command creates global variables which may
#           be accessed later (which is often useful in scripts).  The
#           variables are:
#
#               SOLAR_Individuals  number of individuals included in sample
#               SOLAR_H2r_P        p value for h2r
#               SOLAR_Kurtosis     residual trait kurtosis
#               SOLAR_Covlist_P    list of p values for covariates
#               SOLAR_Covlist_Chi  list of chi values for covariates
#               SOLAR_RhoP         derived estimate of phenotypic correlation
#                                    for bivariate polygenic models, {} if
#                                    not calculated
#               SOLAR_RhoP_P       -testrhop sets this to p value of rhop
#                                    being nonzero
#               SOLAR_RhoP_SE      -rhopse sets this to se value of rhop
#               SOLAR_RhoP_OK      -testrhop sets this if likelihood of rhop
#                                    parameterized model matches polygenic,
#                                    as it should
#
#           The covariate lists are created only if the -screen option
#           is used.  All screened variables are included, regardless of
#           whether they were retained in the final model.  Before you
#           can access any of these variables in a script, you must
#           use a "global" command.  For example:
#
#               global SOLAR_Kurtosis
#               if {$SOLAR_Kurtosis > 4} {puts "Very bad kurtosis!"}
#
#           (9) The default is for the standard error option to be turned
#           on (and temporarily off, when desireable for certain tests).
#           However, if you turn the standard error option off before
#           starting polygenic, it will remain off.
#           
# -

proc screencov args {
    return [eval polygenic -s $args]
}

proc sporadic args {
    return [eval polygenic -sporadic $args]
}

proc polygenic args {

    global SOLAR_H2r_P
    global SOLAR_Kurtosis
    global SOLAR_Covlist_P
    global SOLAR_Covlist_Chi
    global SOLAR_Individuals
    global SOLAR_RhoP
    global SOLAR_RhoP_P
    global SOLAR_RhoP_OK
    global SOLAR_RhoP_SE
    set SOLAR_Kurtosis 0	
    set SOLAR_Covlist_P {}
    set SOLAR_Covlist_Chi {}
    set SOLAR_Individuals ""
    set SOLAR_RhoP {}
    set SOLAR_RhoP_P {}
    set SOLAR_RhoP_OK 0
    set SOLAR_RhoP_SE 0
    set covbase 20  ;# index to covar info in image output

    global SOLAR_old_phenotypes_files
    set SOLAR_old_phenotypes_files [phenotypes -files]

    set qu -q
    ifverbplus set qu ""

    set imoutvalid [imout -valid]
    if {$imoutvalid} {imout -puts -1 -vol 0}

# Remember current standerr status

    set prestanderr [option standerr]

# Examine current covariates...

    set some_covariates_removed 0
    set found_constrained_covariates 0
    set active_covar_list [covariates -active]
    set covar_list $active_covar_list

    if {{} == $active_covar_list} {
	set any_covariates_exist 0
    } else {
	set any_covariates_exist 1

# Check for covariates that are constrained to some value
# Remove them from covar_list, we don't mess with them

	set betalist [covariate -betanames]
	set const_covar_list ""
	foreach beta $betalist {
	    if {![catch {find_simple_constraint $beta}] || \
		    ![catch {find_simple_constraint <$beta>}]} {
		if {!$found_constrained_covariates} {
		    puts " "
		}
		puts "    *** Note: $beta is constrained\n"
		set found_constrained_covariates 1
		lappend const_covar_list [string range $beta 1 end]
	    }
	}
	foreach const $const_covar_list {
	    catch {set covar_list [remove_from_list $covar_list $const]}
	}
    }
	    
    set final_covar_list $covar_list
    set number_of_covariates [llength $covar_list]
    if {0<$number_of_covariates} {
	set free_covariates_exist 1
    } else {
	set free_covariates_exist 0
    }

# Set defaults and read arguments

    set covscreen 0
    set testcovar ""
    set fixall 0
    set test_zero_c2 0
    set testrhoe 0
    set testrhog 0
    set testrhoc 0
    set sporadic 0
    set keephouse 0
    set do_kullback 0
    set got_kullback 0
    set do_var_due2cov 0
    set fix_list {}
    set probability_level 0.1
    set user_probability_level -1
    set vc_probability_level 0.05
    set testrhop 0
    set rhopse 0
    set residinor 0

    set extra_args [read_arglist $args \
	    -screen {set covscreen 1} -s {set covscreen 1} \
	    -prob user_probability_level -p user_probability_level \
	    -fix {lappend fix_list VALUE} \
	    -f {lappend fix_list VALUE} \
	    -residinor {set residinor 1} \
            -keephouse {set keephouse 1} \
	    -testrhoe {set testrhoe 1} \
	    -testrhog {set testrhog 1} \
	    -testrhoc {set testrhoc 1} \
	    -testrhop {set testrhop 1} \
	    -sporadic {set sporadic 1} \
	    -rhopse {set rhopse 1} \
	    -testcovar testcovar \
	    -all {set fixall 1}]

# check arguments for consistency

    if {$covscreen!=0 && $testcovar!=""} {
	error "polygenic -screen and -testcovar are incompatible"
    }
# OK, now that we've determined user isn't using those inconsistently,
# we use testcovar as a special case of covscreen
    if {$testcovar != ""} {
	set covscreen 1
	set fixall 1
	set probability_level 0.05
	if {-1 == [lsearch $active_covar_list $testcovar]} {
# covariate not present try adding it
	    covariate $testcovar
	} elseif {-1 != [lsearch $const_covar_list $testcovar]} {
	    error "covariate $testcovar must be unconstrained first"
	}
	set covar_list $testcovar
	set final_covar_list $testcovar
	set number_of_covariates 1
	set free_covariates_exist 1
    }

    if {$user_probability_level != -1} {
	set probability_level $user_probability_level
    }

    ensure_float $probability_level
    if {$probability_level > 1 || $probability_level < 0} {
	error "Invalid probability level $probability_level"
    }
    if {[llength $extra_args]} {
	error "Invalid argument(s): $extra_args"
    }
    set ihouse [check_house]
    set hmatrix ""
    if {$ihouse} {
	set hmatrix [housematrix]
    }

    set ts [trait]
    set nts [llength $ts]
    set multi 0
    if {$nts > 1} {set multi 1}
    if {$nts != 2} {
	if {$testrhog || $testrhoe || $testrhoc} {
      error "polygenic -testrhoe -testrhog -testrhoc not possible\nexcept in bivariate (2 trait) models"
	}
    }
    if {$nts > 1} {
	if {$covscreen} {
	    error "Covariate screening or testing not supported for multivariate models\nScreen univariate models first"
	}
	if {$sporadic && $testrhog} {
	    error "rhog test not available for household (sporadic) models"
	}
    }
#
# Set up summary output files in output directory
#
    if {$sporadic} {
	set basename sporadic
	purge_sporadic_output_directory
    } else {
	set basename polygenic
	purge_polygenic_output_directory
    }
    set results_filename [full_filename $basename.out]
    set init_results [open $results_filename w]
    puts $init_results "The last run of $basename did not run to completion."
    puts $init_results "Check logs file, or individual fisher output files."
    close $init_results

    set logs_file [full_filename $basename.logs.out]
    putsnew $logs_file

    if {$free_covariates_exist && $covscreen} {


puts "**********************************************************************"
puts "*  (Screening)  Get starting beta values using sporadic type model   *"
puts "*  with diagonal covariance matrices (default for sporadic models)   *"
puts "**********************************************************************"

# Check that all fixed covariates are listed

    foreach fixed $fix_list {
	if {-1 == [lsearch $active_covar_list $fixed]} {
	    error "Fixed covar $fixed not present in model"
	}
    }
    spormod
    option standerr 0
    eval maximize $qu -o s0
    model save [full_filename s0]
    set bll [set sll [loglike]]
    putsa $logs_file \
"    *** Loglikelihood of sporadic model with all covars is $sll\n"

    if {$sporadic} {
	set basemodel s0
    } else {
        set basemodel p0

puts ""
puts "**********************************************************************"
puts "*  (Screening)  Maximize polygenic model with all covariates         *"
puts "**********************************************************************"

        polymod
        eval maximize $qu -o p0
        model save [full_filename p0]
        set bll [set pll [loglike]]
        putsa $logs_file \
"    *** Loglikelihood of polygenic model with all covars is $pll\n"

    }
    if {$ihouse} {
        set basemodel h0
puts " "
puts "**********************************************************************"
puts "*  (Screening)  Maximize household model with all covariates         *"
puts "**********************************************************************"

        load model [full_filename s0]
        house
        if {$hmatrix!=""} {eval $hmatrix}
        eval maximize $qu -o h0
        model save [full_filename h0]
        set bll [set pll [loglike]]
        putsa $logs_file \
"    *** Loglikelihood of household model with all covars is $pll"
        putsa $logs_file " "

        if {!$sporadic} {
	    set basemodel hp0
puts " "
puts "**********************************************************************"
puts "*  (Screening)  Maximize household polygenic model                   *"
puts "*  with all covariates                                               *"
puts "**********************************************************************"

            load model [full_filename p0]
            house
            if {$hmatrix!=""} {eval $hmatrix}
            eval maximize $qu -o hp0
            model save [full_filename hp0]
            set bll [set pll [loglike]]
            putsa $logs_file \
"    *** Loglikelihood of household polygenic model with all covars is:\n\t\t\t\t $pll"
            putsa $logs_file " "
        }
    }

puts " "
puts "**********************************************************************"
    if {$ihouse && !$sporadic} {
puts "*  (Screening)  Maximize household polygenic models                  *"
    } elseif {$ihouse} {
puts "*  (Screening)  Maximize household models                            *"
    } elseif {$sporadic} {
puts "*  (Screening)  Maximize sporadic models                             *"
    } else {
puts "*  (Screening)  Maximize polygenic models                            *"
    }
puts "*  one with each covariate deactivated                               *"
puts "**********************************************************************"

    set final_covar_list {}
    set report_list {}
    set remove_covar_list {}

    set covindex -1
    foreach covar $covar_list {
        incr covindex
        puts " "
	puts "    *** Testing covariate $covar by suspending it ***"
	model load [full_filename $basemodel]
	covariate suspend $covar
        option standerr 0
	eval maximize $qu -o no$covar
	model save [full_filename no$covar]
	set chill [expr 2.0 * ($bll - [loglike])]
	set deg 1
	set testchi [catch {set pstring [chi $chill $deg]}]
	if {$testchi != 0} {set pstring "p = 1.0"}
	lappend SOLAR_Covlist_P [lindex $pstring 2]
	set keep 1    
	set comment "(Significant)"
	if {[lindex $pstring 2] >= $probability_level} {
	    set comment "(Not Significant)"
	    if {-1 != [lsearch $fix_list $covar] || $fixall} {
		set comment "(Not Sig., but fixed)"
	    } else {
		set keep 0
	    }
	}
	if {$keep == 1} {
	    lappend final_covar_list $covar
	} else {
	    lappend remove_covar_list $covar
	}

	set ll [loglike]
        catch {[set ll [format "%.6f" $ll]]}
	putsat $logs_file \
"\n    *** Loglikelihood w/o covar $covar is $ll"

        catch {[set chill [format %.4f $chill]]}
        putsat $logs_file "    *** chi = $chill, deg = $deg"
	lappend SOLAR_Covlist_Chi $chill
        if {0==[string compare [verbosity] "verbosity max"]} {
	    putsat $logs_file "    *** Total process memory is [memory]"
	}

	set report "$pstring  $comment"
	lappend report_list $report
        putsat $logs_file "    *** $report"

        if {$imoutvalid} {
           imout -puts [lindex $pstring 2] -vol [expr $covbase+2+($covindex*4)]
           imout -puts $chill -vol [expr $covbase+3+($covindex * 4)]
        }
    }

puts " "
    set nfinal [llength $final_covar_list]
    model load [full_filename s0]
    if {!$fixall} {
	foreach covar $remove_covar_list {
	    set some_covariates_removed 1
	    covariate delete $covar
	}
    }
}
# end if $covscreen && $free_covariates_exist

# The following is done regardless of whether we are doing covariate screening

puts "**********************************************************************"
    if {$covscreen && $free_covariates_exist} {
puts "*  Covariate screening completed                                     *"
puts "*  Now using models with only significant or fixed covariates        *"
    }
puts "*  Maximize sporadic model                                           *"
puts "**********************************************************************"
putsat $logs_file ""

    option standerr $prestanderr
    spormod
    eval maximize $qu -o spor
    if {[option modeltype] == "evd2"} {
	set tstats [stats -return -q]
	set discrete_trait [stats_get $tstats discrete]
    } else {	
	if {[trait -is-trait-discrete?]} {
	    set discrete_trait 1
	} else {
	    set discrete_trait 0
	}
    }
    set as_quantitative 0
if {{} != [find_string [full_filename spor.out] "quantitative!"]} {
	set as_quantitative 1
    }
    model save [full_filename spor]
    set spor_loglike [loglike]
    putsat $logs_file \
"    *** Loglikelihood of sporadic model is $spor_loglike"
    foreach t $ts {
	if {!$multi} {
	    set suffix ""
	} else {
	    set suffix \($t\)
	}
	set sd_covar [parameter SD$suffix =]
	set sd_noh_covar $sd_covar
    }
    set best_nh_loglike [loglike]
    set best_loglike [loglike]
    set finalmodels spor
    set bestmodel spor
    set best_nh_model spor
    set bestdesc sporadic
    set best_nh_desc sporadic

    if {$residinor} {
putsat $logs_file ""
puts "**********************************************************************"
puts "*  Residualize and Inormalize trait                                  *"
puts "**********************************************************************"
puts ""
	set residinorname [full_filename $basename.residinor]
	residual spor.out -out $residinorname
	load phenotypes $residinorname
	model new
	trait residual
    }	
    if {!$sporadic} {    
putsat $logs_file ""
puts "**********************************************************************"
puts "*  Maximize polygenic model                                          *"
puts "**********************************************************************"
puts ""

        polymod
        eval maximize $qu -o poly
        model save [full_filename poly]
        set poly_loglike [loglike]
        putsat $logs_file \
"    *** Loglikelihood of polygenic model is $poly_loglike"


        foreach t $ts {
	    if {!$multi} {
		set suffix ""
	    } else {
		set suffix \($t\)
	    }
	    set h2r [parameter h2r$suffix =]
	    catch {[set h2r [format %.7f $h2r]]}
	    putsat $logs_file \
"    *** H2r$suffix in polygenic model is $h2r"
            set sd_covar [parameter SD$suffix =]
            set sd_noh_covar $sd_covar
        }
	set best_nh_loglike [loglike]
	set best_loglike [loglike]
	set finalmodels "poly, $finalmodels"
	set bestmodel poly
        set best_nh_model poly
	set bestdesc polygenic
	set best_nh_desc polygenic
    }

    set needhouse 0
    if {$ihouse} {
putsat $logs_file ""
puts "**********************************************************************"
puts "*  Maximize household model                                          *"
puts "**********************************************************************"
puts ""
        load model [full_filename spor]
        house
        if {$hmatrix!=""} {eval $hmatrix}
        eval maximize $qu -o house
        set house_loglike [loglike]
        putsat $logs_file \
"    *** Loglikelihood of household model is $house_loglike"
        model save [full_filename house]
        set c1 0
        set c2 0
        set tno 0
        foreach tr $ts {
	    incr tno
	    set suf ""
	    if {$multi} {
		set suf \($tr\)
	    }
	    set house_c2 [parameter c2$suf =]
	    catch {[set house_c2 [format %.7f $house_c2]]}
	    putsat $logs_file \
"    *** C2$suf in household model is $house_c2"
            set c$tno $house_c2
            set sd_covar [parameter SD$suf =]
        }
	set best_loglike [loglike]
	set bestmodel house
        set bestdesc household
        set finalmodels "house, $finalmodels"

        if {!$sporadic} {
putsat $logs_file ""
puts "**********************************************************************"
puts "*  Maximize household polygenic model                                *"
puts "**********************************************************************"
puts ""
            load model [full_filename poly]
            house
            if {$hmatrix!=""} {eval $hmatrix}
            eval maximize $qu -o housepoly
            model save [full_filename housepoly]
            set housepoly_loglike [loglike]
            putsat $logs_file \
"    *** Loglikelihood of household polygenic model is $housepoly_loglike"
            set c1 0
            set c2 0
            set tno 0
            foreach tr $ts {
	        set suf ""
	        if {$multi} {
		    set suf \($tr\)
	        }
	        set h2r [parameter h2r$suf =]
	        catch {[set h2r [format %.7f $h2r]]}
	        putsat $logs_file "    *** H2r$suf in household polygenic model is $h2r"
		set sd_covar [parameter SD$suf =]
	    }
	    putsat $logs_file " "
	    foreach tr $ts {
		incr tno
		set suf ""
		if {$multi} {
		    set suf \($tr\)
		}
		set housepoly_c2 [parameter c2$suf =]
		catch {[set housepoly_c2_form [format %.7f $housepoly_c2]]}
		putsat $logs_file \
"    *** C2$suf in household polygenic model is $housepoly_c2_form"
                set c$tno $housepoly_c2
            }
	    set best_loglike [loglike]
	    set bestmodel housepoly
	    set bestdesc "household polygenic"
	    set finalmodels "housepoly, $finalmodels"
	}

# The following is for testing purposes...replicating a hard to
# reproduce condition for bivariate household models...bivariate
# models with this troublesome condition usually don't even converge

	if {[if_global_exists SOLAR_test_zero_c2]} {
	    puts "c1 is $c1, c2 is $c2, now setting to 0"
	    set c1 [set c2 0]
	}

# determine retention and significance of C2

	if {$c1 == 0 && $c2 == 0 && !$keephouse} {
	    putsat $logs_file \
"\n    *** Removing C2 because it is 0.0 in $bestdesc model"
            set bestmodel $best_nh_model
            set bestdesc $best_nh_desc
            set sd_covar $sd_noh_covar

# Note: this simply removes the constrained-to-zero C2 from the previously
# maximized polygenic or sporadic model.  It should not need re-maximization.

            load model [full_filename $bestmodel]
            set best_loglike [loglike]
            house -delete 
            save model [full_filename $bestmodel]

        } elseif {$c1 != 0 || $c2 != 0} {
	    if {!$multi} {
		putsat $logs_file "\n    *** Determining significance of C2"
		putsat $logs_file \
"    *** Comparing $bestdesc and $best_nh_desc models"
                set chill [expr 2.0 * ($best_loglike - $best_nh_loglike)]
                set testchi [catch {set chiip [chi -number $chill 1]}]
                if {$testchi != 0} {set chiip 1.0}
		set hpspr [expr $chiip / 2.0]
# Must reformat because we operated on it
                set hpspsign =
                set hpsp [format %.7f $hpspr]
		if {[format %.5f $hpsp] == 0} {
		    set hpsp [format %.8g $hpspr]
		}
		catch {[set chill [format %.4f $chill]]}
		putsat $logs_file \
"    *** chi = $chill, deg = 1, p $hpspsign $hpsp"
            } else {
		putsat $logs_file \
"\n    *** SOLAR does not compute the significance of C2 for multivariate models"
            }
            set needhouse 1
	}
    }
#
# Figure chi (loglikelihood) for either poly/spor or housepoly/house pair
# to calculate significance of h2r (if neither sporadic nor bivariate)
#
    if {!$sporadic && !$multi} {
	if {$needhouse} {
	    set chill [expr 2.0 * ($housepoly_loglike - $house_loglike)]
	    set bestpair "household polygenic and household"
	} else {
	    set chill [expr 2.0 * ($poly_loglike - $spor_loglike)]
	    set bestpair "polygenic and sporadic"
	}
#
# Since in a sporadic model H2 is fixed to a boundary
# (analogous to single ended), we must divide p for H2 by 2
#
	set testchi [catch {set chiip [chi -number $chill 1]}]
	if {$testchi != 0} {set chiip 1.0}
	set SOLAR_H2r_P [expr $chiip / 2.0]
# Must reformat because we operated on it
	set pspsign =
	set pspf [format "%.7f" $SOLAR_H2r_P]
	if {$pspf == 0} {
	    set pspf [format %.8g $SOLAR_H2r_P]
	}
	set SOLAR_H2r_P $pspf

	catch {[set chill [format %.4f $chill]]}
	putsat $logs_file "\n    *** Determining significance of H2r"
	putsat $logs_file "    *** Comparing $bestpair models"
	putsat $logs_file "    *** chi = $chill, deg = 1, p $pspsign $SOLAR_H2r_P"
    }

    if {!$multi && !$found_constrained_covariates} {
	if {$free_covariates_exist && 0!=[llength $final_covar_list]} {
            if {$discrete_trait && !$as_quantitative} {
		set do_kullback 1
	    } elseif {[catch {find_simple_constraint sd}]} {
		set do_var_due2cov 1   ;# Only if SD is *NOT* constrained
	    }
        }
    }

    if {!$residinor && ($do_kullback || $do_var_due2cov)} {
puts ""
puts "**********************************************************************"
puts "*  Maximize $bestdesc model with NO covariates"
	if {$do_var_due2cov} {
puts "*  to determine proportion of variance due to covariates"
        } else {
puts "*  to compute Kullback-Leibler R-squared"
        }
puts "**********************************************************************"

        set finalmodels "$finalmodels, nocovar"
	eval covariate suspend $final_covar_list
	eval maximize $qu -o nocovar
	model save [full_filename nocovar]
        if {$do_var_due2cov} {
	    set sd_nocovar [parameter SD start]
	    set prop_var [expr 1.0 - (($sd_covar * $sd_covar) / \
		  ($sd_nocovar * $sd_nocovar))]
	    catch {[set prop_var [format %.7f $prop_var]]}
	    catch {[set sd_covar [format %.7f $sd_covar]]}
	    catch {[set sd_nocovar [format %.7f $sd_nocovar]]}
	    putsat $logs_file \
"\n    *** Trait SD in model with covariates is $sd_covar"
            putsat $logs_file \
"    *** Trait SD in model without covariates is $sd_nocovar"
            putsat $logs_file \
"    *** Proportion of variance explained by covariates is $prop_var"
            putsa $logs_file " "
        } else {
	    putsat $logs_file \
"\n    *** Loglikelihood of model with no covariates is [loglike]"
	    if {![catch {set kullback [expr 1.0 - \
		  ($best_loglike/[loglike])]}]} {
	        set got_kullback 1
	        if {$kullback < 0.0} {
		    set kullback 0.0
		}
		putsat $logs_file \
	      "    *** Kullback-Leibler R-squared is [format %.7f $kullback]"
		putsa $logs_file " "
	    }

	}
	model load [full_filename $bestmodel]
    }
#
# Compute derived estimate of RhoP (Phenotypic correlation)
#
    if {!$sporadic && $nts == 2 && !$ihouse} {
	set rph2r(1) [parameter h2r\([lindex $ts 0]\) = ]
        set rph2r(2) [parameter h2r\([lindex $ts 1]\) = ]
        set rpre [parameter rhoe =]
        set rprg [parameter rhog =]

        set SOLAR_RhoP [expr sqrt($rph2r(1))*sqrt($rph2r(2))*$rprg + \
    sqrt(1-$rph2r(1))*sqrt(1-$rph2r(2))*$rpre]
#
# Test significance of RhoP if requested
#
        if {$testrhop} {
puts ""
puts "**********************************************************************"
puts "*  Maximize models with estimated rhop and rhop constrained to zero  *"
puts "*  to determine significance of rhop being different from zero       *"
puts "**********************************************************************"
puts ""
            set plike [loglike]
            parameter delete rhoe
	    parameter rhop = $SOLAR_RhoP lower -0.9 upper 0.9
            omega = <sd(ti)>*<sd(tj)>*( phi2*sqrt(<h2r(ti)>)*sqrt(<h2r(tj)>)*(tne*rhog + teq) + I*sqrt(<e2(ti)>)*sqrt(<e2(tj)>) * (teq + tne* ((rhop - sqrt(<h2r(ti)>)*sqrt(<h2r(tj)>)*rhog) / (sqrt(<e2(ti)>)*sqrt(<e2(tj)>)) )))
            option standerr 0
	    if {$rhopse} {
		option standerr 1
	    }
	    eval maximize $qu -o rhop
            save model [full_filename rhop]
	    set rp [parameter rhop =]
	    set rhop_ll [loglike]
            putsat $logs_file \
"    *** Loglikelihood of model rhop with estimated rhop is $rhop_ll"
            puts "\n    *** Warning.  Next model might take awhile."
            puts "    *** Run using 'verbosity max' to see maximization details.\n"
	    if {$rhopse} {
		set SOLAR_RhoP_SE [parameter rhop se]
		option standerr 0
	    }
	    parameter rhop = 0
	    constraint rhop = 0
	    eval maximize $qu -o rhop0
            save model [full_filename rhop0]
	    set rhop0_ll [loglike]
            putsat $logs_file \
"    *** Loglikelihood of model rhop0 with rhop constrained to 0 is $rhop0_ll"
            set rhop_test [expr 2*($rhop_ll - $rhop0_ll)]
            if {$rhop_test < 0} {
	        set rhop_test 0
	    }   
 	    set SOLAR_RhoP_P [chi -number $rhop_test 1]
            putsat $logs_file \
"    *** chi = $rhop_test, deg = 1, p = $SOLAR_RhoP_P"
#rhop-parameterized model should have same likelihood as polygenic...
	    if {$plike == $rhop_ll} {
	        set SOLAR_RhoP_OK 1
	    } else {
		putsat $logs_file \
"    *** Warning.  Loglikehood of RhoP parameterized model doesn't match polygenic"
	    }
            model load [full_filename $bestmodel]
        }
    }
    
#
# Determine significance of rhoe, rhog, and/or rhoc when asked
#
    set p_rhoe0 ""
    set p_rhog0 ""
    set p_rhog1 ""
    set p_rhogn1 ""
    set p_rhoc0 ""
    set p_rhoc1 ""
    set p_rhocn1 ""

    if {$testrhoe && [parameter rhoe =] != 0} {
puts ""
puts "**********************************************************************"
puts "*  Maximize model with rhoe constrained to zero                      *"
puts "*  to determine significance of rhoe being different from zero       *"
puts "**********************************************************************"
puts ""
	global SOLAR_constraint_tol
	parameter rhoe = 0 lower -$SOLAR_constraint_tol upper $SOLAR_constraint_tol
	constraint rhoe = 0
	eval maximize $qu -o rhoe0
	set no_rhoe_ll [loglike]
	putsat $logs_file \
"    *** Loglikelihood of model with rhoe constrained to 0 is $no_rhoe_ll"
	set chill [expr 2.0 * ($best_loglike - $no_rhoe_ll)]
	if {$chill < 0} {
	    set p_rhoe0 "p = 1"
	} else {
	    catch {set p_rhoe0 [chi $chill 1]}
        }
	putsat $logs_file \
"    *** chi = $chill, deg = 1, $p_rhoe0"
	load model [full_filename $bestmodel]
    }

    if {$testrhog} {
	set rhog [parameter rhog =]
	if {$rhog != 0} {
puts ""
puts "**********************************************************************"
puts "*  Maximize model with rhog constrained to zero                      *"
puts "*  to determine significance of rhog being different from zero       *"
puts "**********************************************************************"
puts ""
	    parameter rhog = 0 lower -0.01 upper 0.01
	    constraint rhog = 0
	    eval maximize $qu -o p_rhog0
	    set no_rhog_ll [loglike]
	    putsat $logs_file \
"    *** Loglikelihood of model with rhog constrained to 0 is $no_rhog_ll"
	    set chill [expr 2.0 * ($best_loglike - $no_rhog_ll)]
            if {$chill < 0} {
	        set p_rhog0 "p = 1"
	    } else {
	        catch {set p_rhog0 [chi $chill 1]}
	    }
	    putsat $logs_file \
"    *** chi = $chill, deg = 1, $p_rhog0"
	    load model [full_filename $bestmodel]
	}
	if {$rhog >= 0 && $rhog != 1} {
puts ""
puts "**********************************************************************"
puts "*  Maximize model with rhog constrained to 1.0                       *"
puts "*  to determine significance of rhog being different from 1.0        *"
puts "**********************************************************************"
puts ""
	    parameter rhog = 1 lower 0.99 upper 1.01
	    constraint rhog = 1
	    eval maximize $qu -o p_rhog1
	    set rhog_1_ll [loglike]
	    putsat $logs_file \
"    *** Loglikelihood of model with rhog constrained to 1.0 is $rhog_1_ll"
	    set chill [expr 2.0*($best_loglike - $rhog_1_ll)]
	    if {$chill < 0} {
	        set p_rhog1 "p = 1"
	    } else {
	        catch {
		    set p_rhog1 [chi -number $chill 1]
	            set p_rhog1 [expr $p_rhog1 / 2.0]
	            set f [format %.7f $p_rhog1]
	            if {$f == 0} {
		        set f [format %.8g $p_rhog1]
	            }
		    set p_rhog1 "p = $f"
	        }
	    }
	    putsat $logs_file \
"    *** chi = $chill, deg = 1, $p_rhog1"
	    load model [full_filename $bestmodel]
	}
	if {$rhog <= 0 && $rhog != -1} {
puts ""
puts "**********************************************************************"
puts "*  Maximize model with rhog constrained to -1.0                      *"
puts "*  to determine significance of rhog being different from -1.0       *"
puts "**********************************************************************"
puts ""
	    parameter rhog = -1 lower -1.01 upper -0.99
	    constraint rhog = -1
	    eval maximize $qu -o rhog-1
	    set rhog_n1_ll [loglike]
	    putsat $logs_file \
"    *** Loglikelihood of model with rhog constrained to -1.0 is $rhog_n1_ll"
	    set chill [expr 2.0*($best_loglike - $rhog_n1_ll)]
	    if {$chill < 0} {
	        set p_rhogn1 "p = 1"
	    } else {
	        catch {
		    set p_rhogn1 [chi -number $chill 1]
		    set p_rhogn1 [expr $p_rhogn1 / 2.0]
		    set f [format %.7f $p_rhogn1]
		    if {$f == 0} {
		        set f [format %.8g $p_rhogn1]
		    }
		    set p_rhogn1 "p = $f"
	        }
	    }
	    putsat $logs_file \
"    *** chi = $chill, deg = 1, $p_rhogn1"
	    load model [full_filename $bestmodel]
	}
    }	        
    if {$testrhoc && $needhouse} {
	set rhoc [parameter rhoc =]
	if {$rhoc != 0} {
puts ""
puts "**********************************************************************"
puts "*  Maximize model with rhoc constrained to zero                      *"
puts "*  to determine significance of rhoc being different from zero       *"
puts "**********************************************************************"
puts ""
	    parameter rhoc = 0 lower -0.01 upper 0.01
	    constraint rhoc = 0
	    eval maximize $qu -o p_rhoc0
	    set no_rhoc_ll [loglike]
	    putsat $logs_file \
"    *** Loglikelihood of model with rhoc constrained to 0 is $no_rhoc_ll"
	    set chill [expr 2.0 * ($best_loglike - $no_rhoc_ll)]
            if {$chill < 0} {
	        set p_rhoc0 "p = 1"
	    } else {
	        catch {set p_rhoc0 [chi $chill 1]}
	    }
	    putsat $logs_file \
"    *** chi = $chill, deg = 1, $p_rhoc0"
	    load model [full_filename $bestmodel]
	}
	if {$rhoc >= 0 && $rhoc != 1} {
puts ""
puts "**********************************************************************"
puts "*  Maximize model with rhoc constrained to 1.0                       *"
puts "*  to determine significance of rhoc being different from 1.0        *"
puts "**********************************************************************"
puts ""
	    parameter rhoc = 1 lower 0.99 upper 1.01
	    constraint rhoc = 1
	    eval maximize $qu -o p_rhoc1
	    set rhoc_1_ll [loglike]
	    putsat $logs_file \
"    *** Loglikelihood of model with rhoc constrained to 1.0 is $rhoc_1_ll"
	    set chill [expr 2.0*($best_loglike - $rhoc_1_ll)]
	    if {$chill < 0} {
	        set p_rhoc1 "p = 1"
	    } else {
	        catch {
		    set p_rhoc1 [chi -number $chill 1]
	            set p_rhoc1 [expr $p_rhoc1 / 2.0]
	            set f [format %.7f $p_rhoc1]
	            if {$f == 0} {
		        set f [format %.8g $p_rhoc1]
	            }
		    set p_rhoc1 "p = $f"
	        }
	    }
	    putsat $logs_file \
"    *** chi = $chill, deg = 1, $p_rhoc1"
	    load model [full_filename $bestmodel]
	}
	if {$rhoc <= 0 && $rhoc != -1} {
puts ""
puts "**********************************************************************"
puts "*  Maximize model with rhoc constrained to -1.0                      *"
puts "*  to determine significance of rhoc being different from -1.0       *"
puts "**********************************************************************"
puts ""
	    parameter rhoc = -1 lower -1.01 upper -0.99
	    constraint rhoc = -1
	    eval maximize $qu -o rhoc-1
	    set rhoc_n1_ll [loglike]
	    putsat $logs_file \
"    *** Loglikelihood of model with rhoc constrained to -1.0 is $rhoc_n1_ll"
	    set chill [expr 2.0*($best_loglike - $rhoc_n1_ll)]
	    if {$chill < 0} {
	        set p_rhocn1 "p = 1"
	    } else {
	        catch {
		    set p_rhocn1 [chi -number $chill 1]
		    set p_rhocn1 [expr $p_rhocn1 / 2.0]
		    set f [format %.7f $p_rhocn1]
		    if {$f == 0} {
		        set f [format %.8g $p_rhocn1]
		    }
		    set p_rhocn1 "p = $f"
	        }
	    }
	    putsat $logs_file \
"    *** chi = $chill, deg = 1, $p_rhocn1"
	    load model [full_filename $bestmodel]
	}
    }
puts ""
puts "******************************************************************************"
puts "*                          Summary of Results                                *"
puts "******************************************************************************"
puts ""
    if {!$sporadic} {
	model save [full_filename null0]
    }
    set results [open $results_filename w]
    set outputdir [full_filename {}]
#
# Report data files used
#
    putstee $results "\tPedigree:    [topline pedigree.info]"
    set phenfilelist [phenotypes -files]
    putstee $results "\tPhenotypes:  $phenfilelist"
    if {[llength $phenfilelist] > 1} {
        putstee $results " "
    }
#
# Find # of individuals
#
    set ind_string [find_string [full_filename $bestmodel.out] \
	    "The sample size including probands used is"]
    if {1 != [scan $ind_string " The sample size including probands used is %d" SOLAR_Individuals]} {
	set SOLAR_Individuals ""
    }
    if {$imoutvalid} {
        imout -puts $SOLAR_Individuals -vol 11
    }

    putstee $results \
"\tTrait:       [format %-20s [trait]]  Individuals:  $SOLAR_Individuals\n"



catch {
    set constrained_to_1 0
    if {![catch {set cval [find_simple_constraint sd]}] && $cval == 1} {
	set constrained_to_1 1
    }
    if {(!$discrete_trait || $as_quantitative) && $constrained_to_1} {
	    putstee $results \
"\tWARNING!  YOU HAVE PARAMETER SD CONSTRAINED TO 1.0 !"
	    putstee $results \
"\tThis is probably not what you intended."
	    putstee $results \
"\tYou need to use command \"model new\" before changing from discrete to"
	    putstee $results \
"\tquantitative trait analysis.\n"
    }
    if {$as_quantitative} {
        putstee $results \
"\tWarning!  You are analyzing a discrete trait as quantitative!"
	putstee $results \
"\tSee \"help discrete-notes\" for discussion.\n"
    }
}
#
# Warn if SD below 0.5
#
    if {$multi} {
	foreach tr $ts {
	    if {0.5 > [parameter sd($tr) =]} {
		putstee $results \
"\tWARNING!  Estimated Standard Deviation for $tr is [format %.5f [parameter sd($tr) =]]"
		putstee $results "\tWhen Trait SD is below 0.5, results are sometimes incorrect"
		if {0 < [parameter sd($tr) =]} {
	            set sd_factor [format %.1f [expr 1.0 / [parameter sd($tr) =]]]
	            putstee $results "\tMultiplying trait by a factor such as $sd_factor is recommended.\n"
		}
	    }
	}
    } else {
	if {0.5 > [parameter sd =]} {
       	    putstee $results "\tWARNING!  Estimated Trait Standard Deviation is [format %.5f [parameter sd =]]"
	    putstee $results "\tWhen Trait SD is below 0.5, results are sometimes incorrect"
	    if {0 < [parameter sd =]} {
	        set sd_factor [format %.1f [expr 1.0 / [parameter sd =]]]
	        putstee $results "\tMultiplying trait by a factor such as $sd_factor is recommended"
	    }
	    putstee $results ""
	}
        if {$imoutvalid} {
	    imout -puts [parameter sd =] -vol 7
            imout -puts [parameter sd se] -vol 8
        }
    }
#
# Results for h2r, rhoe, and rhog
#
    set high_h2r 0
    if {$multi} {
	if {!$sporadic} {
	set comment ""
	foreach tr $ts {
	    set h2r [parameter h2r($tr) =]
            if {$h2r > 0.9} {
                set high_h2r 1
            }
            catch {[set h2r [format %.7f $h2r]]}
            putstee $results "\t\t\t H2r($tr) is $h2r  $comment"
            putsa $logs_file "    H2r is $h2r  $comment"

            set h2rse [parameter h2r($tr) se]
            if {0 != $h2rse} {
	        catch {set h2rse [format %.7f $h2rse]}
	        putstee $results "\t       H2r($tr) Std. Error:  $h2rse\n"
	        putsa $logs_file "\t H2r($tr) Std. Error:  $h2rse"
            } else {
	        putstee $results ""
            }
	}
        if {$high_h2r} {
    	    putstee $results "\tWarning.  Unexpectedly high heritabilities might result from"
	    putstee $results "\tnumerical problems, especially if mztwins are present."
	    putstee $results ""
	}
    }
#
# RhoE and RhoG for bivariate models
#
        set pars [parameter -names]
        foreach par $pars {
	    if {"rhoe" == [string range $par 0 3]} {

		set rhoe [parameter $par =]
                catch {set rhoe [format %.7f $rhoe]}
                set rhoese [parameter $par se]
                catch {set rhoese [format %.7f $rhoese]}
                if {0 == $rhoese} {
		    set rhoese "Not Computable"
		}    
	        putstee $results \
	            "\t\t\t [rhocap $par] is $rhoe  $p_rhoe0"
	        putsa $logs_file \
	            "    [rhocap $par] is $rhoe  $p_rhoe0"

                if {$prestanderr} {
		    putstee $results \
	            "\t       [rhocap $par] Std. Error:  $rhoese\n"
		    putsa $logs_file \
	            "    [rhocap $par] Std. Error:  $rhoese\n"
		}
	    }
        }
	if {!$sporadic} {
	foreach par $pars {
	    if {"rhog" == [string range $par 0 3]} {	

                set rhog [parameter $par =]
                catch {set rhog [format %.7f $rhog]}
                set rhogse [parameter $par se]
                catch {set rhogse [format %.7f $rhogse]}
                if {0 == $rhogse} {set rhogse "Not Computable"}
    
	        putstee $results \
	            "\t\t\t [rhocap $par] is $rhog"
	        putsa $logs_file \
	            "    [rhocap $par] is $rhog"

                if {$prestanderr} {
		    putstee $results \
	            "\t       [rhocap $par] Std. Error:  $rhogse\n"
		    putsa $logs_file \
	            "    [rhocap $par] Std. Error: $rhogse\n"
		}
            }
        }

        if {"" != [catenate $p_rhog0 $p_rhog1 $p_rhogn1]} {
            putstee $results ""
            putsa $logs_file ""
        }

	if {"" != $p_rhog0} {
	    putstee $results \
		"\t       RhoG different from zero  $p_rhog0"
	    putsa $logs_file \
		"    RhoG different from zero  $p_rhog0"
	}
	if {"" != $p_rhog1} {
	    putstee $results \
		"\t       RhoG different from 1.0   $p_rhog1"
	    putsa $logs_file \
		"    RhoG different from 1.0   $p_rhog1"
	}
	if {"" != $p_rhogn1} {
	    putstee $results \
		"\t       RhoG different from -1.0  $p_rhogn1"
	    putsa $logs_file \
		"    RhoG different from -1.0  $p_rhogn1"
	}
 #
# Phenotypic correlation
#
        if {[llength $SOLAR_RhoP]} {
	    putstee $results \
"\t       Derived Estimate of RhoP is [format %.7f $SOLAR_RhoP]"
            if {[llength $SOLAR_RhoP_P]} {
		putstee $results \
"\t       RhoP different from zero  p = $SOLAR_RhoP_P"
		if {!$SOLAR_RhoP_OK} {
		    putstee $results \
"\t        Warning.  Loglikelihood of RhoP parameterized model didn't match polygenic"
		}
	    
            }
        }
        putstee $results ""
        putsa $logs_file ""
        }
    } elseif {!$sporadic} {

# Univariate Polygenic

	if {$SOLAR_H2r_P <= $vc_probability_level} {
            set comment "(Significant)"
        } else {
            set comment "(Not Significant)"
        }
        if {$SOLAR_H2r_P != 1.0} {set comment "p $pspsign $SOLAR_H2r_P  $comment"}
	set h2r [parameter h2r =]
        catch {[set h2r [format %.7f $h2r]]}
        putstee $results "\t\t\t H2r is $h2r  $comment"
        putsa $logs_file "    H2r is $h2r  $comment"

        set h2rse [parameter h2r se]
        if {$imoutvalid} {
	    imout -puts $h2r -vol 1
            imout -puts $h2rse -vol 2
	    imout -puts $SOLAR_H2r_P -vol 3
	    imout -puts [loglike] -vol 4
            if {[if_parameter_exists mean]} {
                imout -puts [parameter mean =] -vol 9
                imout -puts [parameter mean se] -vol 10
            }
	}
        if {0 != $h2rse} {
	    catch {set h2rse [format %.7f $h2rse]}
	    putstee $results "\t       H2r Std. Error:  $h2rse\n"
	    putsa $logs_file "\t H2r Std. Error:  $h2rse"
        } else {
	    putstee $results ""
        }
	if {$h2r > 0.9} {
	    putstee $results "\tWarning.  Unexpectedly high heritabilities might result from"
	    putstee $results "\tnumerical problems, especially if mztwins are present."
	    putstee $results ""
	}
    }
#
# Results for household effects
#
    if {$needhouse} {
	if {$multi} {
	    foreach tr $ts {
		set pname C2\($tr\)
		set c2 [parameter $pname =]
		catch {[set c2 [format %.7f $c2]]}
		putstee $results "\t\t\t  $pname is $c2"
		putsa $logs_file "     $pname is $c2"
		set c2se [parameter $pname se]
		if {0 != $c2se} {
		    catch {set c2se [format %.7f $c2se]}
		    putstee $results "\t        $pname Std. Error:  $c2se\n"
		    putsa $logs_file "     $pname Std. Error:  $c2se"
		}
	    }
            set pars [parameter -names]
            foreach par $pars {
                if {"rhoc" == [string range $par 0 3]} {
	            set rhoc [parameter $par =]
	            catch {set rhoc [format %.7f $rhoc]}
	            putstee $results "\t\t\t  [rhocap $par] is $rhoc"
		    putsa $logs_file "     [rhocap $par] is $rhoc"
		    set rhocse [parameter $par se]
		    if {0 != $rhocse} {
			catch {set rhocse [format %.7f $rhocse]}
			putstee $results "\t        [rhocap $par] Std. Error:  $rhocse\n"
			putsa $logs_file "     [rhocap $par] Std. Error:  $rhocse"
		    }
		}
            }
            if {"" != $p_rhoc0} {
                putstee $results \
                "\t       RhoC different from zero  $p_rhoc0"
                putsa $logs_file \
		"    RhoC different from zero  $p_rhoc0"
            }
	    if {"" != $p_rhoc1} {
	        putstee $results \
                "\t       RhoC different from 1.0   $p_rhoc1"
		putsa $logs_file \
		"    RhoC different from 1.0   $p_rhoc1"
	    }
	    if {"" != $p_rhocn1} {
	        putstee $results \
		"\t       RhoC different from -1.0  $p_rhocn1"
		putsa $logs_file \
		"    RhoC different from -1.0  $p_rhocn1"
	    }
	} else {
	    if {$hpsp < $vc_probability_level} {
		set comment "(Significant)"
		set further_comment ""
	    } else {
		set comment "(Not Significant)"
		set further_comment "     (C2 retained because nonzero)"
	    }
	    if {$hpsp != 1.0} {set comment "p $hpspsign $hpsp  $comment"}
	    set c2 [parameter c2 =]
	    catch {[set c2 [format %.7f $c2]]}
	    putstee $results "\t\t\t  C2 is $c2  $comment"
	    putsa $logs_file "     C2 is $c2  $comment"
	    set c2se [parameter c2 se]
	    if {0 != $c2se} {
		catch {set c2se [format %.7f $c2se]}
		putstee $results "\t        C2 Std. Error:  $c2se $further_comment\n"
		putsa $logs_file "\t  C2 Std. Error:  $c2se $further_comment"
	    } else {
		putstee $results ""
	    }
	}
    } elseif {$ihouse} {
	foreach tr $ts {
	    set pname C2
	    if {$multi} {
		set pname C2\($tr\)
	    }
	    putstee $results "\t\t\t  $pname is 0.0000000"
	    putsa $logs_file "    $pname is 0.0000000"
	}
	if {!$keephouse} {
	    if {$multi} {
		putstee $results "\n\tSince they were zero, all C2 parameters have been deleted."
	    } else {
		putstee $results "\n\tSince it was zero, the C2 parameter has been deleted."
	    }
	    putstee $results "\tTo keep C2 parameters even when they are all zero,"
	    putstee $results "\tuse the -keephouse option."
        } else {
	    putstee $results "\n\tC2 parameter(s) retained because of -keephouse option."
	}
    }
#
# Results for each covariate
#
    if {$covscreen} {
	for {set i 0} {$i < $number_of_covariates} {incr i} {
	    set covar [format "%40s" [lindex $covar_list $i]]
	    set report [lindex $report_list $i]
	    putstee $results " $covar  $report"
	}
    }
    if {$imoutvalid} {
	set i 0
        foreach covar $covar_list {
	    if {[if_parameter_exists b$covar]} {
               imout -puts [parameter b$covar =] -vol [expr $covbase+0+($i*4)]
               imout -puts [parameter b$covar se] -vol [expr $covbase+1+($i*4)]
            } else {
                imout -puts 0 -vol [expr $covbase + 0 + ($i * 4)]
                imout -puts 0 -vol [expr $covbase + 1 + ($i * 4)]
            }
            incr i
        }
    }


#
# Pedigree-household group merging
#
    if {$needhouse} {
	putstee $results ""
	if {"" != [set merge_string [find_string \
		[full_filename $bestmodel.out] "pedigrees merged into" ]]} {
	    set merge_string [trim_left $merge_string]
	    putstee $results "\t$merge_string"
	} elseif {"" != [set merge_string [find_string \
		[full_filename $bestmodel.out] "pedigrees shared house" ]]} {
	    set merge_string [trim_left $merge_string]
	    putstee $results "\t$merge_string"
	} elseif {1==[option MergeAllPeds]} {
	    putstee $results "\tAll pedigrees merged into one pedigree-household group"
	} elseif {0==[option MergeHousePeds]} {
	    putstee $results "\tPedigrees not merged into household groups (option MergeHousePeds 0)"
	}
    }
#
# Proportion of variance
#

    if {$some_covariates_removed} {
	putstee $results "\n\tThe following covariates were removed from final models:"
	putstee $results "\t$remove_covar_list"
	putsa $logs_file "\n\tThe following covariates were removed from final models:"
	putsa $logs_file "\t$remove_covar_list"

    }
    if {$any_covariates_exist && !$multi} {
	if {$found_constrained_covariates} {
	    putstee $results \
"\n\tSome covariates are constrained.  This prevents computation of"
            putstee $results \
"\tvariance due to all covariates or Kullback-Leibler R-squared."
        } elseif {0==[llength $final_covar_list]} {
	    putstee $results "\n\tNo covariates were included in the final model"
        } elseif {$residinor} {
	    putstee $results "\n\tProportion of Variance due to Covariates not computed due to -residinor"
        } elseif {$do_var_due2cov} {
	    if {$testcovar == ""} {
		putstee $results \
		    "\n\tProportion of Variance Due to All Final Covariates Is"
	    } else {
		putstee $results \
		    "\n\tProportion of Variance Due to Tested Covariate Is"
	    }
	    if {$prop_var < 0} {
		putstee $results "\t\t\[Not Estimable due to Instability\]"
		putstee $results \
		"\t\t\[Some covariate effects may be relatively small\]"
		putstee $results \
		"\t\t\[Consider running fewer covariates or 'bayesavg -cov'\]"
	    } else {
		putstee $results [format "\t\t\t\t  %s" $prop_var]
                if {$imoutvalid} {
		    imout -puts $prop_var -vol 5
	        }
            }
	} elseif {$got_kullback} {
	    putstee $results \
		    "\n\tKullback-Leibler R-squared is [format %.7f $kullback]"
	} elseif {!$do_kullback && ![catch {find_simple_constraint sd}]} {
	    putstee $results \
		    "\n\tCannot determine variance due to covariates because SD is constrained"
	}
    }
#
# Notes
#
    set outputdir [full_filename {}]
    puts "\n\tOutput files and models are in directory $outputdir"
    putsa $logs_file "\n"
    putsat $logs_file "\tSummary results are in $results_filename"
    puts $results ""
    set rlogsfile $logs_file
    if {[string length [full_filename ""]] > 18} {
        set rlogsfile "\n\t  $rlogsfile"
    }
    putstee $results "\tLoglikelihoods and chi's are in $rlogsfile"
    set and_null0 ""
    if {!$sporadic} {
	set and_null0 " and null0"
	catch {file copy -force $outputdir$bestmodel.out [catenate $outputdir null0.out]}
    }
    puts $results "\tBest model is named $bestmodel$and_null0"
    puts "\tBest model is named $bestmodel$and_null0 (currently loaded)"
    putstee $results "\tFinal models are named $finalmodels"
    if {$covscreen && $free_covariates_exist} {
	if {$sporadic} {
	  puts $results "\tInitial sporadic model is s0"
	} else {
	  puts $results "\tInitial sporadic and polygenic models are s0 and p0"
	}
	if {$ihouse} {
	    if {$sporadic} {
		puts $results "\tInitial household model is h0"
	    } else {
		puts $results "\tInitial household and household polygenic models are h0 and hp0"
	    }
	}
	puts "\tConstrained covariate models are named no<covariate name>"
	puts $results "\tConstrained covariate models are named no<covariate name>"
    }
    close $results

# For univariate, do special residual test.

if {!$multi && !$discrete_trait && !$residinor} {
    set did_residual 0
    catch {
	if {"" == $final_covar_list} {
	    if {-1 != [lsearch -exact [string tolower [define names]] \
		       [string tolower [trait]]]} {
		maximize -q -sampledata
		set st [stats -q trait1 -file [full_filename sampledata.out] -return -out [full_filename $basename.residuals.stats]]
	    } else {
		set st [stats -q -return -out [full_filename $basename.residuals.stats]]
	    }
	} else {
	    residual $bestmodel.out -q -out [full_filename $basename.residuals] -fewest
	    set st [stats residual -q -return -file [full_filename $basename.residuals] -out [full_filename $basename.residuals.stats]]
	}
    set kurt [stats_get $st kurtosis]
    catch {set kurt [format %.4f $kurt]}
    set SOLAR_Kurtosis $kurt
    if {$kurt > 0.8} {
	putsat $results_filename "\n\tWarning!  Residual Kurtosis is $kurt which is too high."
	set kurt_bad 1
	if {[if_parameter_exists t_param]} {
	    putsat $results_filename "\tSince you are using tdist, it is probably OK."
	    set kurt_bad 0
	}
	if {$kurt_bad} {
	    putsat $results_filename "\tSee note 5 in \"help polygenic\"."
	}
    } else {
	putsat $results_filename "\n\tResidual Kurtosis is $kurt, within normal range"
    }
    if {$imoutvalid} {
	imout -puts $kurt -vol 6
    }
    set did_residual 1
    }
    if {!$did_residual} {
	putsat $results_filename "\n\tWarning:  An error occurred while computing residual kurtosis"
	putsat $results_filename "\tTry running residual command to see what happened"
    }
}
if {$residinor} {
    puts "\n\tWarning: Residual phenotypes file loaded."
    puts "\tTo restore original: load phenotypes $SOLAR_old_phenotypes_files"
    puts "\tOr use command restore_phen\n"
}
    if {$imoutvalid} {imout -puts 1 -vol 0}
    return ""
}

proc restore_phen {} {
    if {[if_global_exists SOLAR_old_phenotypes_files]} {
	global SOLAR_old_phenotypes_files
	eval load phen $SOLAR_old_phenotypes_files
    } else {
	error "restore_phen only works after using phenotypes -residinor"
    }
    return ""
}


proc rhocap {word} {
    return [catenate [string toupper [string index $word 0]] \
		[string range $word 1 2] \
		[string toupper [string index $word 3]] \
		[string range $word 4 end]]
}

# solar::relpairs --
# solar::relatives --
#
# Purpose:  Show relationships of relative pairs included in analysis
#             (having all required variables)
#
# Usage:    relatives [-meanf]
#                      -meanf causes Mean f to be reported
#           relpairs             ;# alias for "relatives -meanf"
#
#
# Notes:    output is returned (displayed) and also written to file named
#           relatives.out in current trait/outdir.
#
#           Uses previously stored null0 model in current trait/outdir.
#           Polygenic command should have been run previously to create
#           null0 model.
# -

proc status_message {message} {
    puts -nonewline [format "%-72s\r" $message]
    flush stdout
}

proc relpairs {args} {
    return [relatives -meanf -full $args]
}

proc relatives {args} {
    return [eval pedigree classes -model $args]
}

# solar::residual --
#
# Purpose:  Compute residuals for a maximized model and phenotypes file
#
# Usage:    residual [solarfile] [-out <residualfile>] [-phen <phenfile>] 
#                    [-fewest] [-needped]
#
#           solarfile       solar maximization output file which is expected
#                             in the current outdir.  The default is null0.out
#                             which is created by the polygenic command.
#                             The default directory is the current outdir,
#                             but you may specify a relative pathname to
#                             the current directory.
#
#                           EVD2 models must have actual model currently
#                           in memory (such as is the case immediately after
#                           running polygenic).
#
#                            If the "define" command is used to define the
#                            names used for trait(s) or covariate(s), there
#                             must be a model with the same rootname in
#                             the same directory as the output file.  The
#                             default is null0.mod.
#
#                            Handling of the "scale" (and "noscale") commands
#                             also requires the presence of the model with
#                             the same rootname in the same directory as the
#                             output file.  If this model is not present,
#                             residual will finish but print a warning if
#                             not inside a script.
#
#           residualfile    new phenotypes file with trait 'residual' (the
#                             default is 'residual.out' written to the working
#                             directory).
#           phenfile        the base phenotypes file; the default is to use
#                             the currently loaded phenotypes file(s).
#           -fewest         Copy fewest fields to residualfile (this would be
#                             ID, FAMID (if required), trait, and residual.
#                             The default is to include the above along with
#                             all other (unused) variables from the phenfile.
#           -needped        Only include individuals in the pedigree file.
#                             (By default, all individuals in the phenotypes
#                              file would be included, unless there is a
#                              covariate including sex which require the
#                              pedigree file.)
#
# Example:
#           solar> automodel pheno atrait
#           solar> polygenic -s
#           solar> residual
#
# MOST IMPORTANT!
#
#           This procedure requires a maximization output file.
#           Unless otherwise specified, the default is assumed to be null0.out
#           which is produced by the "polygenic" command.  If this is not
#           what you want, you need to specify the maximization output file.
#           You cannot specify a model file, that is insufficient.
#
# Additional Notes:
#
#           Univariate quantitative trait only.
#           The trait or outdir must be specified first.
#           Must be run in the directory in which pedigree was loaded.
#           FAMID is included if present in both phenotypes and pedigree files.
#           residualfile will be written in comma delimited format.
#           This procedure does not handle hand-written 'mu' equations, only
#             standard covariates.
#           Not applicable to discrete traits.
#-

proc residual {args} {

# Parse arguments

    set pearson 0
    set fewest 0
    set needped 0
    set quiet 0
    set class 0
    set phenfile ""
    set residualfile residual.out
    set outputfile [set default_outputfile [full_filename null0.out]]
    set orargs [read_arglist $args \
	    -out residualfile \
	    -phen phenfile \
	    -phenotypes phenfile \
            -needped {set needped 1} \
	    -q {set quiet 1} \
	    -class {set class 1} \
	    -fewest {set fewest 1}]
    if {1 == [llength $orargs]} {
	set outputfile $orargs
    } elseif {1 < [llength $orargs]} {
	error \
" Invalid argument(s)\n\
Usage: residual \[solarfile\] \[-out residualfile\] \[-phen phenfile\] \[-fewest\]"
    }

# See if user specified path in filename; if not, add 

    if {-1 == [string first / $outputfile]} {
	set outputfile [full_filename $outputfile]
    }

# See if output file exists

    if {![file exists $outputfile]} {
	if {$default_outputfile == $outputfile} {
	    error "$outputfile not found.\nMaybe you need to specify maximization output file."
	} else {
	    error "residual: File $outputfile not found."
	}
    }

    notquiet puts "Using maximization output file: $outputfile"

# Ensure that this was not a discrete maximization
#
#    if {"" != [find_string $outputfile \
#		   "Using SOLAR Discrete and Mixed Trait Modeling"]} {
#	puts "** Discrete trait, results will be scaled to Pearson Residuals"
#	set pearson 1
#    }

# Open "SOLAR output" file

    global solar_residual_ofile
    set ofile [open $outputfile r]
    set solar_residual_ofile $ofile
    set ofiledir [file dirname $outputfile]

# Scan "output" file to find phenotypes file

    if {"" == $phenfile} {
	set phenfile [phenotypes -files]
	if {[llength $phenfile] > 1} {
	    eval joinfiles -o $ofiledir/join.residual.out $phenfile
	    set phenfile $ofiledir/join.residual.out
	}
	if {"" == $phenfile} {
	    close $solar_residual_ofile
	    error \
              "Can't find Phenotypes filename in SOLAR output file $outputfile"
	}
    }

# Open the other files

    if {[catch {set phenfile [solarfile open $phenfile]}]} {
	close $solar_residual_ofile
	error "Can't find phenotypes file $phenfile"
    }

    if {[catch {set rfile [open $residualfile w]}]} {
	close $solar_residual_ofile
	solarfile $phenfile close
	error "Can't open $residualfile for writing"
    }

    if {[catch {set pedfile [tablefile open pedindex.out]}]} {
	close $ofile
	solarfile $phenfile close
	close $rfile
	error "residual must run from directory in which pedigree was loaded"
    }
	
# The actual body is a subroutine so that we can control
# File opening and closing here

# However, in case trait is defined trait, the name of the "maximization
# output" file is included for possible very constrained use.

    if {[catch {res_body $ofile $phenfile $rfile $pedfile $fewest $needped $outputfile $quiet $pearson $class} \
	    errormsg]} {
	close $solar_residual_ofile
	solarfile $phenfile close
	close $rfile
	tablefile $pedfile close
	error $errormsg
    }
    close $solar_residual_ofile
    solarfile $phenfile close
    close $rfile
    tablefile $pedfile close
    return ""
}

proc res_body {ofile phenfile rfile pedfile fewest needped ofilename \
		   quiet pearson ifclass} {

    set allclasses ""
    global solar_residual_ofile

# Get all user definitions

    set definitions [string tolower [define names]]

# Scan "output" file to find all parameters

    set found_parameter_start 0
    set found_parameter_mean 0

    ifdebug puts "scanning output file"
    while {-1 != [gets $ofile line]} {
	if {-1 != [string first "Final Val" $line]} {
	    gets $ofile
	    set found_parameter_start 1
	    break
	}
    }
    if {!$found_parameter_start} {
	error "Output file does not include parameters"
    }
    set pnames {}
    set pvalues {}
    set pcount 0
    set bcount 0

# For evd2 covariates, we must get actual covariate names from the loaded
# model (not the output file as usual).  Once we have actual covariate
# names, we can handle them as usual

    set rocovars [covariates]
    set ocovars {}
    foreach cov $rocovars {
	if {"Suspended\[" != [string range $cov 0 9] &&
	    "\(\)" != [string range $cov end-1 end]} {
	    lappend ocovars $cov
	}
    }

    while {-1 != [gets $ofile line]} {
	if {2 > [llength $line]} {break}
	incr pcount
	set thispname  [lindex $line 0]
	ifdebug puts "thispname is $thispname"
	if {[string range $thispname 0 5] == "bevd2_"} {
	    set thiscname "[lindex $ocovars $bcount]"
#
# If the beta name has bevd2_ prefix, and if it doesn't simply
# match the covariate name, it must be translated name, so we start
# from original covariate name and add "b" prefix.
# Verify this would be the correct name in reverse
#
	    if {"b$thiscname" != $thispname} {
		set thispname2 "bevd2_"
		for {set i 0} {$i < [string length $thiscname]} {incr i} {
		    set ch [string index $thiscname $i]
		    if {$ch == "*"} {
			set ch X
		    } elseif {$ch == "^"} {
			set ch "up"
		    }
		    set thispname2 "$thispname2$ch"
		}
		if {$thispname != $thispname2} {
		    puts "error translating $thispname to $thispname2"
		    error "EVD2 models must run polygenic before residual"
		}
		set thispname b$thiscname
	    }
	    incr bcount
	}
	lappend pnames  $thispname

	lappend pvalues [lindex $line 1]
	gets $ofile
	gets $ofile
	set testpname [string tolower [lindex $line 0]]
	if {!$ifclass} {
	    if {0==[string compare $testpname mean]} {
		set parameter_mean [lindex $line 1]
		set found_parameter_mean 1
	    }
	} else {
	    if {0==[string compare [string range $testpname 0 5] mean_c]} {
		set classindex [string range $testpname 6 end]
		set parameter_mean($classindex) [lindex $line 1]
		set found_parameter_mean 1
	    }
	}
    }
    if {!$found_parameter_mean} {
	error "residual works only for univariate models with mean parameter"
    }


# Get covariate beta's from list of parameters

    set betacount 0
    set betanames {}
    set betavalues {}
    for {set i 0} {$i < $pcount} {incr i} {
	if {"b" == [string range [lindex $pnames $i] 0 0]} {
	    lappend betanames [lindex $pnames $i]
	    lappend betavalues [lindex $pvalues $i]
	    incr betacount
	}
    }
    ifdebug puts "betanames are $betanames"
    if {$betacount < 1} {
	error "Can't find any covariates"
    }
#
# Parse each covariate name and derive Mu expression from all of them
#   Meanwhile collect list of covariate variables (covariables)

    set expcovar 0
    set checkname ""
    set checknames ""
    if {!$ifclass} {
	set expression ""
    }
    set covariables {}
    for {set i 0} {$i < $betacount} {incr i} {
	set name [string range [lindex $betanames $i] 1 end]
	if {$ifclass} {
	    set upos [string last _c $name]
	    set thisclass [string range $name [expr $upos+2] end]
	    set name [string range $name 0 [expr $upos - 1]]
	}

	set checkname $name

	if {-1 != [string first "(" $name]} {
	    set name [string range $name 0 [expr [string first "(" $name] - 1]]
	    set checkname $name
	}
	set term [lindex $betavalues $i]
#
# Parse multipliers (up to final multiplicand)
#
	while {-1 != [string first "*" $name]} {
	    set name1 [string range $name 0 [expr [string first * $name] - 1]]
	    set checkname $name1
	    set name [string range $name [expr [string first * $name] + 1] end]
#
# Parse exponenential expressions (only name^number is possible)
#
	    if {-1 != [string first ^ $name1]} {
		set number [string range $name1 [expr [string first ^ \
			$name1] + 1] end]
		set name1 [string range $name1 0 [expr [string first ^ \
			$name1] - 1]]
		set checkname $name1
		set aname1 "\${v_$name1}-\${x_$name1}"
		set adj_name1 "pow(${aname1},$number)"
	    } else {
#
# No exponent simple case
#
		set adj_name1 "(\${v_$name1}-\${x_$name1})"
	    }
#
# Append multiplier to term
#
	    set term "$term*$adj_name1"
	    setappend covariables $name1
	    setappend checknames $checkname
	}
#
# Now we're at final multiplier in term...
# Parse exponential expression in final multiplier
#
	if {-1 != [string first ^ $name]} {
	    set number [string range $name [expr [string first ^ \
		    $name] + 1] end]
	    set name [string range $name 0 [expr [string first ^ \
		    $name] - 1]]
	    set aname "\${v_$name}-\${x_$name}"
	    set adj_name "pow($aname,$number)"
	} else {
#
# Non-exponent simple case
#
	    set adj_name "(\${v_$name}-\${x_$name})"
	}

# Append multiplier to term

	setappend checknames $name
	set term "$term*$adj_name"
	setappend covariables $name

# Append term to expression

	if {!$ifclass} {
	    set expression "$expression + $term"
	} else {
	    if {[catch {llength $expression($thisclass)}]} {
		set expression($thisclass) ""
	    }
	    set expression($thisclass)  "$expression($thisclass) + $term"
	    setappend allclasses $thisclass
	}
    }

    if {!$ifclass} {
	set expression [string tolower [string range $expression 3 end]]
    } else {
	foreach class $allclasses {
	    set expression($class) [string tolower [string range $expression($class) 3 end]]
	}
    }
    set covariables [string tolower $covariables]
    notquiet puts "Covariate variables: $covariables"

# Find variable means in output file

    set found_variables {}
    set found_variable_means 0

    if {[option modeltype] == "evd2"} {
	close $ofile
	set ofile [open $ofilename r]
	set solar_residual_ofile $ofile
    }
    while {-1 != [gets $ofile line]} {
	if {-1 != [string first "Descriptive Statistics for the Variables" \
		$line]} {
	    gets $ofile
	    gets $ofile line
	    set found_variable_means 1
	    break
	}
    }
    if {!$found_variable_means} {
	error "Couldn't find variable means in output file"
    }

# Determine which covariates are actually definitions

    set checknames [string tolower $checknames]
    set defcovars ""
     foreach checkname $checknames {
	if {-1 != [lsearch -exact $definitions $checkname]} {
	    setappend defcovars $checkname
	}
    }

    set expcovar 0
    if {0 < [llength $defcovars]} {
	notquiet puts "Defined covariates are $defcovars"
	set expcovar 1
    }

# Read variable means from output file

    set traitname ""
    while {-1 != [gets $ofile line]} {
	set discretevar 0
	if {6 != [llength $line]} {break}
	set varname [string tolower [lindex $line 0]]
	set varname_prefix [string range [string tolower $varname] \
				0 3]

	if {"*" == [string range $varname end end]} {
	    set varname [string range $varname 0 [expr \
			[string length $varname] - 2]]
	    set discretevar 1
	}
	
	if {$varname_prefix == "snp_" || \
		$varname_prefix == "hap_" } {
	    eval global x_$varname
	    eval set x_$varname 0
	} elseif {$discretevar} {
	    eval global x_$varname
	    eval set x_$varname \"[lindex $line 3]\"
	} else {
	    eval global x_$varname
	    eval set x_$varname \"[lindex $line 1]\"
	}
	if {"" == $traitname} {
	    set traitname [string tolower $varname]
	    if {$discretevar} {
		puts "** Discrete trait, results will be scaled to Pearson Residuals"
		set pearson 1
	    }
	}
	lappend found_variables [string tolower $varname]
    }
    

# Fix EVD variable name

    if {-1 < [string first _evd $traitname] && [option modeltype] == "evd2"} {
	regsub -all _evd $traitname "" newpname
	set traitname $newpname
    }

# Report trait name

    notquiet puts "Trait variable:      $traitname"
# If FAMID is present in both pedigree and phenotypes files, we assume it
# is needed

    set need_famid 0
    if {[tablefile $pedfile test_name FAMID]} {
	if {[solarfile $phenfile test_name famid]} {
#	    puts "FAMID is present in both pedigree and phenotypes files"
	    set need_famid 1
	}
    }

    set exptrait 0
    if {-1 == [lsearch -exact $definitions $traitname]} {
	if {!$ifclass} {
	    set expression "\${v_$traitname} - (\$parameter_mean + $expression)"
	} else {
	    foreach class $allclasses {
		set expression($class) "\${v_$traitname} - (\$parameter_mean($class) + $expression($class))"
	    }
	}
    } else {
	set exptrait 1
    }

# If defined trait or covars, read in "sampledata.out" to get actual data
# store in array expdata for retrieval during output loop below

    set modelname [file rootname $ofilename]
    if {$exptrait || $expcovar} {

	save model residual.save
	catch {
	    ifdebug puts "Expression trait...maximizing $modelname"
	    load model $modelname
	    set quietmax -q
	    ifdebug set quietmax ""
	    option modeltype Default
	    eval maximize $quietmax -sampledata
	} errmes
	load model residual.save
	if {"" != $errmes} {
	    error "Unable to get definition data for model $modelname"
	}
	set ofiledir [file dirname $ofilename]
	set wfile [tablefile open [file join $ofiledir sampledata.out]]
	set wdcount 0
	catch {
	    tablefile $wfile start_setup
	    tablefile $wfile setup id
	    set tindex 1
	    if {$need_famid} {
		set tindex 2
		tablefile $wfile setup famid
	    }
	    if {$exptrait} {
		tablefile $wfile setup trait1
	    }
	    foreach dcovar $defcovars {
		tablefile $wfile setup $dcovar
	    }
	    while {{} != [set record [tablefile $wfile get]]} {
		if {$exptrait} {
		set tvalue [format %.12g [lindex $record $tindex]]
		if {[is_float $tvalue] && $tvalue != -.1e21} {
		    if {$need_famid} {
			set expdata([lindex $record 0].[lindex $record 1]) $tvalue
		    } else {
			set expdata([lindex $record 0]) $tvalue
		    }
		}
		}
		set bindex [expr $tindex + $exptrait]
		foreach dvar $defcovars {
		    set vvalue [lindex $record $bindex]
		    set fvalue [format %.12g $vvalue]
		    if {[is_float $fvalue] && $fvalue != -.1e21} {
			if {$need_famid} {
			set cexpdata($dvar.[lindex $record 0].[lindex $record 1]) $fvalue
		    } else {
			set cexpdata($dvar.[lindex $record 0]) $fvalue
		    }
		    incr bindex
		}
		}

		incr wdcount
	    }
	} errmes
	tablefile $wfile close
	ifdebug puts "Read $wdcount records from sampledata.out"
	if {"" != $errmes} {
	    error $errmes
	}
	if {$exptrait} {
	    if {!$ifclass} {
		set expression "\$trait1_expression - (\$parameter_mean + $expression)"
	    } else {
		foreach class $allclasses {
		    set expression($class) "\$trait1_expression - (\$parameter_mean($class) + $expression($class))"
		}
	    }
	}
    }

# Report expression

    if {$pearson} {
	if {!$ifclass} {
	    notquiet puts "RawResidual = $expression"
	} else {
	    foreach class $allclasses {
		notquiet puts "RawResidual\($class\) = $expression($class)"
	    }
	}
    } else {
	if {!$ifclass} {
	    notquiet puts "Residual = $expression"
	} else {
	    foreach class $allclasses {
		notquiet puts "Residual\($class\) = $expression($class)"
	    }
	}	    
    }

# If sex is a covariable, we must get sex from pedigree file

    set need_sex 0
    if {$needped || (-1 != [lsearch $covariables sex])} {
	set sex_id_table {}
	set sex_v_table {}
	set need_sex 1
	set x_sex 1
	tablefile $pedfile start_setup
	tablefile $pedfile setup ID
	if {$need_famid} {
	    tablefile $pedfile setup FAMID
	}
	tablefile $pedfile setup SEX
	while {"" != [set record [tablefile $pedfile get]]} {
	    set sex_id_rec [lrange $record 0 $need_famid]

	    set sexcode [lindex $record end]
	    set sexvalue 0
	    if {$sexcode == "1" || "m" == $sexcode || "M" == $sexcode} {
		set sexvalue 1
	    } elseif {$sexcode == "2" || "f" == $sexcode || "F" == $sexcode} {
		set sexvalue 2
	    }
	    if {$sexvalue != 0} {
		lappend sex_id_table $sex_id_rec
		lappend sex_v_table $sexvalue
	    }
	}
	lappend found_variables sex
    }

# Check modelfile for "scale" commands

    set foundmodel 0
    set scaledvars {}
    set scalevals {}
    catch {
	if {[file exists $modelname.mod]} {
	    set modfile [open $modelname.mod]
	    set foundmodel 1
	    while {-1 != [gets $modfile modline]} {
		if {"scale" == [lindex $modline 0]} {
		    lappend scaledvars [string tolower [lindex $modline 1]]
		    lappend scaledvals [lindex $modline 2]
		}
	    }
	    close $modfile
	}
    }
    if {!$foundmodel} {
	notquiet puts "\nWarning.  Can't find model $modelname.mod"
	notquiet puts "Note: Without model file, cannot adjust for scale commands, if any.\n"
    }
		    

# Check to see we got all variable means we need
# Adjust for scale commands
# Print final values

#    puts "found variables $found_variables"
    foreach variable $covariables {
	set variable [string tolower $variable]
	if {-1 == [lsearch $found_variables $variable]} {
	    error "Didn't find variable $variable in output file"
	}
	set scaleindex [lsearch $scaledvars $variable]
	if {$scaleindex != -1} {
	    set scaleval [lindex $scaledvals $scaleindex]
	    eval set x_$variable $scaleval
	}
	notquiet puts -nonewline "x_$variable = "
	notquiet eval puts \${x_$variable}
    }	

# Write first record of residual file
# and setup output format

    set extra_vars {}
    set id_name [string tolower [solarfile $phenfile establish_name id]]

    if {[catch {set famid_name [string tolower [solarfile $phenfile establish_name famid]]}]} {

# The above should not have failed if famid properly mapped
# So, just force it to "famid" in case improperly mapped, and
# this will exclude it from output file anyway

	set famid_name famid
    }

    if {!$exptrait} {
	set formatend "\${v_$traitname},\$residual"
    } else {
	set formatend "\$trait1_expression,\$residual"
    }
    if {$need_famid} {
	puts -nonewline $rfile "$id_name,$famid_name,$traitname,residual"
	set outformat "\$id,\$famid,$formatend"
	set checklist "$id_name $famid_name $traitname"
    } else {
	puts -nonewline $rfile "$id_name,$traitname,residual"
	set outformat "\$id,$formatend"
	set checklist "$id_name $famid_name $traitname"
    }
    set checklist [string tolower $checklist]
    if {!$fewest} {
	set names [string tolower [solarfile $phenfile names]]
	foreach name $names {
	    if {-1 == [lsearch $checklist $name]} {
		if {-1 == [lsearch $covariables $name]} {
		    puts -nonewline $rfile ",$name"
		    set outformat "$outformat,\${v_$name}"
		    lappend extra_vars $name
		}
	    }
	}
    }
    puts $rfile ""
    ifdebug puts "outformat: $outformat"

# Set up phenfile for required variables
# setup list of variable names

    set variablenames {}
    set num_vars_needed 0
    solarfile $phenfile start_setup
    solarfile $phenfile setup [field id]
    lappend variablenames id
    incr num_vars_needed
    if {$need_famid} {
	solarfile $phenfile setup [field famid]
	lappend variablenames famid
	incr num_vars_needed
    }
    if {$ifclass} {
	solarfile $phenfile setup class
	lappend variablenames class
	incr num_vars_needed
    }
    if {!$exptrait} {
	solarfile $phenfile setup $traitname
	incr num_vars_needed
	lappend variablenames v_$traitname
    }
    foreach covar $covariables {
	if {"sex" == $covar} {continue}
	if {-1 != [lsearch -exact $defcovars $covar]} {continue}
	solarfile $phenfile setup $covar
	incr num_vars_needed
	lappend variablenames v_$covar
    }
    foreach extra_var $extra_vars {
	solarfile $phenfile setup $extra_var
	lappend variablenames v_$extra_var
    }
    set vcount [llength $variablenames]

# Main loop:
# Read phenotypes file, calculate, and output residuals

    set indcount 0
    while {"" != [set record [solarfile $phenfile get]]} {
	set incomplete 0
	for {set i 0} {$i < $vcount} {incr i} {
	    set value [lindex $record $i]
	    if {{} == $value && $i < $num_vars_needed} {
		set incomplete 1
		break
	    }
	    eval set [lindex $variablenames $i] \$value
	}
	if {$incomplete} {continue}
	if {$needped || $need_sex} {
	    set sex_found 0
	    set target [list $id]
	    if {$need_famid} {set target [list $id $famid]}
	    set pindex [lsearch -exact $sex_id_table $target]
	    if {$pindex == -1} {
		set incomplete 1
	    }
	    set v_sex [lindex $sex_v_table $pindex]
	}
	if {$exptrait && $need_famid} {
	    if {[catch {set trait1_expression $expdata($id.$famid)}]} {
		set incomplete 1
	    }
	} elseif {$exptrait} {
	    if {[catch {set trait1_expression $expdata($id)}]} {
		set incomplete 1
	    }
	}
	if {$expcovar} {
	    foreach dvar $defcovars {
		set dok 0
		catch {
		    if {!$need_famid} {
			set v_$dvar $cexpdata($dvar.$id)
		    } else {
			set v_$dvar $cexpdata($dvar.$id.$famid)
		    }
		    set dok 1
		}
		if {!$dok} {
		    set incomplete 1
		}
	    }
	}
	if {!$incomplete} {
	    ifdebug puts "Eval for id $id"
	    if {!$ifclass} {
		set use_exp $expression
	    } else {
		set use_exp $expression($class)
	    }
	    ifdebug puts "expression is $use_exp"
	    set residual [eval expr $use_exp]

	    if {$pearson} {
		set predicted $residual
		set phi [alnorm $predicted t]
		set traitval [eval expr [lindex $use_exp 0]]
		ifdebug puts "pearson traitvalue is $traitval"
		set residual [expr (double($traitval)-$phi)/(sqrt($phi)*sqrt(1-$phi))]
	    }
	    set residual [format %.9f $residual]
	    incr indcount
	    if {[catch {eval puts $rfile \"$outformat\"}]} {
		error "\nIllegal character in phenotype name"
	    }
	}
    }
    notquiet puts "$indcount records written"
}
    

# solar::gridh2r
#
# purpose: grid around the h2r value in polygenic model
#
# usage:   gridh2r [-lower <lower>] [-upper <upper>] [-step <step>]
#
#          -lower <lower>  Lowest h2r; default is current value less 0.1
#          -upper <upper>  Highest h2r; default is current value plus 0.1
#          -step <step>    step; default is 0.01
#
# Notes:   polygenic should be run first.  Only univariate models with
#          only e2,h2r parameters are supported.
#
#          Out is written to a file named gridh2r.out in the maximization
#          output directory.  The starting model is listed first regardless
#          of whether it is in the range specified.
#      
#          After completion, the model with the best loglikelihood will
#          be loaded, but with the h2r constraint (if any) deleted.  This
#          might be the starting model even if it isn't in the specified
#          range.
#
#          Each point requires a maximization, so they might come out
#          slowly.  For full maximization detail, give the "verbosity plus"
#          or "verbosity max" command beforehand.
# -

proc gridh2r {args} {

    file delete [full_filename gridh2r.out]

    set lower [highest 0 [expr [parameter h2r =] - 0.1]]
    set upper [lowest 1 [expr [parameter h2r =] + 0.1]]
    set step 0.01

    set badarg [read_arglist $args -lower lower -upper upper -step step]
    if {"" != $badarg} {
	error "gridh2r: Invalid argument: $badarg"
    }

    puts " "
    putsout gridh2r.out "h2r,loglike"
    putsout gridh2r.out "[parameter h2r =],[loglike]"
    set best_loglike [loglike]
    set best_h2r [parameter h2r =]
    save model [full_filename gridh2r.best]

    for {set test $lower} {$test < $upper} {set test [expr $test + $step]} {

# constraint handles parameter and bounds during maximization

	set h2r [format %.10g $test]
	constraint h2r = $h2r
	parameter e2 = [set e2 [expr 1.0 - $h2r]]
	if {[expr $e2 - 0.01] < [parameter e2 lower]} {
	    parameter e2 lower [expr $e2 - 0.01]
	}
	if {[expr $e2 + 0.01] > [parameter e2 upper]} {
	    parameter e2 upper [expr $e2 + 0.01]
	}

	maximize_quietly solar
	putsout gridh2r.out "$h2r,[loglike]"

	if {![is_nan [loglike]] && [loglike] > $best_loglike} {
	    set best_loglike [format %.10g [loglike]]
	    set best_h2r [parameter h2r =]
	    save model [full_filename gridh2r.best]
	}
    }

    load model [full_filename gridh2r.best]
    catch {constraint delete h2r}
    puts "\nBest loglikelihood is $best_loglike at h2r = $best_h2r (loaded)"
    return ""
}


# solar::grid --
#
# Purpose:  Find the highest likelihood in the vicinity of marker(s)
#
# Usage:    grid <marker1> [<marker2> ...]
#
# Example:  grid APOa D6S2436
#
# Notes:    outdir (or trait) and ibddir must previously be specified.
#           ibd matrices for each marker must already have been computed.
#
#           A model named "null0.mod" is expected in the output directory.
#           That can be created with the polygenic command.
#
#           Summary output is displayed on your terminal and written to a file
#           named grid.out.  An output and model file are saved for each
#           marker with the name <markername>_best_theta.
#
#           The twopoint command also has a "-grid" option which will grid
#           around every marker evaluated.
#
#           A special "-bsearch" option sets point at which a "golden section"
#           search begins.  By default, grid single-steps by 0.01 from 0.00 
#           to 0.05 and then begins a golden section search.  (This is on the
#           assumption that the peak will be reached before 0.05 in the vast
#           majority of cases.)  If you have a significant number of cases
#           above 0.05, you might want to change this, for example:
#
#                grid mrk -bsearch 0.01
#
#           would start the golden section search after 0.01 (which will be
#           faster if the value is expected to be greater than 0.05, but
#           slower if the value is going to be less than 0.05).  Note: 0.01 is
#           the smallest value at which the search can begin.  On the other
#           hand if you wanted to single-step all the way to 0.10, you would
#           give the command:
#
#                grid mrk -bsearch .1
#-

proc grid {args} {

# -twopoint is a private option used by "twopoint -grid"

    set begin_search 0.05
    set search_begun 0
    set golden_mean 0.38197

    set twopoint 0
    set markers [read_arglist $args \
	    -twopoint {set twopoint 1} -bsearch begin_search]

# Check things

    ibddir
    full_filename foo
    if {![file exists [full_filename null0.mod]]} {
	error "Model [full_filename null0] not found.\
\nThis can be created with polygenic command."
    }
    foreach marker $markers {
	if {![file exists [ibddir]/ibd.$marker.gz]} {
	    error "IBD matrix not found for marker $marker"
	}
    }
    if {1 > [llength $markers]} {
	error "No markers specified"
    }

    set h2q_index 1
    set boundary_error_flag 0

    if {$twopoint} {
	set outfilename twopoint.out
	if {1 != [llength $markers]} {
	    error "Only one marker allowed with -twopoint option"
	}
    } else {
	set outfilename grid.out
	outheader grid.out 1 LOD 0
	load model [full_filename null0.mod]
    }

    set best_results {}
    set best_result {}
    set best_theta 0

    proc save_best_result {marker theta result} {
	upvar twopoint twopoint
	upvar best_result best_result
	upvar best_ll best_ll
	upvar best_theta best_theta

	set best_result $result
	set best_ll [loglike]
	if {!$twopoint} {
	    set modelname \
		    [full_filename [catenate $marker _best_theta]]
	    save model $modelname
	    file copy -force [full_filename last.out] $modelname.out
	}
    }

    proc evaluate_theta {marker theta} {
	upvar outfilename outfilename
	upvar h2q_index h2q_index
	upvar boundary_error_flag boundary_error_flag

	mibdt $marker $theta
	linkmod2p [full_filename mibd.out.gz]
	set errmsg [maximize_quietly last]
	if {$errmsg != ""} {
	    puts \
	"    *** Error maximizing marker $marker with Theta = $theta"
	    break
	}
	delete_files_forcibly [full_filename mibd.out.gz]

	set result [outresults $outfilename \
		$marker\([string range $theta 1 end]\) [lodn 0] \
		[loglike] $h2q_index $boundary_error_flag]
	return $result
    }


    foreach marker $markers {

# Must remove mibdchr0.loc before first evaluate_theta for each marker

        delete_files_forcibly [full_filename mibdchr0.loc]

	if {$twopoint} {
	    save_best_result $marker 0.00 {}
	} else {
	    set result [evaluate_theta $marker 0.00]
	    save_best_result $marker 0.00 $result
	}

# Use "golden section" method (heavily weighted toward starting position)
# Using begin_theta, left_theta, right_theta, and end_theta

	set begin_theta 0.00
	set begin_ll [loglike]

	if {$search_begun} {
	    set middle_theta 0.19
	} else {
	    set middle_theta 0.01
	}	    

	set end_theta 0.50
	set end_ll [expr [loglike] - 1e20]  ;# can't be >= 0.50 anyway

# Get the ball rolling by doing middle theta

	set result [evaluate_theta $marker $middle_theta]
	set middle_ll [loglike]


	if {$search_begun || ([loglike] > $best_ll)} {
	    save_best_result $marker $middle_theta $result

	    if {$search_begun} {
		set test_theta 0.12
	    } elseif {$begin_search < .02} {
		set test_theta 0.20
		set search_begun 1
	    } else {
		set test_theta 0.02
	    }

	    while {1} {

# Do test evaluation and save results if best so far

		set result [evaluate_theta $marker $test_theta]
		set test_ll [loglike]
		if {[loglike] > $best_ll} {
		    save_best_result $marker $middle_theta $result
		}

# Decide what to do next...

		if {($middle_theta < $test_theta) == ($test_ll > $middle_ll)} {

# Shift to rightmost triad

		    if {$middle_theta < $test_theta} {
			set begin_theta $middle_theta
			set begin_ll $middle_ll
			set middle_theta $test_theta
			set middle_ll $test_ll
			# end unchanged
		    } else {
			set begin_theta $test_theta
			set begin_ll $test_ll
                        # middle unchanged
			# end unchanged
		    }

		} else {

# Shift to leftmost triad

		    if {$middle_theta < $test_theta} {
			# beginning unchanged
			# middle unchanged
			set end_theta $test_theta
			set end_ll $test_ll
		    } else {
			# beginning unchanged
			set end_theta $middle_theta
			set end_ll $middle_ll
			set middle_theta $test_theta
			set middle_ll $test_ll
		    }
		}

# Pick bigger side for next test point using golden mean
# (If there isn't any room, we are done...)

		set left_size [format %.2f [expr $middle_theta - $begin_theta]]
		set right_size [format %.2f [expr $end_theta - $middle_theta]]

		if {$left_size <= 0.01 && $right_size <= 0.01} {
		    break ;# done
		} elseif {!$search_begun && ($middle_theta < $begin_search)} {
		    set test_theta [format %.2f [expr .01 + $middle_theta]]
		} elseif {$left_size >= $right_size} {
		    set search_begun 1
		    set test_theta [format %.2f [expr $middle_theta - \
			    ($golden_mean * ($middle_theta - $begin_theta))]]
		} else {
		    set search_begun 1
		    set test_theta [format %.2f [expr $middle_theta + \
			    ($golden_mean * ($end_theta - $middle_theta))]]
		}
	    }
	}
	lappend best_results $best_result
    }

    delete_files_forcibly [full_filename mibdchr0.loc] [full_filename mibdchr0.mean] \
               [full_filename mibdchr0.mrg.gz]

    if {!$twopoint} {
	if {1==[llength $markers]} {
	    puts \
"\n                                  Best Result\n"
        } else {
	    puts \
"\n                                  Best Results\n"
        }
	foreach result $best_results {
	    puts $result
	}
    } elseif {$best_theta != 0.00} {
	return $best_result
    }
    return ""
}

#dummy version of mibdt
#proc mibdt {markerfile theta} {
#    set fchar [string first . $markerfile]
#    set nstring [string range $markerfile [expr $fchar + 1] end]
#    set number 0
#    scan $nstring %d number
#    set newnum [expr $number + [format %.0f [expr 100*$theta]]]
#    file copy -force [mibddir]/mibd.2.$newnum.gz [full_filename \
#	    mibdchr0.mrg.gz]
#}


proc mibdt {marker theta} {

# If mibdchr0.loc doesn't exist in trait/outdir, this is the first
# evaluate_theta for this marker, so create the files needed by multipnt

    if {![file exists [full_filename mibdchr0.loc]]} {
        set f [open [full_filename mibdchr0.map] w]
        puts $f 0
        puts $f $marker
        close $f
        exec mrgibd [full_filename mibdchr0.map] [ibddir] [full_filename "."]
        file delete [full_filename mibdchr0.map]
        exec getmeans [full_filename mibdchr0.mrg.gz] \
                      [full_filename mibdchr0.mean] 1
        set f [open [full_filename mibdchr0.loc] w]
        puts $f "NLOCI = 1"
        puts $f "LOCATIONS IN CENTIMORGANS:"
        puts $f [format "%-11s 0   0.0" $marker]
        close $f
    }

# Operate in trait/outdir.  Catch errors to restore original directory.
    
    set startdir [pwd]
    set odir [full_filename "."]
    cd $odir
    set rcode [catch {

# Convert theta to map distance in cM using Haldane mapping function

    set qtloc [expr -50 * log(1 - 2*$theta)]
    set cqtloc [format %.0f $qtloc]
    exec multipnt mibdchr0.loc mibdchr0.mrg.gz mibdchr0.mean $qtloc $cqtloc n
    exec gzip -f mibd.out
    matcrc mibd.out.gz

# Be sure to restore original directory.  Re-raise error if error occurred.

    } errmsg]
    cd $startdir
    if {$rcode == 1} {
	global errorInfo errorCode
	return -code $rcode -errorinfo $errorInfo -errorcode $errorCode $errmsg
    }
    return ""
}


# solar::siminf --
#
# Purpose:  Simulate a fully-informative marker and compute its IBDs
#
# Usage:    siminf -out <markerfile> -ibd <ibdfile>
#
#               -out  Write simulated marker genotypes to this filename.
#                     The default is 'siminf.out' in the current working
#                     directory.
#
#               -ibd  Write marker-specific IBDs for the simulated marker
#                     to this filename.  The default is 'ibd.siminf' in the
#                     current working directory.  The file will be gzipped.
# -

proc siminf {args} {
    set outname siminf.out
    set ibdname ibd.siminf
    set badargs [read_arglist $args -out outname -ibd ibdname]
    if {[llength $badargs]} {
	error "Invalid arguments $badargs"
    }

# If IBD filename already has a .gz extension, get rid of it
    if {![string compare [string range $ibdname \
                          [expr [string length $ibdname] - 3] \
                          [expr [string length $ibdname] - 1]] .gz]} {
        set ibdname [string range $ibdname 0 \
                     [expr [string length $ibdname] - 4]]
    }

    if {[catch {set pedfile [tablefile open pedindex.out]}]} {
	error "Pedigree data have not been loaded."
    }

    set outfile [open $outname w]

    tablefile $pedfile start_setup
    tablefile $pedfile setup IBDID
    tablefile $pedfile setup FIBDID
    tablefile $pedfile setup MIBDID

    while {"" != [set record [tablefile $pedfile get]]} {
        set id [lindex $record 0]
        set fa [lindex $record 1]
        if {$fa == 0} {
            set a1($id) [expr $id * 2 - 1]
            set a2($id) [expr $id * 2]
        } else {
            set mo [lindex $record 2]
            if { [drand] < .5} {
                set a1($id) $a1($fa)
            } else {
                set a1($id) $a2($fa)
            }
            if { [drand] < .5} {
                set a2($id) $a1($mo)
            } else {
                set a2($id) $a2($mo)
            }
        }
        puts $outfile [format "%d %d/%d" $id $a1($id) $a2($id)]
    }

    close $outfile
    tablefile $pedfile close

    ibd -inform $outname $ibdname
}

# solar::twopoint --
#
# Purpose:  Perform "Twopoint" analysis on directory of ibd files
#
# Usage:    twopoint [-append] [-overwrite] [-grid] [-cparm {[<parameter>]*}]
#                    -saveall
#
#           -overwrite  (or -ov) Overwrite existing twopoint.out file.
#
#           -append     (or -a)  Append to existing twopoint.out file.
#
#           -cparm {}     Custom parameters.  Scanning will consist of
#                         replacing one matrix with another matrix, everything
#                         else is unchanged.  The starting model MUST be
#                         a "prototype" linkage model with the desired
#                         parameters, omega, and constraints.  Starting
#                         points and boundaries for the parameters must be
#                         explicitly specified.  Following the -cparm tag,
#                         there must be a list of parameters in curly braces
#                         that you want printed out for each model.  The
#                         list can be empty as is indicated with a pair of
#                         curly braces {}.  There must be a model named null0
#                         in the maximization output directory for LOD
#                         computation purposes.  The matrix to be replaced
#                         must have name ibd or ibd1, ibd2, etc.  The highest
#                         such ibd will be replaced.  If the matrix is loaded
#                         with two "columns," such as "d7," each succeeding
#                         matrix will be loaded with two columns also.
#                         See section 9.4 for an example involving dominance.
#                        
#           -grid  Enables the "grid" option, which estimates recombination
#                  fractions in the range theta=0 to 0.45, finding the
#                  optimal value to the nearest 0.01.  (Note: this option is
#                  not important for most twopoint users.  It also
#                  increases evaluation time considerably.  Consider using
#                  the separate "grid" command with only the markers of
#                  greatest interest.)
#
#           -saveall  Save all twopoint models in the maximization output
#                     directory.  The models are named "ibd.<marker>".
# Notes:
#          The trait or outdir must be specified before running twopoint.
#
#          There must be a null0.mod model in the trait or outdir
#          directory.  This can be created with the polygenic command
#          prior to running multipoint.  (This model may include
#          household and covariate effects.  See the help for the
#          polygenic command for more information.)
#
#          An output file named twopoint.out will be created in the trait
#          or outdir directory.  If that file already exists, the user must
#          choose the -append or -overwrite option.
#
#          The best twopoint model is saved as two.mod in the trait or outdir
#          directory.  It is also loaded in memory at the completion of the
#          twopoint command.
#
#          IBDDIR should be set with the ibddir command prior to running
#          twopoint.
#
#          If models have two traits, the 2df LOD scores will be
#          converted to 1df effective LOD scores, with the assumption
#          that parameter RhoQ1 is not intentionally constrained.
#          To override this, use the lodp command (see).  This feature
#          was first included with beta version 2.0.1.
#              
# -

proc twopoint {args} {

    set gridding 0
    set append 0
    set overwrite 0
    set saveall 0
    set plist \'

    set badargs [read_arglist $args \
	    -append {set append 1} -a {set append 1} \
	    -overwrite {set overwrite 1} -ov {set overwrite 1} \
	    -grid {set gridding 1} \
	    -saveall {set saveall 1} \
	    -cparm plist \
	]

    if {{} != $badargs} {
	error "Invalid argument(s) to twopoint: $badargs"
    }

    if {"\'" != $plist} {
	set noparama "-cparm"
	set aparama 1
    } else {
	set noparama ""
	set aparama 0
	set plist ""
    }
    
    set qu -q
    ifverbplus set qu ""

    full_filename test  ;# ensure trait/outdir specified

    set useibddir [ibddir]

    if {![file exists [full_filename null0.mod]]} {
	error "Model [full_filename null0] not found.\
\nThis can be created with polygenic command."
    }

    set highest_old_lod -10000
    set highest_old_record ""
    set done_list {}

    set twopoint_exists 0
    if {[file exists [full_filename twopoint.out]]} {
	set twopoint_exists 1
	if {!$overwrite && !$append} {
	    error \
         "twopoint.out file already exists.  Use -append or -overwrite option."
	}
    }

    if {$overwrite} {
	delete_files_forcibly [full_filename two.mod]
	delete_files_forcibly [full_filename two.out]
    }

    set appending 0
    set open_option -create
    if {$twopoint_exists && $append} {
	set appending 1
	set open_option -append
	set f [open [full_filename twopoint.out] "r"]
	gets $f line
	gets $f line
	while {-1 < [gets $f line]} {
	    set name [lindex $line 0]
	    lappend done_list $useibddir/ibd.$name.gz
	    set lod [lindex $line 1]
	    if {$lod > $highest_old_lod} {
		set highest_old_lod $lod
		set highest_old_record $line
	    }
	}
	close $f
    }

    set wildcard [format %s/ibd.*.gz $useibddir]
    set file_list [glob -nocomplain $wildcard]
    set file_list_len [llength $file_list]

    if {$file_list_len==0} {
	error "No ibd files found in ibddir:\n ($useibddir/ibd.*.gz)"
    }

    set file_list [lsort -dictionary $file_list]

    set do_file_list {}
    for {set i 0} {$i < $file_list_len} {incr i} {
	set test_file [lindex $file_list $i]
	if {-1 == [lsearch -exact $done_list $test_file]} {
	    lappend do_file_list $test_file
	}
    }

    set do_file_list_len [llength $do_file_list]
    if {0 == $do_file_list_len} {
	error "All ibddir ibd files already processed...see [full_filename twopoint.out]"
    }

    lodadj -query -inform stdout

# Setup resultfile

    set headings "Model LOD Loglike"
    set formats "%19s %11s %12.3f"
    set expressions "\$name \$flod \[loglike\]"
    if {$aparama} {
	foreach par $plist {
	    lappend formats %9.6f
	    lappend headings $par
	    lappend expressions "\[parameter $par =\]"

	}
    } elseif {1 < [llength [trait]]} {
	set ts [trait]
	foreach tr $ts {
	    lappend headings H2r($tr)
	    lappend formats %9.6f
	    lappend expressions "\[parameter H2r($tr) =\]"
	    lappend headings H2q1($tr)
	    lappend formats %9.6f
	    lappend expressions "\[parameter H2q1($tr) =\]"
	}
    } else {
	lappend headings "H2r"
	lappend formats "%9.6f"
	lappend  expressions "\[parameter h2r =\]"
	lappend headings "H2q1"
	lappend formats "%9.6f"
	lappend  expressions "\[parameter h2q1 =\]"
    }
    set resultf [resultfile $open_option [full_filename twopoint.out] \
		     -headings $headings -expressions $expressions \
		     -formats $formats -display]
    if {$appending} {
	puts "[centerline "***  New Twopoint Results  ***" 72]\n"
	resultfile $resultf -header -displayonly
    } else {
	resultfile $resultf -header
    }

    set highest_new_lod -10000
    set highest_new_record ""
    set newrs {}
    global Solar_Fixed_Loci
    set Solar_Fixed_Loci 0
    set h2q_index 1
    for {set i 0} {$i < $do_file_list_len} {incr i} {
	set do_file [lindex $do_file_list $i]

	ifverbplus puts "\n    *** Analyzing new ibd $do_file\n"

        if {!$aparama} {
	    load model [full_filename null0.mod]
	}
	eval linkmod $noparama -2p $do_file
	set highest 0
#
# maximize but catch errors
#  unfortunately, maxtry doesn't catch all errors
#
	if {0 != [catch {set max_status [maxtry 1 $h2q_index $do_file [full_filename temp] 1]}]} {
	    set max_status "Unknown retry error"
	} elseif {$saveall} {
	    save model [full_filename [file tail [file rootname $do_file]]]
	}
	set boundary_error_flag 0
	if {$max_status == "" && ![catch {loglike}]} {
	    set loglik [loglike]
	    set flod [lodn 0]
	    if {$flod > $highest_new_lod} {
		save model [full_filename t]
		file copy -force [full_filename temp.out] [full_filename t.out]
		set highest_new_lod $flod
		set highest 1
	    }
	    set flod [format %.4f $flod]
	} else {
	    puts "error maximizing $do_file: $max_status"
	    if {$max_status != "Convergence Error"} {
		set boundary_error_flag 1
	    }
	    set loglik NaN
	    set flod " "
	}
	file delete [full_filename temp.out]
	set first_char [expr 4 + [string first "ibd." $do_file]]
	set last_char [expr [string last ".gz" $do_file] - 1]
	set name [string range $do_file $first_char $last_char]
	set result [resultfile $resultf -write]
	lappend newrs $result
	set gridresult ""
	if {$gridding} {
	    set gridresult [grid -twopoint $name]
	}
	if {"" != $gridresult} {
	    set result $gridresult
	}
	if {$highest} {
	    set highest_new_record $result
	}
    }
    if {$highest_old_lod > -10000} {
	puts "\n[centerline "Highest Old Result" 72]\n"
        puts $highest_old_record
    }

    if {$highest_new_lod > -10000} {
	puts "\n[centerline "Highest New Result" 72]\n"
        puts $highest_new_record
    }
    puts \
      "\n    *** Results have been written to [full_filename twopoint.out]"

# Load best model

    load model [full_filename null0.mod]
    if {$highest_new_lod > $highest_old_lod} {
	file rename -force [full_filename t.mod] [full_filename two.mod]
	file rename -force [full_filename t.out] [full_filename two.out]
	load model [full_filename two]
	puts "\n    *** Best model saved as [full_filename two.mod] which is now loaded."
    } else {
	if {$append} {
	    if {[file exists [full_filename two.mod]]} {
		load model [full_filename two.mod]
		puts "\n    *** Reloaded best old model [full_filename two.mod]"
	    } else {
		puts \
                    "\n    *** Best model from previous twopoint run not found"
	    }
	} else {
	    puts "\n    *** No satisfactory twopoint model found"
	}	
    }
    return ""
}


# solar::e2squeeze -- private
# 
# Purpose:  Set bounds around e2 based on previous value
#
# Usage:    e2squeeze <fraction>  ; 0.05 might be a good choice
#
# Notes:  e2squeeze is applied by multipoint and twopoint.  The default value
#         0.1, so if e2 were maximized to be 0.3 its bounds are set to 0.2
#         nd 0.4.
#
#         The highest lower bound found by applying e2lower and e2squeeze
#         is applied.  e2squeeze is considered a better tool than e2lower.
#
#         Setting e2squeeze to 1 disables the feature.  0 is invalid.
# -

proc e2squeeze {args} {
    if {$args == {}} {
	if {0 == [llength [info globals Solar_E2_Squeeze]]} {
	    return 0.1
	}
	global Solar_E2_Squeeze
	return $Solar_E2_Squeeze
    }
    global Solar_E2_Squeeze
    ensure_float $args
    if {$args <= 0 || $args > 1} {
	error "Invalid number for e2squeeze, must be >0 and <=1"
    }
    set Solar_E2_Squeeze $args
    return ""
}

# soft_lower_bound:
#   You have to "-push" to set bound below 0.01
#   Then you can only go down to 0.01 if above 0.01
#   or down to 0.005 if above 0.005
#   

proc soft_lower_bound {param args} {
    set push 0
    set newbound [read_arglist $args -push {set push 1}]
    ensure_float $newbound
    set oldbound [parameter $param lower]
    if {$newbound >= 0.01} {
	parameter $param lower $newbound
    } elseif {$push} {
	if {$oldbound <= 0.005} {
	    parameter $param lower $newbound
	} elseif {$oldbound <= 0.01} {
	    parameter $param lower [highest 0.0049 $newbound]
	} else {
	    parameter $param lower 0.01
	}
    }
}

# solar::define --
#
# Purpose:  Define an expression to be used in place of a trait or covariate
#
# Usage:    define <name> = <expression>  ; create a definition
#           trait <name> [,<name>]+       ; use definition as trait(s)
#
#           define                        ; show all defininitions
#           define <name>                 ; show definition for <name>
#           define delete <name>          ; delete define for name
#           define new                    ; delete all expressons
#           define delete_all             ; delete all expressons
#           define rename <name1> <name2> ; rename define name
#           define names                  ; return list of all names
#
#           <name> can be any alphanumeric string with underscore, do not
#           use these reserved words as names:
#
#               delete delete_all rename names new
#
#           <expression> is formatted algebraically using standard
#           math operators + - * / and ^ (power) and () parentheses, and
#           also all math functions defined by the C Programming Language
#           which includes "log" for natural logarithm, trig functions,
#           and hyperbolic functions, among others.  Here is a list:
#           erfc, erf, lgamma, gamma, j1, j0, y1, y0, rint, floor, ceil, 
#           tanh, cosh, sinh, atan, acos, asin, tan, cos, sin, expm1, exp,
#           logb, log1p, log10, log, cbrt, sqrt, and abs.  In addition,
#           the inverse normal transformation (see help for "inormal") may
#           be applied using the "inormal_" prefix (for example,
#           inormal_q4 for trait q4).  "inormal_" may be abbreviated
#           down to "inor_".  
#
#           If a phenotype name within the expression contains special
#           characters (anything other than letters, numbers, and underscore)
#           it should be enclosed in angle brackets <>, and the angle brackets
#           must also include any special operator prefix such as "inorm_".
#           For example, given a trait named q.4 (with a dot), you could
#           have a define command like this:
#
#               define i4 = <inorm_q.4>
#
#           Note: similar rules apply within the constraint and omega commands
#           because those commands also allow expressions that could contain
#           decimal constant terms and math operators.
#
#           A debugging function named "print" is also available which
#           prints and return the value of the expression it encloses.
#           After printing, it pauses until the RETURN key is pressed.
#           RETURN may be held down to pass through a lot of prints.
#           Examples of the print command are given in the documentation
#           for the "omega" command.
#
#           The following relational operators may also be used between
#           any two terms.  If the relation is true, 1 is returned,
#           otherwise 0 is returned.  This enables you to construct
#           compound conditional expressions having the same effect as
#           could have been done with "if" statements.  The C operators
#           < and > have been replaced with << and >> so as not to be
#           confused with the <> quotation of variable names in SOLAR.
#
#           C Format    Fortran Format    Test
#           --------    --------------    ----
#
#           ==          .eq.              if equal
#           !=          .ne.              if not equal
#           >=          .ge.              if greather than or equal
#           <=          .le.              if less than or equal
#           >>          .gt.              if greater than
#           <<          .lt.              if less than
#
#
#           An expression is understood to be quantitative unless the
#           top level operator is a relational operator, in which case
#           it is understood to be discrete.
#
#           Names used must not match the names of any phenotype.  When
#           there is an unintended match, the definition can not be used for
#           trait names since it would be ambiguous.
#
#           Once a valid definition has been created, it can be used in
#           the trait command.  Any or all of the traits can be definitions.
#           All definitions will be saved in the model file, and loaded back
#           in when that model is reloaded.  Definitions in a model file
#           will override current definitions.  It is possible to save a
#           model with nothing but definitions if desired.  The only
#           way to delete definitions is with the "new" "delete" or
#           "delete_all" options, or by restarting SOLAR.  The "model new"
#           command has no effect on definitions.
#
#           Expression names are not case sensitive.  Assigning a new
#           expression to a name replaces the expression previously
#           assigned to that name, even if it differs in letter case.
#           Renaming a definition to a name differing only in letter
#           case is possible.
#
#           For covariates only, it is possible to include in definition a
#           constant called "blank".  If an evaluation of the expression
#           returns blank, that individual is counted as missing from the
#           sample.  The best way to use this constant is with one or
#           more conditionals like this:
#
#           define sample = blank*(age<<22)*(sex==2)
#           covariate sample()
#
#           This blanks any male (sex==2) having age less than 22.
#           blank is the number -1e-20, so any numerical operation may
#           change it to a non-blank small number.  It should only be
#           multiplied by 0 or 1.  The empty parentheses after sample() mean
#           that it is not a maximized parameter, it is a null covariate
#           only used to delimit the sample.
#
# Examples:
#
#           define loga = log(a)
#           define eq1 = (q1 - 3.1)^2
#           define dq4 = q4 .gt. 12
# -

# solar::exclude --
#
# Purpose:  Excude phenotypes from use as covariates by automodel
#           and allcovar commands.
# 
# Usage:    exclude <var> <var> ... ; Add variable(s) to exclude
#           exclude                 ; List all excluded variables
#           exclude -reset          ; Reset to default exclude list
#           exclude -clear          ; Remove all variables from list
#
# Notes: You may add to the exclude list with one or more exclude commands.
#
#        By default, all variables named and/or mapped by the FIELD command
#        will be excluded (except for SEX).  The exclude command lets you
#        exclude additional variables.  (The FIELD command variables are
#        pedigree variables such as ID which would never be wanted as
#        covariates.)
#
#        The default exclude list will include the following standard PEDSYS
#        pedigree mnemonics: 
#
#          seq fseq mseq sseq dseq ego id fa mo sire dam pedno famno twin
#          mztwin ibdid fibdid mibdid blank kid1 psib msib fsib birth exit
#
#        If you are excluding more variables that you are keeping, you might
#        consider simply specifying the covariates you want explicitly
#        rather than using the allcovar or automodel commands, or creating
#        a new phenotypes file with fewer fields.
#
#        The variable name you enter will be converted to lower case.  Solar
#        is intended to handle phenotypic and pedigree variables in a case
#        insensitive manner.
# -

proc exclude {args} {
    global Solar_Exclude_List
    if {$args == {}} {
	if {![if_global_exists Solar_Exclude_List]} {
	    return "seq fseq mseq sseq dseq ego id fa mo sire dam pedno famno twin mztwin ibdid fibdid mibdid blank kid1 psib msib fsib birth exit"
	}
	return "$Solar_Exclude_List"
    }
    if {$args == "-clear"} {
	set Solar_Exclude_List ""
	return ""
    }
    if {$args == "-reset"} {
	if {[if_global_exists Solar_Exclude_List]} {
	    unset Solar_Exclude_List
	}
 	return ""
    }
    set Solar_Exclude_List [exclude]
    foreach arg $args {
	set arg [string tolower $arg]
	if {-1 == [lsearch $Solar_Exclude_List $arg]} {
	    if {[string length $Solar_Exclude_List]} {
		set Solar_Exclude_List "[exclude] $arg"
	    } else {
		set Solar_Exclude_List $arg
	    }
	}
    }
    return ""
}

# solar::perdelta --
#
# Purpose:  Set delta used by perturb
# 
# Usage: perdelta <number>
#
# Notes: Defaults to 0.001
# -

proc perdelta {args} {
    if {$args == {}} {
	if {0 == [llength [info globals Solar_Perturb_Delta]]} {
	    return 0.001
	}
	global Solar_Perturb_Delta
	return "$Solar_Perturb_Delta"
    }
    ensure_float $args
    global Solar_Perturb_Delta
    set Solar_Perturb_Delta $args
    return ""
}


# solar::maximize_quietly -- private
#
# Usage: maximize_quietly <outputfilename>
#
# This is the maximize used by most scripts for convenience
#   Verbosity is minimum unless "plus" verbosity was set by user 
# Convergence failure message is returned, all other errors are raised
# (The boundary check is now done by maximize itself.)
#-

proc maximize_quietly {outfilename {p maximize}} {
    set qu -q
    ifverbplus set qu ""

    if {![catch {eval $p $qu -o $outfilename} omessage]} {
	return ""
    }
    if {-1<[string first "convergence failure" [string tolower $omessage]]} {
	return $omessage
    }
    error $omessage
}

# solar::maximize_goodlod -- private
#
# Usage: maximize_goodlod <outfilename> [<procedure> [<minlike>]]
#
# maximize_goodlod is used by maxtry used by multipoint and twopoint.
# It invokes maximize_quietly to do the maximization.  However, if the
# maximization is successful but the resulting likelihood is significantly
# less than <minlike> (the likelihood of null model), it uses a special retry
# strategy to try to get the likelihood better, usually means to get LOD of 0
# or better.  The retries are ended when the loglikelihood is better than
# <minlod>, so only as many retries as needed are actually done.
#
# The retries are as follows:
#
# 1. simply maximize again, this works in 50% of cases.
# 2. start from null model, using standard starting point for h2q = 0.01
# 3. start from previous locus (or null if applicable) starting h2q at 0 
#     (this almost always fixes negative LOD problems)
# 4. start from null model, starting h2q at 0
#
# The intent  is to attempt if at all possible to avoid starting h2q1 at zero
# because it's possible that will miss actual positive heritability.  However,
# if other options fail, starting at zero almost always works to fix negative
# LOD cases,so that is the last resort, tried in two different ways, if
# necessary.  If all these attempts fail, a negative LOD could still result.
#
# Note: twopoint doesn't use these retries, minlike from there defaults to "".
# twopoint would have a problem with retry 2, since the linkmod would 
# then require the -2p option just for the twopoint cases.
#
# Lack of convergence retries are not generally done here as they are done by
# the caller maxtry.  Convergence error in first try is returned immediately,
# but later retries simply continue.
#
# This is intended to be called only from maxtry.  In particular, retries 2 
# and 4 assume that last_maxtry_model.mod exists in the output directory.
# That is done so that this procedure does not have to save the starting
# model a second time for every single maximize (troubled or not).
#
# The actual retries used can be controlled by a global variable called
# SOLAR_maximize_goodlod.  It is set to a sum of the following codes of
# retry types desired (see above list of retry types):
#
# 1.  Code 1
# 2.  Code 2
# 3.  Code 4
# 4.  Code 8
#
# The default is 15, which includes all types. For example, to enable only
# retry types 3 and 4, you would set the code to 12, like this in a script:
#
# global SOLAR_maximize_goodlod
# set SOLAR_maximize_goodlod 12
#
# From the SOLAR prompt, all variables are global, so the global command would
# not be needed.
# -

proc maximize_goodlod {outfilename minlike h2q_index ibdfile} {
    set errmsg [maximize_quietly $outfilename tmaximize]
    if {$errmsg != ""} {return $errmsg}
    if {$minlike == ""}	 {return ""}
    if {$minlike <= [loglike]} {return ""}
# 
# 1. simply maximize again, this works in 50% of cases.
#
    set trymask [use_global_if_defined SOLAR_maximize_goodlod 15]
    ifdebug puts "trymask is $trymask"
    if {$trymask & 1} {
	ifdebug puts "***** retrying to get non-negative lod"
	save model [full_filename last_maximize_goodlod]
	set lastlike [loglike]
	set errmsg [maximize_quietly $outfilename tmaximize]
	if {$errmsg == ""} {
	    if {$minlike <= [loglike]} {return ""}
	}
    }
#
# 2. start from null model, using standard starting point for h2q = 0.01
#
    
    set fixed_loci [expr $h2q_index - 1]
    if {$trymask & 2} {
	ifdebug puts "***** restarting from null to get non-negative lod"
	load model [full_filename null$fixed_loci]
	linkmod $ibdfile ;# this would need to change for twopoint
	set errmsg [maximize_quietly $outfilename tmaximize]
	if {$errmsg == ""} {
	    if {$minlike <= [loglike]} {return ""}
	}
    }
#
# 3. start from previous locus (or null if applicable) with h2q = 0 (usually works)
#
    if {$trymask & 4} {
	ifdebug puts "***** Trying restart from h2q$h2q_index = 0"
	load model  [full_filename last_maxtry_model]
	set ts [trait]
	set nt [llength $ts]
	set tsuf ""
	foreach tr $ts {
	    if {$nt > 1} {
		set tsuf \($tr\)
	    }
	    parameter h2r$tsuf = [expr [parameter h2r$tsuf =] + [parameter h2q$h2q_index$tsuf =]]
	    if {[parameter h2r$tsuf upper] < [parameter h2r$tsuf =]} {
		parameter h2r$tsuf upper [lowest [expr [parameter h2r$tsuf =] + 0.05] 1]
	    }
	    parameter h2q$h2q_index$tsuf = 0 lower 0
	}
	set errmsg [maximize_quietly $outfilename tmaximize]
	if {$errmsg == ""} {
	    if {$minlike <= [loglike]} {return ""}
	}
    }
#
# 4. start from null model, with h2q = 0
#
    if {$trymask & 8} {
	ifdebug puts "***** restarting from null with h2q$h2q_index = 0"
	load model [full_filename null$fixed_loci]
	linkmod -zerostart $ibdfile ;# this would need to change for twopoint
	set errmsg [maximize_quietly $outfilename tmaximize]
	if {$errmsg == ""} {
	    if {$minlike <= [loglike]} {return ""}
	}
    }
#
# Could not get non-negative LOD
#   Just take first successful result.
#
    load model [full_filename last_maximize_goodlod]
    return ""
}


# solar::maxtry -- private
#
# Implements retry strategy for multipoint and twopoint
#
# Retry strategy for non-convergence:
#   1.  Try same model twice again (maximization uses random numbers)
#   2.  If didn't begin with nullX model, reload nullX model (applying
#         new linkage parameters) and retry
#   3.  Set equal H* parameters and retry
#
# If parameter h2q1 doesn't exist, 2 and 3 are not done.
#
# maxtry itself uses maximize_goodlod, which forces re-maximizations of the
# ongoing model if the new likelihood is significantly smaller than minlike,
# the null model loglikelihood, but otherwise convergence occurred.
# -

proc maxtry {try_switches h2q_index ibdfile outfilename verbose {minlike ""}} {

    if {[solardebug]} {
	puts "Entering maxtry"
	set verbose 1
    }

    model save [full_filename last_maxtry_model]
    set errmsg [maximize_goodlod $outfilename $minlike $h2q_index $ibdfile]
    if {$errmsg == ""} {
	return ""
    }
#
#   1. Just try same model all over again 2x
#   This fixes most convergence errors because random numbers are used
#   and sometimes they just don't work out.  maximization is a
#   non-deterministic process, though results most often seem deterministic
#   with problematic likelihood space, the non-deterministic nature is
#   exposed.
#
    load model [full_filename last_maxtry_model]
    set errmsg [maximize_goodlod $outfilename $minlike $h2q_index $ibdfile]
    if {$errmsg == ""} {
	return ""
    }

    load model [full_filename last_maxtry_model]
    set errmsg [maximize_goodlod $outfilename $minlike $h2q_index $ibdfile]
    if {$errmsg == ""} {
	return ""
    }
#
#   2.  If didn't begin with nullX model, reload nullX model (applying
#
    if {$try_switches & 2} {
	set fixed_loci [expr $h2q_index - 1]
	if {$verbose} {
	    puts \
	"\n    *** Retry with null$fixed_loci model starting parameters"
	}
	model load [full_filename null$fixed_loci]
	linkmod $ibdfile
	set errmsg [maximize_goodlod $outfilename $minlike $h2q_index $ibdfile]
	if {$errmsg == ""} {
	    return ""
	}
    }
#
#   3.  Set equal H* parameters and retry
#       Note: this is only done for univariate models, because multi would be
#       much more complicated, and I don't actually think this does much
#       good anyway, but I was asked to include this strategy long ago.
#
    if {1 == [llength [trait]]} {

	if {![if_parameter_exists h2q1] || ![if_parameter_exists h2r] || \
		![if_parameter_exists e2]} {
	    set try_switches 0
	}

	if {$try_switches & 1} {
	    model load [full_filename last_maxtry_model]
	    set hstart [expr (1.0-double([parameter e2 start]))/ \
			    double ($h2q_index + 1.0)]
	    if {$verbose} {
		puts "\n    *** Retry with equal H* parameters: $hstart"
	    }
	    for {set i 0} {$i <= $h2q_index} {incr i} {
		if {$i == 0} {
		    set hname h2r
		} else {
		    set hname h2q$i
		}
		parameter $hname start $hstart
		set hupper [parameter $hname upper]
		set hlower [parameter $hname lower]
		if {$hstart >= $hupper} {
		    parameter $hname upper [expr $hstart + 0.001]
		}
		if {$hstart <= $hlower} {
		    parameter $hname lower [expr $hstart - 0.001]
		}
	    }
	}
	set errmsg [maximize_goodlod $outfilename $minlike $h2q_index $ibdfile]
	if {$errmsg == ""} {
	    return ""
	}
    }

#   pre640_maxtry_sections was included here

# If we get here, all attempts to converge have failed

    if {$verbose} {
	puts \
	    "\n    *** Failure to converge after programmed retries"
    }
    return $errmsg
}

# The following used to be included in maxtry but has been long
# obsolete

proc pre640_maxtry_sections {} {

# If this is a bivariate linkage model, try constraining rhoq1
# This section is now obsoleted.

    if {0} {
    if {$try_switches != 0 && $multi && [if_parameter_exists rhoq1]} {
	if {$verbose} {
	    puts "    *** Constraining RhoQ1 to 1"
	}
	load model [full_filename last_maxtry_model]
	constraint rhoq1 = 1
	set llrhoq11 -1e20
	set errmsg1 [maximize_quietly $outfilename tmaximize]
	if {"" == $errmsg} {
	    set llrhoq11 [loglike]
	    save model [full_filename maxtry_rhoq1]
	    if {$verbose} {
		puts "    *** Model converged"
	    }
	}
	if {$verbose} {
	    puts "    *** Constraining RhoQ1 to -1"
	}
	load model [full_filename last_maxtry_model]
	constraint rhoq1 = -1
	set llrhoq1_1 -1e20
	set errmsg2 [maximize_quietly $outfilename tmaximize]
	if {"" == $errmsg2} {
	    set llrhoq1_1 [loglike]
	    if {$verbose} {
		puts "    *** Model converged"
	    }
	}
	if {$llrhoq11 != -1e20 || $llrhoq1_1 != -1e20} {
	    if {$llrhoq11 > $llrhoq1_1} {
		load model [full_filename maxtry_rhoq1]
	    }
	    return "ConsRhoq"
	}
    }
    }

# OK, as a last resort, try modifiying conv
# This section is now obsoleted.

    if {0} {
    if {$verbose} {
	puts "    *** Reducing CONV to 1e-5"
    }
    load model [full_filename last_maxtry_model]
    option conv 1e-5
    set errmsg [maximize_quietly $outfilename tmaximize]
    option conv 1e-6
    if {"" == $errmsg} {
	puts "    *** Warning.  Parameter accuracy reduced.  See 'help accuracy'."
	return ""
    }

    if {$verbose} {
	puts "    *** Reducing CONV to 1e-4"
    }
    load model [full_filename last_maxtry_model]
    option conv 1e-4
    set errmsg [maximize_quietly $outfilename tmaximize]
    option conv 1e-6
    if {"" == $errmsg} {
	puts "    *** Warning.  Parameter accuracy reduced..  See 'help accuracy'."
	return ""
    }
    }
}


# fels_maxtry is now obsolete and removed from the maxtry calling sequence
# prior to version 6.4.0, this procedure used to be called "maxtry" and the
# now again "maxtry" was "omaxtry"
#
proc fels_maxtry {try_switches h2q_index ibdfile outfilename verbose {minlike ""}} {
    set status [maxtry $try_switches $h2q_index $ibdfile $outfilename \
		    $verbose $minlike]
#
# The following code is switched off, related to "fels problem"
#   to turn on again, change test below to ""
#
    if {"zero_lod_expected" == $status} {

# Check for fels problem (zero lod expected)

	set hi [h2qcount]
	set nu [expr $hi - 1]
	if {$hi >= 1} {
	    if {[if_parameter_exists h2q$hi]} {
		if {0.0==[parameter h2q$hi =]} {
		    if {[file exists [full_filename null$nu.mod]]} {
			if {abs([oldmodel null$nu loglike] - [loglike]) \
				> 0.05} {
			    if {[if_global_exists SOLAR_FELS]} {
				set oldmatrix [matrix]
				save model [full_filename zero_lod_expected]
				load model [full_filename null$nu]
				parameter h2q$hi = 0  upper 0.2
				carve_new_value h2q$hi 0.01
				set cstring "constraint e2 + h2r"
				for {set i 1} {$i <= $hi} {incr i} {
				    set cstring "$cstring + h2q$i"
				}
				set cstring "$cstring = 1"
				eval $cstring
				eval $oldmatrix
				set status [maxtry $try_switches $h2q_index $ibdfile $outfilename $verbose]
				if {"" != $status} {
				   load model [full_filename zero_lod_expected]
				    set status "ZeroLodX"
				} else {
				    if {$hi >= 1} {
					if {[if_parameter_exists h2q$hi]} {
					    if {0.0==[parameter h2q$hi =]} {
						if {[file exists [full_filename null$nu.mod]]} {
						    if {abs([oldmodel null$nu loglike] - [loglike]) \
							    > 0.05} {
							set status "ZeroLodX"
						    }
						}
					    }
					}
				    }
				}
			    } else {
				set status "ZeroLodX"
			    }
			}
		    }
		}
	    }
	}
    }
    return $status
}

# Now, why is h2qcount based on omega and not parameters ?

proc h2qcount {} {
    set count 0
    set omega_equation [omega]
    set suffix [lindex [trait] 1]
    if {{} != $suffix} {set suffix ($suffix)}

    for {set i 1} {1} {incr i} {
	if {-1==[string first h2q$i $omega_equation] && \
		-1==[string first H2q$i $omega_equation] && \
		-1==[string first h2Q$i $omega_equation] && \
		-1==[string first H2Q$i $omega_equation]} break

# Make sure this is an actual parameter	
	
	if {[if_parameter_exists h2q$i$suffix]} {
	    incr count
	}
    }
    return $count
}

proc read_bayes {modelname outfilename what} {
    if {![file exists [full_filename $outfilename]]} {
	error "Output file $outfilename does not exist"
    }
    set soutfile [open [full_filename $outfilename]]
    gets $soutfile  ;# skip over header
    gets $soutfile  ;# skip over ------
    while {-1 != [gets $soutfile line]} {
	set mname [lindex $line 0]
	if {$modelname == $mname} {
	    close $soutfile
	    if {"bic" == $what} {
		return [lindex $line 1]
	    } elseif {"loglike" == $what} {
		return [lindex $line 2]
	    } else {
		error "read_bayes does not read $what"
	    }
	}
    }
    close $soutfile
    error "Record for model $modelname not found"
}
	

proc get_prev_lod_and_h2q {h2q_index chrom loc} {
    if {![file exists [full_filename multipoint$h2q_index.out]]} {
	error "Output file multipoint$h2q_index.out does not exist"
    }
    set soutfile [open [full_filename multipoint$h2q_index.out]]
    while {-1 != [gets $soutfile line]} {
	if {5 != [scan $line "%s %s %s %d %f" cid chromnum lid locnum olod]} {
	    continue
	}
	if {[string compare $cid "chrom"] || \
		[string compare $lid "loc"]} {
	    continue
	}
	if {![string compare "0" [string index $chromnum 0]]} {
	    set chromnum [string range $chromnum 1 end]
	}
	if {$chromnum != $chrom || $locnum != $loc} {
	    continue
	}
	set h2q [lindex $line end]
	if {![is_float $h2q]} {
	    close $soutfile
	    error "Error in record for chrom $chrom loc $loc: $h2q"
	}
	close $soutfile
	return [list $olod $h2q]
    }
    close $soutfile
    error "Record for chrom $chrom loc $loc not found"
    return ""
}

# solar::fatal_error_checks -- private
#
# Does fatal_error_checks.  Returns error that will stop solar
# Called during solar startup before check_os.
# -

proc fatal_error_checks {} {
    catch {ranch_check}
    if {[llength [info procs ranch_check]]} {
	set result [ranch_check]
	if {"" != $result} {
	    error $result
	}
    }
    set result [check_version_compatibility]
    if {"" != $result} {
	error $result
    }
    return ""
}

proc solarversion {} {
    return "SOLAR Eclipse version [solar_tcl_version], last updated on [solar_up_date], [solar_up_year]\nSOLAR main binary was compiled on [solar_compiled_date] at [solar_compiled_time]"
}

proc solar_tcl_startup {} {
 global Solar_Batch
 if {!$Solar_Batch} {
     catch {
     puts "\nSOLAR Eclipse version [solar_tcl_version], last updated on [solar_up_date], [solar_up_year]"
     puts "Copyright (c) 1995-[solar_up_year] Texas Biomedical Research Institute"
     }
  puts "Enter help for help, exit to exit, doc to browse documentation.\n"
  global Solar_Gotcl
  if {[llength $Solar_Gotcl]} {
      puts "Using solar.tcl in $Solar_Gotcl"
  }
 }
}

proc check_version_compatibility {} {
    if {![string compare 8.1.1 \
	      [solar_binary_version]]} {
	return ""
    } else {
	puts " "
	puts stderr "solar.tcl is version [solar_tcl_version]"
	puts stderr "solarmain binary is version [solar_binary_version]"
	global Solar_Gotcl
	if {[llength $Solar_Gotcl]} {
	    set errmsg "\nMismatched version of solar.tcl in $Solar_Gotcl"
	} else {
	    global env
	    set errmsg "\nMismatched version of solar.tcl in $env(SOLAR_LIB)"
	}
    }
    error $errmsg
}
    


# solar::no_check_os -- private
#
# Purpose:  Turn off OS and library advisory checking
#
# Usage:    no_check_os
#
# Notes:   When starting, solar checks for bugs which may result when the
#          version of the OS you are running is lower than the one used
#          when solar was built.  (Forward compatibility is usually
#          available, but backward compatibility is not.)  One of these
#          relates to the proper handling of IEEE NaNs (not a number).
#          Normally, NaN's arise only when convergence fails.
#
#          If the test fails, you are given a warning but must press
#          return to continue.
#
#          If you would like to turn off the checking (at your own
#          risk), you may give the no_check_os command in your .solar
#          startup script.  This will disable OS testing and will enable
#          you to run shell scripts which call solar even with a release
#          that is too early.
# -

proc no_check_os {} {
    eval {proc check_os {} {return "OS checking has been turned off"} }
    return ""
}

proc check_os {} {}

proc check_nan {} {
    set foo NaN
    if {[catch {format %9.6f NaN}] || \
	    $foo > 1} {
	puts \
	"NaN bug: Solar was linked under a later OS than you are running"
	puts -nonewline \
  "This might cause problems.  Press return to continue> "
	flush stdout
	gets stdin junk
	if {$junk == "q"} {error "Script terminated at user request"}
	return ""
    }
    return "NaN handling OK"
}

# solar::usort --
#
# Purpose:  Define unix sort program name 
#           (used for multipoint*.out files)
#
# Usage: usort <program>         ; use program
#        usort ""                ; disables sort feature}
#        usort                   ; show current program
#
# Notes: The default is /usr/bin/sort, which should work on most system.
#        It is necessary to include a path for users which have PEDSYS,
#        which has its own program named "sort."  The program must be
#        compatible with unix sort and have -n -o and -k arguments.
#
# Example:  usort /usr/local/bin/sort
#-

proc usort {args} {
    global Solar_Unix_Sort
    if {$args == {}} {
	if {[if_global_exists Solar_Unix_Sort]} {
	    return $Solar_Unix_Sort
	} else {
	    if {[file exists /bin/sort]} {
		return /bin/sort
	    } elseif {[file exists /usr/bin/sort]} {
		return /usr/bin/sort
	    } else {
	    error "Unix sort not found.  Use usort command to set full path."
	    }
	}
    }
    set Solar_Unix_Sort $args
}


proc topline {filename} {
    if {[catch {set filep [open $filename r]}]} {
	return ""
    }
    set count [gets $filep line]
    close $filep
    if {$count > 0} {
	return $line
    }
    return ""
}


# solar::newtcl --
#
# Purpose:  Recognize new or changed Tcl procedures in Tcl scripts
#
# Usage:    newtcl
#
# Notes:    At the time a SOLAR session is started, all Tcl scripts
#           (files ending with ".tcl") are scanned.  The newtcl
#           command forces another such scan in order to recognize
#           new Tcl procedures (created AFTER the start of the SOLAR
#           session), or to recognize changes to Tcl procedures since
#           the first time those procedures were used (see explanation
#           below).  You could also accomplish this by exiting from
#           and restarting SOLAR, but that is often inconvenient
#           because it causes the loss of session state.
#
#           The following directories are scanned by SOLAR for user scripts:
#
#                .      (the current working directory)
#                ~/lib  (the lib subdirectory of your home directory, if it exists)
#
#           A procedure found in "." will supercede one found in "~/lib" having
#           the same name.  Also beware that if the same procedure name is used
#           in more than one script file, the first one encountered will be
#           the one actually used.  If the same procedure name is found in two
#           files in the same directory, the precedence is not predictable.
#
#           The scanning process simply looks through each script file for
#           "proc" (procedure) statements.  An index of all the procedures
#           is then written to a file named tclIndex in the working directory.
#           This file will only be created if user-defined Tcl scripts are found.
#
#           Tcl procedures are only loaded into SOLAR the first time they
#           used.  Once loaded, they stay loaded, and may no longer reflect
#           the Tcl files in the scan path if those Tcl files are changed.
#           The newtcl command flushes all currently loaded procedures, so
#           the next time any procedure is invoked, it will be reloaded from
#           the file.
#
#           The main Tcl file used by SOLAR is named solar.tcl and is located in
#           the lib subdirectory of the SOLAR installation.  This defines all
#           the fundamental procedures used by SOLAR.  User-defined procedures
#           having the same name as built-in procedures will supercede them.
# -

# newtcl is actually created by the binary solarmain since it must exist in order
# for solar.tcl itself to be scanned.



# solar::needk2 --
#
# Purpose:  Keep K2 (phi2) terms from MIBD matrices
#
# Usage:    needk2
#           needk2 off
#
# Notes:    This command is now obsolescent and should not be used.
#
#           The K2 in MIBD files is obsolescent.  We now maintain
#           a separate phi2.gz file for discrete trait analyses, and
#           for quantitative trait analyses, the K2 (phi2) values are
#           computed as needed.
#
# Old Notes:
#
#           If you need to use any of the K2_* matrix values, issue the needk2
#           command before loading the matrix (or running 'multipoint.')
#
#           Normally the K2 values from matrix files are not used because
#           they are identical to the K2 values computed by SOLAR as needed.
#
#           The default (of not saving K2) cuts matrix memory usage in half.
# -

proc needk2 {args} {
    global Solar_Save_K2
    if {$args == "off"} {
	set Solar_Save_K2 0
    } elseif {$args != ""} {
	error "Invalid needk2 argument"
    } else {
	set Solar_Save_K2 1
    }
    return ""
}

proc ifneedk2 {} {
    global Solar_Save_K2
    if {[if_global_exists Solar_Save_K2]} {
	return $Solar_Save_K2
    } else {
	return 0
    }
}

# solar::memory
#
# Purpose:  Show total memory used by this SOLAR process
#
# Usage:    memory
#
# Notes:    This is intended primarily for internal debugging purposes.
#           Now works on all supported systems.
# -



proc procmem {} {return [memory]}

proc memory {} {
    set memstring 0
    set osname [string tolower [exec uname]]
    if {$osname == "sunos"} {
	catch {set memstring [eval exec du -ks /proc/[pid]/as]}
	if {[llength $memstring] != 2  || $memstring == 0} {
	    set memstring ""
	}
    } else {
	if {$osname == "linux"} {
	    catch {set memstring [eval exec ps -H v [pid]]}
	} else {
	    catch {set memstring [eval exec ps -l [pid]]}
	}
	set first [lsearch $memstring PID]
	if {$first == -1} {
	    puts $memstring
	    error "unable to get memory PID"
	}
	set rsspos [lsearch $memstring RSS]
	if {$rsspos == -1} {
	    puts $memstring
	    error "unable to get memory RSS"
	}
	set lfpos [lsearch $memstring [pid]]
	if {$lfpos == -1} {
	    return $memstring
	    error "unable to get memory data"
	}
	set pos [expr $lfpos + ($rsspos - $first)]
	set memstring [lindex $memstring $pos]
    }
    if {$memstring != ""} {
	    return "[lindex $memstring 0] K Bytes in use by SOLAR"
    }
    return "0 (unable to get mem size)"
}


# solar::benice --
#
# Purpose:  Lower priority of SOLAR to allow more CPU for other jobs
#           or lower priority of one SOLAR run relative to another
#
# Usage:    benice              ; Set "nice" level to 15
#           benice <LEVEL>      ; LEVEL is between 1 and 20
#                               ; 20 is "most nice"
#
# Notes:    This is intended for use on Unix systems which support the
#           "renice" command, including Solaris 2.5 and above
#
#           Once you have set a nice level, you cannot go back to a 
#           higher priority on this process.  You must exit and restart.
#
#           The default unix scheduling allows some time even for
#           very "nice" jobs.  However, they get somewhat less CPU than
#           other jobs.
#
#           On the SFBR Ranch, scheduling is absolute, so that "nice"
#           jobs will be suspended until all other jobs are done (or
#           waiting for a system resource such as disk access).  Nice
#           jobs have minimal (<1%) impact on other jobs, unless they
#           hog huge gobs of memory.
# -

proc benice {args} {
    if {$args == {}} {set args 15}
    eval exec renice $args -p [pid]
}

# solar::qtnm --
# 
# Purpose:  Marginal tests for bayesavg -qtn
#
# Usage:    [allsnp]
#           bayesavg -qtn -max 1
#           [load map snp.map]
#           qtnm [-noplot] [-nomap]
#
#           -noplot  Do not plot results
#           -nomap   Do not use map file; SNP locations are encoded in names
#
# Notes:    You must do bayesavg -qtn [-max 1] first, then qtnm.  qtnm
#           writes a datafile qtnm.out in the outdir, then invokes
#           plotqtn to plot it.  (The -max 1 is optional, however,
#           if you want to do this quickly, you had best include it.)
#
#           To include all snps as covariates in the starting model, use
#           the separate command "allsnp".
#
#           SNP covariate names (after the snp_ or hap_ prefix) will be
#           mapped to locations using the currently loaded map file,
#           which must be loaded prior to running qtnm.  Map files stay
#           loaded from one solar session to the next (in the same working
#           directory) so once you have loaded it, you do not need to load
#           it again.
#
#           Beginning with version 3.0.3, snp names will always be mapped
#           to locations using a loaded map file.  However, you can revert
#           to the previous method, in which the locations are encoded into
#           the snp "names" using the -nomap option.
#
#           Beginning with SOLAR version 3.0.2, the qtnm.out file
#           has the following 5 columns:
#
#           SNP Name (or location if numeric)
#           SNP location
#           Chi Squared
#           p
#           log(p)
#
#          Previously there was no "SNP Name" column because it was
#          assumed to be the location.  Note that plotqtn accepts
#          qtnm.out files with either 4 or 5 columns.
# -

proc qtnm {args} {
    set plot 1
    set nomap 0

    set badargs [read_arglist $args -noplot {set plot 0} -nomap {set nomap 1}]
    if {{} != $badargs} {
	error "qtnm: Bad arguments: $badargs"
    }
#
# Get list of snp covariates from starting model
#
    save model [full_filename bayesavg_ma.orig]
    load model [full_filename cov.orig.mod]
    set covariates [covariates -applicable]

    set poslist {}
    set namelist {}
    set n 0
    foreach cov $covariates {
	set target [string tolower [string range $cov 0 3]]
	if {"snp_" == $target || "hap_" == $target} {
	    set number [set name [string range $cov 4 end]]
	    if {!$nomap} {
		set number [map locn $name]

	    } elseif {![is_float $number]} {
		error "qtnm: Using -nomap and $cov is invalid"
	    }
	    lappend poslist $number
	    lappend namelist $name
	    incr n
	}
    }
    load model [full_filename bayesavg_ma.orig]
    puts "There appear to be $n snps..."
#
# Make list of model names we want
#
    set wanted {}
    for {set i 0} {$i <= $n} {incr i} {
	lappend wanted cov$i
    }
#
# Scan through output file picking up all data we want
#
    set found 0
    if {[catch {set ifile [open [full_filename bayesavg_cov.out]]}]} {
	set ifile [open [full_filename bayesavg_cov.est]]
    }
    gets $ifile
    gets $ifile
    while {$n+1 > $found} {
	if {-1 == [gets $ifile line]} {
	    close $ifile
	    error "Incomplete results: only $found out of $n+1"
	}
	set modelname [lindex $line 0]
	if {-1 != [lsearch $wanted $modelname]} {
	    set loglikes($modelname) [lindex $line 2]
	    incr found
	}
    }
    close $ifile
#
# Capture output to lists and variables also
#
    set all_points {}
    set marker_labels {}
    set outfile [open [full_filename qtnm.out] w]
    set log0 $loglikes(cov0)
    for {set i 1} {$i <= $n} {incr i} {
	set modname cov$i
	set logn $loglikes($modname)
	set chisq [expr 2*($logn - $log0)]
	set p [chi -number $chisq 1]
	set name [lindex $namelist [expr $i - 1]]
	set lp [expr log10($p)]
	set pos [lindex $poslist [expr $i - 1]]
#	puts $outfile [format "%8d  %1g %10.7g %9.6g" $name $chisq $p $lp]
	puts $outfile \
	    [format "%9s %8d  %1g %10.7g %9.6g" $name $pos $chisq $p $lp]
    }
    close $outfile
#
# Do plotting if desired
#
    if {$plot} {
	plotqtn
    }
    return ""
}

# solar::plotqtn
#
# Purpose:  Plot qtn marginal tests (qtnm.out)
#
# Usage:    plotqtn [-nolabels] [-nomarkers] [-file filename] [-local]
#
#           -nolabels     do not include "marker" labels (ticks only)
#           -nomarkers    do not include marker ticks or labels
#           -file         Use named file instead of qtnm.out in outdir
#           -local        Ignore default plot parameters; use only local file
#
# Notes:    You must select the trait or outdir first.  See qtnm for
#           more information.  It must be possible to find the qtnm.out
#           file in the outdir.
#
#           The plot parameter file (in SOLAR_LIB) is qtn.gr.  You
#           may override it with a copy in ~/lib or your working
#           directory.  Your version need only include the parameters
#           you would like to change.  This should work in most cases.
#           If you specify -local, however, the qtn.gr in SOLAR_LIB
#           is completely ignored, and your qtn.gr must be complete,
#           which might get around some very obscure conflict between
#           the two plot parameter files.
#
#           plotqtn accepts either the original 4 or the new 5 column qtnm.out
#           files.  The 5 column files begin with the snp name that is not
#           necessarily the location.
# -

proc plotqtn {args} {


# Important constants
    set x_expansion_factor 1.12  ;# Expansion of X beyond input range

    set debug [solardebug]

    set marker_margin 4.5
    set nolabels 0
    set noticks 0
    set filename ""
    set local_gr 0

    set badargs [read_arglist $args \
		     -nolabels {set nolabels 1} \
		     -nomarkers {set nolabels 1; set noticks 1} \
		     -marker_margin marker_margin \
		     -file filename \
		     -local {set local_gr 1} \
		]
    if {"" != $badargs} {
	error "plotqtn: Invalid arguments: $badargs"
    }
    if {"" == $filename} {
	set filename [full_filename qtnm.out]
    }

    if {$local_gr && $debug} {puts "Warning!  Ignoring qtn.gr IN SOLAR_BIN"}


# Read qtnm.out

    set marker_labels {}
    set all_points {}
    set max_mrk_name_len 0
    set first 1
    set qfile [open $filename]
    set lines 0
    while {-1 != [gets $qfile line]} {
	if {0 == [llength $line]} {
	    continue
	}
	incr lines

# Determine if this is old or new style output file

	if {[llength $line] == 4} {
	    set pos [lindex $line 0]
	    set name $pos
	    set lp [expr 0.0 - [lindex $line 3]]
	} elseif {[llength $line] == 5} {
	    set name [lindex $line 0]
	    set pos [lindex $line 1]
	    set lp [expr 0.0 - [lindex $line 4]]
	} else {
	    error "qtnmplot: Incorrect record length in qtnm.out: $line"
	}
	if {$first} {
	    set min_pos $pos
	    set max_pos $pos
	    set min_lp $lp
	    set max_lp $lp
	    set first 0
	}
	lappend all_points [list $pos $lp]
	lappend marker_labels [list $pos $name $pos]
	if {[string length $name] > $max_mrk_name_len} {
	    set max_mrk_name_len [string length $pos]
	}
	if {$pos < $min_pos} {
	    set min_pos $pos
	}
	if {$pos > $max_pos} {
	    set max_pos $pos
	}
	if {$lp < $min_lp} {
	    set min_lp $lp
	}
	if {$lp > $max_lp} {
	    set max_lp $lp
	}
    }
    close $qfile
    if {$lines < 2} {
	error "File $filename contains only $lines lines"
    }

    ifdebug puts "min_pos $min_pos max_pos $max_pos"
    ifdebug puts "min_lp $min_lp max_lp $max_lp"

# Sort points if required (yes)

    proc sort_by_first_num {a b} {
	set a0 [lindex $a 0]
	set b0 [lindex $b 0]
	if {$b0 > $a0} {
	    return -1
	} elseif {$b0 < $a0} {
	    return 1
	}
	return 0
    }

    set all_points [lsort -command sort_by_first_num $all_points]


# Sort marker ticks and labels too

    set marker_ticks $marker_labels
    set marker_ticks [lsort $marker_ticks]
    set marker_labels [lsort -command sort_by_first_num $marker_labels]

# Begin plotting...

    set setnum 1
    set graphnum 1

# Open new or existing tclgr session

    if {[catch {tclgr open} errmsg]} {
	if {[string compare $errmsg  "tclgr session already opened from this solar session"]} {
	    error $errmsg
	}
    }

# Kill previous set, and graph if not overlaying

    tclgr send kill graphs
    tclgr send clear line
    tclgr send clear string
    tclgr send focus g$graphnum

# Calculate scaling and ticks for x

    set max_x_unscaled $max_pos
    set min_x_unscaled $min_pos
    set delta_x_unscaled [expr $max_x_unscaled - $min_x_unscaled]
    set center_x [expr ($min_pos + $max_pos) / 2]
    ifdebug puts "delta_x_unscaled: $delta_x_unscaled  center_x:  $center_x"

    set mmlist [majorminor $delta_x_unscaled $min_x_unscaled $max_x_unscaled]
    set major_x [lindex $mmlist 0]
    set minor_x [lindex $mmlist 1]
    ifdebug puts "major_x: $major_x  minor_x: $minor_x"

    set delta_x [expr $delta_x_unscaled * $x_expansion_factor]
    set max_x [expr $center_x + ($delta_x / 2.0)]
    set min_x [expr $center_x - ($delta_x / 2.0)]
    set use_x [expr ($min_x + $min_x_unscaled) / 2.0]
    set firstlabel_x [expr $major_x * ceil ($use_x / $major_x)]
    ifdebug puts "xmin: $min_x   xmax: $max_x   firstlabel: $firstlabel_x"

# Setup scaling and ticks for x

    tclgr send world xmin $min_x
    tclgr send world xmax $max_x
    tclgr send xaxis tick op bottom
    tclgr send xaxis tick major $major_x
    tclgr send xaxis tick minor $minor_x

    tclgr send xaxis ticklabel op bottom
    tclgr send xaxis ticklabel start type spec
    tclgr send xaxis ticklabel start $firstlabel_x

    if {abs($min_x) <= 2.0e9 && abs($max_x) <= 2.0e9} {
	tclgr send xaxis ticklabel format decimal
	tclgr send xaxis ticklabel prec 0
    }

# Set up y tick marks (major and minor)

#    ************* Set up Y Axis *****************


# This fiendishly complicated method to set up y axis is borrowed from
# proc plot.  It sets up nice major and minor ticks consistent with xmgr
# regardless of size, following a 1/2/5 rule.

# The design maximum Log10(P) is 2000
#   (highest tick range is therefore 750-1500)
#   (I don't like limits, but this is way beyond anything I can imagine)
# maxy must be 4*10^N or 1*10^N
# the other values are related

    set maxy  1000
    set major 200
    set major_digit 2
    set minor 100

    set maxdiv 2.5
    set majdiv 2
    set mindiv 2

# Since we mess with global variable tcl_precision
# We must catch errors to be sure it gets restored

    global tcl_precision
    set save_tcl_precision $tcl_precision
    set tcl_precision 6
    set not_ok [catch {
    while {1} {
	if {$max_lp > $maxy} {
	    ifdebug puts "Y Axis Major: $major Minor: $minor"
	    tclgr send yaxis tick major $major
	    tclgr send yaxis tick minor $minor
	    break
	}
	set maxy [expr double($maxy) / $maxdiv]
	set major [expr double($major) / $majdiv]
	set minor [expr double($minor) / $mindiv]

	if {$major_digit == 2} {
	    set major_digit 1
	    set maxdiv 2
	    set majdiv 2
	    set mindiv 2
	} elseif {$major_digit == 1}  {
	    set major_digit 5
	    set maxdiv 2
	    set majdiv 2.5
	    set mindiv 2.5
	} else {
	    set major_digit 2
	    set maxdiv 2.5
	    set majdiv 2
	    set mindiv 2
	}
    }
    } caught_error]
    set tcl_precision $save_tcl_precision
    if {$not_ok} {
	error $caught_error
    }

# Create y expansion factor to permit showing markers labels at top

    if {$nolabels || $noticks} {
	set max_mrk_name_len 0
    }
    set marker_margin 2.0
    set chars_that_fit_vertically 55.0  ;# using default font size (was 58)
    set prop_screen_for_markers [expr ($max_mrk_name_len + $marker_margin)  / $chars_that_fit_vertically]
    set prop_screen_for_mticks 0.04
    set prop_screen_for_mleaders 0.04
    set prop_screen_for_ttext 0.015
    set prop_screen_for_plot [expr 1.0 - ($prop_screen_for_mticks +  $prop_screen_for_mleaders +  $prop_screen_for_ttext  +  $prop_screen_for_markers )]
    set expand_y [expr 1.0 / $prop_screen_for_plot]
    ifdebug puts "expand_y is $expand_y"

# Set up y axis and ticks

    set max_y_unscaled $max_lp
    set min_y_unscaled $min_lp

    set max_y_ticklabel [expr $major*ceil(double($max_lp)/$major)]
    ifdebug puts "max_y_ticklabel: $max_y_ticklabel"
    set max_y [expr $max_y_ticklabel * $expand_y]
    set min_y [expr -0.06 * $max_y]
    tclgr send world ymin $min_y
    tclgr send world ymax $max_y
    tclgr send yaxis ticklabel start type spec
    tclgr send yaxis ticklabel stop type spec
    tclgr send yaxis ticklabel start 0
    tclgr send yaxis ticklabel stop $max_y_ticklabel
#
# xmgr spaces Y axis label too close to single digits
#
    if {$max_y_ticklabel > 4 && $max_y_ticklabel < 10} {
	tclgr send yaxis label place spec
	tclgr send yaxis label place 0.043,0
    }
#
# Get user overrides from ma.gr
#
    global env
    if {!$local_gr} {
	if {[file exists $env(SOLAR_LIB)/qtn.gr]} {
	    set mpathname [glob $env(SOLAR_LIB)/qtn.gr]
	    tclgr send read \"$mpathname\"
	}
	if {[file exists ~/lib/qtn.gr]} {
	    set mpathname [glob ~/lib/qtn.gr]
	    tclgr send read \"$mpathname\"
	}
    }
    if {[file exists qtn.gr]} {
	tclgr send read \"qtn.gr\"
    } 
#
#           ***   PLOT POINTS   ***
#
    foreach point $all_points {
	set x [lindex $point 0]
	set y [lindex $point 1]

	tclgr send g$graphnum.s$setnum point $x,$y
	ifdebug ifverbmax puts "graphing g$graphnum.s$setnum point $x,$y"
    }
#
#           ***     PLOT MARKERS    ***
#
    set max_markers 42
    set max_ticks 100
    set max_ticks_modified_xmgr 1000
    if {!$nolabels || !$noticks} {

# Determine if modified xmgr is available, allows 1000 ticks

	set modified_xmgr 0
	set u_name [string tolower [exec uname]]
	if {![string compare $u_name sunos] ||  ![string compare $u_name osf1] ||  [string match *linux* $u_name] ||  [string match *irix* $u_name]} {
	    set modified_xmgr 1
	    set max_ticks $max_ticks_modified_xmgr
	}
#
# See if we can handle this number of markers in any way
#
	set num_markers [llength $marker_labels]
	ifdebug puts "There appear to be $num_markers markers"
	if {$num_markers> $max_ticks} {
	    if {!$modifed_xmgr} {
		ifdebug puts "Modified version of XMGR not available on this system."
	    }
	    set nolabels 1
	    set noticks 1
	    ifdebug puts "Too many many marker ticks to display..."
        }
    }
    if {!$nolabels || !$noticks} {
#
# Remove marker labels down to maximum we can handle
#
	if {!$nolabels && $num_markers > $max_markers} {
	    ifdebug puts "Removing marker labels down to $max_markers"
	    while {$num_markers > $max_markers} {
		set marker [lindex $marker_labels 0]
		set lowest_value [lindex $marker 0]
		set lowest_index 0
		for {set i 0} {$i < $num_markers} {incr i} {
		    set marker [lindex $marker_labels $i]
		    set value [lindex $marker 0]
		    if {$value < $lowest_value} {
			set lowest_value $value
			set lowest_index $i
		    }
		}
		set marker_labels [lreplace $marker_labels  $lowest_index $lowest_index]
		set num_markers [expr $num_markers - 1]
	    }
	}
	if {!$nolabels} {
#
# Move remaining marker labels around to fit
# This should only take 1.5 passes with current algorithm
#   (old algorithm repeated until no more moves)

	    set total_x $delta_x_unscaled
	    set starting_x $min_x_unscaled
	    set min_distance [format %.9f [expr 0.0241 * $total_x]]
	    set markers_moved_left 0
	    set markers_moved_right 0
	    set pass_count 0
	    set max_pass_count 1.5
	    while {$pass_count<1 ||  $markers_moved_left>0|| $markers_moved_right>0} {
		incr pass_count
		set markers_moved_left 0
		set markers_moved_right 0

# Scan left to right, moving "next_marker" right if necessary

		for {set i -1} {$i < $num_markers - 1} {incr i} {
		    if {$i == -1} {
			set nloc [format %.10f [expr $starting_x - $min_distance]]
		    } else {
			set nloc [lindex [lindex $marker_labels $i] 0]
		    }
		    set next_marker [lindex $marker_labels [expr $i + 1]]
		    set next_nloc [lindex $next_marker 0]
		    set distance [format %.10f [expr $next_nloc - $nloc]]
		    if {$distance < $min_distance} {

# Move next marker by 1/2 distance required if this is the first pass
#  (makes offsets more symmetrical)
# Move next marker by entire distance required if this is subsequent pass
# Must maintain rounding to 0.001 precision

			set move_distance [expr $min_distance - $distance]
			if {$pass_count == 1} {
			    set move_distance [expr $move_distance / 2]
			}
			set move_distance [format %.10f $move_distance]
			set new_position [expr $next_nloc + $move_distance]
			set new_position [format %.10f $new_position]
			set j [expr $i + 1]
			set next_marker  [lreplace $next_marker 0 0 $new_position]
			set marker_labels  [lreplace $marker_labels $j $j $next_marker]
			incr markers_moved_right
		    }
		}
#
# This has been proven to take no more than 1.5 passes
#
		if {$pass_count > $max_pass_count} {
		    break
		}
#
# Scan right to left, moving next_marker right if necessary
#
		for {set i $num_markers} {$i > 0} {set i [expr $i - 1]} {
		    if {$i == $num_markers} {
			set nloc [format %.10f [expr $max_x_unscaled +  $min_distance]]
		    } else {
			set nloc [lindex [lindex $marker_labels $i] 0]
		    }
		    set next_marker [lindex $marker_labels [expr $i - 1]]
		    set next_nloc [lindex $next_marker 0]
		    set distance [format %.10f [expr $nloc - $next_nloc]]
		    if {$distance < $min_distance} {
			set move_distance [expr $min_distance - $distance]
			set move_distance [format %.10f $move_distance]
			set new_position [expr $next_nloc - $move_distance]
			set new_position [format %.10f $new_position]
			set j [expr $i - 1]
			set next_marker  [lreplace $next_marker 0 0 $new_position]
			set marker_labels [lreplace $marker_labels $j $j  $next_marker]
			incr markers_moved_left
		    }
		}
	    }
	}

	ifdebug puts "Beginning to draw"

# Draw marker ticks

	proc view_cx {worldc} {
	    if {1} {
		return $worldc
	    } else {
#		upvar min_x min_x
#		upvar max_x max_x		set 
		set min_x -1100
		set max_x 1100
		return [expr 0.15 + (0.7 * ($worldc-double($min_x)) /  ($max_x-double($min_x)))]
	    }
	}

	set vertical_ticks 0

# Vertical ticks only available for Solaris and Alpha versions
#   requires a recompile of xmgr for linux...not done yet

	set tickb [expr $max_y * (1.0 - $prop_screen_for_mticks)]
	set llb [expr $tickb - ($max_y * $prop_screen_for_mleaders)]
	set textt [expr $llb - ($max_y * $prop_screen_for_ttext)]
	ifdebug puts "max_y: $max_y  tickb: $tickb  llb: $llb   textt: $textt"
	ifdebug puts "Drawing marker ticks"
	if {$modified_xmgr || $nolabels} {
	    set vertical_ticks 1
	    foreach mloc $marker_ticks {
		set mloc [lindex $mloc 0]
		tclgr send with line
		tclgr send line loctype world
		tclgr send line [view_cx $mloc], $max_y, [view_cx $mloc],$tickb
		tclgr send line def
	    }
        }

# Write marker labels (when they fit)

	if {!$nolabels} {
	    set last_nloc -100
	    set num_markers [llength $marker_labels]
	    for {set i 0} {$i < $num_markers} {incr i} {
		set marker_label [lindex $marker_labels $i]
		set mname [lindex $marker_label 1]
		set mloc [lindex $marker_label 2]
		set nloc [lindex $marker_label 0]

# setup leader line for label 

		tclgr send with line
		tclgr send line loctype world
		if {$vertical_ticks} {
		    tclgr send line [view_cx $mloc],$tickb,[view_cx $nloc],$llb
		} else {
		    tclgr send line [view_cx $mloc],$max_y,[view_cx $nloc],$llb
		}
		tclgr send line def
		
		# setup marker label
		
		tclgr send with string
		tclgr send string on
		tclgr send string loctype world
		tclgr send string g$graphnum
		tclgr send string [view_cx $nloc], $textt
		tclgr send string rot 270
		tclgr send string char size 0.60
		tclgr send string def \"$mname\"
	    }
	}
    }

# Done with markers

# DRAW !!!

    tclgr send redraw

    return ""

}

# solar::plotqtld
#
# Purpose:  Plot qtld (qtld.out)
#
# Usage:    plotqtld <type> [-nolabels] [-nomarkers] [-file filename] [-local]
#
#           <type> is one of: strat, mgeno, qtdt, qtld
#
#           -nolabels     do not include "marker" labels (ticks only)
#           -nomarkers    do not include marker ticks or labels
#           -file         Use named file instead of qtldm.out in outdir
#           -local        Ignore default plot parameters; use only local file
#
# Notes:    You must select the trait or outdir first.
#
#           The plot parameter file (in SOLAR_LIB) is qtld.gr.  You
#           may override it with a copy in ~/lib or your working
#           directory.  Your version need only include the parameters
#           you would like to change.  This should work in most cases.
#           If you specify -local, however, the qtld.gr in SOLAR_LIB
#           is completely ignored, and your qtld.gr must be complete,
#           which might get around some very obscure conflict between
#           the two plot parameter files.
#
# -

proc plotqtld {type args} {


# Important constants
    set x_expansion_factor 1.12  ;# Expansion of X beyond input range

    set debug [solardebug]

    set marker_margin 4.5
    set nolabels 0
    set noticks 0
    set filename ""
    set local_gr 0

    set badargs [read_arglist $args \
		     -nolabels {set nolabels 1} \
		     -nomarkers {set nolabels 1; set noticks 1} \
		     -marker_margin marker_margin \
		     -file filename \
		     -local {set local_gr 1} \
		]
    if {"" != $badargs} {
	error "plotqtn: Invalid arguments: $badargs"
    }
    if {"" == $filename} {
	set filename [full_filename qtld.out]
    }

    if {$local_gr && $debug} {puts "Warning!  Ignoring qtld.gr IN SOLAR_BIN"}

    if {$type == "strat"} {
	set tindex 2
    } elseif {$type == "geno" || $type == "mgeno"} {
	set tindex 3
    } elseif {$type == "qtdt"} {
	set tindex 4
    } elseif {$type == "qtld"} {
	set tindex 5
    } else {
	error "type must be strat, mgeno, qtdt, or qtld"
    }


# Read qtld.out

    set marker_labels {}
    set all_points {}
    set max_mrk_name_len 0
    set first 1
    set qfile [open $filename]
    set lines 0
    while {-1 != [gets $qfile line]} {
	if {0 == [llength $line]} {
	    continue
	}
	incr lines

	set name [lindex $line 1]
	set pos [map locn $name]
	set lp [lindex $line $tindex]
	set lp [expr 0 - log10($lp)]

	if {$first} {
	    set min_pos $pos
	    set max_pos $pos
	    set min_lp $lp
	    set max_lp $lp
	    set first 0
	}
	lappend all_points [list $pos $lp]
	lappend marker_labels [list $pos $name $pos]
	if {[string length $name] > $max_mrk_name_len} {
	    set max_mrk_name_len [string length $pos]
	}
	if {$pos < $min_pos} {
	    set min_pos $pos
	}
	if {$pos > $max_pos} {
	    set max_pos $pos
	}
	if {$lp < $min_lp} {
	    set min_lp $lp
	}
	if {$lp > $max_lp} {
	    set max_lp $lp
	}
    }
    close $qfile
    if {$lines < 2} {
	error "File $filename contains only $lines lines"
    }

    ifdebug puts "min_pos $min_pos max_pos $max_pos"
    ifdebug puts "min_lp $min_lp max_lp $max_lp"

# Sort points if required (yes)

    proc sort_by_first_num {a b} {
	set a0 [lindex $a 0]
	set b0 [lindex $b 0]
	if {$b0 > $a0} {
	    return -1
	} elseif {$b0 < $a0} {
	    return 1
	}
	return 0
    }

    set all_points [lsort -command sort_by_first_num $all_points]


# Sort marker ticks and labels too

    set marker_ticks $marker_labels
    set marker_ticks [lsort $marker_ticks]
    set marker_labels [lsort -command sort_by_first_num $marker_labels]

# Begin plotting...

    set setnum 1
    set graphnum 1

# Open new or existing tclgr session

    if {[catch {tclgr open} errmsg]} {
	if {[string compare $errmsg  "tclgr session already opened from this solar session"]} {
	    error $errmsg
	}
    }

# Kill previous set, and graph if not overlaying

    tclgr send kill graphs
    tclgr send clear line
    tclgr send clear string
    tclgr send focus g$graphnum

# Calculate scaling and ticks for x

    set max_x_unscaled $max_pos
    set min_x_unscaled $min_pos
    set delta_x_unscaled [expr $max_x_unscaled - $min_x_unscaled]
    set center_x [expr ($min_pos + $max_pos) / 2]
    ifdebug puts "delta_x_unscaled: $delta_x_unscaled  center_x:  $center_x"

    set mmlist [majorminor $delta_x_unscaled $min_x_unscaled $max_x_unscaled]
    set major_x [lindex $mmlist 0]
    set minor_x [lindex $mmlist 1]
    ifdebug puts "major_x: $major_x  minor_x: $minor_x"

    set delta_x [expr $delta_x_unscaled * $x_expansion_factor]
    set max_x [expr $center_x + ($delta_x / 2.0)]
    set min_x [expr $center_x - ($delta_x / 2.0)]
    set use_x [expr ($min_x + $min_x_unscaled) / 2.0]
    set firstlabel_x [expr $major_x * ceil ($use_x / $major_x)]
    ifdebug puts "xmin: $min_x   xmax: $max_x   firstlabel: $firstlabel_x"

# Setup scaling and ticks for x

    tclgr send world xmin $min_x
    tclgr send world xmax $max_x
    tclgr send xaxis tick op bottom
    tclgr send xaxis tick major $major_x
    tclgr send xaxis tick minor $minor_x

    tclgr send xaxis ticklabel op bottom
    tclgr send xaxis ticklabel start type spec
    tclgr send xaxis ticklabel start $firstlabel_x

    if {abs($min_x) <= 2.0e9 && abs($max_x) <= 2.0e9} {
	tclgr send xaxis ticklabel format decimal
	tclgr send xaxis ticklabel prec 0
    }

# Set up y tick marks (major and minor)

#    ************* Set up Y Axis *****************


# This fiendishly complicated method to set up y axis is borrowed from
# proc plot.  It sets up nice major and minor ticks consistent with xmgr
# regardless of size, following a 1/2/5 rule.

# The design maximum Log10(P) is 2000
#   (highest tick range is therefore 750-1500)
#   (I don't like limits, but this is way beyond anything I can imagine)
# maxy must be 4*10^N or 1*10^N
# the other values are related

    set maxy  1000
    set major 200
    set major_digit 2
    set minor 100

    set maxdiv 2.5
    set majdiv 2
    set mindiv 2

# Since we mess with global variable tcl_precision
# We must catch errors to be sure it gets restored

    global tcl_precision
    set save_tcl_precision $tcl_precision
    set tcl_precision 6
    set not_ok [catch {
    while {1} {
	if {$max_lp > $maxy} {
	    ifdebug puts "Y Axis Major: $major Minor: $minor"
	    tclgr send yaxis tick major $major
	    tclgr send yaxis tick minor $minor
	    break
	}
	set maxy [expr double($maxy) / $maxdiv]
	set major [expr double($major) / $majdiv]
	set minor [expr double($minor) / $mindiv]

	if {$major_digit == 2} {
	    set major_digit 1
	    set maxdiv 2
	    set majdiv 2
	    set mindiv 2
	} elseif {$major_digit == 1}  {
	    set major_digit 5
	    set maxdiv 2
	    set majdiv 2.5
	    set mindiv 2.5
	} else {
	    set major_digit 2
	    set maxdiv 2.5
	    set majdiv 2
	    set mindiv 2
	}
    }
    } caught_error]
    set tcl_precision $save_tcl_precision
    if {$not_ok} {
	error $caught_error
    }

# Create y expansion factor to permit showing markers labels at top

    if {$nolabels || $noticks} {
	set max_mrk_name_len 0
    }
    set marker_margin 2.0
    set chars_that_fit_vertically 55.0  ;# using default font size (was 58)
    set prop_screen_for_markers [expr ($max_mrk_name_len + $marker_margin)  / $chars_that_fit_vertically]
    set prop_screen_for_mticks 0.04
    set prop_screen_for_mleaders 0.04
    set prop_screen_for_ttext 0.015
    set prop_screen_for_plot [expr 1.0 - ($prop_screen_for_mticks +  $prop_screen_for_mleaders +  $prop_screen_for_ttext  +  $prop_screen_for_markers )]
    set expand_y [expr 1.0 / $prop_screen_for_plot]
    ifdebug puts "expand_y is $expand_y"

# Set up y axis and ticks

    set max_y_unscaled $max_lp
    set min_y_unscaled $min_lp

    set max_y_ticklabel [expr $major*ceil(double($max_lp)/$major)]
    ifdebug puts "max_y_ticklabel: $max_y_ticklabel"
    set max_y [expr $max_y_ticklabel * $expand_y]
    set min_y [expr -0.06 * $max_y]
    tclgr send world ymin $min_y
    tclgr send world ymax $max_y
    tclgr send yaxis ticklabel start type spec
    tclgr send yaxis ticklabel stop type spec
    tclgr send yaxis ticklabel start 0
    tclgr send yaxis ticklabel stop $max_y_ticklabel
#
# xmgr spaces Y axis label too close to single digits
#
    if {$max_y_ticklabel > 4 && $max_y_ticklabel < 10} {
	tclgr send yaxis label place spec
	tclgr send yaxis label place 0.043,0
    }
#
# Get user overrides from ma.gr
#
    global env
    if {!$local_gr} {
	if {[file exists $env(SOLAR_LIB)/qtld.gr]} {
	    set mpathname [glob $env(SOLAR_LIB)/qtld.gr]
	    tclgr send read \"$mpathname\"
	}
	if {[file exists ~/lib/qtld.gr]} {
	    set mpathname [glob ~/lib/qtld.gr]
	    tclgr send read \"$mpathname\"
	}
    }
    if {[file exists qtld.gr]} {
	tclgr send read \"qtld.gr\"
    } 
#
#           ***   PLOT POINTS   ***
#
    foreach point $all_points {
	set x [lindex $point 0]
	set y [lindex $point 1]

	tclgr send g$graphnum.s$setnum point $x,$y
	ifdebug ifverbmax puts "graphing g$graphnum.s$setnum point $x,$y"
    }
#
#           ***     PLOT MARKERS    ***
#
    set max_markers 42
    set max_ticks 100
    set max_ticks_modified_xmgr 1000
    if {!$nolabels || !$noticks} {

# Determine if modified xmgr is available, allows 1000 ticks

	set modified_xmgr 0
	set u_name [string tolower [exec uname]]
	if {![string compare $u_name sunos] ||  ![string compare $u_name osf1] ||  [string match *linux* $u_name] ||  [string match *irix* $u_name]} {
	    set modified_xmgr 1
	    set max_ticks $max_ticks_modified_xmgr
	}
#
# See if we can handle this number of markers in any way
#
	set num_markers [llength $marker_labels]
	ifdebug puts "There appear to be $num_markers markers"
	if {$num_markers> $max_ticks} {
	    if {!$modifed_xmgr} {
		ifdebug puts "Modified version of XMGR not available on this system."
	    }
	    set nolabels 1
	    set noticks 1
	    ifdebug puts "Too many many marker ticks to display..."
        }
    }
    if {!$nolabels || !$noticks} {
#
# Remove marker labels down to maximum we can handle
#
	if {!$nolabels && $num_markers > $max_markers} {
	    ifdebug puts "Removing marker labels down to $max_markers"
	    while {$num_markers > $max_markers} {
		set marker [lindex $marker_labels 0]
		set lowest_value [lindex $marker 0]
		set lowest_index 0
		for {set i 0} {$i < $num_markers} {incr i} {
		    set marker [lindex $marker_labels $i]
		    set value [lindex $marker 0]
		    if {$value < $lowest_value} {
			set lowest_value $value
			set lowest_index $i
		    }
		}
		set marker_labels [lreplace $marker_labels  $lowest_index $lowest_index]
		set num_markers [expr $num_markers - 1]
	    }
	}
	if {!$nolabels} {
#
# Move remaining marker labels around to fit
# This should only take 1.5 passes with current algorithm
#   (old algorithm repeated until no more moves)

	    set total_x $delta_x_unscaled
	    set starting_x $min_x_unscaled
	    set min_distance [format %.9f [expr 0.0241 * $total_x]]
	    set markers_moved_left 0
	    set markers_moved_right 0
	    set pass_count 0
	    set max_pass_count 1.5
	    while {$pass_count<1 ||  $markers_moved_left>0|| $markers_moved_right>0} {
		incr pass_count
		set markers_moved_left 0
		set markers_moved_right 0

# Scan left to right, moving "next_marker" right if necessary

		for {set i -1} {$i < $num_markers - 1} {incr i} {
		    if {$i == -1} {
			set nloc [format %.10f [expr $starting_x - $min_distance]]
		    } else {
			set nloc [lindex [lindex $marker_labels $i] 0]
		    }
		    set next_marker [lindex $marker_labels [expr $i + 1]]
		    set next_nloc [lindex $next_marker 0]
		    set distance [format %.10f [expr $next_nloc - $nloc]]
		    if {$distance < $min_distance} {

# Move next marker by 1/2 distance required if this is the first pass
#  (makes offsets more symmetrical)
# Move next marker by entire distance required if this is subsequent pass
# Must maintain rounding to 0.001 precision

			set move_distance [expr $min_distance - $distance]
			if {$pass_count == 1} {
			    set move_distance [expr $move_distance / 2]
			}
			set move_distance [format %.10f $move_distance]
			set new_position [expr $next_nloc + $move_distance]
			set new_position [format %.10f $new_position]
			set j [expr $i + 1]
			set next_marker  [lreplace $next_marker 0 0 $new_position]
			set marker_labels  [lreplace $marker_labels $j $j $next_marker]
			incr markers_moved_right
		    }
		}
#
# This has been proven to take no more than 1.5 passes
#
		if {$pass_count > $max_pass_count} {
		    break
		}
#
# Scan right to left, moving next_marker right if necessary
#
		for {set i $num_markers} {$i > 0} {set i [expr $i - 1]} {
		    if {$i == $num_markers} {
			set nloc [format %.10f [expr $max_x_unscaled +  $min_distance]]
		    } else {
			set nloc [lindex [lindex $marker_labels $i] 0]
		    }
		    set next_marker [lindex $marker_labels [expr $i - 1]]
		    set next_nloc [lindex $next_marker 0]
		    set distance [format %.10f [expr $nloc - $next_nloc]]
		    if {$distance < $min_distance} {
			set move_distance [expr $min_distance - $distance]
			set move_distance [format %.10f $move_distance]
			set new_position [expr $next_nloc - $move_distance]
			set new_position [format %.10f $new_position]
			set j [expr $i - 1]
			set next_marker  [lreplace $next_marker 0 0 $new_position]
			set marker_labels [lreplace $marker_labels $j $j  $next_marker]
			incr markers_moved_left
		    }
		}
	    }
	}

	ifdebug puts "Beginning to draw"

# Draw marker ticks

	proc view_cx {worldc} {
	    if {1} {
		return $worldc
	    } else {
#		upvar min_x min_x
#		upvar max_x max_x		set 
		set min_x -1100
		set max_x 1100
		return [expr 0.15 + (0.7 * ($worldc-double($min_x)) /  ($max_x-double($min_x)))]
	    }
	}

	set vertical_ticks 0

# Vertical ticks only available for Solaris and Alpha versions
#   requires a recompile of xmgr for linux...not done yet

	set tickb [expr $max_y * (1.0 - $prop_screen_for_mticks)]
	set llb [expr $tickb - ($max_y * $prop_screen_for_mleaders)]
	set textt [expr $llb - ($max_y * $prop_screen_for_ttext)]
	ifdebug puts "max_y: $max_y  tickb: $tickb  llb: $llb   textt: $textt"
	ifdebug puts "Drawing marker ticks"
	if {$modified_xmgr || $nolabels} {
	    set vertical_ticks 1
	    foreach mloc $marker_ticks {
		set mloc [lindex $mloc 0]
		tclgr send with line
		tclgr send line loctype world
		tclgr send line [view_cx $mloc], $max_y, [view_cx $mloc],$tickb
		tclgr send line def
	    }
        }

# Write marker labels (when they fit)

	if {!$nolabels} {
	    set last_nloc -100
	    set num_markers [llength $marker_labels]
	    for {set i 0} {$i < $num_markers} {incr i} {
		set marker_label [lindex $marker_labels $i]
		set mname [lindex $marker_label 1]
		set mloc [lindex $marker_label 2]
		set nloc [lindex $marker_label 0]

# setup leader line for label 

		tclgr send with line
		tclgr send line loctype world
		if {$vertical_ticks} {
		    tclgr send line [view_cx $mloc],$tickb,[view_cx $nloc],$llb
		} else {
		    tclgr send line [view_cx $mloc],$max_y,[view_cx $nloc],$llb
		}
		tclgr send line def
		
		# setup marker label
		
		tclgr send with string
		tclgr send string on
		tclgr send string loctype world
		tclgr send string g$graphnum
		tclgr send string [view_cx $nloc], $textt
		tclgr send string rot 270
		tclgr send string char size 0.60
		tclgr send string def \"$mname\"
	    }
	}
    }

# Done with markers

# DRAW !!!

    tclgr send redraw

    return ""

}


# solar::majorminor -- private
#
# Purpose:  Determine best major and minor ticks given a range
#           Range must be positive, but can be < 1 or exponential
#
# Usage:    majorminor <range> [<xmin>] [<xmax>]
#
#           <range> is range (e.g. xmin - xmax)
#           <xmin> is min X value, specify if X only
#           <xmax> is max X value, specify if X only
#
# Returns:  list of major and minor tick distances
#
# Note:  Of course this embodies a particular style.  In this case,
#        major ticks begin with either 1, 2, or 5, and they are divided
#        by 2 or 5 minor ticks.  It is intended that there be from 3-7
#        major ticks, which is good for an X axis, not necessarily so
#        good for the Y axis where more ticks might be desired.  (In
#        actual practice, if range is expanded, there might be up to
#        8 major ticks.)
#
#        xmin,xmax is used for xaxis only.
#        This is used to allow extra spacing for very big numbers.
#        Starting when any abx(xmin,xmax) > 10^6, fudge factors
#        are multiplied into the range to provide more spacing:
#
#           abs(xmin,xmax) < 10^6             1.0 (no fudging)
#           abs(xmin,xmax) >= 10^6            1.0 + 0.1 for each digit past 5
# -

proc majorminor {range {xmin 1} {xmax 1}} {

    ifdebug puts "Entering majorminor"
    set debug [solardebug]

    set xmax [expr abs($xmax)]
    set xmin [expr abs($xmin)]
    if {$xmin > $xmax} {
	set xmax $xmin
    }

    if {$xmax >= 1.0e6} {
	set fudge [expr 1.0 + ((log10(1.0*$xmax) - 5.0) * 0.1)]
	if {$debug} {puts "fudging range by $fudge"}
	set range [expr $range * $fudge]
    }

# Get first sig digit
# and, if possible, second sig digit

    set rlen [string length $range]
    set first_digit ""
    for {set i 0} {$i < $rlen} {incr i} {
	set s [string index $range $i]
	if {"." != $s} {
	    if {0 != $s} {
		set first_digit $s
		set second_digit [string index $range [expr $i + 1]]
		if {"e" == $second_digit} {
		    set second_digit ""
		} elseif {"." == $second_digit} {
		    set second_digit [string index $range [expr $i + 2]]
		}
		break
	    }
	}
    }
    if {"" == $first_digit} {
	error "Can't plot when X range is 0"
    }

    if {$debug} {puts "first,second digits are $first_digit,$second_digit"}

    set factor [expr pow (10, floor (log10 ($range)))]
    if {$factor < 1} {
	set factor [format %.1g $factor]
    }
	
    if {$debug} {puts "factor is $factor"}

    while {1} {
	set major .2
	set minor .1
	if {$first_digit == 1} {
	    if {"" != $second_digit} {
		if {$second_digit >= 5} {
		    set major .5
		}
		if {$second_digit >= 8} {
		    set minor .1
		}
	    }
	    break
	}
	if {$first_digit == 2} {
	    set major .5
	    set minor .1
	    break
	}
	if {$first_digit < 7} {
	    set major 1
	    set minor .5
	    break
	}
	set major 2
	set minor 1
	break
    }

    set major [expr $major * $factor]
    set minor [expr $minor * $factor]
    if {$major >= 1} {
	set major [expr round ($major)]
    } else {
	set major [format %.1g $major]
    }
    if {$minor > 3 || $minor == 1} {
	set minor [expr round ($minor)]
    } else {
	set minor [format %.2g $minor]
    }
    if {$debug} {puts "major: $major  minor: $minor"}
    return [list $major $minor]
}


# solar::stepfor --
#
# Purpose: Foward stepwise covariate screening
#
# Usage:   stepfor [-list listfile] [-list list] [-verbose] [-v]
#                  [-fix listfile]  [-fix fixlist] [-max maxdim]
#                  [-p pvalue] [-test othertest] [-par] [-parclean]
#
#          stepclean     ;# Remove fully_typed covariate and unload file
#
#          By default, stepfor will test all covariates in the current
#          model, testing them all and then fixing the best one, and then
#          repeating the process until the best one does not meet the
#          default pvalue of 0.05, or user specified p-value or test (see
#          below).  The final model will contain all the covariates which met
#          the screening test.  A file named stepfor.out is written to the
#          output directory with all the loglikelihoods, and a file named
#          stepfor.history is written with other information.  All of the
#          best models for each number of covariates are saved as
#          stepfor.null<i> where <i> is the number of tested covariates.
#
#          To ensure that all models use the same sample, a new file named
#          fully_typed.out is created in the output directory which
#          defines a variable named "fully_typed" for each fully typed
#          individual.  This file is added to the list of open phenotypes
#          files, and the variable "fully_typed" is added to the
#          model as a "null" covariate which has no effect on the model
#          other than restricting the sample to fully typed individuals.
#
#          To remove the fully_typed covariate and unload the fully_typed.out
#          phenotypes file, give the command "stepclean" after stepfor has
#          completed.
#
#          -list listfile    listfile is a file containing a list of all
#                            covariates to be tested, one on each line.
#                            The filename cannot contain spaces.  These
#                            covariates may or may not be in the model when
#                            the command is given.  If the -list option is
#                            specified, all other covariates in the starting
#                            model are automatically fixed.
#          -list list        Alternatively, a Tcl list of covariates to
#                            be tested can be specified.  Tcl lists are
#                            space delimited and enclosed in quotes or curly
#                            braces.
#
#          -fix list         list is a Tcl list of covariates to be
#                            included in every model and not tested.  Their
#                            values will be estimated by maximum likelihood
#                            for every model, unless you constrain them.
#                            These covariates may or may not in the model
#                            when the command is given.  For -fix, a list
#                            could be simply one phenotype, and that
#                            supercedes a file with the same name.
#          -fix listfile     Alternatively, a file containing a list of all
#                            covariates to be included in every model may
#                            be specified.  The filename cannot contain
#                            spaces.  The list of covariates to be fixed
#                            will supercede the list of covariates to be
#                            tested if the same covariate occurs on both
#                            lists, however a warning will be given.
#
#           -p pvalue        pvalue is the highest p value allowed for
#                            a covariate to be included.  The default is 0.05.
#
#           -max maxdim      maxdim is the maximum number of test covariates
#                            to be included in a model (the maximum dimension).
#
#           -verbose         Show maximization output during maximizations.
#           -v               Same as -verbose
#
#          -par              New and EXPERIMENTAL!  This option turns on Parallel
#                            processing on the SFBR GCC Compute Ranch.
#                            WARNING!  Do not run more than one instance of
#                            stepfor -par from the same working directory.
#                            Parallel stepfor will use many (but not all!) ranch
#                            machines, and access for other users and jobs may
#                            be delayed due to gridware thrashing.  The usual
#                            output is not printed to the terminal to save time
#                            but numerous parallel status messages are printed
#                            to help the developers make this program better.
#                            The parallel operation is automatic and the
#                            parallel status messages may be ignored by most
#                            users most of the time unless there is no output
#                            for more than fifteen minutes.  Note: If model
#                            includes linkage element matrices loaded from
#                            some mibddir, those matrices should be relocated
#                            to the working directory, or specified with an
#                            absolute pathname in the model file.  This is
#                            because in parallel operation the model is loaded
#                            not in the current working directory but in a
#                            subdirectory of /tmp.
#
#          -parclean         Normally, parallel stepfor cleans up after itself.
#                            However, if it is necessary to force a shutdown
#                            of a parallel stepfor, normal cleanup is not
#                            done.  "stepfor -parclean" cleans up all the
#                            junk stepfor files in /tmp directories on all
#                            ranch machines.  This must be run on medusa.  Do
#                            not run if you have any other running parallel
#                            jobs (parallel stepfor, parallel bayesavg, or any
#                            parallel job using "launch" or "doscript") as
#                            their files may be deleted too.
#                            See also "doranch" for other ranch cleanup
#                            procedures.  Cleanup history is written to a file
#                            named cleantmp.out.
#
#           -test othertest  othertest is a user defined Tcl proc that judges
#                            whether or not a covariate should be included.
#                            The test model with the best covariate is loaded
#                            at the time this procedure is called.  This
#                            procedure takes two mandatory arguments (whether
#                            they are needed by the procedure or not).
#
#                            loglike0 nullmodelname
#
#                            loglike0 is the loglikelihood of the null model
#                            which does not contain the current test covariate.
#                            nullmodelname is the pathname to the null model
#                            itself.  The procedure may obtain the
#                            loglikelihood of the current model with the
#                            loglike command.  The default procedure looks
#                            like this:
#
#                        proc stepfortest {loglike0 nullmodel} {
#                            set chisq [expr 2.0 * ([loglike] - $loglike0)]
#                            if {$chisq >= 0} {
#                              	 set pvalue [chi -number $chisq 1]
#                            } else {
#	                         set pvalue 1
#                            }
#                            set pvalue [chi -number $chisq 1]
#                            putsout stepfor.history "\n    *** p = $pvalue"
#                            global SOLAR_stepfor_pvalue
#                            if {$pvalue <= $SOLAR_stepfor_pvalue} {
#                                return 1
#                            }
#                            return 0
#                        }
#
#                            Note that the default procedure does not use
#                            the nullmodel argument, but it does use a
#                            global variable that you will not have to use.
#                            The global supports the -p argument.  The
#                            procedure may load the nullmodel without
#                            restoring the current model; that is handled
#                            by the stepfor procedure itself.
#          
# -

proc stepfor {args} {
#
# Do parallel cleanup if requested
#
    if {$args == "-parclean"} {
	set uname [lindex [exec who -m] 0]
	return [doranch cleantmp $uname.]
    }
#
# Save initial model and delete old files
#
    save model [full_filename stepfor.orig]
    file delete [full_filename stepfor.history]
    file delete [full_filename stepfor.out]
    set modfiles [glob -nocomplain [full_filename stepfor.null*.mod]]
    foreach modfile $modfiles {
	file delete $modfile
    }
#
# read arguments
#
    set stepverbosity -q
    set pvalue 0.05
    set maxdim 0
    set listfile {}
    set fixlist {}
    set ucovlist {}
    set stepfortest stepfortest    ;# the default test, not a typo
    set parallel 0
    set badargs [read_arglist $args \
		     -list listfile \
		     -fix fixlist \
		     -p pvalue \
		     -test stepfortest \
		     -verbose {set stepverbosity ""} \
		     -v {set stepverbosity ""} \
		     -max maxdim \
		     -par {set parallel 1} \
		     ]
    if {{} != $badargs} {
	error "stepfor: Invalid argument $badargs"
    }
    global SOLAR_stepfor_pvalue
    set SOLAR_stepfor_pvalue $pvalue
#
# Get list of covariates to test
#
    if {{} != $listfile} {
	if {-1 == [string first " " $listfile]} {
	    set covlist [listfile $listfile]
	} else {
	    set covlist $listfile
	}
	set ucovlist $covlist
    } else {
	set covlist [covar]
    }
#
# Because a one element fixlist is possible
# If one element is specified, first check if it is a phenotype
#	
    if {{} != $fixlist} {
	if {-1 == [string first " " $fixlist]} {
	    if {-1 == [lsearch -exact [concat sex [lrange [phenotypes] \
						       1 end]] $fixlist]} {
		if {![file exists $fixlist]} {
		    error "stepfor: No phenotype or file named $fixlist"
		}
		set fixlist [listfile $fixlist]
	    }
	}
    }
#
# Be sure fully_typed() is incorporated into fixlist at least once
# and only once
#
    if {-1 == [lsearch -exact $fixlist fully_typed()]} {
	lappend fixlist fully_typed()
    }
#
# Now remove fixed covariates from list of covariates to test
#
    foreach fix $fixlist {
	if {-1 != [set foundpos [lsearch -exact $covlist $fix]]} {
	    set covlist [lreplace $covlist $foundpos $foundpos]
	    if {{} != $ucovlist} {
		if {-1 == [lsearch -exact $ucovlist $fix]} {
		    putsout stepfor.history "Warning.  Removing $fix from test list because fixed"
		}
	    }
	}
    }
#
# Show list of covariates
    set n [llength $covlist]
    set printlist $covlist
    set maxline 78
    while {{} != $printlist} {
	set line "    [lindex $printlist 0]"
	set printlist [lrange $printlist 1 end]
	while {{} != $printlist} {
	    set next [lindex $printlist 0]
	    if {$maxline < [expr [string length $line]+2+[string length $next]\
			       ]} {
		break
	    }
	    set line "$line $next"
	    set printlist [lrange $printlist 1 end]
	}
	putsout stepfor.history $line
    }
    putsout stepfor.history "\n    *** Testing $n covariates"
#
# Add fixed covariates (if not already added) except for fully_typed
#   (fully_typed gets added later)
#
    foreach cov $fixlist {
	if {$cov != "fully_typed()"} {
	    covariate $cov
	}
    }
#
# Add listed covariates (if not already added) and then delete (later).
# This ensures covariates CAN be added, and that they are now deleted.
#
    foreach cov $covlist {
	covariate $cov
    }
#
# If no omega, make it polygenic
#
    if {[omega] == \
	    "omega = Use_polygenic_to_set_standard_model_parameterization"} {
	putsout "No predefined omega; defaulting to polygenic"
	polymod
    }
#
# Now that all covariates have been added to model
# create fully_typed.out file
#
    eval maximize $stepverbosity -sampledata
    set insample [solarfile open [full_filename sampledata.out]]
    solarfile $insample start_setup
    solarfile $insample setup id
    set outsample [open [full_filename fully_typed.out] w]
    puts $outsample "id,fully_typed"
    while {{} != [set record [solarfile $insample get]]} {
	set id [lindex $record 0]
	puts $outsample "$id,1"
    }
    solarfile $insample close
    close $outsample
#
# Removed fully_typed.out from old output directories
# and add in new fully_typed.out from current output directory
#
    set oldphenotypes [phenotype -files]
    set newphenotypes {}
    foreach oldphenotype $oldphenotypes {
	set phentail [file tail $oldphenotype]
	if {[string compare $phentail fully_typed.out]} {
	    lappend newphenotypes $oldphenotype
	}
    }
    lappend newphenotypes [full_filename fully_typed.out]
    eval load phenotypes $newphenotypes

    set oldcovariates [covariate]
    if {-1 == [lsearch -exact $oldcovariates fully_typed()]} {
	covar fully_typed()
    }
#
# OK, now delete the variable covariates
#
    foreach cov $covlist {
	covariate delete $cov
    }
#
# Maximize "null" model
#
    puts "    *** Maximizing null model\n"
    eval maximize $stepverbosity
    save model [full_filename stepfor.null0]
    set null_like0 [loglike]
    putsout stepfor.out [format "%24s   %s   %s               %s      %s" \
			     Model Loglike Covariate Chi^2 p]
    set basemodelname cov
    putsout stepfor.out [format "%24s  %.3f" $basemodelname [loglike]]
#
# Main loop
#
    set loop 1
    set nullindex -1
    set nextindex 0
    set fixcovs {}
    set early_exit 0
    set addedlist {}
    while {[llength $covlist]} {
	if {$maxdim!=0 && $nextindex>=$maxdim} {
	    putsout stepfor.history "    *** Exiting because -max $maxdim"
	    putsout stepfor.history "    *** Final covariates are: $fixcovs"
	    load model [full_filename stepfor.null$nextindex]
	    set early_exit 1
	    break
	}
	incr nullindex
	incr nextindex
	set bestlike ""
	set bestcov ""
	set covindex 0
	set testedcov 0
	if {$parallel} {
	    set bestlikeshort ""
	    set bestcovs ""
	    set number_tests [expr [llength $covlist] - $nullindex]
	    puts "Running $number_tests tests in parallel"

	    puts [exec date]
	    set trynum 1
	    set cfilename [full_filename stepfor.covlist]
	    set cfileout [open $cfilename w]
	    foreach cov $covlist {
		puts $cfileout $cov
	    }
	    close $cfileout
	    set poutfilename [full_filename stepfor.out.d$nextindex]
	    file delete -force $poutfilename
	    file delete -force $poutfilename.tmp

	    set tmodfilename [full_filename stepfor.testmods.1]
	    set tmodfile [open $tmodfilename w]
	    set covindex 0
	    foreach cov $covlist {
		incr covindex
		if {-1 != [lsearch -exact $addedlist $cov]} {
		    continue
		}
		puts $tmodfile $covindex
	    }
	    close $tmodfile

	    global STEP_recordsize
	    set STEP_recordsize 5
	    puts "calling launch"
	    step_parallel_launch stepfor $number_tests [expr $nullindex + 1] \
		$basemodelname
#
# returns only when all results written to stepfor.out.d$nextindex
#
	    set par_infile [open [full_filename stepfor.out.d$nextindex]]
	    set par_outfile [open [full_filename stepfor.out] a]
	    set covindex 0
	    foreach cov $covlist {
		incr covindex
		if {-1 != [lsearch -exact $addedlist $cov]} {
		    continue
		}
		gets $par_infile current_result
		puts $par_outfile $current_result
		set logl [lindex $current_result 1]
		set testedcov 1
		if {"" == $bestlikeshort || $logl > $bestlikeshort} {
		    set bestlikeshort $logl
		    lappend bestcovs $cov
		    set bestindex $covindex
		}
	    }
	    close $par_infile
	    close $par_outfile
	} else {

# Non-parallel

	foreach cov $covlist {
	    incr covindex
	    if {-1 != [lsearch -exact $addedlist $cov]} {
		continue
	    }
	    set testedcov 1
	    load model [full_filename stepfor.null$nullindex]
	    covariate $cov
	    eval maximize $stepverbosity
	    set newmodelname [stepname $basemodelname $covindex]

	    eval set chisq [expr 2.0 * ([loglike] - \$null_like$nullindex)]
	    if {$chisq >= 0} {
		set pval [chi -number $chisq 1]
	    } else {
		set pval 1
	    }
	    set fchisq [format %.4f $chisq]

	    if {-1 != [set epos [string first e $pval]]} {
		set mstring [string range $pval 0 5]
		set estring [string range $pval $epos end]
		set pval "$mstring$estring"
	    }

	    putsout stepfor.out \
		[fformat "%24s  %.3f  %-19s %9s  %s" \
		     $newmodelname [loglike] $cov $fchisq $pval]

	    if {"" == $bestlike} {
		set bestlike [loglike]
		set bestcov $cov
		save model [full_filename stepfor.null$nextindex]
		set bestindex $covindex
	    } elseif {[loglike] > $bestlike} {
		set bestlike [loglike]
		set bestcov $cov
		save model [full_filename stepfor.null$nextindex]
		set bestindex $covindex
	    }
	}
        }

#
# See if criterion is satisfied
#
	if {!$testedcov} {
	    break
	}
	if {!$parallel} {
	    load model [full_filename stepfor.null$nextindex]
	} else {
	    set testbestlike ""
	    foreach testbestcov $bestcovs {
		load model [full_filename stepfor.null$nullindex]
		covariate $testbestcov
		eval maximize $stepverbosity
		if {$testbestlike == "" || [loglike] > $testbestlike} {
		    set bestcov $testbestcov
		    save model [full_filename stepfor.null$nextindex]
		    set bestlike [loglike]
		}
	    }
	}	    
	set null_like$nextindex $bestlike
  	if {![eval $stepfortest \$null_like$nullindex stepfor.$nullindex]} {
	    putsout stepfor.history "    *** Best covariate $bestcov lod did not meet criterion"
	    putsout stepfor.history "    *** Final covariates are: $fixcovs"
	    load model [full_filename stepfor.null$nullindex]
	    set early_exit 1
	    break
	}
	lappend fixcovs $bestcov
	lappend addedlist $bestcov
        putsout stepfor.history "    *** Best covariate $bestcov added to model stepfor.null$nextindex\n"
	set basemodelname [stepname $basemodelname $bestindex]
	set bestmodelname stepfor.null$nextindex
    }
    if {!$early_exit} {
	putsout stepfor.history "\n    *** All covariates were added to final model (stepfor.best):\n$fixcovs"
    }

    save model [full_filename stepfor.best]
    return ""
#
# No longer bother with deleting fully_typed()...
#
#    covariate delete fully_typed()
#    save model [full_filename stepfor.best]
#    puts "\nRe-maximizing stepfor.best with largest possible sample...\n"
#    eval maximize $stepverbosity
#    save model [full_filename stepfor.best]
#
# Alternatively, we could clean up all stepfor models
#
#    for {set i 0} {$i < $bestindex} {incr i} {
#	puts "fixing model stepfor.null$i"
#	load model [full_filename stepfor.null$i]
#	covariate delete fully_typed()
#	save model [full_filename stepfor.null$i]
#    }
#    eval load phen $oldphenotypes
}

proc stepclean {} {
    catch {covariate delete fully_typed()}
    set phenfiles [phenotypes -files]
    set newphens {}
    foreach phenfile $phenfiles {
	if {0!=[string compare [file tail $phenfile] fully_typed.out]} {
	    lappend newphens $phenfile
	}
    }
    if {{} == $newphens} {
	catch {load phenotypes ""}
    } else {
	eval load phenotypes $newphens
    }
    return ""
}


proc stepfortest {loglike0 nullmodel} {
    set chisq [expr 2.0 * ([loglike] - $loglike0)]
    putsout stepfor.history "\n    *** chisq = [format %.8g $chisq]"
    if {$chisq >= 0} {
	set pvalue [chi -number $chisq 1]
    } else {
	set pvalue 1
    }
    putsout stepfor.history "    *** p = $pvalue"
    global SOLAR_stepfor_pvalue
    if {$pvalue <= $SOLAR_stepfor_pvalue} {
	return 1
    }
    return 0
}

proc stepname {basename covindex} {
    set namelist [split $basename .]
    if {[set namelen [llength $namelist]] < 2} {
	return $basename.$covindex
    }
    set newnamelist {}
    for {set i 1} {$i < $namelen} {incr i} {
	if {[lindex $namelist $i] > $covindex} {
	    set newnamelist [concat [lrange $namelist 0 [expr $i - 1]] \
				 $covindex [lrange $namelist $i end]]
	    break
	}
    }
    if {{} == $newnamelist} {
	set newnamelist [concat $namelist $covindex]
    }
    return [join $newnamelist .]
}

# Parallel support for stepfor
#

proc step_parallel_launch {stepname number_tests dimension basemodelname} {

# Clean out old files
#
    set oldfiles [glob -nocomplain [full_filename step.*.*.*.*]]
    foreach oldfile $oldfiles {
	catch {exec rm -rf $oldfile}
    }

#
    set trynum 1
    set nullindex [expr $dimension - 1]
#
#       Make file with list of all files required
#         start with stuff generally needed by SOLAR models...
#
    set lfilename $stepname.files.list
    set outprefix ""
    if {"/" == [string index $lfilename 0]} {
	error "For parallel $stepname, outdir must be relative to working directory"
    }
    set lfile [open $lfilename w]
#
# Determine if matrices
#
    set all_matrixes [matrix]
    set length_all_matrixes [llength $all_matrixes]
    set matrix_names {}
    for {set imatrix 0} {$imatrix < $length_all_matrixes} {incr imatrix} {
	set iname [lindex $all_matrixes $imatrix]
	if {$iname == "matrix"} {
	    incr imatrix
	    set iname [lindex $all_matrixes $imatrix]
	    if {$iname == "load"} {
		incr imatrix
		set iname [lindex $all_matrixes $imatrix]
		if {$iname != ""} {
		    lappend matrix_names $iname
		}
	    }
	}
    }
#
# Add matrices and check for non-local matrices
#
    set matrix_names_needed {}
    foreach matrix_name $matrix_names {
	if {"/" != [string index $matrix_name 0]} {
	    if {-1 != [string first / $matrix_name]} {
		close $lfile
		puts "Model contains matrix $matrix_name"
		error "For parallel $stepname, model matrices should be in working directory\nor specified with absolute pathname in the model file."
	    }
	    puts $lfile $matrix_name
	}
    }
    puts $lfile "pedindex.out"
    puts $lfile "pedindex.cde"
#
# One or more phenotypes files, with or without .cde file
#
    set pifile [open phenotypes.info]
    set pofile [open [full_filename phenotypes.info] w]
    while {-1 != [gets $pifile phenname]} {
	puts $lfile $phenname
	if {-1 == [string first / $phenname]} {
	    puts $pofile $phenname
	} else {
#
# In our phenotypes.info, reference non-wd phenfiles in wd
#   because doscript copies them there
#
	    puts $pofile [file tail $phenname]
	}
	if {[file exists [file rootname $phenname].cde]} {
	    puts $lfile  [file rootname $phenname].cde
	}
	if {[file exists [file rootname $phenname].CDE]} {
	    puts $lfile [file rootname $phenname].CDE
	}
	if {[file exists $phenname.cde]} {
	    puts $lfile $phenname.cde
	}
	if {[file exists $phenname.CDE]} {
	    puts $lfile $phenname.CDE
	}
    }
    close $pifile
    close $pofile
    puts $lfile [full_filename phenotypes.info]
    puts "writing other variables to file"
#
# Write other variables to a file
#
    set vfilename [full_filename $stepname.vars]
    set vfile [open $vfilename w]
#    puts $vfile $null0_like
#    puts $vfile $log_n
    puts $vfile $nullindex
    puts $vfile $basemodelname
    close $vfile
    puts $lfile $vfilename
#
# Required files in outdir
#
    puts $lfile [full_filename $stepname.null$nullindex.mod]
    puts $lfile [full_filename $stepname.testmods.$trynum]
    puts $lfile [full_filename $stepname.covlist]
    close $lfile
#
# Note: all files in lfile will be copied to /tmp working directory
#   in parallel tasks
#
# Continue launch in recursive routine which also handles relaunches
#
     step_launch $stepname $dimension $number_tests $trynum $lfilename \
		$basemodelname
#
# Rename or rewrite temp output file to final one
#
    if {$stepname == "stepfor"} {
	puts "doing one last sort"
# one last sort required now
	set intfilename [full_filename $stepname.out.d$dimension.tmp]
	set sintfilename $intfilename.sort
	exec [usort] -k 1,1n $intfilename >$sintfilename
	puts "reformatting new results"
	set intfile [open $sintfilename]
	set outdfile [open [full_filename $stepname.out.d$dimension] w]
	while {-1 != [gets $intfile line]} {
	    set modelname [stepname $basemodelname [lindex $line 0]]
	    set thislike [lindex $line 1]
	    set use_covar [lindex $line 2]
	    set fchisq [lindex $line 3]
	    set pval [lindex $line 4]
	    set newline [fformat "%24s  %.3f  %-19s %9s  %s" \
			     $modelname $thislike $use_covar $fchisq $pval]
	    puts $outdfile $newline
	}
	close $intfile
	close $outdfile
	exec rm -f $intfilename
	exec rm -f $sintfilename
    } else {
	file rename [full_filename $stepname.out.d$dimension.tmp] \
	    [full_filename $stepname.out.d$dimension]
    }
#
# Done
#
    return ""
}


proc step_launch {stepname dimension number_tests trynum lfilename \
		basemodelname} {
    puts "There are $number_tests tasks"
    puts "Determining number of jobs to launch..."
#
# Determine available machines
#
    set cpus_useable 0
    while {1} {
	set cpus_useable [cpus_available_soft_margin]
	if {$cpus_useable > 100 || $cpus_useable*20 > $number_tests} break
	puts "Too few machines now available, waiting for more..."
	waitdots 60
    }
#
# Now, determine how many we are actually going to use
#
#   (2 below was parallel minjobsize, but that seems to have 2 meanings)
#
#   set maximum_cpus [expr 1 + ($number_tests-1) / 2]
    set maximum_cpus $number_tests
#
# Avoid "one task" situation because it can lead to 100% noncompletion
#   which wastes a lot of time
#
    if {$maximum_cpus <= [parallel minjobsize]} {
	set maximum_cpus $number_tests
    }
    if {$maximum_cpus <= $cpus_useable} {
	set reserved_cpus $maximum_cpus
    } else {
	set reserved_cpus $cpus_useable
    }
#
# Clear launch working directory, if any
#
    set tan [take_a_number]
    set launchdir [full_filename $stepname.[pid].$tan.$dimension.$trynum]
    exec rm -rf $launchdir
    exec mkdir $launchdir
#
# Get current program name
#
    global env
    set programpath $env(SOLAR_PROGRAM_NAME)
    set programname [file tail $programpath]
#
# Launch scripts
#
    set starting_task 1
    set solarname solar
    set jobsize [expr int (ceil ($number_tests / double($reserved_cpus)))]
    puts "precomputed maximum cpus for this dimension is $reserved_cpus"
    puts "precomputed jobsize is $jobsize"
    set launchstat [launch -proc $stepname\_step -extra_args \
			"$trynum" -filelist $lfilename \
			-n $number_tests -jobsize $jobsize \
			-outputdir $launchdir]
    puts "launch status: $launchstat"
#
# Note: launch uses the minimum number of jobs consistent with getting all
# tasks completed with a given jobsize.  Therefore, it might not use all the
# machines we said it could use.  We can get the actual number of machines
# used, which is very important, from the launch.info file.  This is somewhat
# counterintuitively called "maxjobs" it should be "actualjobs".
#
    getinfo launch.info maxjobs
    puts "Launched $maxjobs jobs"

    return [step_shepherd $stepname $dimension $maxjobs $jobsize $trynum \
		$lfilename $tan $launchstat $number_tests $basemodelname]
}


#
# solar::step_shepherd -- private
#
# Purpose:  "Shepherd" parallel tasks, making sure they ultimately complete
#           successfully.  Uses "relaunch" to get all tasks launched OK.
#           Uses "finish" to finish, combined with modified cleanrecords
#           (modified to deal with lack of comma delimiting) to compile
#           all results for a single df.  Results are ultimately written
#           to the output file stepfor.out.
#
#           If this procedure fails to get everything completed, it recurses
#           back through step_launch to launch the remaining stragglers.
#
#-

proc step_shepherd {stepname dimension njobs jobsize trynum lfilename \
			  tan launchstat ntests basemodelname} {
    set start_time [clock seconds]
    set started 0
    set fraction_started 0
    set newtrynum [expr $trynum + 1]
#
# Wait until all jobs have started, or would be expected to be started
#
    puts "Waiting for most jobs to start..."

    set failure_time [expr 60 * 15]
    set initial_wait 10

    set ldirname [full_filename $stepname.[pid].$tan.$dimension.$trynum]
    set expected_total_time 0
    while {1} {
    	after [expr $initial_wait*1000]
	puts "Looking at $ldirname"
	set started [lindex [exec ls $ldirname | wc] 0]
	if {$started == $njobs} {
	    puts "All tasks started, breaking wait"
	    break
	}
	set current_time [clock seconds]
	if {$current_time - $start_time > $failure_time} {
#
# On complete failure, assume launch failed and recurse to do it
#
	    if {$started == 0} {
		puts "Launch failed completely, recursing to redo"
		set oldjobarray [lindex $launchstat 2]
		set oldjobnumber [lindex [split $oldjobarray .] 0]
		puts "Deleting old job array $oldjobnumber"
		catch {exec qdel -f $oldjobnumber}
		set oldfilename [full_filename $stepname.testmods.$trynum]
		set newname [full_filename $stepname.testmods.$newtrynum]
		file copy -force $oldfilename $newname
		set lfile [open $stepname.files.list a]
		puts $lfile $newname
		close $lfile
		return [step_launch $stepname $dimension $ntests \
			    $newtrynum $lfilename $basemodelname]
	    }
#
# On partial failure, this is probably a "slow start" caused by someone
# else taking some of the machines we thought we were going to get before we
# got them.
#
# But what does this means in terms of how long to set the relaunch wait?
# Initial guess: average of this long wait and zero wait
#
	    puts "$failure_time seconds exceeded, breaking wait"
	    set expected_total_time [expr $failure_time / 2]
	    break
	}
	set fraction_started [expr double($started) / $njobs]
	puts "[format %.4g [expr 100.0 * $fraction_started]]% of jobs now started"
	if {$expected_total_time == 0} {
	    if {$fraction_started >= 0.5} {
		puts "Entering second phase of wait"
		set overcompletion_factor 1.4
		set fraction_time [expr [clock seconds] - $start_time]
		set reciprocal_started [expr 1.0 / $fraction_started]
		set expected_total_time [expr round($fraction_time * \
							$overcompletion_factor * \
							$reciprocal_started)]
	    }
	} elseif {$current_time - $start_time > $expected_total_time} {
	    puts "Jobs were expected to be started by now, breaking wait"
	    break
	}
    }
    set relaunch_time $expected_total_time
    if {$relaunch_time == 0} {
	puts "Setting expected to actual time"
	set relaunch_time [expr [clock seconds] - $start_time]
    }
    set relaunch_time [expr $relaunch_time / 2]
    if {$relaunch_time < $initial_wait} {
	puts "Setting expected to initial time"
	set relaunch_time $initial_wait
    }
    puts "Wait time is $relaunch_time"
#
# Now, keep relaunching until everything is started, at total time intervals
# But only launch as allowed by margin
#
    puts "Looking for $ldirname"
    set started [lindex [exec ls $ldirname | wc] 0]
    puts "$started out of $njobs have started"
    if {$started < $njobs} {
	set all_started 0
	set large_count 0
	while {1} {
	    set max_relaunch [cpus_available_soft_margin]
	    puts "Maximum available cpus for relaunching is $max_relaunch"
	    set status [relaunch -y -max $max_relaunch]
	    set third_status [lindex $status 2]
	    set status [lindex $status 0]
	    puts "STATUS is $status"
	    if {$status == "All"} {break}
	    if {$third_status == "relaunched" && \
		    [is_integer $status] && $status > 500} {
		if {$large_count > 2} {
		    puts "Many large relaunches.  Something isn't good.  Wait 30 minutes."
		    set relaunch_time 1800
		} else {
		    puts "Extremely large relaunch.  Wait 5 minutes."
		    set relaunch_time 300
		}
		incr large_count
	    } else {
		puts "Waiting $relaunch_time seconds for all jobs to start"
	    }
	    waitdots $relaunch_time
	}
    }
#
# Gather results ("finish")
# If jobsize==1, we're done
# Otherwise, recheck at relaunch_time intervals
#   (turns out to be better than a big long predicted wait that is usually
#    way too long because start time is far longer than the time of one
#    iteration, there is really no good handle on incremental time)
#    Upon 97% of jobs fully completed, we quit.
#       (A fully completed job has all its tasks completed.)
#    When number of tasks completed in
#      last iteration drops below 5% of the average tasks completed
#      per iteration, we quit.  (Average does not count "first interation"
#      because that included many jobs with more than one complete.)

    set last_jobs_completed ""
    set initial_jobs_completed 0
    set initial_time 0
    set penultimate 0
    while {1} {
	puts [exec date]
	puts "Gathering results..."
	set remaining_time ""
	set gathering_begin [clock seconds]
	set status [finish -any -n]
	set gathering_time [expr [clock seconds] - $gathering_begin]
	puts "Result gathering time: $gathering_time seconds"
	set percent_fully_complete [lindex $status 6]
	set total_jobs [lindex $status 3]
	set status [lindex $status 0]
	puts "$status tasks done"
	if {$jobsize==1} {break}
	if {$status == "All"} {
	    puts "Finish reports all jobs done, breaking wait"
	    break
	}
	if {$percent_fully_complete > 98} {
	    puts "More than 98% of jobs completed fully, breaking wait"
	    break
	}
	if {![is_integer $status]} {continue}
	set percent_complete [expr 100*double($status)/$total_jobs]
	set remaining_time ""
	if {$last_jobs_completed == {}} {
	    set first_tcpi $status
	    set last_jobs_completed $status
	    set initial_jobs_completed $status
	    set initial_time [clock seconds]
	    set timestamps $initial_time
	    set countstamps $status
	    set pent_index -1    ;# pent_index is end-1	    
	} else {
	    set current_time [clock seconds]
	    lappend timestamps $current_time
	    lappend countstamps $status
	    incr pent_index
	    set latest_complete [expr $status - [lindex $countstamps $pent_index]]
	    puts "$latest_complete tasks completed in this iteration"
	    set average_tcps [expr (double($status) - $first_tcpi) / \
				  ($current_time - $initial_time)]
	    puts "About [format %.4g $percent_complete]% tasks completed"
	    puts "About [format %.4g $percent_fully_complete]% of jobs fully complete"
	    puts "Note: this counts some tasks which may be redundant"
	    puts "Average tasks per second: [format %.4g $average_tcps]"
#
# Compute tasks/sec for the most recent minute or more
#
	    set breakout 0
	    for {set i $pent_index} {$i > 0} {incr i -1} {
		set old_time [lindex $timestamps $i]
		if {$current_time - $old_time > 60} {
		    set old_count [lindex $countstamps $i]
		    set tasks_completed [expr $status - $old_count]
		    set time_elapsed [expr $current_time - $old_time]
		    set current_tcps [expr double($tasks_completed) / \
					  $time_elapsed]
		    puts "Current tasks per second: [format %.4g $current_tcps]"
		    if {$current_tcps < 0.02 * $average_tcps} {
			if {$penultimate > 0} {
			    puts "Rate below %2 of average on second attempt"
			    set breakout 1
			} else {
			    if {$average_tcps > 1.0/30} {
				puts "Rate below 2% of average, with high average"
				set breakout 1
			    } else {
				puts "Rate below 2% of average"
				puts "Trying once more after 60 seconds..."
				set penultimate 1
				waitdots 60
			    }
			}
		    } else {
			set penultimate 0
		    }
		    break
		}
	    }
	    if {$breakout} {
		puts "Breaking wait for results"
		break
	    }
	    set last_jobs_completed $status
	    set total_time [expr round($total_jobs * \
				       (double($current_time - $initial_time) / \
					($status - $initial_jobs_completed)))]
	    puts "Projected total time is $total_time seconds"
	    set remaining_time [expr round($total_time - \
					       ($current_time - $initial_time))]
	    puts "Projected remaining time is $remaining_time seconds"
	}
#
# If this is not a special case of being nearly done
# Determine time to wait before collecting results again
#
	if {$penultimate < 1} {
#
# Start with "relaunch_time" which is derived from the time it took to launch
# all the jobs and do 1 iteration and so therefore reflects grid load
#
# (This could all be replaced in a future revision simply based on predicted
#  completion time...)
#
	    set finish_wait $relaunch_time
#
# limit wait time to three minutes, the maximum ordinary start, to filter
# out slow starts that occur if machines are grabbed by others before we get
# them.
#
	    set max_task_wait_time 180
	    if {$finish_wait > $max_task_wait_time} {
		set finish_wait $max_task_wait_time
	    }
#
# But then, allow at least 2x as much time as it takes to gather results
#   so that most of the time, result gathering is not happening, because
#   gathering lowers efficiency of result production
#
	    if {$finish_wait < 2 * $gathering_time} {
		set finish_wait [expr 2 * $gathering_time]
	    }
#
# But do not allow more wait time than computed remaining time + 15 seconds
# OR, 20 seconds on the first go (maximum we are prepared to lose, since this
# might be nearly done already)
#
	    if {{} != $remaining_time} {
		set allow_time [expr $remaining_time + 15]
	    } else {
		set allow_time 20
	    }
	    if {$finish_wait > $allow_time} {
		set finish_wait $allow_time
	    }
	    puts "Waiting $finish_wait seconds for all jobs completed"
	    waitdots $finish_wait
	}
    }
#
# Remove duplicate and incomplete records
#   and append good records to temp copy of ultimate output file
#   accumulate new testmods file of models not yet done
#
    puts [exec date]
    puts "Testing results for completion..."
    exec [usort] -k 1,1n $ldirname.all >$ldirname.sort

    set infile [open $ldirname.sort]
    set nfilename [full_filename $stepname.testmods.$trynum]
    set nfile [open $nfilename]


    set newfilename [full_filename $stepname.testmods.$newtrynum]
    set newfile [open $newfilename w]  ;# this gets filled with undone models

    set outfile [open [full_filename $stepname.out.d$dimension.tmp] a]

    set lastnumber ""
    set testnumber ""
    set missing_count 0
    global STEP_recordsize
    while {-1 != [gets $infile line]} {
	if {"" == $line} {continue}
	if {[llength $line] != $STEP_recordsize} {continue}
	set number [lindex $line 0]
	if {0 == [string compare $number $lastnumber]} {
	    puts "removing duplicate record for $number"
	    continue
	}
	set lastnumber $number
	while {1} {
	    if {-1 == [gets $nfile testnumber]} {
		puts "ERROR getting model number to test"
		break
	    }
	    if {$testnumber == ""} {
		puts "ERROR testnumber is blank"
		continue
	    }
	    if {0==[string compare $number $testnumber]} {
		break
	    } else {
		puts $newfile $testnumber
		puts "Missing model $testnumber"
		incr missing_count
	    }
	}
	puts $outfile $line
    }
    while {-1 != [gets $nfile testnumber]} {
	if {$testnumber == ""} {continue}
	puts "Missing additional model $testnumber"
	puts $newfile $testnumber
	incr missing_count
    }

    close $infile
    close $outfile
    close $nfile
    close $newfile
#
# If nothing is missing, we're done
#
    if {$missing_count == 0} {
	puts "No missing names"
	puts [exec date]
	return OK
    }
#
# Recurse to handle new missing records
#
    set lfile [open $stepname.files.list a]
    puts $lfile [full_filename $stepname.testmods.$newtrynum]
    close $lfile
    puts "Now recursing to handle missing records"
    return [step_launch $stepname $dimension $missing_count $newtrynum $lfilename $basemodelname]
}

proc stepfor_step {directory tasksize taskno trynum} {

    newtcl
    set outputfile $directory/$taskno
    file delete $outputfile
    set covlist [listfile stepfor.covlist]
    set othervars [listfile stepfor.vars]
    set nullindex [lindex $othervars 0]
    set basemodelname [lindex $othervars 1]
    set tmodfile [open stepfor.testmods.$trynum]

# Note: taskno starts with 1, but first_task_num is an index that starts with 0
    set first_task_num [expr ($taskno-1)*$tasksize]
#    set last_task_num [expr $first_task_num + $tasksize - 1]

    for {set i 0} {$i < $first_task_num} {incr i} {
	gets $tmodfile line
    }
    set testmods {}
    for {set i 0} {$i < $tasksize} {incr i} {
	gets $tmodfile line
	lappend testmods $line
    }
    close $tmodfile
#
# Maximize each model and write results
#
    foreach covnumber $testmods {
	puts "covnumber is $covnumber"
	set use_covar [lindex $covlist [expr $covnumber - 1]]
	set modelname "$covnumber"
	set bestlike ""
	catch {file delete stepfor.step.best.mod}
	after 500
	for {set maxcount 0} {$maxcount < 3} {incr maxcount} {
	    set did_maximize 0
	    set did_max_count 0
	    while {$did_maximize == 0 && $did_max_count < 10} {
		incr did_max_count
		catch {
		    puts "Evaluating model $modelname number $maxcount"
		    load model stepfor.null$nullindex
		    set nullike [loglike]
		    covariate $use_covar
		    maximize
		    set newlike [loglike]
		    if {$bestlike == "" || $newlike > $bestlike} {
			puts "[loglike] is best so far"
			save model stepfor.step.best
			set bestlike $newlike
		    }
		    set did_maximize 1
		}
	    }
	}
	puts "loading best model"
	load model stepfor.step.best

	set chisq [expr 2.0 * ([loglike] - $nullike)]

	if {$chisq >= 0} {
	    set pval [chi -number $chisq 1]
	} else {
	    set pval 1
	}
	set fchisq [format %.4f $chisq]

	if {-1 != [set epos [string first e $pval]]} {
	    set mstring [string range $pval 0 5]
	    set estring [string range $pval $epos end]
	    set pval "$mstring$estring"
	}
	puts "Writing to $outputfile"
	set ofile [open $outputfile a]
	puts "formattion oline"
	set oline [fformat "%24s  %.3f  %-19s %9s  %s" \
			 $modelname [loglike] $use_covar $fchisq $pval]
	puts "got oline"
	puts $ofile $oline
	close $ofile
    }
}


# solar::stepup --
#
# Purpose: Covariate screening by Step Up algorithm, useful for QTN analysis
#
# Usage:   stepup [-list listfile] [-list list] [-verbose]
#                 [-fix listfile]  [-fix fixlist]
#                 [-cutoff cutoff] [-logn logn] [-finishlogn]
#                 [-symmetric] [-cornerdf df] [-par]
#                 [-parclean]
#
#          stepup is an fast version of bayesavg and may be used in
#          QTN analysis.
#
#          By default, stepup will test all covariates in the current
#          model one at a time, then add all the new covariate models
#          within the BIC cutoff to the window.  Then the window models are
#          subjected to another round of testing against all covariates,
#          and the process repeats until no more models are added to the
#          window.  Unlike bayesavg, this algorithm doesn't test all
#          possible models, just those that are derived from those in
#          the window.  When completed, it writes files named stepup.win
#          and stepup.avg to the output directgory containing posterior
#          probabilities for the window models and components.
#
#          To ensure that all models use the same sample, a new file named
#          fully_typed.out is created in the output directory which
#          defines a variable named "fully_typed" for each fully typed
#          individual.  This file is added to the list of open phenotypes
#          files, and the variable "fully_typed" is added to the
#          model as a "null" covariate which has no effect on the model
#          other than restricting the sample to fully typed individuals.
#          This covariate is removed from the final best model stepup.best,
#          so you may get a different likelihood in subsequent maximization.
#
#          Up to dimension 3, all models with BIC better than the null model
#          are retained.  (This feature may be controlled with the -cornerdf
#          option.)  Also, the default "strict" rule is only applied to
#          remove apparently redundant higher dimensional models at the
#          very end after all important dimensions have been scanned.
#
#          -list listfile    listfile is a file containing a list of all
#                            covariates to be tested, one on each line.
#                            The filename cannot contain spaces.  These
#                            covariates may or may not be in the model when
#                            the command is given.  If the -list option is
#                            specified, all other covariates in the starting
#                            model are automatically fixed.
#          -list list        Alternatively, a Tcl list of covariates to
#                            be tested can be specified.  Tcl lists are
#                            space delimited and enclosed in quotes or curly
#                            braces.
#
#          -fix list         list is a Tcl list of covariates to be
#                            included in every model and not tested.  Their
#                            values will be estimated by maximum likelihood
#                            for every model, unless you constrain them.
#                            These covariates may or may not in the model
#                            when the command is given.  For -fix, a list
#                            could be simply one phenotype, and that
#                            supercedes a file with the same name.
#          -fix listfile     Alternatively, a file containing a list of all
#                            covariates to be included in every model may
#                            be specified.  The filename cannot contain
#                            spaces.  The list of covariates to be fixed
#                            will supercede the list of covariates to be
#                            tested if the same covariate occurs on both
#                            lists, however a warning will be given.
#
#          -cutoff cutoff    Set the final BIC cutoff.  The default is 6.
#
#          -logn logn        Use this fixed value for log(N) from the
#                            beginning.
#
#          -finishlogn logn  Recompute results of previous analysis with this
#                            log(N) value.  Sometimes stepup fails at the end
#                            because the standard error of the SE parameter
#                            of the best BIC model cannot be computed, and
#                            that is needed to compute the final log(N).
#                            This option allows you to finish such a run that
#                            nearly completed previously.  Be sure that
#                            starting conditions (such as loaded pedigree,
#                            phenotypes, model, outdir) and options are
#                            exactly the same as before.  The original startup
#                            (stepup.orig.mod) and null models from the output
#                            directory will be loaded.  Note that the temporary
#                            log(N) used by stepup by default is simply the
#                            log of the sample size, and this is reported
#                            to the stepup.history file.  You may choose to
#                            use that or some other estimate.  A special file
#                            required is stepup.winmods.prelim, which was
#                            produced by the previous incompleted run of
#                            stepup.
#
#          -verbose          Show maximization output during maximizations.
#          -v                Same as -verbose
#
#          -cornerdf df      EXPERIMENTAL.  This sets the last degree of
#                            freedom that uses a loose test to include models
#                            in the window.  Models need only have a better
#                            BIC than the null model up to and including
#                            this df.  The default is 3.
#
#          -symmetric        Apply symmetric rule rather than strict.  This
#                            results in a larger window.
#
#          -par              This option turns on Parallel
#                            processing on the SFBR GCC Compute Ranch.
#                            WARNING!  Do not run more than one instance of
#                            stepup -par from the same working directory.
#                            Parallel stepup will use many (but not all!) ranch
#                            machines, and access for other users and jobs may
#                            be delayed due to gridware thrashing.  The usual
#                            output is not printed to the terminal to save time
#                            but numerous parallel status messages are printed
#                            to help the developers make this program better.
#                            The parallel operation is automatic and the
#                            parallel status messages may be ignored by most
#                            users most of the time unless there is no output
#                            for more than fifteen minutes.  Note: If model
#                            includes linkage element matrices loaded from
#                            some mibddir, those matrices should be relocated
#                            to the working directory, or specified with an
#                            absolute pathname in the model file.  This is
#                            because in parallel operation the model is loaded
#                            not in the current working directory but in a
#                            subdirectory of /tmp.
#
#          -parclean         Normally, parallel stepup cleans up after itself.
#                            However, if it is necessary to force a shutdown
#                            of a parallel stepup, normal cleanup is not
#                            done.  "stepup -parclean" cleans up all the
#                            junk stepup files in /tmp directories on all
#                            ranch machines.  This must be run on medusa.  Do
#                            not run if you have any other running parallel
#                            jobs (parallel stepup, parallel bayesavg, or any
#                            parallel job using "launch" or "doscript") as
#                            their files may be deleted too.
#                            See also "doranch" for other ranch cleanup
#                            procedures.  Cleanup history is written to a file
#                            named cleantmp.out.
#                            
# -

# Object Design:
#   Domain  {<phenotype names>+}
#   ModelName  {(cov.i.j.k...n) toSet setTo}
#   Model    {<ModelName> loglike BIC}
#   Df       {<Model>+}
#   Window   {<Df>+ New Add Purge makeTests}

#
# Parallel wish list of additional suggested improvements:
#   Additional job collection run in case of 97% of jobs completed fully; one
#     more run might push it to 100% in some cases.
#   Do one-to-three job recursions with inline stepup_step.
#   Don't print missing models to screen.

proc stepup {args} {
#
# Do parallel cleanup if requested
#
    if {$args == "-parclean"} {
	set uname [lindex [exec who -m] 0]
	return [doranch cleantmp $uname.]
    }
    file delete [full_filename stepup.win]
    file delete [full_filename stepup.avg]
#
# read arguments
#
    set parallel 0
    set cornerdf 3
    set stepverbosity -q
    set excluded 0
    set symmetric 0
    set cutoff 6.0
    set p_cutoff 0.1
    set user_log_n 0
    set verbose 1
    set listfile {}
    set fixlist {}
    set ucovlist {}
    set finishlogn 0
    set stepuptest stepuptest    ;# the default test, not a typo
    set badargs [read_arglist $args \
		     -list listfile \
		     -fix fixlist \
		     -cutoff cutoff \
		     -logn user_log_n \
		     -verbose {set verbose 1; set stepverbosity ""} \
		     -v {set verbose 1; set stepverbosity ""} \
		     -par {set parallel 1} \
		     -symmetric {set symmetric 1} \
		     -cornerdf cornerdf \
		     -finishlogn finishlogn \
		     ]
    if {{} != $badargs} {
	error "stepup: Invalid argument $badargs"
    }
#
# Save initial model and delete old files
#
    if {$finishlogn} {
	set user_log_n $finishlogn
	load model [full_filename stepup.orig]
    } else {
	save model [full_filename stepup.orig]
	file delete [full_filename stepup.history]
	file delete [full_filename stepup.out]
	set modfiles [glob -nocomplain [full_filename stepup.null*.mod]]
	foreach modfile $modfiles {
	    file delete $modfile
	}
    }
#
# Get list of covariates to test
#
    if {{} != $listfile} {
	if {-1 == [string first " " $listfile]} {
	    set covlist [listfile $listfile]
	} else {
	    set covlist $listfile
	}
	set ucovlist $covlist
    } else {
	set covlist [covar]
    }
#
# Because a one element fixlist is possible
# If one element is specified, first check if it is a phenotype
#	
    if {{} != $fixlist} {
	if {-1 == [string first " " $fixlist]} {
	    if {-1 == [lsearch -exact [concat sex [lrange [phenotypes] \
						       1 end]] $fixlist]} {
		if {![file exists $fixlist]} {
		    error "stepup: No phenotype or file named $fixlist"
		}
		set fixlist [listfile $fixlist]
	    }
	}
    }
#
# Be sure fully_typed() is incorporated into fixlist at least once
# and only once
#
    if {-1 == [lsearch -exact $fixlist fully_typed()]} {
	lappend fixlist fully_typed()
    }
#
# Now remove fixed covariates from list of covariates to test
#
    foreach fix $fixlist {
	if {-1 != [set foundpos [lsearch -exact $covlist $fix]]} {
	    set covlist [lreplace $covlist $foundpos $foundpos]
	    if {{} != $ucovlist} {
		if {-1 == [lsearch -exact $ucovlist $fix]} {
		    putsout stepup.history "Warning.  Removing $fix from test list because fixed"
		}
	    }
	}
    }
#
# Show list of covariates
    set n [llength $covlist]
    set printlist $covlist
    set maxline 78
    while {{} != $printlist} {
	set line "    [lindex $printlist 0]"
	set printlist [lrange $printlist 1 end]
	while {{} != $printlist} {
	    set next [lindex $printlist 0]
	    if {$maxline < [expr [string length $line]+2+[string length $next]\
			       ]} {
		break
	    }
	    set line "$line $next"
	    set printlist [lrange $printlist 1 end]
	}
	putsout stepup.history $line
    }
    putsout stepup.history "\n    *** Testing $n covariates"
#
# Add fixed covariates (if not already added) except for fully_typed
#   (fully_typed gets added later)
#
    foreach cov $fixlist {
	if {$cov != "fully_typed()"} {
	    covariate $cov
	}
    }
#
# Add listed covariates (if not already added) and then delete (later).
# This ensures covariates CAN be added, and that they are now deleted.
#
    foreach cov $covlist {
	covariate $cov
    }
#
# If no omega, make it polygenic
#
    if {[omega] == \
	    "omega = Use_polygenic_to_set_standard_model_parameterization"} {
	putsout "No predefined omega; defaulting to polygenic"
	polymod
    }
#
# Now that all covariates have been added to model
# create fully_typed.out file
#
    if {[catch {eval maximize $stepverbosity -sampledata} errmess]} {
	error "Unable to maximize model with covariates:\n$errmess"
    }

    if {![file exists [full_filename sampledata.out]]} {
	puts "\nError testing starting model, suspect misspelled snp names"
	catch maximize errmess
	error $errmess
    }
	
    set insample [solarfile open [full_filename sampledata.out]]
    solarfile $insample start_setup
    solarfile $insample setup id
    set outsample [open [full_filename fully_typed.out] w]
    puts $outsample "id,fully_typed"
    while {{} != [set record [solarfile $insample get]]} {
	set id [lindex $record 0]
	puts $outsample "$id,1"
    }
    solarfile $insample close
    close $outsample
#
# Removed fully_typed.out from old output directories
# and add in new fully_typed.out from current output directory
#
    set oldphenotypes [phenotype -files]
    set newphenotypes {}
    foreach oldphenotype $oldphenotypes {
	set phentail [file tail $oldphenotype]
	if {[string compare $phentail fully_typed.out]} {
	    lappend newphenotypes $oldphenotype
	}
    }
    lappend newphenotypes [full_filename fully_typed.out]
    eval load phenotypes $newphenotypes

    set oldcovariates [covariate]
    if {-1 == [lsearch -exact $oldcovariates fully_typed()]} {
	covar fully_typed()
    }
#
# OK, now delete the variable covariates
#
    foreach cov $covlist {
	covariate delete $cov
    }
#
# Maximize "null" model
#
    if {!$finishlogn} {
	puts "    *** Maximizing null model\n"
	eval maximize $stepverbosity -o cov.out
	save model [full_filename stepup.null0]
    } else {
	load model [full_filename stepup.null0]
    }
    set null0_like [loglike]
    set best_bic 0
    set samplesize [outfile_sample_size cov.out]
    putsout stepup.history "    *** Samplesize is $samplesize"
    set log_n [format %.7g [expr log($samplesize)]]
    if {$user_log_n} {
	set log_n $user_log_n
	putsout stepup.history "    *** User-specified log(N) is $log_n\n"
    }
    if {!$finishlogn} {
	putsout stepup.history "    *** Estimated log(N) is $log_n\n"
	putsout stepup.out [format "%30s   %s      %s" Model Loglikelihood BIC]
	set basemodelname cov
	putsout stepup.out [format "%30s     %.3f      %.4f" \
				$basemodelname [loglike] $best_bic]
    }
    
#
# Main loop
#
    global window
    set window [Window_New $null0_like]
    set parallel_infile ""
    set parallel_stepup_out ""
    if {!$finishlogn} {
    for {set testdf 1} {$testdf <= $n} {incr testdf} {
	if {$parallel_infile != ""} {
	    close $parallel_infile
	    set parallel_infile ""
	}
	if {$parallel_stepup_out != ""} {
	    close $parallel_stepup_out
	    set parallel_stepup_out ""
	}
	if {$parallel} {
	    puts [exec date]
	    puts "Making new tests..."
	    set trynum 1
	    set tfilename [full_filename stepup.testmods.$trynum]
	    set number_tests [Window_MakeTestFile $window $n $tfilename]
	    puts "Done at [exec date] with $number_tests tests"
	} else {
	    set newtests [Window_MakeTests $window $n]
	    set number_tests [llength $newtests]
	}
	set newdfwindow {}
	set starting_best_bic $best_bic

	set parallel_launch_done 0

	for {set testnumber 0} {$testnumber < $number_tests} {incr testnumber} {
#
# Parallel operation
#
	    if {$parallel} {
		if {!$parallel_launch_done} {
		    puts "Setting up for parallel launch"
		    set covfilename [full_filename stepup.covlist]
		    set covfile [open $covfilename w]
		    foreach covline $covlist {
			puts $covfile $covline
		    }
		    close $covfile
		    set poutfilename [full_filename stepup.out.d$testdf]
		    file delete -force $poutfilename
		    file delete -force $poutfilename.tmp
		    stepup_begin_launch $testdf $number_tests \
			$null0_like $log_n
#
# returns only when all results written to stepup.out.d$dimension
#
		    set parallel_launch_done 1
		    set parallel_infile [open $poutfilename]
		    set parallel_stepup_out [open [full_filename stepup.out] a]
		    puts "Writing dimension $testdf to stepup.out..."
		}
		gets $parallel_infile current_result
		set test [lindex $current_result 0]
		set loglike [lindex $current_result 1]
		set bic [lindex $current_result 2]

	    } else {
#
# If non-parallel operation
#   Get next test from list of tests
#   Make testmodel
#   Maximize
#   Set up modelname, loglike, and bic from first result
#
		set test [lindex $newtests $testnumber]
		load model [full_filename stepup.null0]
		set testcovs [name2set $test]
		foreach covnumber $testcovs {
		    covariate [lindex $covlist [expr $covnumber - 1]]
		}
		eval maximize $stepverbosity
		set loglike [format %.3f [loglike]]
		set bic [format %8.4f [bic_calculate [loglike] $null0_like \
				      [llength $testcovs] $log_n]]
	    }
#
# Process one result in either parallel or non-parallel mode
#
	    if {$testdf <= $cornerdf} {
		if {$bic <= 0} {
		    ifdebug puts "added to window"
		    lappend newdfwindow [list [lindex $test 0] $loglike $bic]
		}
	    } else {
		if {$bic <= $best_bic + $cutoff} {
		    lappend newdfwindow [list [lindex $test 0] $loglike $bic]
		}
	    }
	    if {$bic < $best_bic} {
		set best_bic $bic
	    }
	    if {!$parallel} {
		putsout stepup.out [format "%30s     %s    %s" \
					$test $loglike $bic]
	    } else {
		puts $parallel_stepup_out [format "%30s     %s    %s" \
					       $test $loglike $bic]
	    }
	}
	if {$parallel_infile != ""} {
	    close $parallel_infile
	    set parallel_infile ""
	}
	if {$parallel_stepup_out != ""} {
	    close $parallel_stepup_out
	    set parallel_stepup_out ""
	}
#
# Since best bic is a moving target, we must go through window again
# to apply last best bic cutoff
#
	if {$testdf > $cornerdf} {
	    set retested_window {}
	    set tested_models {}
	    foreach newmodel $newdfwindow {
		set bic [lindex $newmodel 2]
		if {$bic < $best_bic + $cutoff} {
		    lappend retested_window $newmodel
		    lappend tested_models [lindex $newmodel 0]
		}
	    }
	    set newdfwindow $retested_window
	}

	if {1 > [llength $newdfwindow]} {
	    putsout stepup.history "    *** No models of df $testdf added to window"
	    set early_exit 1
	    break
	}
#
# Add new models to window and purge
#
	set window [Window_Add $window $newdfwindow]
	if {$testdf > $cornerdf} {
	    if {$symmetric || $testdf <= $n} {
		set dosymmetric 1
	    } else {
		set dosymmetric 0
	    }
	    set window [Window_Purge $window [expr $best_bic+$cutoff] \
			    $dosymmetric]
	}
#
# Print models added to window
#
	set lastdf [lindex $window end]
	if {{} != $lastdf} {
	    if {$verbose} {
		putsout stepup.history "\n    *** Models added to window:"
		set modindex 0
		set lastindex [llength $lastdf]
		while {$modindex < $lastindex} {
		    set line "       "
		    set position 7
		    set oldline $line
		    set oldposition $position
		    while {$position < 78 && $modindex < $lastindex} {
			set oldline $line
			set oldposition $position
			set line "$line [lindex [lindex $lastdf $modindex] 0]"
			incr modindex
			set position [string length $line]
		    }
		    if {$position > 78 && [llength $line] > 1} {
			set line $oldline
			set position $oldposition
			incr modindex -1
		    }
		    putsout stepup.history $line
		}
		putsout stepup.history ""
	    }
	}
    }
#
# Save models for possible later -finishlogn option
#
	set winout [open [full_filename stepup.winmods.prelim] w]
	foreach win $window {
	    puts $winout $win
	}
	close $winout
    } else {
#
# Load previous models for -finishlogn option
#
	set window ""
	if {![file exists [full_filename stepup.winmods.prelim]]} {
	    error "Can't find file [full_filename stepup.winmods.prelim] for -finishlogn"
	}
	set winout [open [full_filename stepup.winmods.prelim]]
	while {-1 != [gets $winout line]} {
	    lappend window $line
	}
	close $winout
	set user_log_n $finishlogn
	putsout stepup.history "\nContinuing with window models from previous run..."
    }
#
# Now get sorted list of models in window
#
    set newbic 0
    global winmods
    set winmods [Window_Models $window]
#
# Recompute log_n based on model with best BIC
#
    set best_mod [lindex $winmods 0]
    set best_mod_name [lindex $best_mod 0]
    load model [full_filename stepup.null0]
    set testcovs [name2set $best_mod_name]
    foreach covnumber $testcovs {
	covariate [lindex $covlist [expr $covnumber - 1]]
    }
    option standerr 1
    eval maximize $stepverbosity
    save model [full_filename $best_mod_name]
    if {!$user_log_n} {
	if {[parameter sd se] == 0} {
	    putsout stepup.history "Couldn't get standard error of SE to compute log(n)"
	    putsout -q stepup.history \
		"Re-run stepup using -finishlogn $log_n or other estimate of log(n)"
	    load model [full_filename stepup.orig]
	    error "Re-run stepup using -finishlogn $log_n or other estimate of log(n)"
	}
	set sd [parameter sd =]
	set sdse [parameter sd se]
	set log_n [format %.9g [expr log ($sd*$sd/(2.0*$sdse*$sdse))]]
	putsout stepup.history "\n    *** log(n) calculated from $best_mod_name is $log_n"
#
# Recompute best BIC
#
	set newbic 1
    }
    if {$newbic || $finishlogn} {
	set best_bic [bic_calculate [loglike] $null0_like \
			  [llength $testcovs] $log_n]
	set newbic 1
    }
#
# Write results of window
#  (re-purge now forced to ensure final strict rule, if applicable
#   but most often there will be a new BIC now anyway)
#
    set window [Window_Purge \
		    $window [expr $best_bic+$cutoff] $symmetric $log_n]
    set winmods [Window_Models $window]
    if {$newbic} {
	puts "\n    *** Models in Window (stepup.win) with recomputed BIC:\n"
    } else {
	puts "\n    *** Models in Window (stepup.win):\n"
    }
    foreach mod $winmods {
	set modname [lindex $mod 0]
	set loglike [lindex $mod 1]
	set bic [lindex $mod 2]
	putsout stepup.win [format "%30s     %.3f     %8.4f" \
			      $modname $loglike $bic]
    }
    load model [full_filename $best_mod_name]
    save model [full_filename stepup.best]
    stepup_post $winmods $cutoff $covlist $stepverbosity $log_n $user_log_n
    load model [full_filename stepup.best]
    puts ""
    puts "    *** Averages written to [full_filename stepup.avg]"
    puts "    *** Window file is      [full_filename stepup.win]"
    puts "    *** Messages written to [full_filename stepup.history]"
    puts "    *** Model with best BIC loaded: $best_mod_name"
}

proc stepup_post {winmods cutoff covlist stepverbosity log_n user_log_n} {
#
#  Note: We need to use arcane i/p organization of variable names, because
#  without it we would be using parameter names, which could really
#  foul things up with special characters in them.  (Similar method
#  used in bayesavg_post).
#
#  Even with arrays, there is a similar problem with required internal
#  quotation.  Braces don't seem to work properly in that context.
#
#  This follows because SOLAR parameter names are not restricted to Tcl's
#  name rules.
#
# To fit heritabilities into set of parameters:
# index 1..n for covariates 1..n
# index 0 is h2r
# index -1 is h2q1, etc.
#
# Suffixes for pseudolists use integer or _integer: $p
#   psum$p       Parameter weighted sum
#   sesum$p      Standard Error sum
#   ppprob$p     Parameter posterior probability
#
# Other variables:
#
#   sxbics       Sum of exp(-1/2*BIC)
#   fpprob       Formatted posterior probablity (for models)
#   mpprob       Model posterior probability
#
# Determine range of indexes
#
    set lastindex [llength $covlist]
    set firstindex 1
    set h2rs ""
    if {[if_parameter_exists h2r]} {
	set firstindex 0
	set h2rs h2r
    }
    set numh2qs 0
    for {set i 0} {1} {incr i} {
	if {![if_parameter_exists h2q$i]} {break}
	set firstindex [expr 0 - $i]
	set h2rs "h2q$i $h2rs"
	incr numh2qs
    }
#
# Now initialize sums for parameters in this range
#
    for {set i $firstindex} {$i <= $lastindex} {incr i} {
	set p $i   ;# Make nice suffix
	if {$p < 0} {
	    set p [catenate _ [expr abs($i)]]
	}
	set psum$p 0.0
	set ppprob$p 0.0
	set sesum$p 0.0
    }
#
# Setup allparams...list of all parameter names
#
    set allparams $h2rs
    foreach cov $covlist {
	set allparams "$allparams b$cov"
    }
#
# Compute sxbics: sum of exp(-1/2*BIC)
#
    ifdebug puts "Summing exp(-1/2 * BIC)"
    set bestbic [lindex [lindex $winmods 0] 2]
    set sxbics 0.0
    set modcount 0
    set winnames ""
    foreach mod $winmods {
	set bic [lindex $mod 2]
	if {$bic - $bestbic <= $cutoff} {
	    set ebic [expr exp (-0.5 * $bic)]
	    set sxbics [expr $sxbics + $ebic]
	}
    }
#
# Compute PProbs for each model and write window file
#
    ifdebug puts "Computing model pprobs and writing window file"
    set wheadings "Model BIC PProb SNP_Flags"
    set wformats "%20s %11.4f %10s %[expr 2*$lastindex]s"
    set wexpressions {{$modname} {$modbic} {$fpprob} {$snpflags}}
    set wfile [resultfile -create [full_filename stepup.win] \
		   -headings $wheadings -formats $wformats \
		   -expressions $wexpressions]
    resultfile $wfile -header

    set pprobs {}  ;# why is this needed?
    set wincount 0
    set forced_cov0 0
    foreach mod $winmods {
	set modname [lindex $mod 0]
	set modbic [lindex $mod 2]
	if {$modname == "cov" && $modbic - $bestbic > $cutoff} {
	    ifdebug puts "Calculating pprob for forced null model"
	    set forced_cov0 1
	    set mpprob [expr 1.0 / ($sxbics + 1.0)]
	    set fpprob [format %10.6f $mpprob]
	    if {0.0001 >  $fpprob} {
		set fpprob [format %10.3g $mpprob]
	    }
	} else {
	    lappend winnames [lindex $mod 0]
	    incr wincount
	    ifdebug puts "Calculating mpprob the standard way"
	    set mpprob [expr exp(-0.5*$modbic) / $sxbics]
	    set fpprob [format %10.6f $mpprob]
	}
	set snpflags ""
	set snpset [name2set $modname]
	for {set j 1} {$j <= $lastindex} {incr j} {
	    if {-1 != [lsearch $snpset $j]} {
		set snpflags "$snpflags 1"
	    } else {
		set snpflags "$snpflags 0"
	    }
	}
	resultfile $wfile -write
#
# Now, re-maximize window models for standard errors and compute param sums
#
	ifdebug puts "Re-maximizing $modname"
	load model [full_filename stepup.null0]
	set testcovs [name2set $modname]
	set testparams $h2rs
	foreach cnum $testcovs {
	    set covname [lindex $covlist [expr $cnum - 1]]
	    covariate $covname
	    set testparams "$testparams b$covname"
	}
	option standerr 1
	eval maximize $stepverbosity
	save model [full_filename $modname]

	ifdebug puts "Computing parameter sums"
	if {$forced_cov0 && $modname == "cov"} continue
	for {set i $firstindex} {$i <= $lastindex} {incr i} {
	    set p $i   ;# Make nice suffix
	    if {$p < 0} {
		set p [catenate _ [expr 0 - $i]]
	    }
	    set param [lindex $allparams [expr $i + $numh2qs]]
	    ifdebug puts "  for Parameter $param"
	    if {$i < 0 || 0<=[lsearch -exact $testparams $param]} {
		ifdebug puts "    found parameter in $testparams"
		set value [parameter $param =]
		set weighted [expr $mpprob * $value]
		ifdebug puts \
		 "for $i adding value $value mpprob $mpprob weighted $weighted"
		set psum$p [eval expr \${psum$p} + $weighted]
#
# If element is "non-zero"
#   it adds to the posterior probability of that parameter
#
		if {abs($value) >= 1e-12} {
		    set ppprob$p [eval expr \${ppprob$p} + $mpprob]
		}
#
# Compute SE square sums
#
		set pse [parameter $param se]
		set variance [expr $pse * $pse]
		set sesum$p [eval expr \${sesum$p} + \
				 ($mpprob * ($variance + ($value*$value)))]
	    }
	}
    }
#
# Now compute actual SE values from sums
#
    ifdebug puts "Computing SE values"
    for {set i $firstindex} {$i <= $lastindex} {incr i} {
	set p $i
	if {$p < 0} {
	    set p [catenate _ [expr abs($i)]]
	}
	set emean2 [eval square \${psum$p}]
	ifdebug puts "for $i, emean2 is $emean2"
	ifdebug eval puts \"sesum$p is \${sesum$p}\"
	set sesum$p [expr sqrt ([eval expr \${sesum$p} - $emean2])]
    }
#
# Write stepup.avg
#

    set outfilename [full_filename stepup.avg]
    set soutfile [open $outfilename w]
    putstee $soutfile "    *** Number of Models in Window: $wincount"
    if {$user_log_n} {
	putstee $soutfile "    *** User-supplied log(n) is $user_log_n"
    } else {
	putstee $soutfile "    *** Computed log(n) is $log_n"
    }

    putstee $soutfile "    *** Window:  $winnames"
    putstee $soutfile ""
    close $soutfile

    set headings {Component Average {Std Error} Probability}
    set width_needed 32
    for {set i $firstindex} {$i < $lastindex} {incr i} {
	set component [lindex $covlist $i]
	if {$width_needed < [string length $component]} {
	    set width_needed [string length $component]
	}
    }
    set aformats "%[catenate $width_needed s] %-13.8g %-13.8g %-12.7g"

    set resultf [resultfile -append $outfilename -display \
	    -headings $headings \
	    -formats $aformats \
	    -expressions {$component $average $stderror $postprob}]
    resultfile $resultf -header

    for {set i $firstindex} {$i <= $lastindex} {incr i} {
	set p $i
	if {$i == 0} {
	    set name h2r
	} elseif {$i < 0} {
	    set p [catenate _ [expr abs($i)]]
	    set name h2q$[expr abs($i)]
	} else {
	    set name [lindex $covlist [expr $i - 1]]
	}
	if {$i == 0} {
	    set component H2r
	} elseif {$i < 0} {
	    set component H2q[expr 0 - $i]
	} else {
	    set component [lindex $covlist [expr $i-1]]
	}
	set average [eval expr \${psum$p}]
	set stderror [eval expr \${sesum$p}]
	set postprob [eval expr \${ppprob$p}]
	resultfile $resultf -write
    }
    return ""
}





proc Window_New {like0} {
    set newwindow [list [list [list cov $like0 0]]]
    return $newwindow
}

proc Window_Add {window newdfwindow} {
    return [lappend window $newdfwindow]
}

#
# Window_Purge will purge models below BIC cutoff
#   By design, null model is never purged, callers beware.
# If log_n is provided, BIC's will be recalculated
#
proc Window_Purge {window cutpoint symmetric {log_n 0}} {

    set excluded {}
    ifdebug puts "Window_Purge with cutpoint $cutpoint"
    set null0_like [lindex [lindex [lindex $window 0] 0] 1]
    set newwindow [Window_New $null0_like]
    set dfcnt [llength $window]
    for {set dfin 1} {$dfin < $dfcnt} {incr dfin} {
	set df [lindex $window $dfin]
	set newmodels {}
	foreach model $df {
	    set thisname [lindex $model 0]
	    set thislike [lindex $model 1]
	    set thiset [name2set $thisname]
	    ifdebug puts " testing window model $thisname"
	    if {$log_n} {
		set usedf [llength $thiset]
		set bic [bic_calculate $thislike $null0_like $usedf $log_n]
		ifdebug puts "  with new calculated BIC: $bic"
	    } else {
		set bic [lindex $model 2]
	    }
	    set strict 1
	    if {$bic > $cutpoint} {
		ifdebug puts "   Below cutoff...Removed"
	    } else {
		if {!$symmetric} {
		    ifdebug puts "   Strict Test..."

#
# Apply "strict" test
# Use newwindow because it will contain recalculated BICs, if any
#
		    if {$dfin > 1} {
			for {set tdfin 0} {$tdfin < $dfin} {incr tdfin} {
			    set testdf [lindex $newwindow $tdfin]
			    foreach testmod $testdf {
				ifdebug puts \
			      "    Comparing with [lindex $testmod 0]"
				set testbic [lindex $testmod 2]
				if {$testbic > $bic} {
				    ifdebug puts "     Better BIC!"
				} else {
				    set testname [lindex $testmod 0]
				    set testset [name2set $testname]
				    if {![subset $thiset $testset]} {
					ifdebug puts "     Set mismatch"
				    } else {
					lappend excluded [lindex $model 0]
					set strict 0
					break
				    }
				}
			    }
			    if {!$strict} {
				break
			    }
			}
		    }
		}
		if {$strict} {
		    lappend newmodels [list $thisname $thislike $bic]
		}
	    }
	}
	set newwindow [Window_Add $newwindow $newmodels]
    }
#
# Show list of models excluded by strict rule
#
    if {{} != $excluded} {
	set maxline 78
	putsout stepup.history "\n    *** Models excluded by strict rule:"
	set beyondindex [llength $excluded]
	set exindex 0
	while {$exindex < $beyondindex} {
	    set line "       "
	    set position 7
	    set oldline $line
	    set oldposition $position
	    while {$position < 78 && $exindex < $beyondindex} {
		set oldline $line
		set oldposition $position
		set line "$line [lindex $excluded $exindex]"
		incr exindex
		set position [string length $line]
	    }
	    if {$position > 78 && [llength $line] > 1} {
		set line $oldline
		set position $oldposition
		incr exindex -1
	    }
	    putsout stepup.history $line
	}
    }
    ifdebug puts $window
    return $newwindow
}

#
# Window_Models returns a single list of models in BIC sorted order
#   Assumption: Window_Purge was called earlier

proc Window_Models {window} {
    set mods {}
    foreach dfwindow $window {
	foreach dfmod $dfwindow {
	    lappend mods $dfmod
	}
    }
    set mods [lsort -index 2 -real $mods]
    return $mods
}

#
# Window_MakeTests returns list of "test model names"
# based on last df in window
#
proc Window_MakeTests {window ndf} {
    set tests {}
    set basedf [lindex $window end]
    foreach basemodel $basedf {
	set nullmodelname [lindex $basemodel 0]
	set basecovs [name2set $nullmodelname]
	for {set i 1} {$i <= $ndf} {incr i} {
	    if {-1 == [lsearch -exact $basecovs $i]} {
		set newcovs [concat $basecovs $i]
		set newmodelname [set2name $newcovs]
		setappend tests $newmodelname
	    }
	}
    }
    return $tests
}
proc Window_MakeTestFile {window ndf outfilename} {
    set tests {}
    set basedf [lindex $window end]
    set workfile [open $outfilename.work w]
    foreach basemodel $basedf {
	set nullmodelname [lindex $basemodel 0]
	set basecovs [name2set $nullmodelname]
	for {set i 1} {$i <= $ndf} {incr i} {
	    if {-1 == [lsearch -exact $basecovs $i]} {
		set newcovs [concat $basecovs $i]
		set newmodelname [set2name $newcovs]
		puts $workfile $newmodelname
	    }
	}
    }
    close $workfile
    if {[catch {eval exec [usort] \
		    $outfilename.work >$outfilename.sort}]} {
	puts "/var/tmp has inadequate space for sorting results!"
	global env
	puts "Trying to use $env(HOME)/tmp for sort scratchfile..."
	exec mkdir -p $env(HOME)/tmp
	eval exec [usort] -T $env(HOME)/tmp \
	    $outfilename.work >$outfilename.sort
    }
    exec uniq $outfilename.sort >$outfilename
    file delete $outfilename.work
    file delete $outfilename.sort
    set count [lindex [exec wc $outfilename] 0]
    return $count
}


proc set2name {covlist} {
    set name cov
    set covs [lsort -integer $covlist]
    foreach cov $covs {
	set name "$name.$cov"
    }
    return $name
}

proc name2set {name} {
    return [lrange [split $name .] 1 end]
}

proc bic_calculate {loglike null_loglike df log_n} {
    set lodp [lod -raw $loglike $null_loglike]
    set lambda [expr $lodp * 2 * log (10)]
    set BIC [expr ($df * $log_n) - $lambda]
    return $BIC
}

#
# Parallel support procedures:
#   stepup_begin_launch
#   stepup_launch_shepherd
#   stepup_step
#
# Files:
# outdir/stepup.covlist                    list of all test covariates
# outdir/stepup.vars                       other essential variables
# outdir/stepup.files.list                 list of all files required by p jobs
# outdir/stepup.testmods.<trynum>          all tests for this DF
# outdir/stepup.null0                      null and base model
# outdir/stepup.[pid].<tn>.$dimension.<retrynum>  launch output directory
#                                          <tn> is "take a number" never used
#                                             more than once.
# outdir/setpup.out.d$dimension            output file to be written
# outdir/stepup.launchfiles                file listing files required
#
# Files created or copied to working directory:
# stepup.files.list
#
# -

proc stepup_begin_launch {dimension number_tests null0_like log_n} {
#
# Clean out old files
#
    set oldfiles [glob -nocomplain [full_filename stepup.*.*.*.*]]
    foreach oldfile $oldfiles {
	catch {exec rm -rf $oldfile}
    }
#
    set trynum 1
    set predimension [expr $dimension - 1]
#
#       Make file with list of all files required
#         start with stuff generally needed by SOLAR models...
#
    set lfilename stepup.files.list
    set outprefix ""
    if {"/" == [string index $lfilename 0]} {
	error "For parallel stepup, outdir must be relative to working directory"
    }
    set lfile [open $lfilename w]
#
# Determine if matrices
#
    set all_matrixes [matrix]
    set length_all_matrixes [llength $all_matrixes]
    set matrix_names {}
    for {set imatrix 0} {$imatrix < $length_all_matrixes} {incr imatrix} {
	set iname [lindex $all_matrixes $imatrix]
	if {$iname == "matrix"} {
	    incr imatrix
	    set iname [lindex $all_matrixes $imatrix]
	    if {$iname == "load"} {
		incr imatrix
		set iname [lindex $all_matrixes $imatrix]
		if {$iname != ""} {
		    lappend matrix_names $iname
		}
	    }
	}
    }
#
# Add matrices and check for non-local matrices
#
    set matrix_names_needed {}
    foreach matrix_name $matrix_names {
	if {"/" != [string index $matrix_name 0]} {
	    if {-1 != [string first / $matrix_name]} {
		close $lfile
		puts "Model contains matrix $matrix_name"
		error "For parallel stepup, model matrices should be in working directory\nor specified with absolute pathname in the model file."
	    }
	    puts $lfile $matrix_name
	}
    }
    puts $lfile "pedindex.out"
    puts $lfile "pedindex.cde"
#
# One or more phenotypes files, with or without .cde file
#
    set pifile [open phenotypes.info]
    set pofile [open [full_filename phenotypes.info] w]
    while {-1 != [gets $pifile phenname]} {
	puts $lfile $phenname
	if {-1 == [string first / $phenname]} {
	    puts $pofile $phenname
	} else {
#
# In our phenotypes.info, reference non-wd phenfiles in wd
#   because doscript copies them there
#
	    puts $pofile [file tail $phenname]
	}
	if {[file exists [file rootname $phenname].cde]} {
	    puts $lfile  [file rootname $phenname].cde
	}
	if {[file exists [file rootname $phenname].CDE]} {
	    puts $lfile [file rootname $phenname].CDE
	}
	if {[file exists $phenname.cde]} {
	    puts $lfile $phenname.cde
	}
	if {[file exists $phenname.CDE]} {
	    puts $lfile $phenname.CDE
	}
    }
    close $pifile
    close $pofile
    puts $lfile [full_filename phenotypes.info]
#
# Write other variables to a file
#
    set vfilename [full_filename stepup.vars]
    set vfile [open $vfilename w]
    puts $vfile $null0_like
    puts $vfile $log_n
    close $vfile
    puts $lfile $vfilename
#
# Required files in outdir
#
    puts $lfile [full_filename stepup.null0.mod]
    puts $lfile [full_filename stepup.testmods.$trynum]
    puts $lfile [full_filename stepup.covlist]
    close $lfile
#
# Note: all files in lfile will be copied to /tmp working directory
#   in parallel tasks
#
# Continue launch in recursive routine which also handles relaunches
#
    stepup_launch $dimension $number_tests $trynum $lfilename
#
# Rename temp output file to final one
#
    file rename [full_filename stepup.out.d$dimension.tmp] \
	[full_filename stepup.out.d$dimension]
#
# Done
#
    return ""
}

proc use_outdir_if_available {args} {
    if {[catch {return [full_filename $args]}]} {
	return $args
    }
}


proc cpus_available {} {
    puts "Querying gridware..."
    while {0 != [catch { \
	     exec qstat -g c >[use_outdir_if_available stepup.launch.qstat] \
			 }]} {
	puts "Gridware swamped, trying again..."
	after 2000
    }
    set qsfile [open [use_outdir_if_available stepup.launch.qstat]]
    set availi 4

    gets $qsfile line
    gets $qsfile line
    gets $qsfile line
    set cpus_available [lindex $line $availi]
    puts "qstat -g c: $cpus_available"
    close $qsfile
    return $cpus_available
}

# solar::howmanyranch --
#
# Purpose: Show how many ranch machines a user is using
#
# Usage:  howmanyranch <userid>
#
# Notes:  See also whoranch.  Requires use of Grid Engine software.
# -

# solar::whoranch
#
# Purpose: Show how many ranch machines each ranch user is using
#
# Usage:   whoranch
#
# Notes:   See also howmanyranch.  Requires use of Grid Engine software.
# -

# solar::parallel -- private
#
# Purpose: EXPERIMENTAL parallel operation parameters
#
# Usage:   parallel [margin [<marginsize>]]
#          parallel [minjobsize [<minjobsize>]]
#          parallel [fraction [<fraction>]
#          parallel [ignore [<ignoreid> | <ignoreidlist>]]
#          parallel ignoreadd <ignoreid>
#
# When only the parameter (such as "margin") is named, the procedure
# returns the currently operational value, either from the default or
# previous user selection.  If a value is specified, that replaces the
# current value.  ignoreadd is a special argument used only to add to
# the existing ignore list.
#
# The defaults are programmed for a medium-to-low priority background task,
# however there is a first-come-first-served aspect also.
#
#           margin       the number of machines to be left over after a new
#                          parallel job is launched.  The default is 3500.
#
#           minjobsize   the minimum size of any job.  Often it is more efficient
#                          to have jobs larger than size 1.  The default is 4
#                          meaning that each job has to have at least 2 tasks
#                          unless there are fewer than 2 tasks total.
#
#           fraction     the fraction of available machines which can be used
#                           if the margin is exceeded.  The default is 0.35
#
#           ignore       list of user id's to ignore in evaluating machine
#                        availability, as these user's jobs are temporary
#                        or fallback jobs.  2nd argument can either be single
#                        userid or list of userid's enclosed in braces.
#
#           ignoreadd <newid>    Add <newid> to list of user id's to ignore.
# -

proc parallel {args} {
    global SOLAR_Parallel_Margin
    global SOLAR_Parallel_Fraction
    global SOLAR_Parallel_MinJobsize
    global SOLAR_Parallel_State
    global SOLAR_Parallel_Ignore

    if {$args == "margin"} {
	if {[if_global_exists SOLAR_Parallel_Margin]} {
	    return $SOLAR_Parallel_Margin
	}
	return 3500
    }
    if {"margin" == [lindex $args 0]} {
	set newmargin [lindex $args 1]
	if {![is_integer $newmargin]} {
	    error "margin must be an integer"
	}
	return [set SOLAR_Parallel_Margin $newmargin]
    }
    if {$args == "minjobsize"} {
	if {[if_global_exists SOLAR_Parallel_MinJobsize]} {
	    return $SOLAR_Parallel_MinJobsize
	}
	return 3
    }
    if {"minjobsize" == [lindex $args 0]} {
	set newminjobsize [lindex $args 1]
	if {![is_integer $newminjobsize]} {
	    error "minjobsize must be an integer"
	}
	return [set SOLAR_Parallel_MinJobsize $newminjobsize]
    }
    if {$args == "fraction"} {
	if {[if_global_exists SOLAR_Parallel_Fraction]} {
	    return $SOLAR_Parallel_Fraction
	}
	return 0.35
    }
    if {"fraction" == [lindex $args 0]} {
	set newfraction [lindex $args 1]
	if {![is_float $newfraction]} {
	    error "fraction must be a floating point number"
	}
	return [set SOLAR_Parallel_Fraction $newfraction]
    }
    if {$args == "ignore"} {
	if {[if_global_exists SOLAR_Parallel_Ignore]} {
	    return $SOLAR_Parallel_Ignore
	}
	return eugeneid
    }
    if {"ignore" == [lindex $args 0]} {
	set newignore [lindex $args 1]
	return [set SOLAR_Parallel_Ignore $newignore]
    }
    if {"ignoreadd" == [lindex $args 0]} {
	set newignore [lindex $args 1]
	return [set SOLAR_Parallel_Ignore [concat [parallel ignore] $newignore]]
    }
    error "Invalid argument to parallel"
}

# SOLAR::parallel-tips -- private
#
# Ways to increase machine usage from low priority defaults:
#   parallel margin 0    use all available machines
#   parallel fraction 1  use 100% of available machines
#   parallel ignore ...  ignore the machines used by these users
# -


#
# "margin" is soft
# If fewer than FRACTION (0.3) of the cpus are available after margin,
# We get FRACTION of the cpus instead
#
proc cpus_available_soft_margin {args} {
    set margin [parallel margin]
    set cpus_available [cpus_available]
    set ignore [parallel ignore]
    foreach ig $ignore {
	catch {
	    set thisig [howmanyranch $ig]
	    if {[is_integer $thisig]} {
		set cpus_available [expr $cpus_available + $thisig]
	    }
	}
    }
    set cpus_useable [expr $cpus_available - $margin]
    if {$cpus_useable < $cpus_available * [parallel fraction]} {
	set cpus_useable [expr $cpus_available * [parallel fraction]]
	if {$cpus_useable < 0} {
	    error "No cpus available for parallel operation"
	}
    }
    return [expr int($cpus_useable)]
}

proc take_a_number {} {
    global SOLAR_Take_A_Number
    if {![if_global_exists SOLAR_Take_A_Number]} {
	set SOLAR_Take_A_Number 0
    }
    incr SOLAR_Take_A_Number
    return $SOLAR_Take_A_Number
}

proc stepup_launch {dimension number_tests trynum lfilename} {
    puts "There are $number_tests tasks"
    puts "Determining number of jobs to launch..."
#
# Determine available machines
#
    set cpus_useable 0
    while {1} {
	set cpus_useable [cpus_available_soft_margin]
	if {$cpus_useable > 100 || $cpus_useable*20 > $number_tests} break
	puts "Too few machines now available, waiting for more..."
	waitdots 60
    }
#
# Now, determine how many we are actually going to use
#
    set maximum_cpus [expr 1 + ($number_tests-1) / [parallel minjobsize]]
#
# Avoid "one task" situation because it can lead to 100% noncompletion
#   which wastes a lot of time
#
    if {$maximum_cpus <= [parallel minjobsize]} {
	set maximum_cpus $number_tests
    }
    if {$maximum_cpus <= $cpus_useable} {
	set reserved_cpus $maximum_cpus
    } else {
	set reserved_cpus $cpus_useable
    }
#
# Clear launch working directory, if any
#
    set tan [take_a_number]
    set launchdir [full_filename stepup.[pid].$tan.$dimension.$trynum]
    exec rm -rf $launchdir
    exec mkdir $launchdir
#
# Get current program name
#
    global env
    set programpath $env(SOLAR_PROGRAM_NAME)
    set programname [file tail $programpath]
#
# Launch scripts
#
    set starting_task 1
    set solarname solar
    set jobsize [expr int (ceil ($number_tests / double($reserved_cpus)))]
    puts "precomputed maximum cpus for this dimension is $reserved_cpus"
    puts "precomputed jobsize is $jobsize"
    set launchstat [launch -proc stepup_step -extra_args \
			"$trynum" -filelist $lfilename \
			-n $number_tests -jobsize $jobsize \
			-outputdir $launchdir]
    puts "launch status: $launchstat"
#
# Note: launch uses the minimum number of jobs consistent with getting all
# tasks completed with a given jobsize.  Therefore, it might not use all the
# machines we said it could use.  We can get the actual number of machines
# used, which is very important, from the launch.info file.  This is somewhat
# counterintuitively called "maxjobs" it should be "actualjobs".
#
    getinfo launch.info maxjobs
    puts "Launched $maxjobs jobs"

    return [stepup_shepherd $dimension $maxjobs $jobsize $trynum \
		$lfilename $tan $launchstat $number_tests]
}

proc waitdots {waitsecs} {
    set dots [expr round($waitsecs / 10)]
    set extra [expr round ($waitsecs - 10*$dots)]
    while {$dots > 0} {
	after 10000
	puts -nonewline .
	flush stdout
	incr dots -1
    }
    if {$extra} {
	after [expr 1000 * $extra]
	puts -nonewline .
	flush stdout
    }
    puts ""
}

    


#
# solar::stepup_shepherd -- private
#
# Purpose:  "Shepherd" parallel tasks, making sure they ultimately complete
#           successfully.  Uses "relaunch" to get all tasks launched OK.
#           Uses "finish" to finish, combined with modified cleanrecords
#           (modified to deal with lack of comma delimiting) to compile
#           all results for a single df.  Results are ultimately written
#           to the output file stepup.out.
#
#           If this procedure fails to get everything completed, it recurses
#           back through step_launch to launch the remaining stragglers.
#
#-

proc stepup_shepherd {dimension njobs jobsize trynum lfilename \
			  tan launchstat ntests} {
    set start_time [clock seconds]
    set started 0
    set fraction_started 0
    set newtrynum [expr $trynum + 1]
#
# Wait until all jobs have started, or would be expected to be started
#
    puts "Waiting for most jobs to start..."

    set failure_time [expr 60 * 15]
    set initial_wait 10

    set ldirname [full_filename stepup.[pid].$tan.$dimension.$trynum]
    set expected_total_time 0
    while {1} {
    	after [expr $initial_wait*1000]
	puts "Looking at $ldirname"
	set started [lindex [exec ls $ldirname | wc] 0]
	if {$started == $njobs} {
	    puts "All tasks started, breaking wait"
	    break
	}
	set current_time [clock seconds]
	if {$current_time - $start_time > $failure_time} {
#
# On complete failure, assume launch failed and recurse to do it
#
	    if {$started == 0} {
		puts "Launch failed completely, recursing to redo"
		set oldjobarray [lindex $launchstat 2]
		set oldjobnumber [lindex [split $oldjobarray .] 0]
		puts "Deleting old job array $oldjobnumber"
		catch {exec qdel -f $oldjobnumber}
		set oldfilename [full_filename stepup.testmods.$trynum]
		set newname [full_filename stepup.testmods.$newtrynum]
		file copy -force $oldfilename $newname
		set lfile [open stepup.files.list a]
		puts $lfile $newname
		close $lfile
		return [stepup_launch $dimension $ntests \
			    $newtrynum $lfilename]
	    }
#
# On partial failure, this is probably a "slow start" caused by someone
# else taking some of the machines we thought we were going to get before we
# got them.
#
# But what does this means in terms of how long to set the relaunch wait?
# Initial guess: average of this long wait and zero wait
#
	    puts "$failure_time seconds exceeded, breaking wait"
	    set expected_total_time [expr $failure_time / 2]
	    break
	}
	set fraction_started [expr double($started) / $njobs]
	puts "[format %.4g [expr 100.0 * $fraction_started]]% of jobs now started"
	if {$expected_total_time == 0} {
	    if {$fraction_started >= 0.5} {
		puts "Entering second phase of wait"
		set overcompletion_factor 1.4
		set fraction_time [expr [clock seconds] - $start_time]
		set reciprocal_started [expr 1.0 / $fraction_started]
		set expected_total_time [expr round($fraction_time * \
							$overcompletion_factor * \
							$reciprocal_started)]
	    }
	} elseif {$current_time - $start_time > $expected_total_time} {
	    puts "Jobs were expected to be started by now, breaking wait"
	    break
	}
    }
    set relaunch_time $expected_total_time
    if {$relaunch_time == 0} {
	puts "Setting expected to actual time"
	set relaunch_time [expr [clock seconds] - $start_time]
    }
    set relaunch_time [expr $relaunch_time / 2]
    if {$relaunch_time < $initial_wait} {
	puts "Setting expected to initial time"
	set relaunch_time $initial_wait
    }
    puts "Wait time is $relaunch_time"
#
# Now, keep relaunching until everything is started, at total time intervals
# But only launch as allowed by margin
#
    puts "Looking for $ldirname"
    set started [lindex [exec ls $ldirname | wc] 0]
    puts "$started out of $njobs have started"
    if {$started < $njobs} {
	set all_started 0
	set large_count 0
	while {1} {
	    set max_relaunch [cpus_available_soft_margin]
	    puts "Maximum available cpus for relaunching is $max_relaunch"
	    set status [relaunch -y -max $max_relaunch]
	    set third_status [lindex $status 2]
	    set status [lindex $status 0]
	    puts "STATUS is $status"
	    if {$status == "All"} {break}
	    if {$third_status == "relaunched" && \
		    [is_integer $status] && $status > 500} {
		if {$large_count > 2} {
		    puts "Many large relaunches.  Something isn't good.  Wait 30 minutes."
		    set relaunch_time 1800
		} else {
		    puts "Extremely large relaunch.  Wait 5 minutes."
		    set relaunch_time 300
		}
		incr large_count
	    } else {
		puts "Waiting $relaunch_time seconds for all jobs to start"
	    }
	    waitdots $relaunch_time
	}
    }
#
# Gather results ("finish")
# If jobsize==1, we're done
# Otherwise, recheck at relaunch_time intervals
#   (turns out to be better than a big long predicted wait that is usually
#    way too long because start time is far longer than the time of one
#    iteration, there is really no good handle on incremental time)
#    Upon 97% of jobs fully completed, we quit.
#       (A fully completed job has all its tasks completed.)
#    When number of tasks completed in
#      last iteration drops below 5% of the average tasks completed
#      per iteration, we quit.  (Average does not count "first interation"
#      because that included many jobs with more than one complete.)

    set last_jobs_completed ""
    set initial_jobs_completed 0
    set initial_time 0
    set penultimate 0
    while {1} {
	puts [exec date]
	puts "Gathering results..."
	set remaining_time ""
	set gathering_begin [clock seconds]
	set status [finish -any -n]
	set gathering_time [expr [clock seconds] - $gathering_begin]
	puts "Result gathering time: $gathering_time seconds"
	set percent_fully_complete [lindex $status 6]
	set total_jobs [lindex $status 3]
	set status [lindex $status 0]
	puts "$status tasks done"
	if {$jobsize==1} {break}
	if {$status == "All"} {
	    puts "Finish reports all jobs done, breaking wait"
	    break
	}
	if {$percent_fully_complete > 98} {
	    puts "More than 98% of jobs completed fully, breaking wait"
	    break
	}
	if {![is_integer $status]} {continue}
	set percent_complete [expr 100*double($status)/$total_jobs]
	set remaining_time ""
	if {$last_jobs_completed == {}} {
	    set first_tcpi $status
	    set last_jobs_completed $status
	    set initial_jobs_completed $status
	    set initial_time [clock seconds]
	    set timestamps $initial_time
	    set countstamps $status
	    set pent_index -1    ;# pent_index is end-1	    
	} else {
	    set current_time [clock seconds]
	    lappend timestamps $current_time
	    lappend countstamps $status
	    incr pent_index
	    set latest_complete [expr $status - [lindex $countstamps $pent_index]]
	    puts "$latest_complete tasks completed in this iteration"
	    set average_tcps [expr (double($status) - $first_tcpi) / \
				  ($current_time - $initial_time)]
	    puts "About [format %.4g $percent_complete]% tasks completed"
	    puts "About [format %.4g $percent_fully_complete]% of jobs fully complete"
	    puts "Note: this counts some tasks which may be redundant"
	    puts "Average tasks per second: [format %.4g $average_tcps]"
#
# Compute tasks/sec for the most recent minute or more
#
	    set breakout 0
	    for {set i $pent_index} {$i > 0} {incr i -1} {
		set old_time [lindex $timestamps $i]
		if {$current_time - $old_time > 60} {
		    set old_count [lindex $countstamps $i]
		    set tasks_completed [expr $status - $old_count]
		    set time_elapsed [expr $current_time - $old_time]
		    set time_elapsed [highest 0.5 $time_elapsed]
		    set current_tcps [expr double($tasks_completed) / \
					  $time_elapsed]
		    puts "Current tasks per second: [format %.4g $current_tcps]"
		    if {$current_tcps < 0.02 * $average_tcps} {
			if {$penultimate > 0} {
			    puts "Rate below %2 of average on second attempt"
			    set breakout 1
			} else {
			    if {$average_tcps > 1.0/30} {
				puts "Rate below 2% of average, with high average"
				set breakout 1
			    } else {
				puts "Rate below 2% of average"
				puts "Trying once more after 60 seconds..."
				set penultimate 1
				waitdots 60
			    }
			}
		    } else {
			set penultimate 0
		    }
		    break
		}
	    }
	    if {$breakout} {
		puts "Breaking wait for results"
		break
	    }
	    set last_jobs_completed $status
	    set total_time [expr round($total_jobs * \
		(double($current_time - $initial_time) / \
	     [highest 0.5 [expr ($status - $initial_jobs_completed)]]))]
	    puts "Projected total time is $total_time seconds"
	    set remaining_time [expr round($total_time - \
					       ($current_time - $initial_time))]
	    puts "Projected remaining time is $remaining_time seconds"
	}
#
# If this is not a special case of being nearly done
# Determine time to wait before collecting results again
#
	if {$penultimate < 1} {
#
# Start with "relaunch_time" which is derived from the time it took to launch
# all the jobs and do 1 iteration and so therefore reflects grid load
#
# (This could all be replaced in a future revision simply based on predicted
#  completion time...)
#
	    set finish_wait $relaunch_time
#
# limit wait time to three minutes, the maximum ordinary start, to filter
# out slow starts that occur if machines are grabbed by others before we get
# them.
#
	    set max_task_wait_time 180
	    if {$finish_wait > $max_task_wait_time} {
		set finish_wait $max_task_wait_time
	    }
#
# But then, allow at least 2x as much time as it takes to gather results
#   so that most of the time, result gathering is not happening, because
#   gathering lowers efficiency of result production
#
	    if {$finish_wait < 2 * $gathering_time} {
		set finish_wait [expr 2 * $gathering_time]
	    }
#
# But do not allow more wait time than computed remaining time + 15 seconds
# OR, 20 seconds on the first go (maximum we are prepared to lose, since this
# might be nearly done already)
#
	    if {{} != $remaining_time} {
		set allow_time [expr $remaining_time + 15]
	    } else {
		set allow_time 20
	    }
	    if {$finish_wait > $allow_time} {
		set finish_wait $allow_time
	    }
	    puts "Waiting $finish_wait seconds for all jobs completed"
	    waitdots $finish_wait
	}
    }
#
# Remove duplicate and incomplete records
#   and append good records to temp copy of ultimate output file
#
    puts [exec date]
    puts "Testing results for completion..."
    exec [usort] -k 1,1 $ldirname.all >$ldirname.sort

    set infile [open $ldirname.sort]
    set nfile [open [full_filename stepup.testmods.$trynum]]

    set newfilename [full_filename stepup.testmods.$newtrynum]
    set newfile [open $newfilename w]

    set outfile [open [full_filename stepup.out.d$dimension.tmp] a]
    set lastname ""
    set testname ""
    set missing_count 0
    while {-1 != [gets $infile line]} {
	puts "got line $line"
	if {"" == $line} {continue}
	if {[llength $line] != 3} {continue}
	set name [lindex $line 0]
	puts "Testing name $name"
	if {0 == [string compare $name $lastname]} {
	    puts "removing duplicate record for $name"
	} else {
	    while {1} {
		if {-1 == [gets $nfile testname]} {
		    puts "ERROR getting model name to test"
		    break
		}
		puts "got testname $testname"
		set testname [lindex $testname 0]
		if {$testname == ""} {continue}
		if {0==[string compare $name $testname]} {
		    break
		} else {
		    puts $newfile $testname
		    puts "Missing model $testname"
		    incr missing_count
		    set testname ""
		}
	    }
#	    puts "  Write record for $name"
	    puts $outfile $line
	    set lastname $name
	}
    }
    while {-1 != [gets $nfile testname]} {
	if {$testname == ""} {continue}
	puts "Missing additional model $testname"
	puts $newfile $testname
    }

    close $infile
    close $outfile
    close $nfile
    close $newfile
#
# If nothing is missing, we're done
#
    if {$missing_count == 0} {
	puts "No missing names"
	puts [exec date]
	return OK
    }
#
# Recurse to handle new missing records
#
    set lfile [open stepup.files.list a]
    puts $lfile [full_filename stepup.testmods.$newtrynum]
    close $lfile
    puts "Now recursing to handle missing records"
    return [stepup_launch $dimension $missing_count $newtrynum $lfilename]
}

proc stepup_step {directory tasksize taskno trynum} {

    newtcl
    set outputfile $directory/$taskno
    file delete $outputfile
    set all_tests [listfile stepup.testmods.$trynum]
    set covlist [listfile stepup.covlist]
    set number_tests [llength $all_tests]
    set othervars [listfile stepup.vars]
    set null0_like [lindex $othervars 0]
    set log_n [lindex $othervars 1]
    
# Note: taskno starts with 1, but first_task_num is an index that starts with 0
    set first_task_num [expr ($taskno-1)*$tasksize]
    set last_task_num [expr $first_task_num + $tasksize - 1]
#
# Maximize each model and write results
#
    for {set task $first_task_num} {$task <= $last_task_num} {incr task} {
	set modelname [lindex $all_tests $task]
	puts "Evaluating model $modelname"
	load model stepup.null0
	set testcovs [name2set $modelname]
	foreach covnumber $testcovs {
	    covariate [lindex $covlist [expr $covnumber - 1]]
	}
	eval maximize
	set loglike [format %.3f [loglike]]
	set bic [format %8.4f [bic_calculate [loglike] $null0_like \
				   [llength $testcovs] $log_n]]
	puts "Writing to $outputfile"
	set ofile [open $outputfile a]
	puts $ofile "$modelname $loglike $bic"
	close $ofile
    }
}


# solar::doranch --
#
# Purpose:  execute a script on every ranch machine (usually for /tmp cleanup)
#
# DO NOT USE THIS FOR SUBMISSION OF REGULAR JOBS because it bypasses
# the Gridware queing system, which it must do for cleanup of ALL machines.
#
# MUST BE RUN ON MEDUSA (only medusa addresses all other ranch machines)
#
# See also "stepup -parclean" which uses doranch to cleanup junk created by
# forcing a "stepup -par" job to quit.
#
# Usage:    doranch <procname> <argument>
#
#           doranch cleanuser <username>   ;# delete ALL user's /tmp files on
#                                          ;# the ranch (Note: you can only
#                                          ;# delete files for which you have
#                                          ;# delete privilege, usually because
#                                          ;# of owning them.)
#
#           doranch finduser <username>    ;# find all my /tmp files on the
#                                          ;# ranch but do not delete them.
#                                          ;# Findings are written
#                                          ;# to finduser.out.  If -all is
#                                          ;# used, all users are shown.
#
#           doranch cleantmp <dirname>.    ;# same as "stepup -parclean"
#                                          ;# delete all /tmp/<dirname>.*
#                                          ;# files.  (parallel stepup dirs
#                                          ;# are prefixed with <dirname>
#                                          ;# followed by dot.
#
#           doranch findtmp <dirname>      ;# find all name* directories
#                                          ;# but do not delete them.  Findings
#                                          ;# are written to findtmp.out.
#
#           doranch cleanme now            ;# same as
#                                          ;# doranch cleantmp <username>
#
#           make_rhosts                    ;# make a new .rhosts file, or
#                                          ;# append to existing one to
#                                          ;# make it complete.  It may be
#                                          ;# useful to delete old .rhosts
#                                          ;# file first if it contains errors.
#
#           showspace                      ;# Return sorted list of /tmp
#                                          ;# storage used by all users
#                                          ;# in showspace.out.  Uses
#                                          ;# doranch finduser -all, unless
#                                          ;# existing finduser.out is found.
#
#
#           <procname> is the name of the procedure to be run on every
#                      machine.  procedures cleanuser, finduser, cleantmp,
#                      findtmp, and cleanme are provided, but user-written
#                      scripts could be used also.
#
#           <username> is the username.
#
#           cleantmp is a procedure that deletes all files and directories
#           in /tmp which match the specified prefix, after which a wildcard
#           * is assumed.  For example "cleantmp charlesp." would delete a
#           directory named "/tmp/charlesp.11019.2"
#
# Notes:  It is useful to run ranch jobs in subdirectories of the /tmp
#         directory to minimize network traffic.  Jobs should be designed to
#         cleanup after themselves in normal operation by deleting the
#         /tmp subdirectory that was used as a working directory.
#
#         However, even when jobs are designed to cleanup after themselves,
#         if the jobs do not run to completion, the cleanup code might never
#         be run.  This is especially true when a user or administrator
#         shuts down a large array job (such as "stepup -par") because of
#         a mistake or emergency.
#
#         That is when "doranch" may be useful.  The "cleanuser" procedure
#         deletes all files owned by the user in /tmp directories on
#         all ranch machines.  The "cleantmp" procedure deletes all files
#         and directories in /tmp prefixed by the cleantmp argument on all
#         ranch machines.
#
#         The doranch procedures listed above may be used in creating custom
#         cleanup options for other scripts.
#
#         Such an emergency cleanup option is already built into the stepup
#         command as option "-parclean".  That uses doranch and cleantmp
#         as shown above.  Authors of other parallel scripts for general
#         create similar script options tailored to the names of /tmp
#         subdirectories they use.
#
#         To see what the "finduser" script looks like, in order to write
#         something similar, use the solar command "showproc finduser".
#
#         All the doranch procedures write to a file named by the specified
#         procname, for example cleanuser writes to a file named cleanuser.out
#         for each file found.  Usually this has two columns, node name
#         and filename.  However, for "finduser" a middle column is added
#         which lists total diskspace used in kbytes.
#
#         Note that a valid .rhosts file is required for usage, and
#         the make_rhosts file will make one.  doranch will complain
#         if the .rhosts file is not present or incomplete.
#
#         If doranch reports failure in connecting to some hosts, it is
#         probably because the passwd and shadow files involved in userid
#         authentication have not been properly updated on those hosts.
#
#         If doranch reports failure in connecting to every ranch host, it
#         probably means that the .rhosts file is invalid, and you should then
#         delete the old .rhosts file and run make_rhosts.
#
#         If doranch hangs at a particular host, that machine is probably
#         down in some unusual way that is not known to gridware.
#         
#-


proc doranch {procname argument} {

    catch {file delete -force $procname.out}
    set hostlist [gethostlist]
    check_rhosts $hostlist
    global env
    set solarpath $env(SOLAR_PROGRAM_NAME)
    puts "Using solarpath $solarpath"
    foreach host $hostlist {
	puts "doing $host..."
	if {[catch {exec rsh $host "$solarpath $procname \
          $argument"}]} {
	    puts "    $host not accessible"
	}
    }
}


proc check_rhosts {hostlist} {
    global env
    set home $env(HOME)
    if {![file exists $home/.rhosts]} {
	error "No .rhosts file; use make_rhosts to create one"
    }
    set rhostlist [listfile $home/.rhosts]
    set nodelist {}
    foreach rhost $rhostlist {
	lappend nodelist [lindex $rhost 0]
    }
    lappend hostlist medusa
    lappend hostlist medusa-gw
    foreach host $hostlist {
	if {-1 == [lsearch -exact $nodelist $host.txbiomedgenetics.org]} {
	    error ".rhost file missing $host: use make_rhosts to update"
	}
    }
    return OK
}

proc getcondoruid {} {
    set infile [open .job.ad]
    while {-1 != [gets $infile line]} {
	if {"Owner" == [lindex $line 0]} {
	    set quotedname [lindex $line 2]
	    return $quotedname
	}
    }
    error "Condor Owner not found in .job.ad file"
}


proc make_rhosts {} {
    global env
    set home $env(HOME)
    set user [lindex [exec /usr/bin/who -m] 0]

    exec touch $home/.rhosts

# Get current .rhosts (if any)

    set oldrs [listfile $home/.rhosts]
    set oldhosts {}
    foreach oldr $oldrs {
	lappend oldhosts [list [lindex $oldr 0] [lindex $oldr 1]]
    }

# Get required high host number

    set hosts [gethostlist]
    set highhost 0
    foreach host $hosts {
	set hostnumber [string range $host 1 end]
	if {[is_integer $hostnumber]} {
	    if {$hostnumber > $highhost} {
		set highhost $hostnumber
	    }
	}
    }
    set highhost [expr $highhost + 500]

# First make sure .rhosts has medusa lines
    
    add_to_rhosts_if_needed medusa $user $oldhosts
    add_to_rhosts_if_needed medusa-gw $user $oldhosts

    for {set i 1} {$i < $highhost} {incr i} {
	add_to_rhosts_if_needed n$i $user $oldhosts
    }
    return OK
}    

proc add_to_rhosts_if_needed {nodename user oldhosts} {
    if {-1 == [lsearch -exact $oldhosts "$nodename.txbiomedgenetics.org $user"]} {
	add_to_rhosts $nodename $user
    }
}

proc add_to_rhosts {nodename user} {
    global env
    set home $env(HOME)
    exec echo "$nodename.txbiomedgenetics.org   $user" >>$home/.rhosts
}


proc cleantmp {name} {
    return [findtmp $name delete]
}


proc findtmp {name args} {
    global env
    set home $env(HOME)
    cd /tmp
    set nodename [exec uname -n]
    set alldirs [glob $name*]
    foreach dir $alldirs {
	if {"" != $args} {
	    catch {exec rm -rf /tmp/$dir}
	    set procname cleantmp
	} else {
	    set procname findtmp
	}
	exec echo "$nodename $dir" >>$home/$procname.out
    }
}


proc cleanuser {username} {
    return [finduser $username delete]
}


proc finduser {username args} {
    set procname finduser
    global env
    set home $env(HOME)
    cd /tmp
    set nodename [exec uname -n]
    set alldirs [glob *]
    foreach dir $alldirs {
	set fileuser ""
	catch {
	    set fulldir [exec ls -ld $dir]
	    set fileuser [lindex $fulldir 2]
	}
	if {$fileuser == $username} {
	    set nameandsize $dir
	    if {"delete" == $args} {
		catch {exec rm -rf /tmp/$dir}
		set procname cleanuser
	    } else {
		set nameandsize [exec du -sk $dir]
	    }
	    exec echo "$nodename $nameandsize" >>$home/$procname.out
	} elseif {$username == "-all"} {
	    set nameandsize [exec du -sk $dir]
	    exec echo "$fileuser $nodename $nameandsize" >>$home/$procname.out
	}
    }
}

# -f  run finduser
proc showspace {args} {
    exec rm -f showspace.out
    set finduser 0
    set badargs [read_arglist $args -f {set finduser 1}]
    if {$finduser || ![file exists finduser.out]} {
	exec rm -f finduser.out
	doranch finduser -all
    } else {
	puts "Using existing finduser file, use -f option to make new one"
    }
    exec [usort] finduser.out -k1,1 >sortuser.out
    after 1000
    set insort [open sortuser.out]
    set username ""
    set space 0
    set records 0
    while {-1 != [gets $insort line]} {
	if {[llength $line]} {
	    set this_username [lindex $line 0]
	    set this_space [lindex $line 2]
	    if {$this_username != $username} {
		if {""!=$username} {
		    lappend usernames [list $username $space]
		    incr records
		}
		set username $this_username
		set space $this_space
	    } else {
		set space [expr $this_space + $space]
	    }
	}
    }
    if {$space > 0} {
	lappend usernames [list $username $space]
	incr records
    }
    set sortnames [lsort -decreasing -integer -index 1 $usernames]
    foreach name $sortnames {
	putsout -d. showspace.out "[lindex $name 0]    [lindex $name 1]"
    }
    return ""
}


proc cleantest {} {
    global env
    set home $env(HOME)
    set ohm [file tail $home]
    exec rsh n10 mkdir /tmp/$ohm
    exec rsh n10 mkdir /tmp/$ohm/foobar
    exec rsh n10 touch /tmp/foobar
}


proc gethostlist {} {
    set hostlist {}
    exec qhost > qhost.data
    set hostdata [listfile qhost.data]
    set first 0
    foreach host $hostdata {
	if {$first < 2} {
	    incr first
	    continue
	}
	if {"-" != [lindex $host 3]} {
	    lappend hostlist [lindex $host 0]
	}
    }
    return $hostlist
}

proc cleanme {name args} {
    global env
    set home $env(HOME)
    cd /tmp
    set alldirs [glob *]
    foreach dir $alldirs {
	if {[file owned $dir]} {
	    catch {exec rm -rf /tmp/$dir}
	    exec echo "[exec uname -n] $dir" >>$home/cleanme.out
	}
    }
}

# solar::bayesavg --
#
# Purpose:  Perform bayesian oligogenic model averaging
#           on covariates or linkage components of the current model.
#
# Usage:    bayesavg [-cov[ariates]] [-ov[erwrite]] [-redo]
#                    [-max <max>] [-cutoff <cutoff>] [-symmetric]
#                    [-list <listfile>] [-fix [cov|param]]
#                    [-size_log_n] [-nose] [-old_log_n]
#                    [-sporadic] [-h2rf h2r_factor] [-saveall]
#                    [-qtn] [-stop] [-nostop]
#
#           bayesavg -r[estart]   ;# (see also -redo)
#
#   SPECIAL NOTE:  THE ALGORITHMS CHANGED in VERSION 1.7.3.  SEE NOTES 1-4.
#                  NUMERIC RESULTS MAY DIFFER FROM PREVIOUS VERSIONS.
#
#               -covariates (or -cov)  Perform bayesian model averaging
#                  on the covariates only.  (The default is to perform
#                  bayesian model averaging on the linkage elements.)
#
#               -overwrite (or -ov) means force overwrite of existing output
#                  files
#
#               -max  Only include this number of components, or fewer,
#                     at one time.  This reduces the number of models
#                     enormously, particularly for large N.
#
#               -list file contains a list of the elements to use.
#                     There is one line for each covariate or linkage
#                     parameter.  Remaining covariates or linkage parameters
#                     in the starting model are automatically fixed.
#                     Covariates need not be present in the starting model,
#                     but linkage parameters (and their matrices, etc.) must
#                     be included in the starting model.
#
#               -fix (or -f) fix (lock in) this covariate.  A fixed element
#                  covariate (specified by covariate name, e.g. "age") or 
#                  linkage element (by linkage parameter name, e.g. "h2q1")
#                  is carried through all models.  (Note: a -fix or -f 
#                  qualifier is required for each covariate to be fixed,
#                  for example:  -f age -f sex.)  When fixed elements are
#                  included, it is adviseable to run "polygenic" on the
#                  starting model first.
#                   
#               -cutoff (optional) sets the BIC limit for occam's window 
#                  (default: 6) 
#
#               -log_n specify the log(n) to use.  Normally this is first
#                  estimated from the samplesize of the unsaturated model,
#                  then recalculated from the standard deviation of the
#                  mean and it's standard error in the model with the best BIC.
#
#               -symmetric (or -sym) use "symmetric" Occam's window.
#                   The default is a "strict" Occam's window, which excludes
#                   superset models with higher BIC; symmetric Occam's window
#                   includes ALL models within BIC cutoff.
#
#               -stop   Stop when no models in the last group with the same
#                       size (degrees of freedom) have entered the window.
#                       (This is the default for -qtn.)
#
#               -nostop  Do not stop when no models in the last group with
#                        the same size have entered the window.  (Useful
#                        for overriding the default for -qtn.)  If -stop
#                        or -qtn is specified, however, the report if any
#                        models have entered the window is still given.
#
#               -restart (or -r) means restart previous bayesavg run that was
#                  terminated before completion.  This begins with the
#                  model after the last one in the output file.  Do not use
#                  -restart if last run completed.  When restarting, set
#                  the trait or outdir, then give the command "bayesavg
#                  -restart" with no other arguments.  The original model
#                  and other arguments are automatically recalled.
#                  Previous command arguments are read from
#                  bayesavg.command and the previous starting model is
#                  c.orig or cov.orig.  If you need to change anything, use
#                  the -redo option instead.  You will also need to use
#                  the -redo option if the first pass through all models
#                  completed, or if the bayesavg was started under
#                  a previous version of SOLAR.
#
#                -redo is a special form of restart that allows you to change
#                  some options.  Unlike -restart, -redo REQUIRES YOU TO
#                  SPECIFY ALL OPTIONS AND LOAD ORIGINAL STARTING MODEL.
#                  Only models not already found in the output file will be
#                  maximized.  
# 
#                  There are several cases where you must use -redo instead
#                  -restart:  (1) If you need to
#                  re-maximize models which had convergence problems
#                  previously (edit them out of bayesavg*.est file, change
#                  boundaries, then -redo).  (2) If previous bayesavg run
#                  completed but you want to try a different window cutoff or
#                  type.  (3) You deleted all files except the bayesavg.est
#                  file.  (4) You need to restart from a previous version of
#                  SOLAR.  Unlike -restart, with -redo you must set up the
#                  starting model and commands either as they were previously
#                  or with desired changes.  Since you must set up the
#                  original model EXACTLY, and specify options either EXACTLY
#                  as they were originall specified, or with the desired
#                  changes, you are advised to use this option carefully.
#                  It is a good idea to make a backup copy of the outdir
#                  first.
#
#               -saveall will force the saving of all models.  Normally only
#                  the models within Occam's window are saved.  (Note:
#                  models outside the window will not have standard errors.)
#
#               -size_log_n Use the log(n) estimated from sample size as the
#                           final log(n).  This bypasses the computation of
#                           log(n) from the S.E. of the SD parameter of the
#                           model with the best BIC.
#
#               -nose       Do not compute standard errors for any models
#                           (normally they are only computed for models
#                           in the window).  Unless you specify a particular
#                           -log_n, the log(n) estimated from sample size
#                           will be used (as with -size_log_n).
#
#               -old_log_n  This calculates log(n) the old fashioned way,
#                           using the saturated model for covariate analysis
#                           or the unsaturated model for linkage analysis.
#                           This option is provided for comparison with
#                           earlier releases, and may soon be removed.
#
#               -h2rf (optional) is used to set upper bound of H2r
#                  (default: 1.1)  See notes below.  Use of this option
#                  is now unnecessary because of automated boundary control.
#
#               -sporadic  This option is depricated.  Force all models
#                  to sporadic.  Valid only with -covariate.  Now you can
#                  accomplish the same thing by making the starting model
#                  sporadic.
#
#               -qtn   Quantitative Trait Nucleotide Analysis:
#                      A "covariate" analysis is done with "-stop" in effect.
#                      Covariates with name snp_* or hap_* are automatically
#                      included but other covariates are excluded.  A special
#                      "windowfile" named bayesavg_cov.win is also
#                      produced.  The -stop default can be overridden with
#                      -nostop.  To include all snps in the starting model,
#                      use the separate command "allsnp".
#
# Output:   In addition to the terminal display, the following files are
#           created (<outname> is "bayesavg" for linkage analysis or 
#           "bayesavg_cov" for covariate analysis):
#
#           <outname>.avg         Final averaged results
#           <outname>.out         Final BIC and other info for each model
#                                   (standard errors for models in window)
#           <outname>.history     History of analysis messages
#           <outname>.est         Estimated BIC for each model (pass 1)
#           <outname>.nose        Final BIC but no standard errors (pass 2)
#
#           Models are saved with "c" <prefix> for linkage analysis and "cov"
#           prefix for covariate analysis:
#
#           <prefix>0.mod         Unsaturated model, with standard errors
#           <prefix>1.mod         Model with element 1 (if saved)
#           <prefix>12.mod        Model with elements 1 and 2 (if saved)
#           <prefix>12_11.mod     Model with elements 1, 2, and 11.
#           <prefix>.orig.mod     Original user model when started
#           <prefix>.start.mod    Base model (unsaturated) before maximization
#           <prefix>.base.mod     Maximized base model
#
# Notes:    1)  bayesavg determines the number of variable (non-fixed)
#               elements and sets N automatically.  N and the number of
#               models are reported near the beginning.  A new algorithm
#               is used to determine all the element combinations; this
#               results in a more logical ordering in which the smallest
#               models are evaluated first.
#
#           2)  The first pass through all models is done with an approximate
#               log(n) computed from the sample size.  The resulting file
#               is bayesavg.est (or bayesavg_cov.est).  The final log(n) is
#               then computed from the model with the best BIC, and all
#               BIC's are recalculated with the resulting file being
#               bayesavg.nose (or bayesavg_cov.nose).  Then, standard
#               errors for only the models within Occam's window are
#               recalculated.  The resulting final output file is
#               bayesavg.out (or bayesavg_cov.out).  The output summary
#               averages are reported in bayesavg.avg (or
#               bayesavg_cov.avg).  This is a new algorithm designed to
#               save time (by only calculating standard errors from the
#               models in the window), be more robust, and give more
#               accurate results.  Results may differ somewhat from those
#               in earlier versions (prior to 1.7.3) of SOLAR.  Additional
#               history of the analysis (the messages starting with "***")
#               are saved in bayesavg.history (or bayesavg_cov.history).
#
#
#           3)  To permit special models (with household effects, epistasis,
#               etc.) to be handled, bayesavg no longer forces the starting
#               model to be sporadic first.  It merely maximizes the current
#               model, with all non-fixed elements removed, but with no
#               change(s) to the starting omega or constraints.
#               If the starting model cannot be maximized, the user is
#               advised to run "polygenic" first.  Running "polygenic"
#               first is probably a good idea in all -covariate cases,
#               particularly if there are non-fixed elements.
#
#           4)  Models are now "built-up" from the unsaturated model
#               rather than being "constrained down" from the saturated
#               model.  The unsaturated model itself is usually created
#               by "constraining down" the starting model.
#
#           5)  bayesavg may not support bivariate models.
# -

# John decided not to allow this option, so it is undocumented.
#
#               -sporadic_first  (For covariate analysis only.)  For each
#                  model, maximize sporadic first, then polygenic.  This
#                  will probably take longer on average (and is not compatible
#                  with special models having household effects or other
#                  special features), but may help handle some otherwise
#                  intractible models.
#
#
#   -excludenull now obsolescent...uninvokable in 1.7.3

proc bayesavg {args} {

# Ensure sort exists

    usort

# Check for parallel mode

    if {[if_global_exists PAR_jobs]} {
	return [eval parbayesavg $args]
    }
    if {[if_global_exists GRID_jobs]} {
	return [eval gridbayesavg $args]
    }

# Setup verbosity

    set verbose 0
    set qu -q

    ifverbplus set verbose 1
    ifverbplus set qu ""

    ifdebug set verbose 1

# Allow for old-Sun tail command

    if {"SunOS" == [exec uname]} {
	set head2 "-2"
	set tailn ""
    } else {
	set head2 "-n 2"
	set tailn "-n"
    }

# Initialize variables and default options

    set fix_list {}   ;# As specified; not included items automatically fixed
    set cutoff 6
    set h2r_factor 1.1
    set restart 0
    set overwrite 0
    set covar 0
    set covarlist {}
    set betalist {}
    set excludenull 0
    set saveall 0
    set savewindow 0
    set redo 0
    set rere 0
    set minBICmodel ""
    set last_minBICmodel ""
    set symmetric 0
    set use_sporadic 0
    set use_cov0 0
    set max_comb 0
    set list_file ""
    set minBIC 1e36
    set bad_results {}
    set max_e2 0.9999          ;# higher than this, and we use spormod
    set missing 0
    set no_se 0
    set winse 0
    set paramlist {}
    set sporadic_first 0
    set use_log_n {}
    set old_log_n 0
    set size_log_n 0
    set qtn 0
    global Solar_Bay_Qtn
    set Solar_Bay_Qtn 0
    set stop 0
    set nostop 0
    set maxcomb 2000000
    set maxcomb_increment 1000000

# Read arguments

    set extra_args [read_arglist $args -cutoff cutoff -h2rf h2r_factor \
	    -cov {set covar 1} -covariates {set covar 1} \
	    -qtn {set covar 1; set qtn 1; set stop 1; set Solar_Bay_Qtn 1} \
	    -stop {set stop 1} \
	    -sporadic {set covar 1; set use_sporadic 1} \
	    -spor {set covar 1; set use_sporadic 1} \
	    -restart {set restart 1} -r {set restart 1} \
	    -rere {set restart 1; set rere 1} \
	    -redo {set redo 1} \
	    -saveall {set saveall 1} \
	    -savewindow {set savewindow 1} \
	    -symmetric  {set symmetric 1} \
	    -sym  {set symmetric 1} \
	    -fix {lappend fix_list VALUE} \
	    -f {lappend fix_list VALUE} \
            -max max_comb \
            -list list_file \
	    -nose {set no_se 1} \
	    -winse {set winse 1} \
	    -size_log_n {set size_log_n 1} \
	    -log_n use_log_n \
	    -old_log_n {set old_log_n 1} \
	    -nostop {set nostop 1} \
	    -ov {set overwrite 1} -overwrite {set overwrite 1}]

    set fix_list [string tolower $fix_list]

# Check for invalid arguments

    if {[llength $extra_args]} {
	error "Invalid arguments $extra_args"
    }
    if {$restart && $redo} {
	error "Arguments -restart and -redo are incompatible"
    }
    if {$sporadic_first && $use_sporadic} {
	error "Arguments -sporadic_first and -sporadic (only) are incompatible"
    }

# Remember previous arguments if restarting

    if {$restart && !$rere} {
	if {[llength $args] > 1} {
	    error \
	       "Use 'bayesavg -restart' to restart; other arguments remembered"
	}
	set arguments ""
	set got_arguments 0
	if {[file exists [full_filename bayesavg.command]]} {
	    set cfile [open [full_filename bayesavg.command]]
	    if {-1 < [gets $cfile arguments]} {
		set got_arguments 1
	    }
	    close $cfile
	}
	if {!$got_arguments} {
	    error \
       "Unable to read previous arguments from [full_filename bayesavg.command]"
	}
	puts "    *** Restarting bayesavg with arguments: $arguments"
	return [eval bayesavg -rere $arguments]
    }
    if {$rere} {
	set overwrite 0
    }

# Setup names for covariate and non-covariate cases

    if {$covar} {
	set prefix cov
	set outname bayesavg_cov
	set basemodel cov.base
    } else {
	set prefix c
	set outname bayesavg
	set basemodel c.base
    }

# Now, if actually restarting or redoing, check for required files

    if {$restart || $redo} {
	if {![file exists [full_filename $outname.est]]} {
	    error "Unable to restart: $outname.est not saved"
	}
    }
    if {$restart} {
	if {![file exists [full_filename $prefix.orig.mod]]} {
	    error "Unable to restart: $prefix.orig.mod not saved"
	}
    }



# Check restart/overwrite status; Purge old files as appropriate

    if {$restart && $overwrite} {
	error "-restart and -overwrite arguments are incompatible"
    } elseif {!$overwrite && !$restart && !$redo} {
	if {0<[llength [bayesavg_purge -outname $outname -prefix $prefix \
		-testonly]]} {
	   error "Bayesavg output files exist.  Use -overwrite or -restart option."
	}
    } elseif {$overwrite} {
	bayesavg_purge -outname $outname -prefix $prefix
    }

# Save user's original model as ".orig" if not restarting
# If -restart, load previous ".orig" 

    if {$restart} {
	load model [full_filename $prefix.orig]
    } else {
	save model [full_filename $prefix.orig]
	exec echo $args >[full_filename bayesavg.command]
    }

# Turn off standard error and save model as ".start"

    option standerr 0
    save model [full_filename $prefix.start]

# Announce restart or redo

    if {$restart} {
	putsout $outname.history \
"\n    ********* Restarting bayesavg *********"
    }
    if {$redo} {
	putsout $outname.history \
"\n    ********* Redo bayesavg started *********"
    }


# ****************************************************************************
#     Setup for covariate case (covarlist, betalist, and N)
#
#        Note: fix_list is just what is specified by user.
#        It doesn't include automatically fixed items:
#        items not included in "-list" if list used, and constrained items.
# ****************************************************************************

    if {$covar} {

# If no -list,
# Setup covarlist and betalist with all but suspended covariates
# and covariates constrained to zero.
#
# Covariates constrained to non-zero values are automatically "fixed."

	if {{} == $list_file} {
	    set tcovarlist [covariates -applicable]
	    set tbetalist  [covariates -betanames]
	    for {set i 0} {$i < [llength $tcovarlist]} {incr i} {
		set tcovar [lindex $tcovarlist $i]
		set tbeta [lindex $tbetalist $i]
		set target [string tolower [string range $tcovar 0 3]]
		if {!$qtn || 0==[string compare "snp_" $target] || \
		    0==[string compare "hap_" $target]} {
		    if {-1 == [string first "Suspended\[" $tcovar]} {
			if {![is_constrained_to_zero $tbeta] && \
				![is_constrained_to_nonzero $tbeta]} {
			    lappend covarlist $tcovar
			    lappend betalist $tbeta
			}
		    }
		}
	    }

# If -list, we get covariate names from list
# Covariates are temporarily loaded into empty model to get their betanames
#   with the correct ordering.
# Then, listed covariates are actually added to model
# 
	} else {
	    set lfile [open $list_file r]
	    covariate delete_all
	    while {-1 != [gets $lfile newcov]} {
		covariate $newcov
	    }
	    close $lfile
	    set covarlist [covariates -applicable]
	    set betalist  [covariates -betanames]
	    load model [full_filename $prefix.start]
	    foreach ensure_covar $covarlist {
		covariate $ensure_covar
	    }
	    save model [full_filename $prefix.start]
	}

#  If there are fixed covariates, remove them from lists
#  Also, check to see that fixed covariates are actually in the model
#    If not, add them (ain't I nice)

	if {"" != $fix_list} {
	    set augmented_model 0
	    set cov_in_model_l [string tolower [covariates -applicable]]
	    foreach lfixed $fix_list {
#
# Remove fixed cov's from covarlist and betalist
# It's not important if they were there already or not
#
		for {set i 0} {$i < [llength $covarlist]} {incr i} {
		    set ucovar [lindex $covarlist $i]
		    set lcovar [string tolower $ucovar]
		    if {0==[string compare $lcovar $lfixed]} {
			set covarlist [lreplace $covarlist $i $i]
			set betalist [lreplace $betalist $i $i]
			break
		    }
		}
#
# Now, make sure fixed covariates are actually IN MODEL
#
		set found 0
		foreach cov_l $cov_in_model_l {
		    if {![string compare $lfixed $cov_l]} {
			set found 1
			break
		    }
		}
		if {!$found} {
		    covariate $fixed
		    putsout $outname.history \
                     "    *** Adding -fixed covariate $fixed to starting model"
		    set augmented_model 1
		}
	    }
	    if {$augmented_model} {
		save model [full_filename $prefix.start]
	    }
	}

# Report variable and fixed covariates

	putsout $outname.history "    *** Testing covariates: $covarlist"
	ifdebug puts "betalist is $betalist"
	set n [llength $covarlist]
    }

# ****************************************************************************
#     Setup for non-covariate case: paramlist and N
# ****************************************************************************

    if {!$covar} {
#
# If using -list...
#
	set paramlist {}
	if {{} != $list_file} {
	    set n 0
	    set lfile [open $list_file r]
	    while {-1 != [gets $lfile newparam]} {
		set newparam [string tolower [string trim $newparam]]
		if {-1 == [lsearch -exact $fix_list $newparam]} {
		    lappend paramlist $newparam
		    incr n
		}
	    }
	    close $lfile
#
# Not using list file, use all h2qi parameters, i=1,2,3...
#
	} else {
	    set ne [h2qcount]
	    set n 0
	    set paramlist {}
	    for {set i 1} {$i <= $ne} {incr i} {
		if {-1 == [lsearch -exact $fix_list h2q$i]} {
		    lappend paramlist H2q$i
		    incr n
		}
	    }
	}
#
# Make sure all fixed elements are present
#
	foreach lfixed $fix_list {
	    if {![if_parameter_exists $lfixed]} {
		error "Fixed parameter $lfixed not present in starting model"
	    }
	}
    }

# ****************************************************************************
#     Check and report N
# ****************************************************************************

    if {$n < 2} {
	if {$covar} {
	    error \
	      "Covariate averaging not possible with one or fewer covariates"
	} else {
	    puts "Use 'bayesavg -cov' if analyzing covariates"
	    error \
                "Linkage averaging not possible with one or fewer linkage elements"
	}
    }
    putsout $outname.history  "\n    *** N is $n"

# ****************************************************************************
#     Generate list of combinations (subsets?):   allcomb
# ****************************************************************************

    set allcomb {}
    if {$max_comb} {
	combinations $n -max $max_comb -list allcomb -stop $maxcomb
    } else {
	combinations $n -list allcomb -stop $maxcomb
    }
    set ncomb [llength $allcomb]
    if {$ncomb == $maxcomb} {
	putsout $outname.history "    *** Number of possible models is greater than $maxcomb"
	putsout $outname.history "    ***   (Additional models will be computed when and if needed)"
    } else {
	putsout $outname.history "    *** Number of models is [expr $ncomb + 1]\n"
    }

# ****************************************************************************
# Maximize base model (from which other models are built) and set samplesize
#   Base models include all fixed elements, but no variable ones
# ****************************************************************************

# Consider covariate case

    if {$covar} {
#
# See if basemodel already exists (if not restarting, it was deleted)
# Make sure it is sporadic if -sporadic option used
#
	set use_previous_basemodel 0
	if {$restart && [file exists [full_filename $basemodel.mod]] && \
		[file exists [full_filename $basemodel.out]]} {
	    if {$use_sporadic && ![oldsporadic $basemodel]} {
		error "-sporadic option not used in previous run"
	    }
	    set use_previous_basemodel 1
	}
#
# Suspend non-fixed elements from starting model, resave as cov.start
#
	if {!$use_previous_basemodel} {
	    set c_suspended 0
	    foreach current_covar $covarlist {
		covariate suspend $current_covar
		ifdebug puts \
			"Suspended covariate $current_covar for base model"
		set c_suspended 1
	    }
	    if {$c_suspended} {
		save model [full_filename cov.start]
	    }
#
# Force model to sporadic if -sporadic or -sporadic_first 
#   and not sporadic already
#
	    if {($sporadic_first || $use_sporadic) \
		    && ![oldsporadic cov.start]} {
		spormod
		save model [full_filename cov.start]
#
# If only sporadic_first, we maximize sporadic, then polygenic
#
		if {$sporadic_first} {
		    putsout $outname.history \
			    "    *** Maximizing unsaturated sporadic model"
		eval maximize $qu -o cov.spor.out
		putsout $outname.history \
		       "    *** Loglikelihood of sporadic model is [loglike]\n"
		    polymod
		}
	    }
#
# Maximize base model
#
	    putsout $outname.history \
		    "    *** Maximizing base model cov0 (unsaturated)"
	    
# If omega has not been defined, default to polygenic

	    if {-1 != [set foo [string first \
		   Use_polygenic_to_set_standard_model_parameterization \
				    [omega]]]} {
		puts "\n    *** Undefined omega in starting model, defaulting to polygenic\n"
		polymod
	    }

	    eval maximize $qu -o $basemodel.out
	    if {($max_e2 >= [parameter e2 =]) || $use_sporadic || \
		    [oldsporadic cov.start]} {
		save model [full_filename $basemodel]
		putsout $outname.history \
			"    *** Loglikelihood of cov0 is [loglike]\n"
	    } else {
		putsout $outname.history \
		"    *** H2r below threshold, maximize base model as sporadic"
		load model [full_filename cov.start]
		spormod
		eval maximize $qu -o $basemodel.out
		save model [full_filename $basemodel]
		putsout $outname.history \
			"    *** Loglikelihood of sporadic cov0 is [loglike]\n"
	    }
	}
    }
#
# Consider linkage case, create base model
#
    if {!$covar} {
	ifdebug puts "creating base model for linkage case"
#
# Constrain variable linkage elements to 0, resetting h2r
#
	foreach param $paramlist {
	    parameter h2r = [expr [parameter h2r =] + [parameter $param =]]
	    parameter $param = 0 lower -0.01
	    if {-1==[string first "-" $param]} {
		constraint $param = 0
	    } else {
		constraint <$param> = 0
	    }
	}
	parameter h2r upper [lowest 1.0 [highest [parameter h2r upper] \
		[expr $h2r_factor * [parameter h2r =]]]]
#
# See if basemodel already exists (if not restarting, it was deleted)
#
	set use_previous_basemodel 0
	if {$restart && [file exists [full_filename $basemodel.mod]] && \
		[file exists [full_filename $basemodel.out]]} {
	    putsout $outname.history "    *** Using previous $basemodel.mod"
	    set use_previous_basemodel 1
	}
#
# Maximize basemodel if not already found
#
	if {!$use_previous_basemodel} {
	    putsout $outname.history \
		    "    *** Maximizing base model c0 (unsaturated)"
	    set max_status [maximize_quietly [full_filename \
		    $basemodel.out]]
	    if {"" != $max_status} {
		error "Convergence error maximizing c0:  $max_status"
	    }
	    putsout $outname.history "    *** Loglikelihood of c0 is [loglike]\n"
	save model [full_filename $basemodel]
	}
    }

# ****************************************************************************
# If -old_log_n, set old_log_n now from fully saturated model (covariate)
#   or unsaturated model (linkage)
# ****************************************************************************

    if {$old_log_n} {
	load model [full_filename $basemodel]
	option standerr 1
	if {$covar} {
	    for {set i 0} {$i < $n} {incr i} {
		set covi [lindex $covarlist $i]
		covariate restore $covi
	    }
	    if {$sporadic_first} {
		spormod
		puts "    *** Maximizing saturated sporadic model"
		maximize -q -o $basemodel.spor.se.out
		polymod
	    }
	    puts "    *** Maximizing saturated model to calculate log(n)"
	    maximize -q -o $basemodel.se.out
#
# If linkage, we use unsaturated model just done, but re-maximize for S.E.
#
	} else {
	    puts \
           "    *** Maximizing unsaturated model with S.E. to calculate log(n)"
	    maximize -q -o $basemodel.se.out
	}
	set sd [parameter sd =]
	set sdse [parameter sd se]
	if {$sdse == 0} {
	    error "Attempt to use -old_log_n failed; can't get SE of SD"
	}
	set use_log_n [expr (log ($sd*$sd / (2*$sdse*$sdse)))]
    }

# ****************************************************************************
# Load basemodel, set samplesize and log_n_est
# ****************************************************************************

    load model [full_filename $basemodel]
    set samplesize [outfile_sample_size $basemodel.out]
    putsout $outname.history "    *** Samplesize is $samplesize"
    set log_n_est [expr log($samplesize)]
    if {{} != $use_log_n} {
	set log_n_est $use_log_n
	putsout $outname.history \
		"    *** log(n) specified as [format %.7f $log_n_est]\n"
    } elseif {$no_se || $size_log_n} {
	set use_log_n $log_n_est
	putsout $outname.history \
		"    *** log(n) computed from sample size is [format %.7f $log_n_est]\n"
    } elseif {[is_constrained_to_nonzero sd] || [is_constrained_to_zero sd]} {
	set use_log_n $log_n_est
	putsout $outname.history \
		"    *** SD is constrained so will use estimated log(n): [format %.7f $log_n_est]\n"
    } else {
	putsout $outname.history \
		"    *** Estimated log(n) is [format %.7f $log_n_est]\n"
    }
    set null_loglike [oldmodel $basemodel loglike]
#
# Save basemodel as c[ov]0 with suspended covariates now constrained instead
#
    suspend2constrain
    save model [full_filename [catenate $prefix 0]]

# ****************************************************************************
#   Setup output file using "resultfile" object
# ****************************************************************************

# Start with model, BIC, loglike

    set headings "Model BIC Loglike"
    set formats "%16s %11.4f %12.3f"
    set expressions {{$cname} {$BIC} {$c_loglike}}
#
# Add H2r and H2r SE
#
    lappend headings H2r
    lappend headings "H2r SE"
    set formats "$formats %-12.7g %-12.7g"
    lappend expressions "\[oldmodel \$cname h2r\]"
    lappend expressions "\[oldmodel \$cname h2r -se\]"
#
# If covariate analysis and h2q's are present, add them to the output
#
    if {$covar && [if_parameter_exists h2q1] && ![is_constrained_to_zero h2q1]} {
	for {set i 1} {[if_parameter_exists h2q$i]} {incr i} {
	    lappend headings H2q$i
	    lappend headings "H2q$i SE"
	    set formats "$formats %-12.7g %-12.7g"
	    lappend expressions "\[oldmodel \$cname h2q$i\]"
	    lappend expressions "\[oldmodel \$cname h2q$i -se\]"
	}
    }
#
# For covariate analysis, add all covariates
#   For linkage analysis, add all H2qi components
#
    for {set i 0} {$i < $n} {incr i} {
	if {$covar} {
	    set cov [lindex $covarlist $i]
	    set beta [lindex $betalist $i]
	    lappend headings $beta
	    lappend headings "$beta SE"
	    set formats "$formats %-13.8g %-13.8g"
	    lappend expressions "\[oldmodel \$cname $beta\]"
	    lappend expressions "\[oldmodel \$cname $beta -se\]"
	} else {
	    set vparam [lindex $paramlist $i]
	    lappend headings $vparam
	    lappend headings "$vparam SE"
	    set formats "$formats %-12.7g %-12.7g"
	    lappend expressions "\[oldmodel \$cname $vparam\]"
	    lappend expressions "\[oldmodel \$cname $vparam -se\]"
	}
    }
    set open_option -create
#
# For both restart and redo, must scan old output file first
#
    if {$restart} {
	set this_df 0
	set best_bic_in_df 0.0
	set models_in_this_df 1
	set early_exit 0
	set best_bic 0.0
	set best_model cov0
	set best_model_in_df cov0
	set combno 0
    }

    if {$restart || $redo} {
	delete_files_forcibly $outname.avg
	set endcomb [lindex $allcomb end]
	set endnum [lindex $endcomb 0]
	set endname [catenate $prefix $endnum]
	for {set cni [expr $endnum+1]} {$cni <= $n} {incr cni} {
	    set endname [catenate $endname _$cni]
	}
#	puts "Looking for $endname..."
	set icount [expr 5 + 2 * [llength $endcomb]]
	puts "    *** Each record must have $icount entries"
	if {!$redo && \
		![catch {read_bayes $endname $outname.est bic}]} {
	    error "Last model already done...you must use -redo"
	}
	set open_option -append
	set ofile [open [full_filename $outname.est] r]
	gets $ofile   ;# Get headings
	gets $ofile   ;# Get -------- under headings
	set last_model ""
	set number_models_read 0
	set first 1
	while {-1 != [gets $ofile lastline]} {
	    if {[llength $lastline] >= $icount} {
		incr number_models_read
		set last_model [lindex $lastline 0]
		set BIC [lindex $lastline 1]
		if {$BIC < $minBIC} {
		    set minBIC $BIC
		    set minBICmodel $last_model
		}
		if {$restart && !$first} {
		    incr combno
# Note: combno will be index for *next* combination
		}
		set first 0
		if {$stop && $restart} {
		    set df [llength [bayesavg_elements $last_model]]
#		    puts "Model $last_model has df $df and BIC $BIC"
		    if {$df != $this_df} {
			putsout $outname.history \
		"    *** Best BIC in degree $this_df is $best_bic_in_df for model $best_model_in_df"
			if {$models_in_this_df==0} {
			    putsout $outname.history \
                        "    *** No models with degree $this_df were in window"
			    if {!$nostop} {
				puts $outname.history \
				"    *** Exiting main loop by stop rule"
				set early_exit 1
				break
			    }
			}
			set this_df $df
			set models_in_this_df 0
			set best_bic_in_df $BIC
			set best_model_in_df $last_model
		    }
		    if {$BIC - $cutoff < $best_bic} {
			set models_in_this_df 1
		    }
		    if {$BIC < $best_bic} {
			set best_bic $BIC
			set best_model $last_model
		    }
		    if {$BIC < $best_bic_in_df} {
			set best_bic_in_df $BIC
			set best_model_in_df $last_model
		    }
		}
	    } else {
		close $ofile
		error "Incomplete ending record in output file: $outname.est"
	    }
	}
	if {$restart} {
	    if {$number_models_read >= 1+$ncomb} {
	error "Enough or more than enough models already...you must use -redo"
	    }
	    set should_be [lindex $allcomb [expr $combno - 1]]
	    set found_comb [bayesavg_elements $last_model]
	    if {[string compare $found_comb $should_be]} {
#		puts "found: $found_comb  should be: $should_be"
		error "Models missing or duplicated:  use -redo"
	    }
	}
	
	close $ofile
	if {"" == $last_model} {
	    error "Last bayesavg left empty output file: $outname.est"
	}
    }

    set resultf [resultfile $open_option [full_filename $outname.est] \
	    -headings $headings \
	    -formats $formats -expressions $expressions -display]
    if {$restart || $redo} {
	resultfile $resultf -header -displayonly
    } else {
	resultfile $resultf -header

# Output results for basemodel (currently loaded)

	set cname [catenate $prefix 0]
	set c_loglike [loglike]
	set BIC 0  ;# Yes, this one is easy !
	resultfile $resultf -write
    }

# if restarting and only unsaturated model in file, reset restart status now

    set end_restart 0
    set still_restarting 0
    if {$restart && 0 != [string compare $last_model [catenate $prefix 0]]} {
	set still_restarting 1
    }

# ********************************************************************
#       Evaluate each combination
# ********************************************************************

#
# Implement stopping rule.  combno initialized here unless restart.
#
    if {!$restart} {
	set this_df 0
	set models_in_this_df 1
	set early_exit 0
	set best_bic 0.0
	set best_bic_in_df 0.0
	set best_model $basemodel
	set best_model_in_df $basemodel
	set combno 0
    }

    set number_models_written 0
#
# Big loop begins here
#
    for {} {!$early_exit && ($combno < $ncomb)} {incr combno} {
	set output_needed 1
	set maximize_sporadic_first 0
	ifdebug puts "Doing combno $combno"
#
# If this is the last combno because of artificial limit, increase here
#
	if {$combno+1 == $maxcomb} {
	    set maxcomb [expr $maxcomb + $maxcomb_increment]
	    putsout $outname.history "    *** Recalculating combinations to $maxcomb"
	    set allcomb {}
	    if {$max_comb} {
		combinations $n -max $max_comb -list allcomb -stop $maxcomb
	    } else {
		combinations $n -list allcomb -stop $maxcomb
	    }
	    set ncomb [llength $allcomb]
	}
	    

#
# Get current combination, cname, and df
#
	set comb [lindex $allcomb $combno]
	set cname [catenate $prefix [lindex $comb 0]]
	for {set cni 1} {$cni < [llength $comb]} {incr cni} {
	    set cname [catenate $cname _[lindex $comb $cni]]
	}
	set df [llength $comb]
#
# Check for early exit
#
	if {$df != $this_df} {
	    putsout $outname.history \
		"    *** Best BIC in degree $this_df is $best_bic_in_df for model $best_model_in_df"
	    if {$stop && $models_in_this_df == 0} {
		putsout $outname.history \
		    "        *** No models with degree $this_df were in window"
		if {!$nostop} {
		    putsout $outname.history \
			    "        *** Exiting main loop by stop rule"
		    set early_exit 1
		    continue
		}
	    }
	}
#
# If redo, procede only if model not found in output file
#
	set use_existing_model 0
	if {$redo} {
	    if {![catch {set BIC [read_bayes $cname $outname.est bic]}]} {
		if {$df != $this_df} {
		    set this_df $df
		    set models_in_this_df 0
		    set best_bic_in_df $BIC
		    set best_model_in_df $cname
		}
		if {$BIC < $best_bic_in_df} {
		    set best_bic_in_df $BIC
		    set best_model_in_df $cname
		}
		if {$BIC - $cutoff < $best_bic} {
		    set models_in_this_df 1
		}
		if {$BIC < $best_bic} {
		    set best_bic $BIC
		}
		continue
	    }
	}
#
# Create model from base by restoring elements
#
	load model [full_filename $basemodel]
	if {$covar} {
	    for {set i 0} {$i < $n} {incr i} {
		set covi [lindex $covarlist $i]
		if {-1 != [lsearch $comb [expr $i + 1]]} {
		    covariate restore $covi
		}
	    }
	} else {
	    foreach ce $comb {
		set param [lindex $paramlist [expr $ce - 1]]
		if {-1==[string first "-" $param]} {
		    constraint delete $param
		} else {
		    constraint delete <$param>
		}
		carve_new_value $param 0.01 h2r
		parameter $param lower 0
	    }
	}
#
# Maximize and save results
#
#  covariate
#
	if {$covar} {
	    if {$sporadic_first} {
		spormod
		set error_msg [maximize_quietly last.out]
		if {"" == $error_msg} {
		    polymod
		    set error_msg [maximize_quietly last.out]
		}
	    } else {
		set error_msg [maximize_quietly last.out]
	    }
#
# linkage
#
	} else {  ;# linkage model
	    set error_msg [max_bayesavg last.out $comb]
	}
	if {0 < [llength $error_msg]} {
	    lappend bad_results $error_msg
	    if {[llength $bad_results] > 5} {
		bayesavg_bad_results $outname.est $bad_results 0
	    }
	    continue
	}
	suspend2constrain    ;# convert suspended cov's to constrained
	model save [full_filename $cname]
#
# For all models
#   Calculate BIC
#
	set save_this_one 0
	set c_loglike [oldmodel $cname loglike]
	set lodp [lod $c_loglike $null_loglike]
	set lambda [expr $lodp * 2 * log (10)]
	set BIC [expr ($df * $log_n_est) - $lambda]
	if {$BIC < $minBIC} {
	    set save_this_one 1
	    set minBIC $BIC
	    set last_minBICmodel $minBICmodel
	    set minBICmodel $cname
	}
	if {$output_needed} {
	    resultfile $resultf -write
	    incr number_models_written
	}
#
#  Delete this model if no longer needed or desired
#
        if {!$save_this_one && !$saveall} {
	    delete_files_forcibly [full_filename $cname.mod]
	}
#
# Update stuff for early stop rule
#
	if {$df != $this_df} {
	    set this_df $df
	    set models_in_this_df 0
	    set best_bic_in_df $BIC
	    set best_model_in_df $cname
	}
	if {$BIC < $best_bic_in_df} {
	    set best_bic_in_df $BIC
	    set best_model_in_df $cname
	}
	if {$BIC - $cutoff < $best_bic} {
	    set models_in_this_df 1
	}
	if {$BIC < $best_bic} {
	    set best_bic $BIC
	}

#
# Delete previous best if this is a new best
#
	if {!$saveall && $save_this_one == 1} {
	    if {"" != $last_minBICmodel && \
		    [catenate $prefix 0] != $last_minBICmodel} {
		delete_files_forcibly [full_filename $last_minBICmodel.mod]
	    }
	}
    }
    load model [full_filename $minBICmodel]
#
# Sort output file; lowest BIC first
#
    putsout $outname.history "\n    *** Sorting output file"
    if {0 != [string length [usort]]} {
	set fullname [full_filename $outname.est]
	if {0==[catch {eval exec head $head2 $fullname >$fullname.tmp}]} {
	    if {0==[catch {eval exec tail $tailn +3 $fullname | [usort] -n -k 2 \
		    >>$fullname.tmp}]} {
		if {0==[catch {eval file rename -force $fullname.tmp $fullname}]} {
		    if {[llength $bad_results] > 0} {
			bayesavg_bad_results $outname.est $bad_results 1
		    }
		    if {{} == $use_log_n} {
			return [bayesavg_rewrite $cutoff $n $outname \
				$covarlist $excludenull $symmetric $no_se \
				$null_loglike $log_n_est $basemodel \
				$paramlist $formats $headings $betalist \
				0]
		    } else {
#
# We had "exact" log_n provided...don't need to recalculate BIC's
#
			file copy -force [full_filename $outname.est] \
				[full_filename $outname.nose]
			return [bayesavg_post $cutoff $n $outname \
				$covarlist $excludenull $symmetric $no_se \
				$basemodel $paramlist $formats $headings \
				$betalist $minBICmodel]
		    }
		}
	    }
	}
    }
#
# Sorting error
#
    if {[llength $bad_results] > 0} {
	puts stderr "   *** Sort of $outname.est failed"
	bayesavg_bad_results $outname.est $bad_results 2
    }
    error "    *** Sort of $outname.est failed"
}


proc hack_mibd_index {modfilename old new} {
    exec sed s/mibd$old/mibd$new/g $modfilename > $modfilename.1
    exec sed s/h2q$old/h2q$new/g $modfilename.1 > $modfilename.2
    file rename -force $modfilename.2 $modfilename
    file delete $modfilename.1
    return ""
}

#
# bayesavg postprocessing part one...rewrite with recalculated BIC's
# using log_n from model with best BIC
#
# First, we need to rewrite bayesavg.est (or .tmp if depth > 0) to bayesavg.out
# using real log_n value from best BIC model

proc bayesavg_rewrite {cutoff maxindex outname covarlist excludenull \
	symmetric no_se null_loglike log_n_est basemodel paramlist formats \
        headings betalist depth} {
#
# Set input extension according to depth
#
    if {$depth == 0} {
	set ext est
    } else {
	set ext nose.tmp
    }
#
# Allow for old-Sun tail command
#
    if {"SunOS" == [exec uname]} {
	set head2 "-2"
	set tailn ""
    } else {
	set head2 "-n 2"
	set tailn "-n"
    }
#
# Build up resultfile for output
#
    set columns [llength $formats]
    set expressions {}
    for {set i 0} {$i < $columns} {incr i} {
	if {$i == 1} {
	    lappend expressions \$BIC
	} else {
	    lappend expressions "\[lindex \$inline $i\]"
	}
    }
    set outfile [resultfile -create [full_filename $outname.nose] \
	    -headings $headings -formats $formats -expressions $expressions]
    resultfile $outfile -header
#
# Start reading $ext file
#
    set null_loglike [format %12.3f $null_loglike]
    set infile [open [full_filename $outname.$ext] r]
    gets $infile inline
    gets $infile inline
    if {"----------" != [string range $inline 1 10]} {
	close $infile
	error "Error(s) in $outname.$ext"
    }
#
# The first post-header record is the best-bic model
#
    gets $infile inline
    set bestmodel [lindex $inline 0]
#
# Maximize best model with standard errors turned on
#
    putsout $outname.history "    *** Maximizing $bestmodel for standard errors"
    set saveiter [option maxiter]
    if {[file exists [full_filename $bestmodel.mod]]} {
	load model [full_filename $bestmodel]
	option maxiter 1
    } else {
#
# If model wasn't saved, we have to rebuild it
#
	puts "Rebuilding $bestmodel from $basemodel..."
	load model [full_filename $basemodel]
	set cfilename [file rootname [file tail $bestmodel]]
	set comb [bayesavg_elements $cfilename]
	if {{} != $covarlist} {
	    for {set i 0} {$i < $maxindex} {incr i} {
		if {-1 != [lsearch $comb [expr $i + 1]]} {
		    set covi [lindex $covarlist $i]
		    covariate restore $covi
		}
	    }
	} else {
	    foreach ce $comb {
		set param [lindex $paramlist [expr $ce - 1]]
		constraint delete $param
		carve_new_value $param 0.01 h2r
		parameter $param lower 0
	    }
	}
    }
    option standerr 1
    if {[catch {maximize -q -o $bestmodel}] || \
	[parameter sd se] == 0} {
	close $infile
        if {{} == $covarlist} {
	    load model [full_filename c.orig.mod]
	} else {
	    load model [full_filename cov.orig.mod]
	}
	error "Couldn't get standard error of SD; suggest -redo using -log_n [format %.7f $log_n_est]"
    } else {
	set sd [parameter sd =]
	set sdse [parameter sd se]
	set log_n [expr log ($sd*$sd / (2*$sdse*$sdse))]
	putsout $outname.history "    *** log(n) calculated from $bestmodel is [format %.7f $log_n]"
	suspend2constrain
	option maxiter $saveiter
	save model [full_filename $bestmodel]
#
# Copy input records to output records, but correcting BIC
#
	while {{} != $inline} {
	    set cfilename [lindex $inline 0]
	    set inbic [lindex $inline 1]
	    set inloglike [lindex $inline 2]
#
# Calculate correct BIC
#
	    if {![is_float $inloglike]} {
		set BIC [format %11s NaN]
	    } else {
		ifdebug puts "likes: $inloglike  $null_loglike"
		set lodp [lod $inloglike $null_loglike]
		set lambda [expr $lodp * 2 * log (10)]
		set df [llength [bayesavg_elements $cfilename]]
		ifdebug puts "$cfilename:  df: $df  lambda: $lambda"
		set BIC [expr ($df * $log_n) - $lambda]
		catch {set BIC [format %11.4f $BIC]}
	    }
	    ifdebug puts "Model: $cfilename   estBIC: $inbic   BIC: $BIC"
#
# splice BIC into result line
#
#	    set firstcut [expr [string first $inbic $inline] - 1]
#	    set resumec [expr $firstcut + 1 + [string length $inbic]]
#	    ifdebug puts "prefix: [string range $inline 0 $firstcut]"
#	    set outline [string range $inline 0 $firstcut]$BIC[string range $inline $resumec end]
	    set outline [replace_using_format $inline $formats 1 $BIC]
	    resultfile $outfile -write
	    gets $infile inline
	}
    }
    close $infile
#
# Resort new output file (unlikely, but might need it)
#
    putsout $outname.history \
	    "    *** Re-sorting output file with changed BIC's"
    if {0 != [string length [usort]]} {
	ifverbplus puts "\n    *** Re-sorting $outname.nose"
	set fullname [full_filename $outname.nose]
	if {0==[catch {eval exec head $head2 $fullname >$fullname.tmp}]} {
	    if {0==[catch {eval exec tail $tailn +3 $fullname | [usort] -n -k 2 \
		    >>$fullname.tmp}]} {
		set infile [open $fullname.tmp]
		gets $infile
		gets $infile
		set inline ""
		gets $infile inline
		close $infile
		if {0!=[string compare $bestmodel [lindex $inline 0]]} {
		    if {$depth < 10} {
			putsout $outname.history \
            "\n    *** Now a different model has best BIC: [lindex $inline 0]"
			return [bayesavg_rewrite $cutoff $maxindex \
				    $outname $covarlist $excludenull \
				    $symmetric $no_se $null_loglike \
				    $log_n_est $basemodel $paramlist \
				    $formats $headings $betalist \
				    [expr $depth + 1]]
		    } else {
			putsout $outname.history \
 "\n    *** Warning!  Changes in log(n) keep causing best BIC model to change"
			putsout $outname.history \
   "    *** Locking in best value after 10 retries\n"
		    }
		}
		if {0==[catch {eval file rename -force $fullname.tmp \
				   $fullname}]} {
		    return [bayesavg_post $cutoff $maxindex $outname \
			    $covarlist $excludenull $symmetric $no_se \
			    $basemodel $paramlist $formats $headings \
			    $betalist $bestmodel]
		}
	    }
	}
    }
#
# Sorting error
#
    error "    *** Sort of $outname.est failed"
}


proc bayesavg_post {cutoff maxindex outname covarlist excludenull symmetric \
    no_se basemodel paramlist formats headings betalist bestmodel} {
#
    ifdebug puts "Starting bayesavg_post"
#
#
# IMPORTANT to allow link/cov/qtn types and
# h2r in all types and
# h2q1 in cov/qtn types (when present):
#
# firstindex==0 is normal case
#   in that case, i==0 is h2r, represented by h2q0
# firstindex==-1 means h2q1 included in cov model
#   in that case, i==-1 is h2r, represented by h2q_1
#   and i==0 is h2q1, represented by h2q0
#
    set firstbelement 3
    set firstindex 0
    if {{} == $covarlist} {
	set n [llength $paramlist]
    } else {
	set n [llength $covarlist]
	if {[if_parameter_exists h2q1] && ![is_constrained_to_zero h2q1]} {
	    set firstindex -1
	    set firstbelement 5
	}
    }
#
# setup result variables
# "h2q0" is actually h2r (or h2q1, see above)
#
# OR, for covariates, 
# h2qX is really the covariate numbered X
#
#
    for {set i $firstindex} {$i <= $maxindex} {incr i} {
	set p $i
	if {$p < 0} {
	    set p [catenate _ [expr abs($i)]]
	}
	set h2q$p 0
	set h2qse$p 0
	set h2qp$p 0
    }
#
# Open file and skip over header
#
    set infile [open [full_filename $outname.nose] r]
    gets $infile inline
    gets $infile inline
    if {"----------" != [string range $inline 1 10]} {
	close $infile
	error "Error(s) in bayesavg.out"
    }
    set data_offset [tell $infile]
#
# Make windowfile for reporting about models in window
#
    global Solar_Bay_Qtn
    if {[if_global_exists Solar_Bay_Qtn] && $Solar_Bay_Qtn==1} {
	set wheadings "Model BIC PProb SNP_Flags"
	set wformats "%20s %11.4f %10s %[expr 2*$maxindex]s"
	set wexpressions {{$wfilename} {$wbic} {$wpprob} {$snp_flags}}
	set wfile [resultfile -create [full_filename $outname.win] \
		       -headings $wheadings -formats $wformats \
		       -expressions $wexpressions]
	resultfile $wfile -header
    }
#
# Pass through file summing exp(-1/2 * BIC)
#
    ifdebug puts "Summing exp(-1/2 * BIC)"
    set eBICsum 0
    set count 0
    set window {}
    set window_of_filenames {}
    set force_cov0 0
    set forced_cov0 0
    if {[if_global_exists Solar_Bay_Qtn] && $Solar_Bay_Qtn==1} {
	set force_cov0 1
    }

    while {0 < [gets $infile inline]} {
	if {2 != [scan $inline "%s %f" filename BIC]} {
	    close $infile
	    error "Error detected in $outname.out"
	}
	if {$excludenull && ("c0" == $filename || "cov0" == $filename)} continue
	if {$count == 0} {
	    set max_bic [expr $BIC + $cutoff]
	} else {
	    if {$BIC > $max_bic} {
		if {!$force_cov0} {
		    break
		} else {
		    if {-1 != [lsearch $window_of_filenames cov0]} {
			set force_cov0 0   ;# No need to force it
			break
		    }
		    set forced_cov0 1
		    set filename cov0
		    set BIC 0
		    set model_elements {}
		}
	    }
	}
	incr count
#
# For STRICT Occam's window
# See if we have encountered any models (having better BICs and)
# which are subsets of this model
#
	if {!$forced_cov0} {
	set model_elements [bayesavg_elements $filename]
	if {0==$symmetric} {
	    set found_subset 0
	    foreach comb $window {
		if {1==[subset $model_elements $comb]} {
		    set found_subset 1
		    break
		}
	    }
	    if {$found_subset} {
		continue
	    }
	}
	set eBIC [expr exp(-0.5 * $BIC)]
	set eBICsum [expr $eBICsum + $eBIC]
	}
	lappend window $model_elements
	lappend window_of_filenames $filename
	lappend window_of_bic $BIC
    }
#
# Write window file
#
    if {[if_global_exists Solar_Bay_Qtn] && $Solar_Bay_Qtn==1} {
	set window_size [llength $window]
	for {set i 0} {$i < $window_size} {incr i} {
	    set wfilename [lindex $window_of_filenames $i]
	    set wbic [lindex $window_of_bic $i]
	    if {"cov0" == $wfilename && $forced_cov0} {
		set cpprob [expr 1.0 / ($eBICsum + 1.0)]
		ifdebug puts "calculating pprob for forced cov0: $wpprob"
		set wpprob [format %10.6f $cpprob]
                if {0.0001 >  $wpprob} {
                    set wpprob [format %10.3g $cpprob]
		}
	    } else {
		ifdebug puts "calculating pprob old way"
		set wpprob [expr exp(-0.5*$wbic) / $eBICsum]
		set wpprob [format %10.6f $wpprob]
	    }
	    set snp_flags ""
	    set wset [bayesavg_elements $wfilename]
	    for {set j 1} {$j <= $maxindex} {incr j} {
		if {-1 != [lsearch $wset $j]} {
		    set snp_flags "$snp_flags 1"
		} else {
		    set snp_flags "$snp_flags 0"
		}
	    }
	    resultfile $wfile -write
	}
    }
#
# If we "forced" window to include cov0, remove it now
#   Note: not necessary to remove from $window because it's remade anyway
#         not necessary to remove from $window_of_bic since it's not reused
#
    if {$forced_cov0} {
	set last_index [expr [llength $window] - 2]
	set window_of_filenames [lrange $window_of_filenames 0 $last_index]
    }
#
# Pass through file again computing PProbs and h2q$p sums
# 
    ifdebug puts "computing h2qi sums"
    seek $infile $data_offset
    set window {}
    while {0 < [gets $infile inline]} {
	if {2 != [scan $inline "%s %f" filename BIC]} {
	    close $infile
	    error "Error detected in $outname.out"
	}
	if {$excludenull && ("c0" == $filename || "cov0" == $filename)} continue
	set BIC [lindex $inline 1]
	if {$BIC > $max_bic} break
	set model_elements [bayesavg_elements $filename]
	if {0==$symmetric} {
	    set found_subset 0
	    foreach comb $window {
		if {1==[subset $model_elements $comb]} {
		    set found_subset 1
		    break
		}
	    }
	    if {$found_subset} {
		continue
	    }
	}
	lappend window $model_elements

	set PProb [expr exp(-0.5 * $BIC) / $eBICsum]

	for {set i $firstindex} {$i <= $maxindex} {incr i} {
	    set h2q [lindex $inline [expr $firstbelement + ($i * 2)]]
	    set p $i
	    if {$p < 0} {
		set p [catenate _ [expr abs($i)]]
	    }
	    if {[is_nan $h2q]} {
		close $infile
		error "NaN(s) found in bayesavg.nose"
	    }
	    set weighted [expr $h2q * $PProb]
	    ifdebug puts "for $i adding value $h2q mpprob $PProb weighted $weighted"
	    set h2q$p [eval expr \${h2q$p} + $weighted]
#
# If this model includes element i, and if it is "non-zero"
#   it adds to the posterior probability of that element
#
	    if {($i<=0) || (0<=[lsearch -exact $model_elements $i])} {
		if {abs($h2q) >= 1e-12} {
		    set h2qp$p [eval expr \${h2qp$p} + $PProb]
		}
	    }
	}
    }
    if {$no_se} {
#
# Rename output file since s.e.'s not needed
#
	file rename [full_filename $outname.nose] [full_filename $outname.out]

    } else {
#
# Open resultfile for output of file with std errors
#
    set columns [llength $formats]
    set expressions {}
    for {set i 0} {$i < $columns} {incr i} {
	lappend expressions "\[lindex \$inline $i\]"
    }
    set outfile [resultfile -create [full_filename $outname.out] \
	    -headings $headings -formats $formats -expressions $expressions]
    resultfile $outfile -header
#
# Pass through file again computing h2qse$p sums
#
    ifdebug puts "computing h2qse sums"
    seek $infile $data_offset
    set window {}
    set finished 1
    while {0 < [gets $infile inline]} {
	set filename [lindex $inline 0]
	if {! ($excludenull && ("c0" == $filename || "cov0" == $filename))} {
	    set BIC [lindex $inline 1]
	    if {$BIC > $max_bic} {
		set finished 0
		break
	    }
	    set model_elements [bayesavg_elements $filename]
	    set found_subset 0
	    if {0==$symmetric} {
		foreach comb $window {
		    if {1==[subset $model_elements $comb]} {
			set found_subset 1
			break
		    }
		}
	    }
	    if {!$found_subset} {
		lappend window $model_elements

		if {$filename == $bestmodel  && \
			0 != [oldmodel $filename mean -se]} {
		    load model [full_filename $bestmodel]
		} else {
		    putsout $outname.history \
			    "    *** Maximizing $filename for Standard Errors"
#
# Create model from base by restoring elements
#
		    load model [full_filename $basemodel]
		    set cfilename [file root [file tail $filename]]
		    set comb [bayesavg_elements $cfilename]
		    if {{} != $covarlist} {
			for {set i 0} {$i < $n} {incr i} {
			    if {-1 != [lsearch $comb [expr $i + 1]]} {
				set covi [lindex $covarlist $i]
				covariate restore $covi
			    }
			}
		    } else {
			foreach ce $comb {
			    set param [lindex $paramlist [expr $ce - 1]]
			    if {-1==[string first "-" $param]} {
				constraint delete $param
			    } else {
				constraint delete <$param>
			    }
			    carve_new_value $param 0.01 h2r
			    parameter $param lower 0
			}
		    }
#
# Maximize model with standard errors turned on, save w/o covariates
#
		    option standerr 1
		    if {[catch {maximize -q}]} {
			close $infile
			error "Error maximizing model for Standard Errors"
		    }
		    suspend2constrain
		    save model [full_filename $filename]
		}
#
# Accumulate statistics for each variable parameter (including h2r)
#
		set PProb [expr exp(-0.5 * $BIC) / $eBICsum]
		for {set i $firstindex} {$i <= $maxindex} {incr i} {
		    set parvalueindex [expr $firstbelement + ($i * 2)]
		    set parvalue [lindex $inline $parvalueindex]
		    if {$parvalue != 0.0} {
			if {$i == $firstindex} {
			    set ename h2r
			} elseif {$i == 0} {
			    set ename h2q1
			} else {
			    if {"" != $covarlist} {
				set ename [lindex $betalist [expr $i - 1]]
			    } else {
				set ename  [lindex $paramlist [expr $i - 1]]
			    }
			}
			set se [oldmodel $filename $ename -se]
			set parcheck [oldmodel $filename $ename]
			set changeparvalue [expr abs($parvalue - $parcheck)]
			if {$changeparvalue > 0.1} {
			    puts "$ename was $parvalue now $parcheck"
			    error "Re-maximized model $filename is different"
			}
			ifdebug puts "$ename was $parvalue now $parcheck"
			if {[is_nan $se]} {
			    close $infile
			    error \
		        "NaN(s) for standard error in model [lindex $inline 0]"
			}

# Update standard error for final output file
			set p $i
			if {$p < 0} {
			    set p [catenate _ [expr abs($i)]]
			}

			set se_index [expr $firstbelement + 1 + ($i * 2)]
			set inline [lreplace $inline $se_index $se_index $se]
			set variance [expr $se * $se]
			set h2qse$p [eval expr \${h2qse$p} + \
			      ( $PProb * ( $variance + ($parvalue*$parvalue)))]
		    }
		}
	    }
	}
	resultfile $outfile -write
    }
#
# Copy rest of file (outside of window, no se's added)
#
    if {!$finished} {
	resultfile $outfile -write   ;# Write line just read
	while {-1 != [gets $infile inline]} {
	    resultfile $outfile -write
	}
    }
    close $infile
#
# Compute actual h2qse$p values
#
    for {set i $firstindex} {$i <= $maxindex} {incr i} {
	set p $i
	if {$p < 0} {
	    set p [catenate _ [expr abs($i)]]
	}
	set emean2 [eval square \${h2q$p}]
 	set sevalue [expr sqrt ([eval expr \${h2qse$p} - $emean2])]
 	set h2qse$p $sevalue
    }
    }  ;# End of if not -no_se

# *****************************************************************************
# Write bayesavg.avg with bayesian model averages
# *****************************************************************************

    set outfilename [full_filename $outname.avg]
    set soutfile [open $outfilename w]
    putstee $soutfile "    *** Number of Models in Window: [llength $window]"
    putstee $soutfile "    *** Window:  $window_of_filenames"
    putstee $soutfile ""
    close $soutfile


    set headings {Component Average {Std Error} Probability}
    if {{} != $covarlist} {
	set width_needed 32
	for {set i $firstindex} {$i < $maxindex} {incr i} {
	    set component [lindex $covarlist $i]
	    if {$width_needed < [string length $component]} {
		set width_needed [string length $component]
	    }
	}
	set aformats "%[catenate $width_needed s] %-13.8g %-13.8g %-12.7g"
    } else {
	set aformats "%25s %-12.7g %-13.8g %-12.7g"
    }

    set resultf [resultfile -append $outfilename -display \
	    -headings $headings \
	    -formats $aformats \
	    -expressions {$component $average $stderror $postprob}]
    resultfile $resultf -header

    for {set i $firstindex} {$i <= $maxindex} {incr i} {

	set p $i
	if {$p < 0} {
	    set p [catenate _ [expr abs($i)]]
	}

	if {$i == $firstindex} {
	    set component H2r
	} elseif {$i == 0} {
	    set component H2q1
	} else {
	    if {{} != $covarlist} {
		set component [lindex $covarlist [expr $i-1]]
	    } else {
		set component H2q$p
	    }
	}
	set average [eval expr \${h2q$p}]
	set stderror [eval expr \${h2qse$p}]
	set postprob [eval expr \${h2qp$p}]
	resultfile $resultf -write
    }
#
# Delete .nose file if it's the same length
#
    catch {
	if {[file exists [full_filename $outname.nose]] && \
		[file exists [full_filename $outname.out]] && \
		[file exists [full_filename $outname.est]] && \
		[file size [full_filename $outname.nose]] == \
		[file size [full_filename $outname.out]]} {
	    file delete [full_filename $outname.nose]
	}
    }
#
# Helpful messages
#
    putsout $outname.history "\n    *** Averages written to [full_filename $outname.avg]"
    putsout $outname.history "    *** Model results written to [full_filename $outname.out]"
    puts "    *** Messages written to [full_filename $outname.history]"
#
# Load best model
#
    load model [full_filename $bestmodel]
    putsout $outname.history "    *** Model with best BIC loaded: $bestmodel"
    return ""
}

proc subset {listall listsub} {
    set mismatch 0
    foreach el $listsub {
	if {-1==[lsearch $listall $el]} {
	    set mismatch 1
	    break
	}
    }
    if {0==$mismatch} {
	return 1
    }
    return 0
}

#
# Return a list of the numbered elements (1..N) included in this model
#
# _ is used between all numbers
#
proc bayesavg_elements {cfilename} {
    set elemstr [string range $cfilename 1 end]
    if {"ov" == [string range $elemstr 0 1]} {
	set elemstr [string range $elemstr 2 end]
    }
    set elements [split $elemstr _ ]
    if {![string compare $elements 0]} {
	set elements ""
    }
    return $elements
}

	
# solar::bayesmod -- private
#
# Purpose: To build a model containing only the numbered elements:
#          linkage components or elements
#
# Usage:   bayesmod [-cov] [-h2rf] [-nomax] [-new] i j k ...
#          i,j,k are the element numbers, starting with 1, that are to be
#          included.  Model is constructed from the previously saved
#          saturated model.
#
#           -cov            ;# Covariate elements (not linkage)
#           -h2rf           ;# h2r factor...see help bayesavg...default is 1.1
#           -max            ;# maximize model
#           -new            ;# build model from model in memory
#                           ;#   NOT from previous bayesavg saturated model
#
# Example:  trait bmi       ;# Set trait so output directory can be found
#           bayesmod 1 3    
#           bayesmod -cov 1 3 7 8
#
# Notes:    (1) Unlike the bayesavg procedure,
#               excluded covariates are constrained to zero rather than 
#               being "suspended."  This is equivalent, but constrained
#               covariates are easier for users to deal with.
# -

proc bayesmod {args} {

    set covar 0
    set h2r_factor 1.1
    set max 0
    set new 0
    set retval ""
    set last_verbosity [verbosity]

    set numlist [read_arglist $args -cov {set covar 1} -h2rf h2r_factor \
	    -max {set max 1} -new {set new 1}]

    foreach num $numlist {
	ensure_integer $num
    }
#
# Covariate model
#
    if {$covar} {
	if {!$new} {
	    load model [full_filename cov.sat.mod]
	}
	set betalist [covariates -betanames]
	set numcov [llength $betalist]
	foreach num $numlist {
	    if {$num > $numcov} {
		error "There are only $numcov covariates"
	    }
	}
	for {set i 1} {$i <= $numcov} {incr i} {
	    if {-1==[lsearch $numlist $i]} {
		set betaname [lindex $betalist [expr $i - 1]]
		parameter $betaname = 0
		if {-1==[string first "-" $param]} {
		    constraint $betaname = 0
		} else {
		    constraint <$betaname> = 0
		}
	    }
	}
	if {$max} {
	    puts "Maximizing..."
	    verbosity plus
	    set retval [maximize_quietly bayesmod.out]
	}
#
# Linkage model
#
    } else {
	if {!$new} {
	    load model [full_filename c.sat.mod]
	}
	set h2qc [h2qcount]
	foreach num $numlist {
	    if {$num > $h2qc} {
		error "There are only $h2qc linkage elements"
	    }
	}
	for {set i 1} {$i <= $h2qc} {incr i} {
	    if {-1==[lsearch $numlist $i]} {
		parameter h2r start [expr [parameter h2r start] + \
			[parameter h2q$p start]]
		parameter h2q$p start 0
		parameter h2q$p lower 0.01
		constraint h2q$p = 0
	    }
	}
	parameter h2r upper [lowest \
		[expr [parameter h2r start] * $h2r_factor] 1]
	if {$max} {
	    puts "Maximizing..."
	    verbosity plus
	    set retval [max_bayesavg [full_filename bayesmod.out] $numlist]
	}
    }
    eval $last_verbosity
    return $retval
}    

proc square {value} {return [expr $value * $value]}

proc bayesavg_bad_results {cfilename bad_results sorted} {
    set tempname [full_filename $cfilename.tmp]
    set permname [full_filename $cfilename]
    set tmpoutput [open $tempname w]
    foreach result $bad_results {
	if {[llength $result] > 0} {
	    puts $tmpoutput $result
	    puts stderr $result
	}
    }
    if {$sorted == 0} {
	puts $tmpoutput \
		"Convergence error limit exceeded; terminating prematurely"
	puts $tmpoutput \
		"Results are incomplete and not sorted"
    } elseif {$sorted == 1} {
	puts $tmpoutput "Results are incomplete but sorted"
    } elseif {$sorted == 2} {
	puts $tmpoutput \
		"Convergence errors occurred"
	puts $tmpoutput \
		"Results are incomplete and not sorted due to sort problem"
    }
    puts $tmpoutput ""
    close $tmpoutput
    eval exec cat $permname >>$tempname
    eval file rename -force $tempname $permname
    error "Terminating with convergence errors"
}
#
# Maximize with retries suitable for bayesavg linkage models
#
proc max_bayesavg {outfilename comb} {
    set errmsg [maximize_quietly $outfilename]
    if {$errmsg == ""} {
	return ""
    }
    if {"verbosity min" != [verbosity]} {
	puts "\n    *** Retry with equal active H* parameters"
    }
    model load [full_filename last.mod]
    set hstart [expr (1-[parameter e2 start])/ \
	    ([llength $comb] + 1)]
    set elements [linsert $comb 0 0]
    foreach i $elements {
	if {$i == 0} {
	    set hname h2r
	} else {
	    set hname h2q$i
	}
	parameter $hname start $hstart
	set hupper [parameter $hname upper]
	set hlower [parameter $hname lower]
	if {$hstart >= $hupper} {
	    parameter $hname upper [expr $hstart + 0.001]
	}
	if {$hstart <= $hlower} {
	    parameter $hname lower [expr $hstart - 0.001]
	}
    }
    set errmsg [maximize_quietly $outfilename]
    if {$errmsg == ""} {
	return ""
    }
    return "Convergence problem with combination: $comb"
}

# solar::combinations --
#
# Purpose:  Make a list or count combinations of integers 1..N of size K
#
# Usage:    combinations <N> [<K>] [-max <maxsize>] [-list list] [-force]
#                        [-count] [-counts] [-start <number>] [-stop <number>]
#
#           N        defines the range of integers 1..N of interest.  If no
#                    other arguments are specified, and N <= 10, the set of
#                    all combinations of this range of integers is returned.
#                    To get a list of combinations where possibly N > 10, 
#                    add either the -list or -force option, with -list being
#                    the preferred method.
#
#           K        only include combinations of exactly this size (as
#                    in traditional "combinations").  If this argument is
#                    not specified, the default is to include combinations
#                    of all sizes, starting from the smallest size.
#
#           -count   Only return the NUMBER of combinations, not a list
#                    of the actual combinations.
#
#           -counts   Return a list containing the number of combinations for
#                     each "size" (i.e. "K").
#
#           -max     include all combinations up to and including this size
#                    (the default is to include combinations of all sizes).
#                    The K and -max arguments may not be used at the
#                    same time.
#
#           -list    APPEND combinations to this list rather than returning
#                    them.  Specify the list variable by name, as with the
#                    Tcl lappend command (see example below).  If the variable
#                    is not already set, a new variable is created.  When this
#                    argument is used, nothing is returned.  For example:
#
#                        set comblist {}
#                        combinations 20 -max 10 -list comblist
#
#                    Be sure to empty the list first (as shown above) if you
#                    do not want to append to the previous contents, if the
#                    variable was used previously in the same procedure.  This
#                    option may save memory (as compared with -force) for
#                    very large N since only one copy of the list is ever
#                    created.
#                  
#           -force   return list ("by value") even if N > 10.  This
#                    is required for N > 10 unless the -list, -count, -counts,
#                    -start, or -stop arguments are given.  Only use this
#                    option if you are sure this is what you want to do.
#                    Read all the following paragraphs to be sure.  Generally,
#                    you would only use it inside a script, where the
#                    returned combinations are going to be immediately saved
#                    to a variable, such as:
#
#                        catch {set comblist [combinations $N -force]}
#
#                    The reason to require a -force option is that if a
#                    large N is given in an interactive session, the
#                    terminal window could be locked up for hours displaying
#                    all the combinations, with no way to break out until
#                    the terminal output buffer is empty.  If that were to
#                    happen, you would probably want to kill the whole
#                    terminal session from another terminal window.  For
#                    some users, that would probably require calling the
#                    system administrator.
#
#                    The -force option may require more memory than the -list
#                    option because a copy of the list is created in the
#                    process of "returning" it to the caller; that's just
#                    the way Tcl works, and it becomes important when creating
#                    lists with huge numbers of elements.
#
#                    If you are using this form of the command in a script,
#                    be careful that it is not the last command in the
#                    script, which Tcl automatically returns.  Then, if
#                    the user runs the script from the terminal, the
#                    terminal window would be locked up.  If you must
#                    use it as the last command in a script, you should
#                    use a "catch" command around it, as in the example
#                    above.  The catch command only returns 0 (for success)
#                    or 1 (for error).
#
#           The following options are useful when dividing up the set of
#           combinations into jobs of an equal size.  Otherwise, they may
#           seem a bit obscure.
#
#           -start   Start with combination number <number>
#
#           -stop    Stop with combination number <number>
#
# Notes:
#
# CAUTION!  The list can get VERY BIG!  Be careful if n > 15 because
# memory requirements double for each [incr n], unless you are setting k
# or -max.  ("BIG" means 100's of megabytes, gigabytes, etc.  I am not
# kidding. On Solaris systems, you can use the SOLAR "memory" command to see
# just how much memory SOLAR has consumed.)
#
# -

proc combinations {n args} {

    if {$n < 1} {
	error "N must be greater than zero"
    }

    set minsize 1
    set maxsize $n
    set max ""
    set size 0
    set listname ""
    set force 0
    set start 0
    set stop 0
    set count 0
    set do_count_only 0
    set do_count_list 0
    set count_list {}

    set ksize [read_arglist $args \
	    -max max \
	    -list listname \
	    -force {set force 1} \
	    -start start \
	    -stop stop \
	    -count {set do_count_only 1} \
	    -counts {set do_count_only 1; set do_count_list 1} \
	]

    if {"" != $ksize} {
	if {![is_integer $ksize]} {
	    error "Invalid argument(s): $ksize"
	}
	if {"" != $max} {
	    error "-max and -size arguments are incompatible"
	}
	if {$ksize < 1 || $ksize > $n} {
	    error "size $ksize is invalid"
	}
	set minsize $ksize
	set maxsize $ksize
    }

    if {"" != $max} {
	if {![is_integer $max]} {
	    error "-max requires integer value"
	}
	if {$max < 1} {
	    error "Positive -max required"
	}
	if {$max > $n} {
	    error "-max cannot be greater than N"
	}
	set maxsize $max
    }

    if {!$do_count_only && !($start && $stop)} {
	if {$n > 10 && 0 == [llength $listname] && !$force} {
	    error "N > 10 not permitted without -list or -force"
	}
    }

    if {0 == [llength $listname]} {
	set comb {}
    } else {
	upvar $listname comb
    }

    for {set i $minsize} {$i <= $maxsize} {incr i} {
	combinations_ref $n $i comb $start $stop
	if {$do_count_list} {
	    lappend count_list $count
	    set count 0
	}
	if {$stop && ($count >= $stop)} {
	    break
	}
    }

    if {$do_count_list} {
	return $count_list
    }

    if {$do_count_only} {
	if {$start} {
	    set count [expr $count + 1 - $start]
	}
	return $count
    }

    if {0 == [llength $listname]} {
	return $comb
    }
    return ""
	
}

#
# Compute traditional combinations(n,k) where k is fixed size
#
# This is NOT a user interface.  proc combinations (above) is the
# user interface.
#
# Caller's list passed by name (like call by reference) and augmented
#
# Iterative "quirky odometer" method
#   Each digit is a element
#   Start from "floor"
#   Twirl last digit and carry
#   On carry, following digits are successive integers (e.g. 5 6 7)
#   Each digit can only go to final "ceiling" value
#
proc combinations_ref {n k combname start stop} {
#
# Test for caller error
#
    if {$n < $k} {
	error "N cannot be less than K"
    }
#
# Link to caller's array
#
    upvar $combname combinations_r
#
# Link to caller's count and do_count_only
#
    upvar count count
    upvar do_count_only do_count_only
#
# Setup first ("floor") combination
#
    set floor {}
    for {set i 1} {$i <= $k} {incr i} {
	lappend floor $i
    }
    incr count
    if {!$do_count_only} {
	if {!$start || ($count >= $start)} {
	    lappend combinations_r $floor
	}
    }
    if {$stop && ($count >= $stop)} {
	return ""
    }
#
# Setup ceiling combination (for comparison use)
#
    set delta [expr $n - $k]
    set ceiling {}
    for {set i [expr 1 + $delta]} {$i <= $n} {incr i} {
	lappend ceiling $i
    }
#
# Now generate remaining combinations
#
    set done 0
    set last [expr $k - 1]
    set base $floor
    while {1} {
#
# Twirl last digit
#
	set startnum [lindex $base $last]
	set endnum [lindex $ceiling $last]
	while {$startnum < $endnum} {
	    incr startnum
	    incr count
	    set base [lreplace $base $last $last $startnum]
	    if {!$do_count_only} {
		if {!$start || ($count >= $start)} {
		    lappend combinations_r $base
		}
	    }
	    if {$stop && ($count >= $stop)} {
		return ""
	    }
	}
#
# Carry and start over (or finish)
#
	set previous [expr $last - 1]
	if {$previous < 0} {
	    break
	}
	while {[lindex $base $previous] == [lindex $ceiling $previous]} {
	    incr previous -1
	    if {$previous < 0} {
		return ""
	    }
	}
	set newvalue [expr [lindex $base $previous] + 1]
	for {set pntr $previous} {$pntr < $k} {incr pntr} {
	    set base [lreplace $base $pntr $pntr $newvalue]
	    incr newvalue
	}
	incr count
	if {!$do_count_only} {
	    if {!$start || ($count >= $start)} {
		lappend combinations_r $base
	    }
	}
	if {$stop && ($count >= $stop)} {
	    return ""
	}
    }
    return ""
}


#
# Old recursive method (now obsolete but maintained for testing purposes)
#
proc oldcombinations {n {max -1}} {
    if {$n == 1} {return 1}
    set minus1 [oldcombinations [expr $n - 1] $max]
    foreach e $minus1 {
	if {$max != -1 && $max <= [llength $e]} {continue}
	lappend comb [concat $e $n]
    }
    foreach e $minus1 {
	lappend comb $e
    }
    return [lappend comb $n]
}

proc combfile {n {max -1}} {
    if {$n == 1} {
	set ofile [open combfile.dat w]
	puts $ofile 1
	close $ofile
	return ""
    }
    combfile [expr $n - 1] $max
    set ofile [open combfile.dat a+]
    puts $ofile $n
    set end_file [tell $ofile]
    seek $ofile 0
    while {$end_file > [tell $ofile]} {
	gets $ofile cline
	set read_position [tell $ofile]
        if {$max != -1 && [llength $cline] >= $max} {continue}
	seek $ofile 2 end
	puts $ofile "$cline $n"
        flush $ofile
	seek $ofile $read_position
    }
    close $ofile
    return ""
}

# solar::resultfile -- private
#
# Purpose: output results to file and/or screen
#
# Usage:  resultfile -create <filename> -headings <headings> -formats <formats>
#             -expressions <expressions> [<options>]
#
#         (-append may be substituted for -create if appending to an existing
#           file)
#
#         This returns a resultfile object which is used in subsequent
#         commands. The resultfile object will NOT be returned if the
#         -write or -header option is used, instead, the string written
#         to file and/or terminal is returned.  [NOTE: This changed
#         with revision 2.0.2.]
#
#         resultfile <resultfile> -header
#
#         resultfile <resultfile> -write
#
#         options are:  -display (tee to standard output)
#                       -displayonly (display ONLY on standard output)
#                       -default <default result>  {0 is default default}
#
# Note:  This is intended as a replacement for outheader/outresults which
#        was becoming increasingly difficult to use.
#
#        -create, -append, -header, and -write may not be used in combination.
#
#        Closing is not required.  The file is re-opened and closed each
#        time results are written.
# -

proc resultfile {args} {

    set terminator 0
    set cfilename ""
    set afilename ""
    set header 0
    set write 0
    set formats {}
    set expressions {}
    set display 0
    set displayonly 0
    set default_result 0

    set oldresultf [read_arglist $args -create cfilename \
	    -append afilename \
	    -header {set header 1} \
	    -write {set write 1}\
            -default default_result \
	    -display {set display 1} \
	    -displayonly {set display 1; set displayonly 1} \
	    -returnonly {set displayonly 1} \
	    -headings headings -formats formats \
	    -terminator {set terminator 1} \
	    -expressions expressions]

    if {{} == $oldresultf && "" == $cfilename && "" == $afilename} {
	error "Resultfile requires -create, -append, or previous object"
    }
#
# Create a new 'resultfile'
#
    if {"" != $cfilename || $afilename != ""} {
	if {{} == $headings || {} == $formats || {} == $expressions} {
           error "Resultfile creation missing headings, formats or expressions"
	}
	if {"" != $cfilename && "" != $afilename} {
	    error "Resultfile create and append options are incompatible"
	}
	if {!$displayonly} {
	    if {"" != $cfilename} {
		set filename $cfilename
		set outfile [open $cfilename w]
		close $outfile
	    } else {
		set filename $afilename
	    }
	}
#
# Create header (regardless of whether we write it now)
#
	set headerstring ""
	set understring ""
	set widths {}
	for {set i 0} {$i < [llength $headings]} {incr i} {
	    set heading [lindex $headings $i]
	    set format [lindex $formats $i]
	    if {"" == $heading} continue   ;# Allows for optional postfix
#
# Width of field is width of heading, or width of format, whichever is
# longer
#
	    set header_length [string length $heading]
	    set format_length [format_width $format]
	    if {$format_length < 0} {
		set format_length [expr 1 - $format_length]
	    }
	    if {$format_length > $header_length} {
		set width $format_length
	    } else {
		set width $header_length
	    }
	    lappend widths $width
#
# Center the title in a field of appropriate width
#   If centering requires an uneven number of spaces, put the odd
#   space in front
#
	    set blanks [expr $width - [string length $heading]]
	    set front [expr ceil($blanks/2.0)]
	    set back [expr floor($blanks/2.0)]
	    for {set j 0} {$j < $front} {incr j} {
		set headerstring "$headerstring "
	    }
	    set headerstring "$headerstring$heading"
	    for {set j 0} {$j < $back} {incr j} {
		set headerstring "$headerstring "
	    }
#
# Create the underline string:
# One hyphen under each letter of the header, expanding out to the
#   width of the field, but centered abound the title
#
	    if {$front > $back} {
		set understring "$understring "
	    }
	    for {set j 0} {$j < $back} {incr j} {
		set understring [catenate $understring -]
	    }
	    for {set j 0} {$j < $header_length} {incr j} {
		set understring [catenate $understring -]
	    }
	    for {set j 0} {$j < $back} {incr j} {
		set understring [catenate $understring -]
	    }
#
# If there is going to be another field, add a space to each
#
	    if {$i < [llength $headings] - 1} {
		set headerstring "$headerstring "
		set understring "$understring "
	    }
	}
	return "resultfile $args -widths \{$widths\} \
	    -headerstring \{$headerstring\} -understring \{*$understring\}"
    }
#
# Load previously created object and write something
#
    set widths {}
    set formats {}
    set expressions {}
    set headings {}
    set filename ""
    set headerstring ""
    set understring ""
    set default_result 0
    set terminator 0

    set resultflen [string length $oldresultf]
    set resultf [string range $oldresultf 1 [expr $resultflen-2]]
    set foo [read_arglist $resultf -widths widths -formats formats \
		 -expressions expressions -create filename -append filename \
		 -headings headings -header {set foo 1} -write {set foo 1} \
                 -default default_result \
		 -displayonly {set displayonly 1; set display 1} \
		 -returnonly {set displayonly 1} \
		 -display {set display 1} -headerstring headerstring \
		 -terminator {set terminator 1} \
		 -understring understring]
    set understring [string range $understring 1 end]
#
# Write header if requested
#
    if {$header} {
	if {$display} {
	    puts $headerstring
	    puts $understring
	}
	if {!$displayonly} {
	    set outfile [open $filename a]
	    puts $outfile $headerstring
	    puts $outfile $understring
	    close $outfile
	}
	return $headerstring
    }
#
# Write results if requested
#
    if {$write} {
	if {{} == $widths || {} == $formats || {} == $expressions} {
	    error "resultfile requires -formats and -expressions"
	}

	set resultstring ""
	for {set i 0} {$i < [llength $expressions]} {incr i} {
	    set expression [lindex $expressions $i]
	    set formatstr [lindex $formats $i]
	    set width [lindex $widths $i]
	    set express "set resultfile_FoObAr03 $expression"
	    set result $default_result
	    catch {set result [uplevel $express]}  ;# evauation done here
#
# If format fails (maybe numeric format with alpha value)
# just procede
#
	    if {[catch {set result [format $formatstr $result]}]} {
	    }
#
# Whether format failed or not, adjust to specified width
#
	    set result [format [catenate "%$width" s] $result]
	    set resultstring "$resultstring$result"
	    if {$i < [llength $expressions]} {
		set resultstring "$resultstring "
	    }
	}
	if {$display} {
	    puts $resultstring
	}
	if {!$displayonly} {
	    set outfile [open $filename a]
	    if {!$terminator} {
		puts $outfile $resultstring
	    } else {
		puts $outfile "$resultstring *"
	    }
	    close $outfile
	}
	return $resultstring
    }

# Should not get here unless no operative option
    return ""
}


proc format_width {format} {
    set format_width_string [string range $format 1 end]
    set count [scan $format_width_string %d width]
    if {1 != $count} {
	error "Invalid format $format"
    }
    return $width
}

# solar::outheader -- private
#
# Purpose: Output header for analysis scripts
#
# Usage:   outheader filename h2qindex col_2 show_se
#
# Notes:   This is for use in combination with outresults script.
# -

proc outheader {filename h2qindex col_2 show_se {epi 0}} {
    if {"/dev/null" != $filename} {
	set soutfile [open [full_filename $filename] w]
	fioutheader $soutfile $h2qindex $col_2 $show_se $epi
	close $soutfile
    }
    fioutheader stdout $h2qindex $col_2 $show_se $epi
    return ""
}

proc fioutheader {soutfile h2qindex col_2 show_se {epi 0}} {

    puts -nonewline $soutfile "           Model   "
    set shadow                "-------------------"
    if {[llength $col_2]} {
# expand col_2 to 9 spaces centered
	set blanks [expr 9 - [string length $col_2]]
	set prefix [format [catenate % [expr ceil($blanks/2.0)] s] ""]
	set suffix [format [catenate % [expr floor($blanks/2.0)] s] ""]
	set col_2 $prefix$col_2$suffix
	puts -nonewline $soutfile "   $col_2"
	set shadow         "$shadow   ---------"
    }
    puts -nonewline $soutfile "    Loglike  "
    set shadow         "$shadow  -----------"
    puts -nonewline $soutfile "     H2r  " 
    set shadow         "$shadow  --------"
    if {$show_se} {
	puts -nonewline $soutfile [format "%10s" "H2r SE "]
	set shadow "$shadow  --------"
    }

    for {set i 1} {$i <= $h2qindex} {incr i} {
	puts -nonewline $soutfile [format "%8s  " H2q$i]
	set shadow "$shadow  --------"
	if {$show_se} {
	    puts -nonewline $soutfile [format "%10s" "H2q$i SE"]
	    set shadow "$shadow  --------"
	}
    }
    if {$epi} {
	puts -nonewline $soutfile [format "%8s  " H2qE1]
	set shadow "$shadow  --------"
    }
    puts $soutfile "\n$shadow"
}

#  solar::outresults -- private
#
# Purpose: Output a line showing LOD and other stats for current model
#
# Usage:   outresults filename modelname lod loglike h2qindex err sefile
#
# Notes:   Model should be maximized before using this script.
# -

proc outresults {filename modelid lod loglik h2qindex {err 0} {sefile none} \
	{modelfile -none} {score none} {lcse none}} {
    set outstring [formatresults $modelid $lod $loglik $h2qindex $err $sefile \
	    $modelfile $score $lcse]
    puts stdout $outstring

    set soutfile [open [full_filename $filename] a]
    puts $soutfile $outstring
    close $soutfile

    return $outstring
}


proc formatresults {modelid lod loglik h2qindex {err 0} {sefile none} \
	{modelfile -none} {score none} {lcse none}} {

    set outstring [format %19s $modelid]

    if {"NA" != $lod} {
	if {[llength $lod]} {
	    set outstring "$outstring [format %11.4f $lod]"
	} else {
	    set outstring "$outstring [format %11s ""]"
	}
    }

    if {0 != [string compare [string toupper $loglik] NAN]} {
	set outstring "$outstring [format %12.3f $loglik]"
    } else {
	set outstring "$outstring [format %12s NaN]"
    }

    if {"-none" == $modelfile} {
	set par_h2rs [parameter h2r start]
    } else {
	set par_h2rs [oldmodel $modelfile h2r]
    }

    if {[string compare [string toupper $par_h2rs] NAN]} {
	set outstring "$outstring [format %9.6f $par_h2rs]"
    } else {
	set outstring "$outstring [format %9s NaN]"
    }

    for {set i 1} {$i <= $h2qindex} {incr i} {

	if {"-none" == $modelfile} {
	    set par_h2q [parameter h2q$i start]
	} else {
	    set par_h2q [oldmodel $modelfile h2q$i]
	}

	if {[string compare [string toupper $par_h2q] NAN]} {
	    set outstring "$outstring [format %9.6f $par_h2q]"
	} else {
	    set outstring "$outstring [format %9s NaN]"
	}
    }
    if {[check_epistasis]} {
	if {"-none" == $modelfile} {
	    set par_h2q [parameter h2qe1 start]
	} else {
	    set par_h2q [oldmodel $modelfile h2qe1]
	}

	if {[string compare [string toupper $par_h2q] NAN]} {
	    set outstring "$outstring [format %9.6f $par_h2q]"
	} else {
	    set outstring "$outstring [format %9s NaN]"
	}
    }
    if {"none" != $score} {
	if {[is_nan $score]} {
	    set outstring "$outstring [format %9s NaN]"
	} else {
	    set outstring "$outstring [format %9.6f $score]"
	}
    }
    if {"none" != $lcse} {
	if {[is_nan $lcse] || $lcse == 0} {
	    set outstring "$outstring [format %9s NaN]"
	} else {
	    set outstring "$outstring [format %9.6f $lcse]"
	}
    }

    if {$err == 1} {
	set outstring "$outstring BoundrErr"
    }
    if {$err == 2} {
	set outstring "$outstring ConvrgErr"
    }
    if {$err == 3} {
	set outstring "$outstring NoStdErr"
    }
    return $outstring
}

# solar::solartcl --
# Purpose:  Check SOLAR version compatibility of tcl file
#
# solar::solarmodel --
#
# Purpose:  Check SOLAR version compatibility of model
#
# solarmodel appears a the top of all new model files and identifies the
# model version.  If the version is incompatible with the current
# version, an error message is displayed.
#
# solartcl appears at the top of all upgraded script files.  SOLAR
# programmers are encoured to use solartcl as well.
#
# To upgrade solar models, use the "upgrade" command.
# -

proc solarmodel {version {beta ""}} {
# Currently there are no earlier versions except unidentified ones
    return "";
}

proc solartcl {version {beta ""}} {
# Currently there are no earlier versions except unidentified ones
    return "";
}

# solar::upgrade --
#
# Purpose:  Upgrade model files and scripts
#
# Usage:    upgrade modelname
#           upgrade scriptname.tcl
#
# Notes:   If successful, the new file will replace the original one.
#             The old file is saved with ".old" tacked on to the end of
#             the name (e.g. amodel.mod.old).
#
#          If an error is reported, the original file remains unchanged.
#
#          If the file is a model, the ".mod" extension is assumed even if
#             not specified.  Solar always tacks on ".mod" to the end of
#             model filenames.
#
#          If the file is a script, it must end with the ".tcl" extension,
#             and the extension must be specified in the command as shown.
#             Upgrade looks for this, and if found it assumes that a script
#             is being upgraded.
#
#          solartcl appears at the top of all upgraded script files.  SOLAR
#             programmers are encoured to use solartcl as well.
#
#
# -

proc upgrade {givenname} {
    set scriptmode 0
    set modelname $givenname
#
# Check for .tcl and .mod extensions
#
    set glength [string length $givenname]
    if {$glength > 4} {
	if {![string compare ".tcl" [string range $givenname \
		[expr $glength - 4] end]]} {
	    set scriptmode 1
	} elseif {[string compare ".mod" [string range $givenname \
		[expr $glength - 4] end]]} {
	    set modelname "$givenname.mod"
	}
    }
#
# Open model file and working output file
#
    set oldfile [open $modelname r]
    set newfile [open $modelname.new w]
#
# Skip over leading comments
#   (might be #!solar statement)
#
    while {[string length [set nextline [gets $oldfile]]]} {
	if {"#" != [string range $nextline 0 0]} {
	    break
	}
	puts $newfile $nextline
    }
#
# Output new version statement
#
    set newversion [lindex [solarversion] 0]
    set newbeta [lindex [solarversion] 1]
    if {$scriptmode} {
        puts $newfile "solartcl $newversion $newbeta"
    } else {
	puts $newfile "solarmodel $newversion $newbeta"
    }
#
# Set preliminary version (gets overwritten if solarmodel statment
#
    set oldversion 0
#
# Read and convert each line
#
    set parameter_count 0
    while {[string length $nextline]} {
	set token1 ""
	set token2 ""
	set token3 ""
	scan $nextline "%s %s %s" token1 token2 token3
#
# If this is a solarmodel or solartcl statement, record actual version
#
    if {"solarmodel" == $token1 || \
	    "solartcl" == $token1} {
	scan $token2 "%f" junk oldversion
#
# Convert phenfile line (That's going way back)
#
    } elseif {"phenfile" == $token1 || \
		"phenfile" == $token2} {
	    puts $newfile "phenotypes load token3"
#
# Convert old-style covariate line
#
	} elseif {"covariate" == $token1} {
	    set varname $token2
	    set xsex 0
	    set xexp ""
	    for {set i 2} {$i < [llength $nextline]} {incr i} {
		set modifier [lindex $nextline $i]
		if {"sex" == $modifier} {
		    set xsex 1
		} elseif {"exp" == $modifier} {
		    incr i
		    set xexp [lindex $nextline $i]
		}
	    }
	    if {0==[llength $xexp] || ![string compare $xexp 1]} {
		puts $newfile "covariate $varname"
		if {$xsex} {
		    puts $newfile "covariate $varname*sex"
		}
	    } elseif {![string compare $xexp 2]} {
		puts $newfile "covariate $varname^2"
		if {$xsex} {
		    puts $newfile "covariate $varname^2*sex"
		}
	    } elseif {![string compare $xexp 12]} {
		puts $newfile "covariate $varname"
		if {$xsex} {
		    puts $newfile "covariate $varname*sex"
		}
		puts $newfile "covariate $varname^2"
		if {$xsex} {
		    puts $newfile "covariate $varname^2*sex"
		}
	    } else {
		error "Invalid covariate statement: $nextline"
	    }
#
# Convert parameter names
#
	} elseif {"parameter" == $token1 && !$scriptmode} {
	    incr parameter_count
	    if {2 < $parameter_count} {
		set name $token2
		set firstch [string range $name 0 0]
#
#   Convert 2 to ^2
#
		if {"b" == $firstch || "m" == $firstch || "f" == $firstch} {
		    if {0==[string compare "2" \
			    [string range $name end end]]} {
			set pentul [expr [string length $name] - 2]
			set name "[string range $name 0 $pentul]^2"
		    }
		}
#
#   Convert 'm' and 'f' covariate prefixes
#
		if {"m" == $firstch} {
		    set name "b[string range $name 1 end]"
		} elseif {"f" == $firstch} {
		    set name "b[string range $name 1 end]*sex"
		}
		set nextline [lreplace $nextline 1 1 $name]
	    }
	    puts $newfile $nextline
#
# Check for 'multipoint -f' which is now 'multipoint -overwrite'
#
	} elseif {("multipoint" == $token1 || "scanloci" == $token1) && \
		-1 != [lsearch -exact $nextline "-f"]} {
	    set position [lsearch -exact $nextline "-f"]
	    set nextline [lreplace $nextline $position $position "-overwrite"]
	    set nextline [lreplace $nextline 0 0 "multipoint"]
	    puts $newfile $nextline
#
# Default: just write old line out
#
	} else {
	    puts $newfile $nextline
	}
	set nextline [gets $oldfile]
    }
    close $newfile
    close $oldfile
    eval file rename -force $modelname $modelname.old
    eval file rename -force $modelname.new $modelname
}

# solar::read_arglist --
#
# Purpose:  Read hyphenated optional arguments and argument-value pairs
#
# Usage:    read_arglist arglist [identifier_name value_var]+
#           value_var := varname | {statement block}
#
# Notes:
#          This provides a general way of handling argument lists of the
#          form:
#
#          [[arg] | [-identifier [value]]]+
#
#          Which is to say that there may be any number of "regular"
#          arguments and "hyphenated" arguments.  The "hyphenated"
#          arguments may be stand-alone or paired with values.  (Unlike
#          typical Unix syntax, stand-alone hyphenated arguments MAY NOT be
#          strung together, and hyphenated arguments with values must be
#          separated by space and not with some other character such as =).
#
#          The "regular" arguments (those not hyphenated or part of
#          a hyphenated pair) are put into a list which is returned by
#          this procedure.
#
#          Hyphenated arguments may either require following "value"
#          arguments or not allow them (in which case the hyphenated
#          argument acts like a switch).  Value arguments must be separated
#          from the hyphenated argument by a space (as is typical in Tcl).
#          For example
#
#              bar -height 1.5
#
#          There are two ways read_arglist can handle a hyphenated argument.
#
#          (1) The first, specified by the 'varname' expansion of value_var,
#          performs an assignment of the "value" argument to the caller's
#          varname variable.  For example:
#
#              read_arglist $args -height h
#
#          If $args contains "-height 1.5", then 1.5 will be assigned to the
#          caller's 'h' variable.  Note that this method always requires
#          a value argument and so does not work for switch arguments.
#
#          (2) The second, specified by the '{statement block}' expansion
#          of value_var executes an arbitrary set of expressions in
#          the caller's context.  This allows a simple switch or more
#          complex list-building.  The the statement block contains the
#          token VALUE, a value argument is required and the token
#          VALUE is replaced by the actual value argument.  Substitution
#          is performed only once and only for the first occurance of
#          VALUE.
#
#          A simple switch is implemented like this:
#
#              read_arglist $args -bottom {set bottom 1}
#
#          If $args contains "-bottom," bottom (in the caller's context) is
#          set to 1.  A value argument is neither required nor allowed.
#
#          A list-building argument is implemented like this:
#
#              read_arglist $args -include {lappend ilist VALUE}
#
#          If $args contains "-include foo" then "lappend ilist foo" is
#          executed in the caller's context.
#
#          NOTE that in the {statement block} form, the statement block
#          IS REQUIRED to have more than one list element.  A llength is
#          done to determine which form is being used.  Thus, you cannot
#          have:
#
#              read_arglist $args -exit {exit}  ;# *** BAD ***
#
#          but you could have
#
#              read_arglist $args -exit {eval exit}
#
#          If -* is used as an identifier_value, it matches any argument
#          in the argument list and causes that argument do be added to
#          the return list.  Normally -* should be the last identifier
#          value; all following identifier values will be ignored.
#          Also, the value_var (or statement block) following -* is never
#          assigned or executed and so can be anything.  This is intended
#          as an escape to permit only certain arguments to be processed
#          leaving other variables for processing by a later procedure.
#
#    More notes:
#
#          It is the responsibility of the caller to assign default
#          values before calling this procedure.
#
#          Hyphenated arguments may not have hyphenated strings for values.
#          However, hyphenated arguments may have negative numbers (e.g.
#          -1.2e5) for values.  If the value does not parse as an integer
#          or floating point number, it must not begin with hyphen.  If
#          the token following a hyphenated argument begins with - but is
#          not a number, it is considered to be another hyphenated argument
#          (which may cause the preceding argument to be flagged as having
#          a missing value).
#
#          Hyphenated argument names must not be numbers (integer or floating
#          point).  For example, you may not have "-1" or "-1.2e5" as a
#          hyphenated argument.
#          
#          Hyphenated arguments which do not match any of the templates
#          given raise the "Invalid argument %s".
#
#          The identifier matching rule is case insensitive.
#
# -

proc read_arglist {arg_list args} {
    set case_insensitive 1

    set pname [lindex [info level [expr [info level] - 1]] 0]
    set target_pairs $args
    set return_list {}
    for {set i 0} {$i < [llength $arg_list]} {incr i} {
	set identifier [lindex $arg_list $i]
	if {"-" != [string range $identifier 0 0]} {
	    lappend return_list $identifier
	    continue
	}
	if {$case_insensitive} {
	    set identifier [string tolower $identifier]
	}
	set found 0
	for {set j 0} {$j < [llength $target_pairs]} {incr j 2} {
	    set target [lindex $target_pairs $j]
	    if {"-*" == $target} {
		set found 1
		lappend return_list $identifier
		break
	    }
	    if {$case_insensitive} {
		set target [string tolower $target]
	    }
	    if {![string compare $target $identifier]} {
		set varname [lindex $target_pairs [expr $j + 1]]
		if {[llength $varname] > 1} {
#
# statement evaluation specified
#
		    if {-1 != [string first VALUE $varname]} {
#
# expression has VALUE substitution so value is required
#
			incr i
			if {[llength $arg_list] < [expr $i + 1]} {
			error \
		   "Missing value for argument $identifier to procedure $pname"
			}
			set value [lindex $arg_list $i]
			if {"-" == [string range $value 0 0] && \
			    ![is_float [lindex $value 0]]} {
			    error \
		   "Missing value for argument $identifier to procedure $pname"
			}
#
# perform ONE substitution
#
			set position [string first VALUE $varname]
			set length [string length $varname]
			set left [expr $position - 1]
			set right [expr $position + $length]
			set block \
"[string range $varname 0 $left]$value[string range $varname $right end]"
                    } else {
			set block $varname
		    }
		    uplevel $block
#
# "simple" assignment specified ; argument always required
#
		} else {
		    incr i
		    if {[llength $arg_list] < [expr $i + 1]} {
			error \
		   "Missing value for argument $identifier to procedure $pname"
		    }
		    set value [lindex $arg_list $i]
		    if {"-" == [string range $value 0 0] && \
			    ![is_float [lindex $value 0]]} {
			error \
		   "Missing value for argument $identifier to procedure $pname"
		    }
		    upvar $varname callervar
		    set callervar $value
		}
		set found 1
		break
	    }
	}
	if {!$found} {
	    error "Invalid argument [lindex $arg_list $i] to procedure $pname"
	}
    }
    return $return_list
}

proc test_read_arglist {args} {
    set cutoff 6
    set h2r_factor 1.1
    set restart 0
    set force 0
    set include_list {}

    read_arglist $args -cutoff cutoff -h2rf h2r_factor -restart {restart 1} \
	    -f {set force 1} \
	    -include {lappend include_list VALUE}

    puts "cutoff:     $cutoff"
    puts "h2r_factor: $h2r_factor"
    puts "restart:    $restart"
    puts "force:      $force"
    puts "include_list: $include_list"
}    

# solar::markertest --
#
# Purpose:  Test markerfile for discrepancies; list blankable ID's
#
# Usage:    markertest <markerfile> [<marker>]* [<option>]
#           <option> := -1 | -many | -ped | -fam <famfile> | -2
#
#           <markerfile> is either exact filename or pattern including *
#           <marker> (optional) is either exact name or pattern including *
#           If no markers are specified, all markers in markerfile are tested.
#           Each marker is tested individually.
#
#           Results are recorded in markertest.out in current directory.
#           During normal operation, many error and warning messages may
#           appear.  Ignore these messages until markertest has finished.
#
#           If no options are specified, a flexible procedure is used that
#           should work in nearly all cases.  It is the same as markertest -1
#           followed by markertest -many if necessary.  IMPORTANT: Read the 
#           following two paragraphs to understand how those options work.
#
#           -1     Blank one individual at a time.  Report all individual
#                  blankings (if any) that fix discrepancy.  If this succeeds
#                  only one of the reported individuals needs to be blanked
#                  and it is up to user to pick the best one.  However, this
#                  procedure is good (if it works) because it will list all
#                  the possibilities, and it is relatively fast.  But if it
#                  is necessary to blank more than one individual AT THE 
#                  SAME TIME, this procedure will fail, so it is frequently
#                  inadequate by itself.
#
#           -many  Blank the first individual, and, if that doesn't fix
#                  discrepancy, blank the second individual, and then the
#                  third, and so on, until the discrepancy is fixed.  Then, 
#                  unblank all the individuals that can be unblanked without
#                  a discrepancy returning.  The result is one set of
#                  individuals that ALL NEED TO BE BLANKED AT THE SAME TIME.
#                  It is not necessarily the only such set of individuals,
#                  or the smallest set.  This procedure should always succeed
#                  in finding a set of blankable individuals.  (This
#                  option used to be named -r.)
#
#           -ped    Rather than blanking only one ID at a time, blank
#                   whole "extended pedigrees" at a time.  Blankable
#                   pedigrees are identified by pedindex.out PEDNO
#                   (the index created by "load pedigree") and
#                   by the first ID found in that pedigree.  This procedure
#                   is the fastest, and is useful in identifying errant
#                   pedigrees, provided there is only one errant pedigree.
#
#           -fam    Rather than blanking only one ID at a time, blank
#                   nuclear families (or other groups) identified by
#                   "famno."  The "famfile" must contain records
#                   including "ID" (permanent ID) and "FAMNO" (other
#                   fields will be ignored).  There may be more than
#                   one record for each ID.  Records may not use
#                   "FAMID" to disambiguate ID's.
#
#           -2      Try blanking combinations of 2 ID's until one such pair
#                   of blankings fixes the discrepancy.  Because this is an
#                   N x N problem, it may take a VERY LONG TIME to finish, but
#                   if you are convinced there is one pair that needs to be
#                   blanked, this procedure will find it.
#
# Notes:    Pedigree file must already have been loaded
#
#           Markerfile must have ID and and marker(s) only.  Each marker is
#           analyzed separately.  Results for all markers are reported in
#           markertest.out, which is divided into one section for each
#           marker.
#
#           Output is written to markertest.out which is divided into one
#           section for each marker.
# -

# proc markertest is now a selector which selects the appropriate subroutines

proc markertest {args} {
    set markertest1 0
    set markertest2 0
    set markertestmany 0
    set markertestped 0
    set markertestfam 0
    set famnofile ""

# Clean out old markertest.out

    set ofile [open markertest.out w]
    close $ofile

# Parse arguments

    set moreargs [read_arglist $args \
	    -1 {set markertest1 1} \
	    -2 {set markertest2 1} \
	    -many {set markertestmany 1} \
	    -r {set markertestmany 1} \
	    -ped {set markertestped 1} \
	    -fam famnofile]

# If famnofile, ensure that it exists

    if {"" != $famnofile} {
	if {![file exists $famfilename]} {
	    error "File $famfilename not found"
	}
	set markertestfam 1
    }

# Ensure that more than one option hasn't been specified

    set optioncount [expr $markertest1 + $markertest2 + $markertestmany + \
	    $markertestped + $markertestfam]
    if {$optioncount > 1} {
	error "Only one option may be specified at a time or use default"
    }

# First non-option argument is markerfilename(s)

    if {0==[llength $moreargs]} {
	error "Usage: markertest <markerfile> \[<marker>\]* \[<option>\]"
    }
    set markerfiles [lindex $moreargs 0]
    if {-1 != [string first * $markerfiles]} {
	set markerfiles [glob $markerfiles]
    }
    if {{} == $markerfiles || ![file exists [lindex $markerfiles 0]]} {
	error "No such file(s): $markerfiles"
    }

# Following arguments, if any, are markername(s)

    set markernames [lrange $moreargs 1 end]

# MAIN LOOP (for each markerfile)

    set markers_tested 0
    set discrep_found 0
    set discrep_solved 0
    foreach markerfile $markerfiles {
	set mfile [solarfile open $markerfile]
	if {![solarfile $mfile test_name ID]} {
	    putsteer markertest.out \
	      "Error!  Markerfile $markerfile does not have ID or EGO field\n"
	    solarfile $markerfile close
	    continue
	}
	set mfnames [lrange [solarfile $mfile names] 1 end]

# Make a list of markernames to test from this markerfile

	if {{} == $markernames} {
	    set testmarkers $mfnames
	} else {
	    set testmarkers {}
	    foreach markername $markernames {
		foreach mfname $mfnames {
		    if {-1 == [string first * $markername]} {
			if {[string_imatch $mfname $markername]} {
			    lappend testmarkers $mfname
			}
		    } else {
			set mf [string tolower $mfname]
			set m [string tolower $markername]
			if {[string match $m $mf]} {
			    lappend testmarkers $mfname
			}
		    }
		}
	    }
	}
	putsteer markertest.out \
		"\nMarkers to test in file $markerfile:\n$testmarkers"

# Build temporary file(s) with just one marker

	foreach marker $testmarkers {
	    incr markers_tested
	    putsteer markertest.out \
"\n   **********   Testing $marker   ******************************\n"

            solarfile $mfile rewind
	    solarfile $mfile start_setup
	    solarfile $mfile setup ID
	    solarfile $mfile setup $marker
	    set ofile [open markertest.$marker.tmp w]
	    puts $ofile "ID,$marker"
	    while {{} != [set record [solarfile $mfile get]]} {
		puts $ofile "[lindex $record 0],[lindex $record 1]"
	    }
	    close $ofile

# See if there are discrepancies for this marker

	    if {[catch {load marker markertest.$marker.tmp}]} {
		error "Error loading markerfile generated for $marker"
	    }
	    if {![catch {marker discrep}]} {
		putsteer markertest.out \
			"\nNo discrepancies found for $marker\n"
	    } else {
		incr discrep_found
		putsteer markertest.out \
			"Analyzing discrepancies found for $marker...\n"

# Apply user-selected method

		if {$markertestfam} {
		  set status [markertest_fam markertest.$marker.tmp $famnofile]
		} elseif {$markertestmany} {
		   set status [markertest_r markertest.$marker.tmp]
		} elseif {$markertest2} {
		    set status [markertest_2 markertest.$marker.tmp]
		} elseif {$markertest1} {
		    set status [markertest_1 markertest.$marker.tmp]
		} elseif {$markertestped} {
		    set status [markertest_ped markertest.$marker.tmp]
		} else {

# Apply default method: first do markertest_1, if that fails, try _r

		    putsteer markertest.out \
			    "First, try blanking one person at a time...\n"
		    set status [markertest_1 markertest.$marker.tmp]
		    if {$status} {
			putsteer markertest.out \
				"\nCould not find any one-person solutions"
			putsteer markertest.out \
				"Searching for a many-person solution...\n"
			set status [markertest_r markertest.$marker.tmp]
		    }
		}
		if {$status} {
		    putsteer markertest.out \
	       "No solutions could be found for discrepancy in marker $marker"
		} else {
		    incr discrep_solved
		}
		file delete markertest.$marker.tmp
	    }
	}
	solarfile $mfile close
    }
    putsteer markertest.out \
"\n************************* Summary of Results ***************************\n"

    putsteer markertest.out \
	    "$markers_tested markers tested"
    putsteer markertest.out \
	    "$discrep_found discrepancies found"
    if {$discrep_found} {
	putsteer markertest.out "$discrep_solved discrepancies solved"
    }
    puts "\nResults written to file markertest.out"
}


#
# Subroutine to fix discrepancy by blanking one person at a time
#
proc markertest_1 {markerfilename} {

    set DEBUG 1

# Open markerfile and setup ID,MARKER

    set mfile [solarfile open $markerfilename]
    set names [solarfile $mfile names]
    solarfile $mfile start_setup
    solarfile $mfile setup ID
    set markername [lindex [solarfile $mfile names] 1]
    solarfile $mfile setup $markername

# Read marker file into memory

    set Id {}
    set Marker {}
    set count 0
    while {{} != [set record [solarfile $mfile get]]} {
	lappend Id [lindex $record 0]
	lappend Marker [lindex $record 1]
	incr count
    }
    solarfile $mfile close

    if {$DEBUG} {puts "Read $count records from $markerfilename"}

# MAIN LOOP

    set Bad_Id {}
    for {set i 0} {$i < $count} {incr i} {
	if {"" == [lindex $Marker $i]} {
	    continue
	} else {

# Write temporary marker file

	    if {$DEBUG} {puts "Blanking [lindex $Id $i]..."}

	    set tfile [open "markertest.tmp" w]
	    puts $tfile "id,$markername"
	    for {set j 0} {$j < $count} {incr j} {
		if {$j != $i} {
		    puts $tfile "[lindex $Id $j],[lindex $Marker $j]"
		} else {
		    puts $tfile "[lindex $Id $j],"
		}
	    }
	    close $tfile

# Load temporary marker file and check for discrepancies

	    load marker markertest.tmp
	    if {![catch {marker discrep}]} {

# Write suspect ID to file and add to list

		putsteer markertest.out \
	       "Blanking this one ID fixes marker $markername: [lindex $Id $i]"
		lappend Bad_Id [lindex $Id $i]
	    }
	}
    }

    file delete markertest.tmp

# See if we got anything

    if {0==[llength $Bad_Id]} {
	return 1
    }
    return 0

}


#
# Subroutine to fix discrepancy by blanking everyone until fixed
#
proc markertest_r {markerfilename} {

    set DEBUG 1

# Open markerfile and setup ID,MARKER

    set mfile [solarfile open $markerfilename]
    set names [solarfile $mfile names]
    solarfile $mfile start_setup
    solarfile $mfile setup ID
    set markername [lindex [solarfile $mfile names] 1]
    solarfile $mfile setup $markername

# Read marker file into memory

    set Id {}
    set Marker {}
    set count 0
    while {{} != [set record [solarfile $mfile get]]} {
	lappend Id [lindex $record 0]
	lappend Marker [lindex $record 1]
	incr count
    }
    solarfile $mfile close

    if {$DEBUG} {puts "Read $count records from $markerfilename"}

# MAIN LOOP 1
#   Remove each ID until problem goes away

    for {set i 0} {$i < $count - 1} {incr i} {
	if {"" == [lindex $Marker $i]} {
	    continue  ;# It's already blank, no need to test
	} else {

# Write temporary marker file

	    puts "Blanking [lindex $Id $i]..."

	    set tfile [open "markertest.tmp" w]
	    puts $tfile "id,$markername"
	    for {set j 0} {$j < $count} {incr j} {
		if {$j > $i} {
		    puts $tfile "[lindex $Id $j],[lindex $Marker $j]"
		} else {
		    puts $tfile "[lindex $Id $j],"
		}
	    }
	    close $tfile

# Load temporary marker file and check for discrepancies

	    load marker markertest.tmp
	    if {![catch {marker discrep}]} {

# This final straw fixed the problem.  Break here.
		
		break
	    }
	}

# Didn't fix problem.  Keep on going.

    }

# If we removed more than first ID (more than likely) we must continue

    set Bad_Index 0   ;# If $i==0, this is answer
    if {$i > 0} {
	set Bad_Index {}

# MAIN LOOP 2
#   Add back in all that can be added back in

	set last_removed $i
	for {set i 0} {$i <= $last_removed} {incr i} {
	    
# Skip over already blank markers

	    if {"" == [lindex $Marker $i]} {
		continue
	    }

# Write temporary marker file

	    puts "Unblanking [lindex $Id $i]..."

	    set tfile [open "markertest.tmp" w]
	    puts $tfile "id,$markername"
	    for {set j 0} {$j < $count} {incr j} {

		set removed_list {}

		if {"" == [lindex $Marker $j]} {
		    puts $tfile "[lindex $Id $j],"
		} else {
		    if {$j <= $i && -1 == [lsearch $Bad_Index $j]} {
			puts $tfile "[lindex $Id $j],[lindex $Marker $j]"
		    } elseif {$j > $last_removed} {
			puts $tfile "[lindex $Id $j],[lindex $Marker $j]"
		    } else {
			puts $tfile "[lindex $Id $j],"
			lappend removed_list $j
		    }
		}
	    }
	    close $tfile

	    if {$DEBUG} {puts "Removed: $removed_list"}

# Load temporary marker file and check for discrepancies

	    load marker markertest.tmp
	    if {[catch {marker discrep}]} {

# This individual brough problem back.  Add to Bad_Index list

		lappend Bad_Index $i
	    }
	}
    }

    if {0==[llength $Bad_Index]} {
	putsteer markertest.out \
	       "Error 1 in markertest algorithm; contact solar@txbiomedgenetics.org"
	exit
    }

    file delete markertest.tmp

    if {$count == [llength $Bad_Index]} {
	putsteer markertest.out \
		"All ID's must be blanked for marker $markername !!!"
	return 1
    }

# Write output file
    
    putsteer markertest.out \
	    "All of the following ID's must be blanked for marker $markername:"
    foreach i $Bad_Index {
	putsteer markertest.out \
	     "        [lindex $Id $i]"
    }
    putsteer markertest.out " "
    return 0
}


proc markertest_ped {args} {

    set DEBUG 1

# Open markerfile and setup ID,MARKER

    set mfile [solarfile open $markerfilename]
    set names [solarfile $mfile names]
    solarfile $mfile start_setup
    solarfile $mfile setup ID
    set markername [lindex [solarfile $mfile names] 1]
    solarfile $mfile setup $markername

# Read marker file into memory

    set Id {}
    set Marker {}
    set count 0
    while {{} != [set record [solarfile $mfile get]]} {
	lappend Id [lindex $record 0]
	lappend Marker [lindex $record 1]
	incr count
    }
    solarfile $mfile close

    if {$DEBUG} {puts "Read $count records from $markerfilename"}

# Read pedindex.out and setup Pedno and First_ID associative arrays

    set pfile [solarfile open pedindex.out]
    solarfile $pfile setup ID
    solarfile $pfile setup PEDNO
    set highest_pedno 0
    while {{} != [set record [solarfile $pfile get]]} {
	set this_pedno [lindex $record 1]
	set Pedno([lindex $record 0]) $this_pedno
	if {$highest_pedno < $this_pedno} {
	    set highest_pedno $this_pedno
	    set First_ID($this_pedno) [lindex $record 0]
	}
    }
    solarfile $pfile close

# MAIN LOOP

    set Bad_Pedno {}
    for {set i 1} {$i < $highest_pedno} {incr i} {

# Write temporary marker file

	if {$DEBUG} {puts "Blanking pedindex.out PEDNO $i ..."}
	
	set tfile [open "markertest.tmp" w]
	puts $tfile "id,$markername"
	for {set j 0} {$j < $count} {incr j} {
	    if {$Pedno([lindex $Id $j]) != $i} {
		puts $tfile "[lindex $Id $j],[lindex $Marker $j]"
	    } else {
		puts $tfile "[lindex $Id $j],"
	    }
	}
	close $tfile
	
# Load temporary marker file and check for discrepancies

	load marker markertest.tmp
	if {![catch {marker discrep}]} {

# Write suspect PEDNO to file and add to list

	    putsteer markertest.out \
 "Fixed by blanking pedindex PEDNO [format %s $i] with first ID: $First_ID($i)"
	    lappend Bad_Pedno $i
	}
    }

    file delete markertest.tmp

# See if we got anything

    if {0==[llength $Bad_Pedno]} {
	return 1
    }
    return 0
}


proc markertest_fam {markerfilename famfilename} {

    set DEBUG 1

# Open markerfile and setup ID,MARKER

    set mfile [solarfile open $markerfilename]
    set names [solarfile $mfile names]
    solarfile $mfile start_setup
    solarfile $mfile setup ID
    set markername [lindex [solarfile $mfile names] 1]
    solarfile $mfile setup $markername

# Read marker file into memory

    set Id {}
    set Marker {}
    set count 0
    while {{} != [set record [solarfile $mfile get]]} {
	lappend Id [lindex $record 0]
	lappend Marker [lindex $record 1]
	incr count
    }
    solarfile $mfile close

    if {$DEBUG} {puts "Read $count records from $markerfilename"}

# Read famfile and setup Members (array of lists of members for each famno)
# and Families_Found (list of famnos)

    set ffile [solarfile open $famfilename]
    solarfile $ffile setup ID
    solarfile $ffile setup FAMNO
    set Families_Found {}
    while {{} != [set record [solarfile $ffile get]]} {
	set this_id [lindex $record 0]
	set this_fam [lindex $record 1]
	if {[catch {set memlist $Members($this_fam)}]} {
	    set Members($this_fam) $this_id
	} else {
	    lappend memlist $this_id
	    set Members($this_fam) $memlist
	}
	if {-1==[lsearch $Families_Found $this_fam]} {
	    lappend Families_Found $this_fam
	}
    }
    solarfile $ffile close

# MAIN LOOP

    set Bad_Famno {}
    foreach famno $Families_Found {

# Write temporary marker file

	if {$DEBUG} {puts "Blanking members of Famno $famno..."}
	set memlist $Members($famno)
#	puts $memlist
	
	set tfile [open "markertest.tmp" w]
	puts $tfile "id,$markername"
	for {set j 0} {$j < $count} {incr j} {
	    set this_person [lindex $Id $j]
	    if {-1==[lsearch $memlist $this_person]} {
		puts $tfile "$this_person,[lindex $Marker $j]"
	    } else {
		puts $tfile "$this_person,"
	    }
	}
	close $tfile
	
# Load temporary marker file and check for discrepancies

	load marker markertest.tmp
	if {![catch {marker discrep}]} {

# Write suspect FAMNO to file and add to list

	    set ofile [open markertest.out a]
	    puts $ofile "FAMNO $famno"
	    close $ofile
	    lappend Bad_Famno $famno
	}
    }

    file delete markertest.tmp

# See if we got anything

    if {0==[llength $Bad_Famno]} {
	return 1
    }
    return 0
}


proc markertest_2 {markerfilename} {

    set DEBUG 1

# Open markerfile and setup ID,MARKER

    set mfile [solarfile open $markerfilename]
    set names [solarfile $mfile names]
    solarfile $mfile start_setup
    solarfile $mfile setup ID
    set markername [lindex [solarfile $mfile names] 1]
    solarfile $mfile setup $markername

# Read marker file into memory

    set Id {}
    set Marker {}
    set count 0
    while {{} != [set record [solarfile $mfile get]]} {
	lappend Id [lindex $record 0]
	lappend Marker [lindex $record 1]
	incr count
    }
    solarfile $mfile close

    if {$DEBUG} {puts "Read $count records from $markerfilename"}

# MAIN LOOP

    set Bad_Id {}
    for {set i 0} {$i < $count} {incr i} {
	for {set j [expr $i + 1]} {$j < $count} {incr j} {
	    if {$i == $j || "" == [lindex $Marker $i] || \
		    "" == [lindex $Marker $j]} {
		continue
	    } else {

# Write temporary marker file

		if {$DEBUG} {puts "Blanking [lindex $Id $i] and [lindex $Id $j]"}

		set tfile [open "markertest.tmp" w]
		puts $tfile "id,$markername"
		for {set out_i 0} {$out_i < $count} {incr out_i} {
		    if {$out_i != $i && $out_i != $j} {
			puts $tfile "[lindex $Id $out_i],[lindex $Marker $out_i]"
		    } else {
			puts $tfile "[lindex $Id $out_i],"
		    }
		}
		close $tfile

# Load temporary marker file and check for discrepancies

		load marker markertest.tmp
		if {![catch {marker discrep}]} {

# Write suspect ID to file and add to list
		    putsteer markertest.out \
       "Blanking this pair fixes $markername: [lindex $Id $i],[lindex $Id $j]"
		    lappend Bad_Id [list [lindex $Id $i] [lindex $Id $j]]
		}
	    }
	}
    }

# See if we got anything

    file delete markertest.tmp

    if {0==[llength $Bad_Id]} {
	return 1
    }
    return 0
}

# End of markertest and subroutines


#
# General SOLAR Utilities...little programs could be useful to many scripts
#
# (Unfortunately this section wasn't created earlier, so general SOLAR
#  utilities are actually scattered throughout the code.)
#

proc wordcap {word} {
    return [catenate [string toupper [string index $word 0]] \
		[string range $word 1 end]]
}

proc append_mod {given_name} {return [append_extension $given_name ".mod"]}

proc append_extension {given_name extension} {
    if {-1 == [string first $extension $given_name]} {
	return [format "%s%s" $given_name $extension]
    } else {
	return $given_name
    }
}
	
# solar::if_parameter_exists
#
# Purpose:  Check if a parameter exists without creating it
#
# Usage:    if_parameter_exists <parameter_name>
#
#           Returns 1 if parameter exists, 0 otherwise.
#
# Notes:    This is used in scripts in a "if" statement.  For example:
#
#           if {[if_parameter_exists h2q1]} {
#               constraint e2 + h2r + h2q1 = 0
#           }
#
# -

proc if_parameter_exists {pname} {
    if {[catch {parameter $pname start}]} {
	return 0
    }
    return 1
}


# solar::startclock
# solar::stopclock
#
# Purpose: simple local timer (elapsed time)
#
# Usage: startclock;<timed command>;stopclock
#
# See Also: timediff
#-

proc startclock {} {
    global SOLAR_startclock
    set SOLAR_startclock [exec date]
    puts $SOLAR_startclock
    return $SOLAR_startclock
}

proc stopclock {} {
    global SOLAR_startclock
    set outstring "[timediff $SOLAR_startclock [exec date]] seconds"
    puts $outstring
    return $outstring
}

# solar::timediff --
#
# Purpose:  Calculate seconds between two system time strings
#
# Usage: timediff <start-time> <end-time>
#
# See Also: startclock, stopclock
#
# set starttime [exec date]
#  ... procedure to be timed
# set endtime [exec date]
# return "seconds: [timediff $starttime $endtime]"
#

proc timediff {first last} {
    set firstsub [findcolons $first]
    set lastsub [findcolons $last]
    set flist [split $firstsub :]
    set llist [split $lastsub :]
    if {3 != [llength $flist]} {
	error "invalid time specifier: $firstsub"
    }
    if {3 != [llength $llist]} {
	error "invalid time specifier $lastsub"
    }
    set fseconds [makenumber [lindex $flist 2]]
    set lseconds [makenumber [lindex $llist 2]]

    set fseconds [expr $fseconds + 60 * [makenumber [lindex $flist 1]]]
    set lseconds [expr $lseconds + 60 * [makenumber [lindex $llist 1]]]

    set fseconds [expr $fseconds + 3600 * [makenumber [lindex $flist 0]]]
    set lseconds [expr $lseconds + 3600 * [makenumber [lindex $llist 0]]]

    if {$lseconds < $fseconds} {
	set lseconds [expr $lseconds + 3600*24]
    }

    set total [expr $lseconds - $fseconds]

    return $total
}

proc findcolons {alist} {
    foreach term $alist {
	if {-1!=[string first : $term]} {
	    return $term
	}
    }
    error "No time found in $alist"
}

proc makenumber {string} {
    if {0==[string first 0 $string]} {
	set string [string range $string 1 end]
    }
    return $string
}

# solar::d2e2 --
# solar::d2e --
#
# Purpose: convert Fortran D style exponents to E form
#
# Usage: d2e <inputfilename> <outputfilename>
#        d2e2 <inputfilename> <outputfilename>
#
# d2e2 starts with line 2, so as not to disturb D's in the header line of
# comma delimited files.  d2e is more suitable for pedsys files.
#
# SOLAR now understands Fortran D style in phenotypes files in most
# circumstances anyway, so this conversion is not generally needed.  You
# will know you need this if you see error messages.
# -

proc d2e2 {infilename outfilename} {

    set infile [open $infilename]
    set outfile [open $outfilename w]

    set linecount 0
    while {-1 != [gets $infile line]} {
	set outline $line
	if {$linecount} {
	    catch {
		set outline [regsub -all D $line E]
	    }
	}
	incr linecount
	puts $outfile $outline
    }
    close $infile
    close $outfile
}

proc d2e {infilename outfilename} {

    set infile [open $infilename]
    set outfile [open $outfilename w]

    while {-1 != [gets $infile line]} {
	set outline $line
	catch {
	    set outline [regsub -all D $line E]
	}
	puts $outfile $outline
    }
    close $infile
    close $outfile
}




# solar::read_output  --
#
# Purpose:  Read variable statistics from maximization output file
#
# Usage:    read_output <outfile> <varname> [-mean | -min | -max | -std]
#
#           -mean Get variable mean (default)
#           -min  Get variable minimum
#           -max  Get variable maximum
#           -std  Get variable standard deviation
#           -d    1 if discrete, 0 otherwise
#
# Note:     If outfile is not full pathname, current trait/outdir is assumed.
#           Statistics pertain to actual sample used in maximization.
#
# Example:  read_output null1.out q4 -std
# -

proc read_output {outfile varname {getfield -mean}} {
    return [getvar $getfield $outfile $varname]
}


proc getvar {mean_min_max_std outfile varname} {

# Set index for kind of value required

    if {![string compare $mean_min_max_std -mean]} {
	set get_index 1
    } elseif {![string compare $mean_min_max_std -min]} {
	set get_index 3
    } elseif {![string compare $mean_min_max_std -max]} {
	set get_index 4
    } elseif {![string compare $mean_min_max_std -std]} {
	set get_index 2
    } elseif {![string compare $mean_min_max_std -d]} {
	set get_index 0
    } else {
	error "getvar needs -mean -min -max -std -d"
    }

# Expand pathname if not specified

    if {-1 == [string first / $outfile]} {
	set outfile [full_filename $outfile]
    }

# Check for file

    if {![file exists $outfile]} {
	error "No such file $outfile"
    }

# Normalize varname to lowercase

    set varname [string tolower $varname]

# Scan output file for variable statistics

    set ofile [open $outfile r]
    set found_variables {}
    set found_variable_means 0
    while {-1 != [gets $ofile line]} {
	if {-1 != [string first "Descriptive Statistics for the Variables" \
		$line]} {
	    gets $ofile line      ;# blank
	    gets $ofile line      ;# headings
	    set found_variable_means 1
	    break
	}
    }
    if {!$found_variable_means} {
	close $ofile
	error "Did not find variable means in output file $outfile"
    }

# Scan for the variable we want (or variable* because * means discrete)

    while {-1 != [gets $ofile line]} {
	if {6 != [llength $line]} {break}
	set thisname [string tolower [lindex $line 0]]
	if {![string compare $varname $thisname] || \
		![string compare $varname* $thisname]} {

# return the mean value

	    close $ofile
	    if {$get_index} {
		return [lindex $line $get_index]
	    } else {
		if {[string index $thisname end] != "*"} {
		    return 0
		} else {
		    return 1
		}
	    }
	}
    }
    close $ofile
    error "Did not find variable $varname in output file $outfile"
}
    
# solar::selectrecords --
#
# Purpose:  Select records from a file and copy them to a new file
#
# Usage:    selectrecords <infile> [<outfile>] [{<conditions>}]*
#
#           If not specified, <outfile> defaults to selectrecords.out
#
#           Each <condition> is a Tcl conditional expression which includes
#           field names in the file preceded by dollar sign $.
#           Field names are case insensitive (you need not match
#           capitalization used in file itself).  Each condition
#           must be enclosed in curly braces and spaced from other conditions
#           if any.  
#
#           Conditions may also include actual Tcl variables, preceded by $$
#           Tcl variables are Case Sensitive.
#
#           Simple examples are shown, but any valid Tcl expression operators
#           and functions may be used, and expressions may be arbitrarily
#           complex...they are evaluated by the Tcl expression parser, with
#           the exception of special pre-substitution of $$ variables.
#           Internally, $$ variables are upvar'd into a local variables having
#           leading capital S.
#
#           If a condition includes a non-existant field, it will never be
#           satisfied, producing an empty result file.  (In future, error
#           detection may be added.)  If a condition includes a undefined $$
#           tcl variable, an error will result.
#
#           Input file may be either PEDSYS or Comma Delimited format.
#           Output file is comma delimited.
#
#           If the first condition does not include any dollar signs,
#           it must include spaces (for example, {1 == 1}).  No such
#           requirement is made for subsequent conditions.  It seems pointless
#           to have condition without dollar signs anyway; if no condition
#           is given you get all records (the "null condition" is always true).
#               
# Example:  selectrecords phen.dat out.dat {$bmi > 0.3} {$famid == 10}
#
#           for {set F 1} {$F < 100} {incr F} {
#               selectrecords phen.dat out$F.dat {$bmi > 0.3} {$famid == $$F}
#           }
#
# Note:     Records are "selected" when they match ALL conditions given (unless
#           condition includes a non-existing field or has other error).
#
# -

proc selectrecords {args} {
#
# one of the tricks here is that file fields ultimately become
# variables.  This is fine unless the user specifies a variable that
# doesn't exist as a field, then it *may* inherit the value of a variable
# within this procedure (!).  To prevent this, all local variables have
# capitalization, never starting with capital S (used for Tcl variables).
#
    set inConditions {}
    set Files {}

# we used to test for presence of spaces, just to determine whether 2nd
# argument was filename or not.  Problem is, we can't test directly for
# braces.  But we can test for dollar sign, which would also be proof that
# argument was enclosed in braces (otherwise any dollar signs would be
# pre-applied in caller's context and not visible here).
#
# Now we permit either test, so spaces are not required except in unusual
# constant case like {1 == 1}, and only for first condition, so now it is
# highly unlikely for non-documentation-reading user (like most) to make error.
#
    if {[llength $args] == 0} {
	error "usage: selectrecords <input> [<output>] [<conditions>]*"
    }

    for {set I 0} {$I < [llength $args]} {incr I} {
	set Arg [lindex $args $I]
	if {$I == 0} {
	    lappend Files $Arg

	} elseif {$I == 1} {
	    if {[llength $Arg] > 1 || -1 != [string first "\$" $Arg]} {
		lappend inConditions $Arg
	    } else {
		lappend Files $Arg
	    }
	} else {
	    lappend inConditions $Arg
	}
    }

    set Conditions [string tolower $inConditions]

    set inFilename [lindex $Files 0]
    set outFilename selectrecords.out
    if {2<=[llength $Files]} {
	set outFilename [lindex $Files 1]
    }

    set iFile [tablefile open $inFilename]
    set Names [string tolower [tablefile $iFile names]]
    set oFile [open $outFilename w]

    tablefile $iFile start_setup
    set Header ""
    set Name_count 0
    foreach Name $Names {
	if {![unique $Name $Names]} {
	    if {[string tolower $Name] != "blank"} {
		puts "field Name $Name not included because it is not unique"
	    }
	} else {
	    tablefile $iFile setup $Name
	    incr Name_count
	    if {"" == $Header} {
		set Header $Name
	    } else {
		set Header "$Header,$Name"
	    }
	}
    }
#
# This needs more work, intended to
# give warning if Conditions use non-existing fieldName
#
#    set testc $Conditions
#    foreach icon $testc {
#	set con $icon
#	while {1} {
#	    set fdd [string first "\$" $con]
#	    if {$fdd < 0} break
#	    set termstart [expr $fdd + 1]
#	    set termend [expr [string wordend $con $termstart] - 1]
#	    set Name [string range $con $termstart $termend]
#	    puts "fieldName is $Name"
#	    if {-1 == [lsearch $Names $Name]} {
#		puts "warning: $Name not present in file"
#		puts "this condition will be ignored: $icon"
#		break
#	    }
#	    set con [string range $con [expr $termend + 1] end]
#	}
#    }
	

# permit Tcl variables using $$
# insert modified case-sensitive version back into case lowered string

    set Debugvar 0
    set Varlist {}
    while {1} {
	set fDD [string first "\$\$" $inConditions]
	if {$fDD < 0} break
	set Termstart [expr $fDD + 2]
	set Termend [expr [string wordend $inConditions $Termstart] - 1]
	set Name [string range $inConditions $Termstart $Termend]
	if {$Debugvar} {puts "variable Name is $Name"}
	if {-1 == [lsearch $Varlist $Name]} {
	    lappend Varlist $Name
	    if {[catch {upvar $Name S$Name}]} {
		error "Missing Tcl variable: $Name"
	    }
	    if {[catch {eval set fooBAR \$S$Name}]} {
		error "Missing tcl variable: $Name"
	    }
	    set preConditions [string range $Conditions 0 \
				   [expr $fDD - 1]]
	    set postConditions [string range $Conditions \
				    [expr $Termend + 1] end]
	    set Conditions "$preConditions\$S$Name$postConditions"
	    set preinConditions [string range $inConditions 0 \
				     [expr $fDD - 1]]
	    set postinConditions [string range $inConditions \
				      [expr $Termend + 1] end]
	    set inConditions "$preinConditions\$S$Name$postinConditions"
	} else {
	    error "did find Name"
	}
    }
    if {$Debugvar} {puts "Conditions is $Conditions"}
    if {$Debugvar} {puts "inConditions is $inConditions"}

    puts $oFile $Header

    set Written 0
    while {{} != [set Line [tablefile $iFile get]]} {
	for {set i 0} {$i < $Name_count} {incr i} {
	    set Name [lindex $Names $i]
	    set Value [lindex $Line $i]
	    eval set $Name \$Value
	}
	set tEST 1
	foreach CoN $Conditions {
#	    puts "CoN is $CoN"
	    set test 0
	    set foOBAR [catch {eval set tEST \[expr $CoN \] }]
#	    puts "foOBAR is $foOBAR"
#	    puts "tEST is $tEST"
	    if {$foOBAR || !$tEST} {
		set tEST 0
		break
	    }
	}
	if {$tEST} {
	    incr Written
	    puts $oFile [join $Line ,]
	}
    }

    tablefile $iFile close
    close $oFile

    return "$Written records written"
}

# solar::ped2csv
#
# Purpose:  Convert Pedsys format file to comma delimited format
#
# Usage:    ped2csv <pedfilename> [<outfilename>]
#
#           If <outfile> is not specified, filename is <pedfile>.csv
#
# Notes:    BLANK fields are removed.  Duplicate field names will cause an
#           error.
#
#           This command uses the "selectrecords" command, which makes it
#           very easy: "selectrecords <pedfilename> <outfilename>".  Since
#           no condition is specified, all records are selected, and since
#           selectrecords uses the tablefile command, it can read pedsys files.
# -

proc ped2csv {args} {
    set nargs [llength $args]
    set inname [lindex $args 0]

    if {$nargs == 0 || $nargs > 2} {
	error "invalid arguments to ped2csv"
    }
    if {$nargs == 1} {
	set outname [lindex $args 0].csv
    } else {
	set outname [lindex $args 1]
    }
    return [selectrecords $inname $outname]
}


# solar::selectfields --
#
# Purpose:  Select fields (columns) from data file(s) and copy to a new file
#
# Usage:    selectfields [-noid] [<infile>]* [.] [-np] [<field-name>]* 
#                        [-o <outfile>]  [-sample] [-list filename] [-noid]
#                                        
#           A optional period (aka dot) ends the list of filenames and starts
#           the list of field names.  If there is no dot, the first argument
#           is assumed to be the one and only data filename.  The currently
#           loaded phenotypes files are automatically included at the end of
#           the list of files.  If nothing precedes the dot, only the
#           phenotypes files are used.  Fields found in multiple files default
#           to the first file in which they are found, however a warning is
#           given when this happens.  The -np argument forces the loaded
#           phenotypes files to be ignored.  The -sample argument forces
#           only the inclusion of individuals having all field values
#           defined.  Otherwise, a record is written for every ID encountered
#           in the file(s) from which data is read, however one or more
#           data value(s) might be blank.  
#
#           -list filename   Use all the field names in this file, listed
#                            one per line.  These are appended to the list
#                            of field names given in the command line, if
#                            any.
#
#           If the -noid switch is given, the old version of selectfiles
#           is used.  This takes one and only one <infile> followed by a
#           list of fieldnames, with no dot in between.  The only other
#           option allowed is -o.  No ID field is required in the input
#           file, and no ID field is written unless included in the list
#           of fieldnames.  The loaded phenotypes file is not used unless
#           that is the one file named.
#
#           If not specified, <outfile> defaults to selectfields.out
#
#           <field-names> follow rules for phenotypes files and are also
#             affected by field command specifications.  For example,
#             if you specify "ID" as field name, this would also match a
#             field name "EGO" in the file.
#
#           Input file may be either PEDSYS or Comma Delimited format.
#           Output file is comma delimited.
#
# Example:  selectfields phen.dat out.dat ID AGE -o age.dat
#
# -

proc selectfields {args} {

    set outfilename selectfields.out
    set loadedfiles [phenotypes -files]
    set sample 0
    set listfile ""
    set noid 0
    set phenfiles ""

    set argsonly [read_arglist $args -out outfilename -o outfilename \
		      -list listfile -noid {set noid 1} \
		      -np {set loadedfiles ""} -sample {set sample 1}]

    if {$noid} {
	return [eval oldselectfields $args]
    }

    if {-1 != [set dashpos [lsearch $argsonly "."]]} {
	set phenfiles [lrange $argsonly 0 [expr $dashpos - 1]]
	set fields [lrange $argsonly [expr $dashpos + 1] end]
    } else {
	set phenfiles [lindex $argsonly 0]
	set fields [lrange $argsonly 1 end]
    }
    foreach loadedfile $loadedfiles {
	setappend phenfiles $loadedfile
    }
    if {[llength $phenfiles] < 1} {
	error "selectfields: No input files selected"
    }
    if {"" != $listfile} {
	set listfilelist [listfile $listfile]
	set fields [concat $fields $listfilelist]
    }

    if {[llength $fields] < 1} {
	error "selectfields: No fields selected"
    }

# remove redundant files phenfiles -> usefiles
# open all phen files as phenf(i) for field scanning

    set usefiles {}
    set j 0
    for {set i 0} {$i < [llength $phenfiles]} {incr i} {
	set preceding [lrange $phenfiles 0 [expr $i - 1]]
	set this [lindex $phenfiles $i]
	if {-1 != [lsearch $preceding $this]} {
	    puts "Warning: file $this listed twice"
	} else {
	    set phenf($j) [solarfile open $this]
	    set file_needed($j) 0
	    lappend usefiles $this 
	    incr j
	}
    }

# Fill Fields array for each file used
# set file_needed

    catch {unset Fields}
    catch {unset Data}
    for {set i 0} {$i < [llength $fields]} {incr i} {
	set field [lindex $fields $i]
	set found ""
	set reported_dup 0
	for {set j 0} {$j < [llength $usefiles]} {incr j} {
	    if {[solarfile $phenf($j) test_name $field]} {
		set filename [lindex $usefiles $j]
		if {$found==""} {
		    set found $filename
		    set file_needed($j) 1
		    if {[catch {set fields_here $Fields($filename)}]} {
			set fields_here ""
		    }
		    lappend fields_here $field
		    set Fields($filename) $fields_here
		} else {
		    if {!$reported_dup} {
			set reported_dup 1
			puts "Warning.  Using $field from file $found"
		    }
		        puts "          Ignoring $field in file $filename"
		}
	    }
	}
	if {$found==""} {
	    for {set j 0} {$j < [llength $usefiles]} {incr j} {
		solarfile $phenf($j) close
	    }
	    error "selectfields: Could not find field $field in any of the files:\n$usefiles"
	}
    }

# Close files not needed
# or Read data from each file into Data arrays

    set ids {}
    for {set j 0} {$j < [llength $usefiles]} {incr j} {
	if {!$file_needed($j)} {
	    solarfile $phenf($j) close
	} else {
	    set filename [lindex $usefiles $j]
#	    puts "Reading $filename"
	    set fields_here $Fields($filename)
	    solarfile $phenf($j) start_setup
	    solarfile $phenf($j) setup ID
	    foreach field_here $fields_here {
		solarfile $phenf($j) setup $field_here
	    }
	    set linecount 0
	    while {{} != [set record [solarfile $phenf($j) get]]} {
		incr linecount
		set id [lindex $record 0]
		if {-1 == [lsearch $ids $id]} {
		    lappend ids $id
		}
		set fieldno 0
		foreach field_here $fields_here {
		    incr fieldno ;# starts at 1
		    set Data($field_here,$id) \
			[lindex $record $fieldno]
		}
	    }
	}
    }

# Data is read, now close remaining input files

    for {set j 0} {$j < [llength $usefiles]} {incr j} {
	if {$file_needed($j)} {
	    solarfile $phenf($j) close
	}
    }

# Output 1 record for any ID found

    set written 0
    set outfile [open $outfilename w]
    puts $outfile ID,[join $fields ,]
    foreach id $ids {
	set record "$id"
	set skip 0
	foreach field $fields {
	    set datum ""
	    catch {set datum $Data($field,$id)}
	    if {$datum == "" && $sample} {
		set skip 1
	    }
	    set record "$record,$datum"
	}
	if {$skip} {continue}
	puts $outfile $record
	incr written
    }
    close $outfile
    return "$written records written OK"
}


proc oldselectfields {args} {

    set outfilename selectfields.out
    set argsonly [read_arglist $args -out outfilename -o outfilename \
		      -noid {set foobar 0} ]
    if {[llength $argsonly] < 2} {
	error "selectfields requires <infile> and <field-name>"
    }
    set inFilename [lindex $argsonly 0]
    set columns [lrange $argsonly 1 end]

    if {![file exists $inFilename]} {
	error "selectfields: file not found: $inFilename"
    }
    set tfile [solarfile open $inFilename]
    solarfile $tfile start_setup
    set header ""
    foreach col $columns {
	set header "$header,$col"
	if {![solarfile $tfile test_name $col]} {
	    solarfile $tfile close
	    error "selectfields: Missing field $col"
	}
	solarfile $tfile setup $col
    }
    set outfile [open $outfilename w]
    set header [string range $header 1 end]
    puts $outfile $header
    set count 0
    while {{} != [set line [solarfile $tfile get]]} {
	set outline [join $line ,]
	puts $outfile $outline
	incr count
    }
    close $outfile
    solarfile $tfile close
    return "$count records written"
}

# solar::joinfiles --
#
# Purpose:  Join files horizontally based on ID's
#
# Usage:    joinfiles [-all] [<filename>]* [-out <filename>] [-key <keylist>]
#                     -list <filename> -chunk <chunksize>
#
#           -out <filename>  Write joined records to this file (default is
#                            joinfiles.out in the working directory)
#           -key <keylist>   Base join on these key(s) (default is ID or EGO,
#                            and also FAMID if FAMID is present in all files)
#           -all             Filenames may be patterns with wildcards
#                            (including *  to match any sequence of characters
#                            and ? to match any one character) and/or names of
#                            directories whose files will be included.
#                            (Files in subdirectories are not included.)
#                            When using -all, no system limit on open files
#                            is applicable.
#            -list <filename> Include all files listed in <filename>, which
#                             has one filename in each line, which may be
#                             a pattern with wildcards.  Only one -list
#                             may be used.  When using -list, no system limit
#                             on open files is applicable.
#             -chunk <chunksize>  The chunk size used in joining files under
#                                 -all and -list options.  By joining only one
#                                 chunk of files at a time, the system limit
#                                 on open files is bypassed.  The default is
#                                 100.
#
#           Some additional esoteric options are described below in Note 7.
#
# Notes:
#
# 1)  Each file may either be Comma Delimited or Pedsys, and sucessive files
#     may use different formats.
#
# 2)  The output file will be Comma Delimited, thus this command also serves
#     to translate one or more Pedsys files to Comma Delimited format.
#
# 3)  Any field mapping of ID and FAMID to some other name through the 
#     "field" command will be applied if the keys are defaulted.  Key
#     matching is case insensitive, so the key might be "ID" in one file
#     and "id" in the next.
#
# 4)  Records will be output for every ID encountered regardless of whether
#     that ID is found in all files.
#
# 5)  If keys are specified, you'd better know what you are doing.
#     No field name mapping or testing of whether FAMID is required
#     will be done.  However, whether defaulted or not, the availability
#     of keys in every file will be tested.
#
# 6)  If the same filename is repeated in the list of files to join, the
#     repeats are ignored (for technical reasons).  If you must join the
#     same file to itself for some legitimate reason (???), copy
#     to a another filename first.
#
# 7)  If the same field name(s), other than the key(s), are found in more
#     than one file, the default action is to rename them in the output
#     file in a way so as to be unique.  The following format is used:
#
#     <field name>.<filename>[.<number>]
#
#     If adding the filename makes the field name unique, that is all that
#     is done, which makes for a nice new name.  For example:
#
#         q4.qaw10.phen    (phenotype q4 in file gaw10.phen)
#
#     Otherwise, <number> is applied, using the first number (starting
#     from 2 and ending with 30002) that makes the field name unique.
#     Unless there are more than 30000 matching field names, this will
#     guarantee that a unique name will be found and  used.  Also, 
#     with reasonably short names, it is likely that the resulting name
#     will be unique within 18 characters, which is currently required
#     for trait and covariate names.  However, uniqueness within 18 
#     characters is not guaranteed as that would require ugly renaming
#     and it's quite possible the 18 character limit may be removed
#     eventually anyway.  Uniqueness testing is case insensitive.
#
#     There are two other optional ways of handling field names which are
#     not unique.  These option specifiers may be used anywhere after the
#     command name but apply globally to all files.
#
#           -uniqueonly    Remove fields which are not unique among files
#                          (except keys).
#
#           -norename      Don't rename fields that are not unique, just
#                          include them.  (Note: If this option is applied,
#                          the resulting file may cause problems with
#                          various SOLAR commands.  For example, the
#                          "residual" command won't like it even if the
#                          duplicated field is NOT used as a trait or
#                          covariate.)
#
# 8)  If the same fieldname is repeated in one file, that field is
#     not included in the output.  (Such fields could not be selected
#     as traits or covariates in SOLAR anyway.)  This typically occurs
#     when there is a field named BLANK to separate columns in a Pedsys
#     file.  Also, fields with the "null" name (zero characters or all
#     blanks) are not included.
#-

proc joinfiles {args} {
#
# Define variables and defaults
#
    set debug 0                      ;# local debugging
    set keylist {}                   ;# list of keys (see note below)
    set allnames {}                  ;# all fieldnames
    set oname joinfiles.out          ;# output filename
    set default_keys 0               ;# default key name rules
    set xfile tablefile              ;# tablefile or solarfile
    set make_unique rename           ;# rename, only, norename
    set listfile ""
    set many ""
    set chunksize 100
#
# Note: Herein I use "key" to refer to the name(s) of field indentifiers which
#       must match in order to "join" records.  I use "id" as the name of
#       the "data" in a particular "key" field or fields.  However, user is
#       permitted to use the name "-id" to refer to "key" since "id" is
#       otherwise meaningless at user level.
#
# Read arguments
#
    set nuinames [read_arglist $args \
		    -output oname -out oname -o oname \
		    -ids keylist -id keylist \
		    -keys keylist -key keylist \
		    -uniqueonly {set make_unique only} \
		    -norename {set make_unique norename} \
		    -all {set many many} \
		    -list listfile \
		    -chunk chunksize \
		    -clump chunksize \
		   ]
    file delete $oname

    if {0 == [llength $keylist]} {
	set keylist {id famid}
	set default_keys 1
	set xfile solarfile
    }

#
# Remove non-unique files 
#
    set inames {}
    foreach iname $nuinames {
	setappend inames $iname
    }
#
# Handle -all and -list options
# These options work by re-calling joinfiles
#
    if {"" != $listfile || "" != $many} {
	set patternglob ""
	if {"" != $inames} {
	    foreach iname $inames {
		set pnames [glob -nocomplain $iname]
		if {{} == $pnames} {
		    error "joinfiles: File(s) $iname not found"
		}
		set patternglob [concat $patternglob $pnames]
	    }
	}
	if {"" != $listfile} {
	    set biglist [listfile $listfile]
	    foreach line $biglist {
		set pnames [glob -nocomplain $line]
		if {{} == $pnames} {
		    error "joinfiles: Listed file(s) not found: $line"
		}
		set patternglob [concat $patternglob $pnames]
	    }
	}
	if {"" == $patternglob} {
	    error "joinfiles: No files found"
	}

	set options [read_arglist $args -list ignore -all ignore \
			 -o ignore -out ignore -output ignore \
			 -* foo]
	foreach in $inames {
	    set options [remove_from_list $options $in]
	}
	if {"" != $options} {
	    puts "joinfile options are $options"
	}
	
	set j1 [file dirname $oname]/solar.joinfiles.alllist.tmp
	set j2 [file dirname $oname]/solar.joinfiles.alllist.temp
	file delete -force $j1
	file delete -force $j2
	set first 1
	set finalglob {}
	foreach patordir $patternglob {
	    if {[file isdirectory $patordir]} {
		set pat [glob -nocomplain $patordir/*]
		if {"" == $pat} {
		    puts "joinfiles: Warning.  Directory $patordir is empty."
		    continue
		}
	    } else {
		set pat $patordir
	    }
	    set finalglob [concat $finalglob $pat]
	}
	set finalset {}
	foreach ele $finalglob {
	    if {[file isdirectory $ele]} {
		continue
	    }
	    if {-1 == [lsearch -exact $finalset $ele]} {
		lappend finalset $ele
	    } else {
		puts "joinfiles: Warning.  File will only be included once:\n$ele\n"
	    }
	}
	if {{} == $finalset} {
	    error "joinfiles: Error.  No files found"
	}
	puts "Joining files $finalset\n"
	set chunk_include [expr $chunksize - 1]
	set first 1
	while {{} != $finalset} {
	    set thisbatch [lrange $finalset 0 $chunk_include]
	    set finalset [lrange $finalset $chunksize end]
	    foreach rfile $thisbatch {
		puts "Joining $rfile"
	    }
	    if {$first} {
		set returnout [eval joinfiles $thisbatch -o $j2 $options]
		set first 0
	    } else {
		set returnout [eval joinfiles $j2 $thisbatch -o $j1 $options]
		file rename -force $j1 $j2
	    }
	}
	file rename -force $j2 $oname
	set returnout "[lrange $returnout 0 end-1] $oname"
	file delete -force $j1
	file delete -force $j2
	return $returnout
    }
#
# Open each file as a tablefile into array tfile(iname)
# Get list of field names: allnames
# and per-file list: fnames(iname)
#    
    foreach iname $inames {
	set tf [set tfile($iname) [eval $xfile open $iname]]

# Get names, removing repeats and nulls

	set goodnames {}
	set tempnames [eval $xfile $tf names]
	set badnames {}  ;# already seen twice, don't try adding again
	foreach tempname $tempnames {
	    if {{} != $tempname} {
		if {-1 == [lsearch -exact [string tolower $badnames] \
			       [string tolower $tempname]]} {
		    if {-1 == [lsearch -exact [string tolower $goodnames] \
				   [string tolower $tempname]]} {
			lappend goodnames $tempname
		    } else {
			set goodnames [remlist $goodnames $tempname]
			lappend badnames $tempname
		    }
		}
	    }
	}
	set allnames [concat $allnames [set fnames($iname) $goodnames]]
	if {$default_keys} {
	    if {![eval $xfile $tf test_name famid]} {
		set keylist id
	    }
	}
	foreach key $keylist {
	    if {![eval $xfile $tf test_name $key]} {
		foreach jname $inames {
		    catch {eval $xfile $tfile($jname) close}
		}
		error "joinfiles:  Key $key not found in file $iname"
	    }
	}
    }
    if {$debug} {puts "allnames is $allnames"}
#
# Handle keys and duplicate names from each set
# creating "unique names": unames
#   (Note: case insensitivity here)
#	
    set nkeys [llength $keylist]

    set lkeylist [string tolower $keylist]      ;# "l" for lower case testing
    set lallnames [string tolower $allnames]
    foreach iname $inames {
	if {$xfile == "solarfile"} {
	    set lkeylist {}
	    foreach keyname $keylist {
		lappend lkeylist [string tolower [eval $xfile $tfile($iname) \
						      establish_name $keyname]]
	    }
	}
	set names $fnames($iname)
	set ns ""
	set uns ""
	foreach name $names {
	    set lname [string tolower $name]
	    if {-1 == [lsearch -exact $lkeylist $lname]} {
		if {[string_imatch $make_unique norename]} {
		    lappend ns $name
		    lappend uns $name
		} elseif {[unique $lname $lallnames]} {
		    lappend ns $name
		    lappend uns $name
		} elseif {[string_imatch $make_unique rename]} {
		    set test "$name.$iname"
		    set ltest [string tolower $test]
		    set lt_allnames [concat $lallnames $ltest]
		    if {[unique $ltest $lt_allnames]} {
			lappend ns $name
			lappend uns $test
			lappend allnames $test
			lappend lallnames $ltest
		    } else {
			for {set ii 2} {$ii < 30002} {incr ii} {
			    set test $name.$iname.$ii
			    set ltest [string tolower $test]
			    set lt_allnames [concat $lallnames $ltest]
			    if {[unique $ltest $lt_allnames]} {
				lappend ns $name
				lappend uns $test
				lappend allnames $test
				lappend lallnames $ltest
				break
			    }
			}
		    }
		}
	    }
	    set unames($iname) $uns
	    set fnames($iname) $ns
	}
    }
#
# Read elements of each file into an associative array (hashtable)
# named data(iname,id) where "id" is comma delimited key string of id's
# Also create set/list of all id's: allids
#
# Detect same id's used twice in same file as an error
#
    set allids {}
    set header [join $keylist ,]
    foreach iname $inames {
	set tf $tfile($iname)
	eval $xfile $tf start_setup
	foreach key $keylist {
	    eval $xfile $tf setup $key
	}
	foreach fname $fnames($iname) {
	    eval $xfile $tf setup $fname
	}
	foreach uname $unames($iname) {
	    set header "$header,$uname"
	}
	while {{} != [set record [eval $xfile $tf get]]} {
	    set id [lindex $record 0]
	    for {set i 1} {$i < $nkeys} {incr i} {
		set id "$id,[lindex $record $i]"
	    }
	    setappend allids $id
	    set fullid "$iname,$id"
	    if {![catch {set foo $data($fullid)}]} {
		foreach jname $inames {
		    catch {eval $xfile $tfile($jname) close}
		}
		error "joinfiles:  Found multiple records with key <$id> in file $iname"
	    }
	    set data($fullid) [lrange $record $nkeys end]
	}
    }
#
# WRITE OUTPUT FILE, starting with header
# then, for each id found, output composite record from all files
#
    putsout -q -d. $oname $header
    set count 0
    foreach id $allids {
	set record $id
	foreach iname $inames {
	    if {0 == [llength $fnames($iname)]} continue
	    if {[catch {set record "$record,[join $data($iname,$id) ,]"}]} {
		foreach n $fnames($iname) {
		    set record "$record,"
		}
	    }
	}
	putsout -q -d. $oname $record
	incr count
    }
    foreach jname $inames {
	catch {eval $xfile $tfile($jname) close}
    }
    return "$count records written to $oname"
}

proc unique {name list} {
    if {-1 != [set start [lsearch -exact $list $name]]} {
	if {-1 != [lsearch -exact [lrange $list [expr $start + 1] end] $name]} {
	    return 0
	}
    }
    return 1
}

# solar::invert
#
#
# invert from version 6 has been renamed "transpose" in version 7
# see "help transpose" for more information
#-

# solar::transpose
#
# Purpose:: transpose on MathMatrix or comma delimited file
#
# Usage: transpose <MathMatrix>  ;# returns id of transposed MathMatrix
#        transpose <infile> <outfile>  ;# transposes CSV file
#
# Note: All records must have same length.  First record is treated like all
# others.  To invert Pedsys file, use ped2csv first.  Memory usage for
# extremely large files (>100mb) could be a problem.  If memory is exhausted
# while caching the file in memory, solar might crash to the shell prompt.
# -

#
# this transpose below stores original file in an array of lists
# each list element must therefore be accessed through a lindex
# in theory, that might lead to more compute time than using an array
# to store each element, rather than each line.  However, this approach
# uses far less overall memory, and actually seems to run faster for large
# files.  The original invert based on element storage is 
# now called oldinvert and retained below.
#

proc transpose {args} {

    set nargs [llength $args]
    if {$nargs == 1} {
	return [ctranspose $args]
    } elseif {$nargs != 2} {
    error "Usage: transpose <csvinput> <csvoutput>  OR  transpose <MathMatrix>"
    }
    return [eval transpose_csv $args]
}

proc transpose_csv {infilename outfilename} {
    set infile [open $infilename]
    set outfile [open $outfilename w]

    set last_maxj -1
    for {set i 0} {1} {incr i} {
	if {-1 == [gets $infile inline]} {
	    break
	}
	set inlist [split $inline ,]
	set maxj [llength $inlist]
	if {$last_maxj == -1} {
	    set last_maxj $maxj
	} else {
	    if {$maxj != $last_maxj} {
		puts "Length of record $i is $maxj"
		puts "Length of previous records was $last_maxj"
		close $infile
		close $outfile
		error "invert: Error! All records must be same length!"
	    }
	}
	set mlist($i) $inlist
    }
    close $infile
    set maxi $i

    for {set j 0} {$j < $maxj} {incr j} {
	set first 0
	for {set i 0} {$i < $maxi} {incr i} {
	    if {$first==0} {
		set outline "[lindex $mlist($i) $j]"
		set first 1
	    } else {
		set outline "$outline,[lindex $mlist($i) $j]"
	    }
	}
	puts $outfile $outline
    }
    close $outfile
    return ""
}

proc oldtranspose {infilename outfilename} {
    set infile [open $infilename]
    set outfile [open $outfilename w]

    set last_maxj -1
    for {set i 0} {1} {incr i} {
	if {-1 == [gets $infile inline]} {
	    break
	}
	set inlist [split $inline ,]
	set maxj [llength $inlist]
	if {$last_maxj == -1} {
	    set last_maxj $maxj
	} else {
	    if {$maxj != $last_maxj} {
		puts "Length of record $i is $maxj"
		puts "Length of previous records was $last_maxj"
		close $infile
		close $outfile
		error "invert: Error! All records must be same length!"
	    }
	}
	for {set j 0} {$j < $maxj} {incr j} {
	    set atom [lindex $inlist $j]
	    set matrix($i,$j) $atom
	}
    }
    close $infile
    set maxi $i

    for {set j 0} {$j < $maxj} {incr j} {
	set first 0
	for {set i 0} {$i < $maxi} {incr i} {
	    if {$first==0} {
		set outline "$matrix($i,$j)"
		set first 1
	    } else {
		set outline "$outline,$matrix($i,$j)"
	    }
	}
	puts $outfile $outline
    }
    close $outfile
    return ""
}




#
# General Purpose Utilities...could be useful outside SOLAR
#
# (Unfortunately this section wasn't created earlier, so general purpose
#  utilities are actually scattered throughout the code.)
#


proc remove_from_list {alist ename} {
    set epos [lsearch -exact $alist $ename]
    return [lreplace $alist $epos $epos]
}

# solar::remlist
#
# Purpose:  Remove element from list by name
#
# Usage:    remlist <list> <element>
#
# Notes:    Input list is not modified, but new list is returned.
#
#           Only first matching element is removed.  This works well
#           with setappend for set-like behavior: use setappend to add
#           elements to "set" and remlist to remove
#           elements from set.
#
#           Match testing is case insensitive.
#
#           No error is raised if thre is no matching element; input
#           list is returned unchanged.
#
# See Also: setappend
#
# -

proc remlist {list element} {
    return [remove_from_list_if_found $list $element]
}

proc remove_from_list_if_found {alist ename} {
    set llist [string tolower $alist]
    set lname [string tolower $ename]
    set epos [lsearch -exact $llist $lname]
    if {$epos >= 0} {
	return [lreplace $alist $epos $epos]
    }
    return $alist
}


proc listfile {filename} {
    set retlist {}
    set infile [open $filename]
    while {-1 != [gets $infile line]} {
	if {"" != $line} {
	    lappend retlist [string trim $line]
	}
    }
    close $infile
    return $retlist
}

proc line_index {text lineno} {
    for {set i 0} {$i <= $lineno } {incr i} {
	set nindex [string first "\n" $text]
	if {$nindex < 0} {
	    if {$i == $lineno} {
		return $text
	    } else {
		return ""
	    }
	}
	if {$i == $lineno} {
	    return [string range $text 0 [expr $nindex - 1]]
	}
	set text [string range $text [expr $nindex + 1] end]
    }
    return ""
}

    
# solar::remove_global --
#
# Purpose:  Remove a global variable (so it no longer exists)
#
# Usage:    remove_global <variable_name>
#
# Notes:    It is not necessary to declare variable as global first,
#           and there is no error if no such global actually exists.
#
# See Also: if_global_exists
# -

proc remove_global {name} {
    purge_global $name
}

proc purge_global {name} {
    if {0 != [llength [info globals $name]]} {
	global $name
	unset $name
    }
}

# solar::is_nan --
#
# Purpose:  Check if value is NaN (Not a Number)
#
# Usage:    is_nan <number>
#
#           Returns 1 if number is NaN, 0 otherwise.
#
# Notes:    This is most useful in scripts, when getting the likelihood or
#           other value using read_model, you should check to be sure it
#           is not NaN due to maximization convergence error.
# -


proc is_nan {numstr} {
    if {![string compare "NAN" [string toupper [string range $numstr 0 2]]]} {
	return 1
    }
    return 0
}

proc is_integer {string} {
    if {1 == [scan $string "%d%s" test junk]} {
	return 1
    }
    return 0
}

proc is_float {string} {
    if {1 == [scan $string "%g%s" test junk]} {
	return 1
    }
    return 0
}

proc ensure_integer {string} {
    if {![is_integer $string]} {
	error "Invalid integer: $string"
    }
    return {}
}

proc ensure_float {string} {
    if {![is_float $string]} {
	error "Invalid floating point number: $string"
    }
    return {}
}

proc is_digit {ch} {
    return [string match {[0-9]} $ch]
}

proc is_alpha {ch} {
    return [string match {[A-Za-z]} $ch]
}

proc is_alnum {ch} {
    return [string match {[A-Za-z0-9]} $ch]
}


# solar::fformat -- 
#
# Purpose:  Replace Tcl format with fixed width fields for numbers
#
# Usage:    fformat <spec>+ <value1>+
#
#           <spec>   format specifier(s) as for Tcl format command.
#                    f, e, or g format required for "fixed width"
#                    operation, like this:
#
#                   %[--][W][.P]T  where T is e, f, g, or y
#                      default right justification
#                   -  specifies left justification
#                   --  specifies center justification
#                   W is desired width
#                   P is desired precision (before and after decimal)
#                   T is format type:
#                     f is floating decimal
#                     e is exponential
#                     g is floating decimal if suitable, then exponential
#                     y same as g, except that exponential format is not
#                       used until the output would otherwise be 0.0 or
#                       nearly so for a non-zero value.  At least one
#                       significant digit is preserved for P 1-4, two 
#                       digits for P 4-6, and three digits for P  7-*.
#                       This is more consistent with readability, 
#                       retaining the fixed format nearly as long as
#                       possible.  Sometimes, more space will be used than W,
#                       but this is much less likely than with the standard
#                       G format.  However, unlike F format, the the result
#                       will not go to zero unless it is zero.  When possible,
#                       allow more space in "width" than the precision seems
#                       to require.  That way, under special circumstances,
#                       there is extra space for signs, "e", decimal point,
#                       etc.
#                     z same as y, except resulting string is trimmed to
#                     minimum space for csv files
#
#                    Note: For fractional numbers, make width at least 2
#                    than precision, to allow for leading "0."  Then allow
#                    one more for - sign, if that is possible.
#
#          This is intended as a drop-in replacement for the Tcl "format"
#          command, modifying "minimum width" to "fixed width" for
#          the f, e, and g formats ("fixed width" makes for more
#          readable columns) and adding a center justification option.
# -

proc fformat {spec args} {

    set result ""
    set argindex 0

    set speclen [string length $spec]
    for {set i 0} {$i < $speclen} {incr i} {
	set newchar [string index $spec $i]
	set nextchar [string index $spec [expr $i + 1]]

	if {"%" == $newchar} {
	    if {"%" == $nextchar} {
		incr i
		set result [catenate $result % ]
		continue
	    }
	    set subspec "%"
	    for {incr i} {$i < $speclen} {incr i} {
		set subchar [string index $spec $i]
		set subspec [catenate $subspec $subchar]
		if {"." == $subchar || "-" == $subchar || \
			"+" == $subchar || "l" == $subchar || \
			[is_digit $subchar]} {
		    continue
		}
		set newresult [fformat1 $subspec [lindex $args $argindex]]
		incr argindex
		set result [catenate $result $newresult]
		break
	    }

	} else {
	    set result [catenate $result $newchar]
	}
    }
    return $result
}


# solar::fformat1 -- private
#
# Purpose: Subroutine used by fformat to format individual numbers
#
# Usage:   fformat <spec> <value>
#
#           See fformat for details.
# -

proc fformat1 {spec value} {

    if {"%" != [string index $spec 0]} {
	error "invalid spec: must start with %"
    }

    set prefix [string range $spec 1 [expr [string length $spec] - 2]]
    set letter [string range $spec end end]


# Parse leading + and -

    set justify right
    set signed ""
    set minuscount 0

    while {1} {
	if {"-" == [string index $prefix 0]} {
	    incr minuscount
	} elseif {"+" == [string index $prefix 0]} {
	    set signed "+"
	} else {
	    break
	}
	set prefix [string range $prefix 1 end]
    }

    if {$minuscount == 1} {
	set justify left
    } elseif {$minuscount > 1} {
	set justify center
    }

# Parse width, if any

    set width 0
    set gotwidth [scan $prefix %i width]

    set segment ""
    if {$gotwidth > 0 || "y" == $letter || "z" == $letter} {
	set output [fformat1w $signed $width $prefix $letter $value]
    } else {
	set output [format %$signed$prefix$letter $value]
    }

# Perform justification or centering  (only if width specified)

    if {$width != 0} {
	set s s
	if {"left" == $justify} {
	    set output [string trim $output]
	    set output [format %-$width$s $output]
	} elseif {"center" == $justify} {
	    set output [string trim $output]
	    set next left
	    while {[string length $output] < $width} {
		if {"left" == $next} {
		    set output " $output"
		    set next right
		} else {
		    set output "$output "
		    set next left
		}
	    }
	} ;# no need to do anything for right justification
    }
    return $output
}


proc fformat1w {signed width prefix letter value} {

    set yformat 0
    set zformat 0
    if {"y" == $letter} {
	set yformat 1
	set letter f
    } elseif {"z" == $letter} {
	set yformat 1
	set letter f
	set zformat 1
    }
    set default [format %$signed$prefix$letter $value]
    set new $default


# "s" format truncates to precision, which is dangerous
# (this is not the case for i or d format)

    if {"s" == $letter} {
	return $default
    }

    set catchval [catch {

# Get precision

	set dotpos [string first . $prefix]
	if {$dotpos == -1} {
	    set prec $width
	} else {
	    set prefixlen [string length $prefix]
	    set prec [string range $prefix [expr $dotpos + 1] [expr $prefixlen - 1]]
	    if {"" == $prec} {
		set prec $width
	    }
	}

	
#   puts "width >$width<   prec  >$prec<  letter  >$letter<"


# For "y" format, use "f" format if rules satisfied, else use "g"

	if {$yformat} {
	    set minsig 1
	    if {[string length $new] <= $width} {
		if {$value == 0} {
		    return $new
		}
		if {$prec < 5} {
		    if {$new != 0} {
			return $new
		    }
		} elseif {$prec < 7} {
		    set minsig 2
		    set target [string trimright $new]
		    set masklen [expr [string length $target] - 2]
		    set masked [string range $target 0 $masklen]
		    if {$masked != 0} {
			return $new
		    }
		} else {
		    set minsig 3
		    set target [string trimright $new]
		    set masklen [expr [string length $target] - 3]
		    set masked [string range $target 0 $masklen]
		    if {$masked != 0} {
			return $new
		    }
		}
	    }
	    set letter e
#	    puts "CALLING FORMAT %$signed$prefix$letter"
	    set new [format "%$signed$prefix$letter" $value]
	}

	if {[string length $new] <= $width} {
	    return $new
	}
	
	set minprec 0
	if {$yformat} {
	    set minprec [expr $minsig - 1]
	}

	for {set test $prec} {$test >= $minprec} {incr test -1} {
	    set newprefix "%$signed$width.$test$letter"
	    set new [format $newprefix $value]
	    if {[string length $new] <= $width} {
		return $new
	    }
	}
	return $new  ;# OK, this appears to be the best we can do

    } catchstring ]
    

# see if we just did a "return" from catch block
    if {$catchval == 2} {
	if {$zformat} {
	    set catchstring [string trim $catchstring]
	}
	return $catchstring
    }
	
# Otherwise, use default
    return $default
}


# solar::if_global_exists --
#
# Purpose:  Check if a Tcl global variable exists
#
# Usage:    if_global_exists <global_name>
#
#           Returns 1 if global exists, 0 otherwise.
#
# Notes:    This is used in scripts in an "if" statement.  For example:
#
#           if {[if_global_exists SPECIAL_CASE]} {
#               global SPECIAL_CASE
#               puts "This is case $SPECIAL_CASE"
#           }
#
#           You do not need to declare the variable "global" before
#           calling if_global_exists.  However, you will need to declare it
#           global before setting or using it in a script.  Note that all
#           variables declared at the interpreter level (at the solar>
#           prompt) are automatically global.  Global variables should
#           not be confused with "shell" variables such as SOLAR_BIN
#           (though, all shell variables may be found in the global
#           array "env", for example, $env(SOLAR_BIN)).
#
#           Global variables are a convenient way of passing variables
#           through many levels of procedure calls without rewriting all
#           the intervening procedures, or across commands on an ad hoc basis.
#           Use of global variables is considered "bad style" by programming
#           purists and other bores.  But if they're so smart, why aren't they
#           writing your program?  It is true, however, that use of global
#           variables can sometimes introduce bugs and other unintended
#           consequences.
#
#           Globals variables prefixed with SOLAR_ are reserved for use by
#           the standard SOLAR procedures defined in solar.tcl.  But solar.tcl
#           might also use unprefixed globals, so it is recommended that users
#           use their own unique prefix to be safe.
#
# See Also: remove_global
#
# -

proc if_global_exists {name} {
    if {0 == [llength [info globals $name]]} {
	return 0
    } else {
	return 1
    }
}

# solar::stringsub --
#
# Purpose:  Simple verbatim string substitution (not regsub)
#
# Usage:    stringsub <original> <target> <replacement>
#
# -

proc stringsub {original target replacement} {
    set out $original
    if {-1 != [set bpos [string first $target $original]]} {
	set out [catenate [string range $original 0 [expr $bpos - 1]] \
		$replacement \
		[string range $original \
		[expr $bpos + [string length $target]] end]]
    }
    return $out
}

# Replace element in pre-formatted string without changing format
# whitespace between each element assumed
proc replace_using_format {line formats index newitem} {
    set items {}
    set line_length [llength $line]
    for {set i 0} {$i < $line_length} {incr i} {
	if {$i == $index} {
	    lappend items $newitem
	} else {
	    lappend items [lindex $line $i]
	}
    }
    return [eval format \$formats $items]
}    

# solar::catenate --
#
# Purpose:  Concatenate strings
#
# Usage:    catenate [<string>]*
#
# Example:  set modelname [catenate $basename 0 .mod]
#
# -

proc catenate {args} {
    set base ""
    foreach arg $args {
	set base $base$arg
    }
    return $base
}

# solar::string_imatch --
#
# Purpose:  Case insensitive string match testing
#
# Usage:    string_imatch <string1> <string2>
#
#           Returns 1 for case insensitive match, 0 otherwise.
#
# Note:     Useful in SOLAR scripts.
#
# -

proc string_imatch {string1 string2} {
    if {[string compare [string tolower $string1] [string tolower $string2]]} {
	return 0
    }
    return 1
}

# Trimming

proc is_blank {testchar} {
    if {" " == $testchar || "\t" == $testchar} {
	return 1
    }
    return 0
}

proc trim_left {istring} {
    set leftchar [string range $istring 0 0]
    while {[is_blank $leftchar]} {
	set istring [string range $istring 1 end]
	set leftchar [string range $istring 0 0]
    }
    return $istring
}

	

proc remove_whitespace {istring} {
    set slength [string length $istring]
    set result ""
    for {set i 0} {$i < $slength} {incr i} {
	set char [string index $istring $i]
	if {[string compare $char " "] && \
		[string compare $char "\t"] && \
		[string compare $char "\n"] && \
		[string compare $char "\r"] && \
		[string compare $char "\v"] && \
		[string compare $char "\f"]} {
	    set result $result$char
	}
    }
    return $result
}

# solar::setappend --
#
# Purpose:  Append only new elements to a list (keeping it like a set)
#
# Usage:    setappend <listname> element
#
# Note:     The list is identified by name, and may be modified, as with
#           lappend.
#
# Example:  set friends "John Tom Laura Jeff"
#           setappend friends Harald
#
# See Also: remlist
#
# -

# Append only new atoms to list
proc setappend {aset atom} {
    upvar $aset a
    if {-1 == [lsearch -exact $a $atom]} {
	lappend a $atom
    }
    return $a
}


# solar::setxor --
#
# Purpose:  Perform exclusive-or (xor) on two sets (Tcl lists)
#
# Usage:    setxor aset bset
#
# Note:     If element appears multiple times in one list, but not in other,
#           it will appear multiple times in output.
# -

proc setxor {list1 list2} {
    set xor {}
    foreach e $list1 {
	if {-1 == [lsearch -exact $list2 $e]} {
	    lappend xor $e
	}
    }
    foreach e $list2 {
	if {-1 == [lsearch -exact $list1 $e]} {
	    lappend xor $e
	}
    }
    return $xor
}


proc lowest {args} {
    if {0 == [llength $args]} {
	error "No arguments to lowest"
    }
    set low [lindex $args 0]
    foreach arg $args {
	if {$arg < $low} {
	    set low $arg
	}
    }
    return $low
}

proc highest {args} {
    if {0 == [llength $args]} {
	error "No arguments to highest"
    }
    set high [lindex $args 0]
    foreach arg $args {
	if {$arg > $high} {
	    set high $arg
	}
    }
    return $high
}

proc sum_of_squares {list} {
    set sum 0.0
    foreach l $list {
	set sum [expr $sum + ($l * $l)]
    }
    return $sum
}

proc sum {args} {
    set sum 0
    foreach arg $args {
	set sum [expr $sum + $arg]
    }
    return $sum
}

# procedure to make absolute pathname
proc make_absolute_pathname {name} {
    if {"absolute" == [file pathtype $name]} {
	return $name
    }
    if {"." != $name} {
	set newname [file join [pwd] $name]
    } else {
	set newname [pwd]
    }
    return $newname
}

#proc to center a line in an (assumed) 80 column line
proc centerline {instring {linewidth 80}} {
    set slength [string length $instring]
    if {$slength > [expr $linewidth - 1]} {return $instring}
    set needs [expr ($linewidth - $slength) / 2]
    set width [expr $needs + $slength]
    return [format %[catenate $width s] $instring]
}

# proc to count occurrances of a particular character in a string
proc char_count {string target_char} {
    set slist [split $string ""]
    set count 0
    foreach char $slist {
	if {0 == [string compare $char $target_char]} {
	    incr count
	}
    }
    return $count
}

# proceedure to use global if it exists or altvalue otherwise
proc use_global_if_defined {globalname altvalue} {
    if {0 != [llength [info globals $globalname]]} {
	upvar #0 $globalname g
	return $g
    } else {
	return $altvalue
    }
}

# Procedure to forcibly delete arbitrary number of files
# (Tcl's "file delete -force" doesn't like the null file)
proc delete_files_forcibly {args} {
    if {{} == $args} {
	return ""
    } else {
	eval file delete -force $args
    }
}

# solar::countfields
#
# Purpose: determine consistency of number of columns in a comma delimited file
#
# Usage: countfields <filename>
#
# An information report is returned like this:
#
# longest: 8 (#1) x 1497    shortest: 8 (#1) x 1497
#
# This means that the longest record had 8 fields, the first such record was
# #1, and it was followed by 1497 others of same length in the file.
#
# As it happens, the shortest record also had 8 fields, it was #1, and followed
# by 1497 of the same length in the file.
# -

proc countfields {filename} {
    set ifile [open $filename]
    set shortest 0
    set shortcount 0
    set shortest_line 1
    set longest_line 1
    set longcount 0
    set longest 0
    set count 0
    while {-1 != [gets $ifile line]} {
	set rlist [split $line , ]
	set len [llength $rlist]
	incr count
	if {$count == 1} {
	    set shortest $len
	    set longest $len
	} else {
	    if {$len < $shortest} {
		set shortest $len
		set shortest_line $count
		set shortcount 1
	    } elseif {$shortest == $len} {
		incr shortcount
	    }
		
	    if {$len > $longest} {
		set longest $len
		set longest_line $count
		set longcount 1
	    } elseif {$longest == $len} {
		incr longcount
	    }
	}
    }
    close $ifile
    return "longest: $longest (\#$longest_line) x $longcount    shortest: $shortest (#$shortest_line) x $shortcount"

}
# solar::copybin --
#
# Purpose:  Install new executable file without disturbing current users
#
# Usage:    copybin <filename> <directory>
#
# Note:    The way this works is quite simple.  The original version of the
#          file is not overwritten or deleted, but instead renamed.  
#          Running processes continue to access the original version
#          through the inode, regardless of the name change, while new
#          processes will access the new version.  The renaming scheme
#          simply appends dot followed by a number to the filename.
#          The first available number starting from 1 is used.  For
#          example, the old "solarmain" becomes "solarmain.1" or
#          "solarmain.2" if a "solarmain.1" already exists, etc.  At some
#          point you might want to clear out some of the older versions, but
#          that is up to you, and it would lead to numbering that is not
#          sequential, since copybin always takes the first available
#          number.
#
#          This is similar in design to the Unix "install -f" command.
#          It lacks some of install's checking features, but in one way
#          is much more capable: it allows any number of new versions to
#          be installed without disturbing users of the first or any other
#          previous version.  The Unix install command only has one level
#          of backup since it merely prepends "OLD" to the original name.
#          If you do two install's in a row over a timespan in which jobs
#          are continuing to run (as, unfortunately, is often required)
#          copies of the original version are lost and users are likely
#          to get a memory mapping error of some kind.
#
#          This seems to work across NFS mounted filesystems, but it might
#          not work for you, so be wary.  Actually, in ancient Unix days this
#          command might not have been necessary, but now that memory mapping
#          is used to load image files, it is necessary now.
# -


proc copybin {newfile dir} {

    if {![file exists $newfile]} {
	error "No such file $newfile"
    }
    if {![file exists $dir]} {
	error "No such directory $dir"
    }
    if {[file isdirectory $newfile]} {
	error "File $newfile is a directory"
    }
    if {![file isdirectory $dir]} {
	error "File $dir is not a directory"
    }

    set filename [file tail $newfile]
    if {[file exists [file join $dir $filename]]} {
	set index 1
	while {[file exists [file join $dir $filename.$index]]} {
	    incr index
	}
	puts "Moving [file join $dir $filename] to [file join $dir $filename.$index]"
	exec mv [file join $dir $filename] [file join $dir $filename.$index]
    }

    puts "Copying $newfile to $dir"
    exec cp $newfile $dir
}

proc copybini {newfile dir} {

    if {![file exists $newfile]} {
	error "No such file $newfile"
    }
    if {![file exists $dir]} {
	error "No such directory $dir"
    }
    if {[file isdirectory $newfile]} {
	error "File $newfile is a directory"
    }
    if {![file isdirectory $dir]} {
	error "File $dir is not a directory"
    }

    exec /usr/sbin/install -f $dir -o $newfile
}

	
	
# procedure to make command aliases
#   Note: by design the longname can be longer than (or not same as) procname
#   But, shortname must be a right truncation of longname (only its length

proc make_shortcut {shortname longname {procname ""}} {
    if {"" == $procname} {set procname $longname}
    if {$longname == $shortname} {set longname [catenate $longname _]}
    set shortlen [string length $shortname]
    while {[string length $longname] >= $shortlen} {
	set newname $longname
	set longname [string range $longname 0 \
		[expr [string length $longname] - 2]]
	if {0==[string compare $newname $procname]} {continue}
	eval proc $newname args \{return \[eval $procname \$args\]\}
    }
    return ""
}

# solar::showproc --
#
# Purpose:  Show SOLAR procedure or write to a file
#
# Usage:    showproc <procname> [<filename>]
#
# If <filename> is not specified, procedure is displayed on terminal using
# the 'more' pager.  If <filename> is specified, renamed proc is written
# to that file.
#
# This procedure will show any SOLAR procedure (script), whether built-in
# or user-defined.  Some, but not all, built-in SOLAR commands
# are implemented as scripts, and can be shown by this command.  Other
# SOLAR commands are implemented in C++ and FORTRAN, and cannot be shown
# by this command.
#
# User-defined scripts must be used once before they can be shown.
#
# The formatting shown by showproc may not be as pretty as it actually is
# in the source file because it will concatenate lines which are extended
# using backslash.  showproc is based on the Tcl command "info body" which
# has this "feature."
#
# To protect built-in procedures from being accidentally superceded
# through the use of this command, the procedure name is suffixed with
# ".copy".  If you choose to edit the script, IT IS RECOMMENDED THAT
# YOU DO NOT RENAME IT TO HAVE THE SAME NAME AS THE ORIGINAL PROCEDURE
# UNLESS YOU REALLY KNOW WHAT YOU ARE DOING.  If you do that anyway,
# it would probably be ignored.  SOLAR prevents you from overriding
# built-in procedures by putting the directory containing the active
# solar.tcl file to the front of the auto-load list.  Normally, that
# directory is the SOLAR_BIN directory defined when SOLAR starts up.
# Even if you did have a copy of the solar.tcl file in your working
# directory when SOLAR started up, procedures might be resolved either
# to the solar.tcl file or to separate script files in your working
# directory, depending on which appears earlier in an alphabetical
# list.
#
# Before new procedures can be used in SOLAR you must restart SOLAR or give
# the newtcl command.
#
#-

proc writeproc {args} {
    error "Use showproc instead"
}

proc showproc {procname {filename ""}} {
    set display 0
    if {"" == $filename} {
	set filename /tmp/[catenate $procname _proc].[pid]
	set display 1
    }
    set procbody [info body $procname]
    set procargs [info args $procname]
    set ofile [open $filename w]
    puts $ofile "proc [catenate $procname .copy] \{$procargs\} \{"
    puts $ofile $procbody
    puts $ofile "\}"
    close $ofile
    if {$display} {
	exec more $filename >&@stdout
	delete_files_forcibly $filename
    }
}
#
# Solar command shortcuts
#

# solar::shortcut --
#
# Purpose:  Show command shortcuts legal in scripts
#
# Usage:    shortcut <command>
#
# -

proc shortcut {command} {
    global Solar_Shortcut_List
    set foundshort ""
    set foundlong ""
    foreach sc $Solar_Shortcut_List {
	set shortname [lindex $sc 0]
	set longname [lindex $sc 1]
	set shortend [expr [string length $shortname] - 1]
	set commandend [expr [string length $command] - 1]
	if {$commandend < $shortend} continue
	if {0==[string compare $shortname \
		[string range $command 0 $shortend]]} {
	    if {0==[string compare $command \
		    [string range $longname 0 $commandend]]} {
		set foundshort $shortname
		set foundlong $longname
		break
	    }
	}
    }
    if {"" == $foundshort} {
	error "Shortcuts for $command not found"
    }
    return "Shortcuts: $foundshort - $foundlong"
}

proc make_solar_shortcut {args} {
    global Solar_Shortcut_List
    set shortname [lindex $args 0]
    set longname [lindex $args 1]
    lappend Solar_Shortcut_List [list $shortname $longname]
    eval make_shortcut $args
    return ""
}

proc make_solar_aliases {} {
    global Solar_Shortcut_List
    set Solar_Shortcut_List {}
    make_solar_shortcut abou        about
    make_solar_shortcut allc        allcovars              allcovar
    make_solar_shortcut alnorm      alnorm
    make_solar_shortcut analy       analysis-examples      analysis-example
    make_solar_shortcut ascer       ascertainment
    make_solar_shortcut autom       automodel
    make_solar_shortcut bayesa      bayesavg
    make_solar_shortcut bayesm      bayesmodel             bayesmod
    make_solar_shortcut beni        benice
    make_solar_shortcut bou         boundary               boundary
    make_solar_shortcut boundari    boundaries             boundary
    make_solar_shortcut chang       change-notes
    make_solar_shortcut chi         chi
    make_solar_shortcut chro        chromosomes            chromosome
    make_solar_shortcut clod        clod
    make_solar_shortcut comb        combinations
    make_solar_shortcut cons        constraints            constraint
    make_solar_shortcut cov         covariates             covariate
    make_solar_shortcut discrete-note  discrete-notes
    make_solar_shortcut doc         doc
    make_solar_shortcut dran        drand
    make_solar_shortcut e2l         e2lower
    make_solar_shortcut e2s         e2squeeze
    make_solar_shortcut epista      epistasis
    make_solar_shortcut examp       example
    make_solar_shortcut excl        exclude
    make_solar_shortcut fie         fields                 field
    make_solar_shortcut file-f      file-freq
    make_solar_shortcut file-map    file-map
    make_solar_shortcut file-mar    file-markers           file-marker
    make_solar_shortcut file-pe     file-pedigrees         file-pedigree
    make_solar_shortcut file-ph     file-phenotypes
    make_solar_shortcut fine        finemap
    make_solar_shortcut fix         fix
    make_solar_shortcut fre         freq
    make_solar_shortcut getvar      getvar
    make_solar_shortcut grid        grid
    make_solar_shortcut h2qf        h2qf
    make_solar_shortcut h2rf        h2rf
    make_solar_shortcut hel         help
    make_solar_shortcut help-d      help-document
    make_solar_shortcut helpa       helpadd
    make_solar_shortcut hou         houses                 house
    make_solar_shortcut ibd         ibd
    make_solar_shortcut ibdd        ibddir
    make_solar_shortcut ibdo        ibdoptions             ibdoption
    make_solar_shortcut ibs         ibs
    make_solar_shortcut interv      intervals              interval
    make_solar_shortcut key         key
    make_solar_shortcut linkm       linkmodel              linkmod
    make_solar_shortcut linkmod2    linkmod2p
    make_solar_shortcut load        load
    make_solar_shortcut loadk       loadkinship            loadkin
    make_solar_shortcut lod         lod
    make_solar_shortcut lodn        lodn
    make_solar_shortcut logl        loglikelihood          loglike
    make_solar_shortcut madj        madj
    make_solar_shortcut map         map
    make_solar_shortcut mark        markers                marker
    make_solar_shortcut markert     markertest
    make_solar_shortcut matr        matrixes               matrix
    make_solar_shortcut maxi        maximize
    make_solar_shortcut mem         memory
    make_solar_shortcut mibd        mibd
    make_solar_shortcut mibdd       mibddir
    make_solar_shortcut minipl      miniplot
    make_solar_shortcut model       model
    make_solar_shortcut mu          mu
    make_solar_shortcut mul         multipoint
    make_solar_shortcut needk       needk2
    make_solar_shortcut newt        newtcl
    make_solar_shortcut newm        newmodels              newmod
    make_solar_shortcut no_ch       no_check_os
    make_solar_shortcut null        null
    make_solar_shortcut nulln       nulln
    make_solar_shortcut oldm        oldmodel
    make_solar_shortcut ome         omega
    make_solar_shortcut opt         options                option
    make_solar_shortcut outd        outdir
    make_solar_shortcut par         parameters             parameter
    make_solar_shortcut ped         pedigrees              pedigree
    make_solar_shortcut pedli       pedlike
    make_solar_shortcut pedlo       pedlod
    make_solar_shortcut perd        perdelta
    make_solar_shortcut perturb     perturb
    make_solar_shortcut phen        phenotypes
    make_solar_shortcut plo         plotmulti              plot
    make_solar_shortcut polyg       polygenic
    make_solar_shortcut polym       polymodel              polymod
    make_solar_shortcut quadrat     quadratic
    make_solar_shortcut qtn         qtnmarker              qtnm
    make_solar_shortcut regi        register
    make_solar_shortcut relat       relatives
    make_solar_shortcut resi        residuals              residual
    make_solar_shortcut sav         save
    make_solar_shortcut scree       screencov
    make_solar_shortcut shortc      shortcuts              shortcut
    make_solar_shortcut simin       siminf
    make_solar_shortcut simqt       simqtl
    make_solar_shortcut slod        slod
    make_solar_shortcut solarm      solarmodel
    make_solar_shortcut solart      solartcl
    make_solar_shortcut solarv      solarversion
    make_solar_shortcut spor        spormodel               spormod
    make_solar_shortcut stringp     stringplot
    make_solar_shortcut tabl        tablefile
    make_solar_shortcut tclg        tclgr
    make_solar_shortcut tdis        tdist
    make_solar_shortcut trai        traits                  trait
    make_solar_shortcut twop        twopoint
    make_solar_shortcut upg         upgrade
    make_solar_shortcut usag        usages                  usage
    make_solar_shortcut usor        usort
    make_solar_shortcut verb        verbosity
    make_solar_shortcut mgas        mgassociation           mgassoc
    return ""
}

#
# The following section documents commands implemented in C++
#
# solar::model --
#
# Purpose:  Describe, save, or load a model
# 
# Usage:    save model <modelname>     ; save current model to a file
#           load model <modelname>     ; load model from a file
#           model                      ; display model on terminal
#           model new                  ; reset to new empty model
# 
# Notes:    An extension .mod is automatically appended if not specified.
#           You must specify directory path if you want to save model
#           in a subdirectory of the current directory.
# - 

# solar::load --
#
# Purpose:  Load a user data file (pedigree, phenotype, marker, etc.)
#
# Usage:    load <object-type> [<options>] <arguments>
#
#           load pedigree <filename>
#           load phenotypes <filename>
#           load matrix [-sample | -allow] <filename> <name1> [<name2>]
#           load matrix [-cols <tcl-list>] <filename> ;# MathMatrix
#           load model <filename>
#           load freq [-nosave] <filename>
#           load marker [-xlinked] <filename>
#           load map [-haldane | -kosambi] <filename>
#
# Notes:    There is much more information available for each variant of
#           the load command.  See the documentation for the
#           particular object type, for example, "help pedigree".
#           For information about a particular file format, see the
#           applicable file-* documentation, for example, "help file-pedigree".
#-

# solar::save --
#
# Purpose:  save <object-type> <arguments>
#
#           save model <filename>
#
#           More information is available under "help model"
#-

proc load {args} {

    if {[llength $args] < 1} {
	error "Usage: load <object-type> [<options>] <args>"
    }
    return [eval [lindex $args 0] load [lrange $args 1 end]]
}

proc save {args} {

    if {[llength $args] < 1} {
	error "Usage: save <object-type> [<options>] <args>"
    }
    return [eval [lindex $args 0] save [lrange $args 1 end]]
}


# solar::mu --
#
# Purpose:  Set or Display the Mu equation (trait value estimator)
#
#           Usually the covariate command is used to set this automatically,
#           but the mu command provides more advanced capabilities, such as
#           using log or sine functions.
# 
# Usage:    mu                        ; displays Mu equation
#           mu = mu + <expression>    ; add new terms to Mu equation
#           mu = <expression>         ; replaces Mu equation (SEE DISCUSSION!)
#           mu reset                  ; restores default Mu equation
#
#           <expression> may include mathematical operators (+-*/^), 
#           constants, parentheses, any mathematical functions defined in
#           the C programming language, any phenotypic variables included
#           in the analysis, sex, and for any variable "var" x_var (the
#           sample mean), min_var (the sample minimum), and max_var
#           (the sample maximum).  Parameters whose names include
#           any erstwhile operators including parentheses, *, or ^ must
#           be enclosed in angle brackets <> to prevent being parsed as
#           functions; note this always happens for bivariate models, or when
#           there are interaction covariates such as age*sex, or squared
#           covariates such as age^2.  For bivariate models, you can also
#           include "t1" and "t2": t1 is 1 if the mu is being evaluated
#           for the first trait, and 0 otherwise, and t2 has this behavior
#           for the second trait.  All variables included in the mu will be
#           required in the sample.
#
#           Also it is possible for the mu to include inequality operators
#           (such as >=) and the "print" function (for debugging purposes).
#           In these regards, the mu expression is like the omega expression.
#           See "help omega" for more about inequalities and print, and
#           a complete listing of the mathematical functions available.
#           
# Discussion:
#
#  The "mu" embodies the estimation of the trait value for any individual
#  based on the sample mean and their covariate values.  It does not
#  normally include genetic effects.  The difference from this estimation
#  and the actual value is used to determine genetic and other
#  intra-individual effects.  Thus, "mu" is evaluated in the context of
#  of a single individual, and NOT a pair of individuals as with "omega".
#
#  You can get examples of many possible "mu" commands by using the
#  mu command to display the current mu equation for different
#  models.  For example:
#
#  solar> model new
#  solar> trait q1
#  solar> covar age
#  solar> mu
#  mu = \{Mean+bage*(age-x_age)\}
#
#  First notice that the entire body of this default mu equation is
#  delimited by \{ and \} characters.  This is the portion which is
#  automatically generated by SOLAR and will be changed automatically
#  if your covariates are changed.  You should not normally edit this
#  portion of the mu.  If you need to change the mu, you can either
#  augment this portion with an additional expression, or replace the
#  mu altogether with a new expression, in which case you must leave
#  out the \{ and \} delimiters.  If you replace the mu altogether with
#  a new expression, you are then responsible for including terms for
#  covariates (if any) and it is not necessary to use the "covariate"
#  command at all.
#
#  The Mean and bage terms refer to parameters in the model, the age term
#  refers to a data variable "age" found in the phenotypes file, and the
#  term x_age refers to the average age of all individuals in this sample.
#  You may include similar terms in any new mu expression.
#
#  Adding To The mu
#
#  You can add additional terms either by appending them onto the mu shown
#  by the mu command (using terminal cut and paste makes this convenient)
#  or using the "mu = mu + ..." shorthand.  For example, using the
#  shorthand, you could add a new term for log(weight-100) as follows:
#
#  solar> mu = mu + log(weight-100)
#
#  OR by entering the following:
#
#  solar> mu = \{Mean+bage*(age-x_age)\} + log(weight-100)
#
#  in either case, the result would be the same:
#
#  solar> mu
#  mu = \{Mean+bage*(age-x_age)\} + log(weight-100)
#
#  If you then added another covariate, that would be included automatically
#  in the default portion of the mu:
#
#  solar> covar sex
#  solar> mu
#  mu = \{Mean+bage*(age-x_age)+bsex*Female\} + log(weight-100)
#
#  Notice here that the variable "Female" changes according to the sex.
#  It is "0" for male and "1" for female.
#
#  Replacing the Mu
#
#  You can also replace the Mu altogether, removing the "default portion."
#  If you remove the specially delimited "default portion" from the mu,
#  your covariate commands will have no effect on the mu, and you will
#  either have to write the beta parameters into the mu yourself or
#  remove the covariates altogether.  All phenotypic variables you
#  write into the model will be required for all individuals to be
#  included in the sample.
#
#  Continuing our example:
#
#  solar> covariate delete_all
#  solar> mu
#  mu = \{Mean]}
#  solar> mu = Mean + log(weight-min_weight)
#  solar> mu
#  mu =  Mean + log(weight-min_weight)
#
#  The Mu can be as elaborate as you like, including any mathematical
#  functions defined in the "C" programming language.  It need not include
#  the "Mean" parameter (in fact you do not even need a Mean parameter in
#  SOLAR anymore).
#
#  If you removed the default mu by mistake and need to restore it,
#  use the "mu reset" command.
#
#  Bivariate Mu
#
#  solar> model new
#  solar> trait q1 q2
#  solar> covar sex
#  solar> mu
# mu = \{t1*(<Mean(q1)>+<bsex(q1)>*Female) + t2*(<Mean(q2)>+<bsex(q2)>*Female)\}
#
#  Notice that the mu for this bivariate model has separte terms for the first
#  and second traits, which are identified by "t1" and "t2".  (The variable
#  "t1" is true if the first trait is being estimated, and false if the
#  second trait is being estimated.  If you replace the mu, any terms not
#  multiplied by "t1" or "t2" will be applied to the estimation of both
#  traits, and you may have as many (or as few) t1 and/or t2 terms as you
#  need.
#
# Additional Notes:
#
#  (1) Use the "mu = mu + <expression>" as described above instead of
#      the now obsolescent "mu = <expression> + mu" to add to the mu.
#      Also, you may notice that square brackets are no longer used
#      to delimit the default mu.  They did not work as had been intended.
#      The default portion of the mu is now delimited by \{ and \} which
#      may be included in a user specified mu.  Everything within the
#      delimiters is maintained by SOLAR and editing this portion will
#      have no effect.  It is simply displayed for informational purposes.
#      If the mu is defaulted, models will be saved with a mu "comment"
#      for informational purposes only; the actual mu is determined by
#      the covariates.
#
#  (2) As terms in the mu equation, you may use any constant, any
#      parameter, Sex, Mean, and any Phenotypic variable.  There are
#      also predefined terms for any phenotype named 'var': x_var
#      (the sample mean), min_var (the sample min), and max_var (the
#      sample max).  Any math operator (+,-,*,/) and function defined 
#      in the C programming language may be used.  Also, the ^ character
#      may be used to indicate exponentiation.
#
#  (3) Parameter names which include * or ^ should be enclosed in
#      <> angle brackets to prevent the names from being interpreted
#      as multiplication and/or exponentiation expressions:
#
#           mu = Mean + <bage*sex>*(age-x_age)*Female
#
#  (4) The default mu expression will display all variables as being
#      adjusted to their mean (e.g. age-x_age).  However, during
#      maximization, if a variable is found to be binary, the
#      variable is adjusted to its minimum (e.g. diabet-min_diabet)
#      instead.  This will be reflected after the first maximization.
#      User-created mu equations must always correctly specify
#      either the mean (x_) or min (min_) variable as required.
#-

# solar::option --
#
# Purpose:  Set or read the value of model-specific options.
# 
# Usage:    option <option name> <value>    ; sets option value
#           option <option name>            ; reads option value
#           option                          ; shows all option values
#
# Notes:    ibd-specific options are set by ibdoption.
#
#           Most options control fairly obscure aspects of SOLAR operation 
#           and are not normally changed directly by SOLAR users.  Many are 
#           automatically controlled by other SOLAR commands during
#           normal operation.
#
#           Model-specific options are stored in saved model files.
#           Starting a new model (with the "model new" command) will
#           reset all options to the default value.  Loading a model
#           will reset all options to the default, then load the options
#           in the model file.  It is recommended to specify options
#           after specifying the trait and covariates, but before
#           giving the "polygenic" or other model maximizing command.
#
#           Warning: RARELY USED options may have become buggy in the 
#           context of more current features since they haven't been recently
#           tested.
#
#           Here is a list of options and their default values:
#
#    CMDiagonal 0      Covariance matrices diagonal (1=yes; 0=no; automatically
#                        set to 1 for sporadic models, 0 for all others)
#
#    StandErr 1        Compute Standard Errors (1=yes; 0=no; defaults to 1
#                        except during a multipoint scan where it is set to
#                        0 while scanning to improve speed)
#
#    StandLogLike 0    Standardize Loglikelihood (0=no; 1=yes) RARELY USED.
#
#    AutoCovarBound 1.25   Factor used in estimating covariate boundaries:
#                            AutoCovarBound*(tmax-tmin)/(cmax-cmin)
#                            This is a fairly wide but useable estimate
#                            which has never needed adjustment.
#
#    Grid 0            Method used in likelihood estimation 0=Search; 1=Grid
#                        RARELY USED, and not to be confused with grid command
#    GridPoints 1      Points to be used if Grid option (above) is applied
#
#    MaxIter 1000      Maximum Iterations allowed in a loglikelihood search
#                        (If you need more than this, something is probably
#                        wrong.  Usually only 10-20 iterations of searching
#                        is sufficient.  MaxIter is to prevent SOLAR from
#                        iterating forever in troublesome cases.)
#                        
#    Outlier 0         0=keep; 1=remove outliers (It's probably better to
#                        to remove them yourself than rely on this RARELY USED
#                        option.)
#    CutPeople 0.05    Factor used to remove outlying people
#    CutPed 1.0        Factor used to remove outlying pedigrees
#
#    TDist 0           Automatically set by the "tdist" command.  Don't set
#                        this option yourself unless you are an expert.  Use
#                        the tdist command instead, which sets this option
#                        and sets up the required parameter for you.
#
#    Conv 1.0E-6            Convergence improvement factor for quantitative
#                             models (For experts only!  See Chapter 6.)
#    Conv(Discrete) 1.0e-4  Convergence improvement factor for discrete models
#    NConv 4                Convergence count (For experts only!) Conv has to
#                             satisfied this many times.
#    Tol 1.0E-8             Tolerance for parameters (for experts only!)
#    MaxStep 5              Maximum steps (for experts only!) This many
#                             decrements are allowed in the attempt to improve
#                             loglikelihood.  This may need to be increased in
#                             troublesome cases
#    BCliffs 0.1            Backup from NaN cliffs by this factor (for experts
#                             only!) currently used only for discrete models
#    MaxCliffs 15           Maximum steps to backup from NaN cliffs (for
#                             experts only) currently used only for discrete
#
#    ScoreOnlyIndex -1   Automatically set by "multipoint -score"; otherwise 
#                          don't touch
#
#    MergeHousePeds 1    Merge pedigrees sharing households (1=yes; 0=no)
#                          Necessary for accurate C2 estimation; 1 is default.
#    MergeAllPeds 0      Merge ALL pedigrees  (Earlier merging method; use
#                          only if MergeHousePeds fails inexplicably)
#
#    RobustEst 0         Robust Estimation (new and barely tested)
#    Tune 3              Factor used with robust estimation
#
#    PedSelect 0         Select only this pedigree for maximization.  The
#                        zero default means "all pedigrees."  Otherwise
#                        use integer to select pedigree as indexed in
#                        pedindex.out created by "load pedigree."
#                        Alternatively use commands "pedlike" and
#                        "pedlod" to get pedigree specific likelihoods
#                        and lods using the same parameter values. It
#                        is also possible to select multiple pedigrees
#                        by using + operator: option pedselect 1;
#                        option pedselect + 2; and so on.  + is optional
#                        before the first selection.  The list of selected
#                        pedigrees is saved to model file, and can be
#                        cleared out either with "option pedselect 0" or
#                        "model new."
#
#    EnableDiscrete 1    Use SOLAR "Discrete Trait" modeling if trait is
#                        found to be discrete (2 integer values separated
#                        by one, e.g. 0,1).  (0) means use quantitative
#                        modeling regardless of inferred trait type.
#
#    DiscreteOrder 1     Ordering for discrete pedigrees.  You are strongly
#                        discouraged from changing this by Dr. Jeff Williams
#                        who has done considerable study of discrete trait
#                        modeling.  The default ordering (1) puts affecteds
#                        first if prevalence < 0.5, else unaffecteds first.
#                        No ordering is done for (0).  (-1) reverses the
#                        standard ordering.  (2) does per-pedigree ordering.
#                        (-2) does per-pedigree reverse ordering.  Per-pedigree
#                        ordering is not available for multiple traits.  Set
#                        DiscreteOrder to 3 to an create output file named
#                        "discrete.out" in output directory containing up
#                        to 7 variables of the ordered data.
#
#    DiscreteMethod 1    Version of discrete code used.  The default (1)
#                        seems the most robust, and use of the alternate
#                        method (2) is discouraged.
#
#    UnbalancedTraits 1  Default is to use "unbalanced traits" in bivariate
#                        models.  Individuals will be included in the
#                        analysis if they have either trait; they do not have
#                        to have both.  Individuals are converted to
#                        individual-trait's.  (0) Excludes individuals
#                        missing either trait.  (-1) Also exclude individuals
#                        missing either trait, use "bivariate" feature
#                        built-in to Search.f.  
#
#    EnforceBounds 1     Numerical errors during the quadratic-solving phase
#                        of maximization can result in overshooting parameter
#                        boundaries by a tiny amount.  The default enforces
#                        boundaries whenever a new "point" of parameter values
#                        is computed.  This was essential for bivariate models
#                        to prevent attempted square roots of tiny negative
#                        numbers.  (0) turns this feature off.  This is
#                        not effective during some phases of maximization,
#                        such as estimation of standard errors, where the
#                        point is not computed in Search.f, which is why
#                        the AbsVarianceParms option was added.
#
# EnforceConstraints 0   Experimental.  (1) turns on attempted enforcement of
#                        constraints when the derivatives of the tableau
#                        become too large to maintain constraint numerical
#                        accuracy better than 1e-4.  (Otherwise, convergence
#                        may fail with the "Numerical constraint failure"
#                        note.)  Unfortunately, when this is done, models are
#                        usually so out of whack for some reason or other that
#                        convergence will ultimately fail by exceeding the
#                        maximum iteration limit anyway.  (-1) turns off the
#                        constraint numerical accuracy test at the end of
#                        maximization.  The default (0) tests constraint
#                        accuracy at the end of maximization only.
#
#    CorrectDeltas 0     Experimental.  1 turns on attempted correction of
#                        deltas for numerical errors during the quadratic
#                        problem solving phase of maximization.  The default
#                        (0) leaves this turned off because the default 
#                        EnforceBounds option accomplishes the intended
#                        result more efficiently in most cases.
#
#    AbsVarianceParms 1  The default (1) forces the the abs() function to be
#                        applied to all known parameters used in the omega in
#                        bivariate maximization.  This prevents tiny negative
#                        values from causing NaN's to arize from the
#                        application of square root.  (-1) forces the abs() 
#                        function to be applied in univariate maxmization as
#                        well.  That might cause trouble with some discrete
#                        models.  (0) forces the use of the actual parameter
#                        values in the omega, negative or not.
#
#    BounDiff 0          This option controls the method used to compute
#                        "forward" and "central" differences in likelihood
#                        (derivatives) when a parameter is at a boundary.
#                        (Currently, this option is applied only in discrete
#                        trait modeling where it has been shown to be
#                        necessary to ensure complete maximization.)
#
#                        The default (0) prevents the parameter from going
#                        beyond the boundary if the boundary is -1 or 0 for
#                        lower bounds and 0 or 1 for upper bounds since 
#                        these are the typical "hard" boundaries for variance
#                        components, correlations, roots, and logs.  Otherwise,
#                        parameters are allowed to go beyond boundaries
#                        during derivative calculation by very tiny amounts.
#
#                        (1) always enforces the boundaries.
#                        (-1) never enforces the boundaries, allowing
#                        tiny excursions beyond them in all cases (which,
#                        if possible, might give the best results).  
#                        (2) applies the historic rule: only upper boundaries
#                        are (always) enforced, but only for forward
#                        differences, and the simple method of substituting
#                        the negated backward difference is used.
#                        For options (0) and (1) a more sophisticated
#                        algorithm is used when otherwise the boundary 
#                        would be crossed illegally.  An inner difference
#                        is taken AND adjusted for the slope of the next
#                        nearest change in differences.  In other words,
#                        the second derivative is used to compute an
#                        expected first derivative.
#
#    PedLike 0           Intended for use by pedlike and pedlod commands only.
#                        Produces files "pedexclude.dat" and "pedlike.dat"
#                        during maximization.
#
#    SampleSameTrustMe   This option declares to SOLAR that the sample for
#                        the previous model is identical to the current model.
#                        Therefore, the same EVD matrices may be used, and
#                        the default checking that is done to determine if the
#                        sample is the same is bypassed.  If this option is
#                        specified before EVD matrices have been created for
#                        any model, the required matrices are created, but the
#                        storage of additional data to determine when the
#                        sample changes is bypassed.  Thus maximization speed
#                        of all EVD models is increased, but only slightly,
#                        about 1%.  The downside is that if the user has
#                        specified this option in error, the results will be
#                        wrong and SOLAR might even crash.  Hence the suffix
#                        "TrustMe".  THIS IS A SPECIAL NO-WRITE OPTION which is
#                        not written to model files, because when the model
#                        is reloaded it may not be applicable at the time it
#                        is reloaded.  It should be used only in scripts
#                        where the null and test models are absolutely certain
#                        to have the same sample.
#
#    evdphase 0          If option modeltype is evd2, the evdphase option
#                        is used internally to handle the 3 model phases.
#                        In the evdphase 1, the sample is determined and
#                        evddata.out is written.  In the second phase a
#                        model is created using evddata.out for traits and
#                        data with all unrelated individuals and this model
#                        is actually maximized using standard quantitative
#                        maximization.  In the third phase the maximized
#                        model is translated back into the original parameters.
#
#    Eigenvectors 0      If set to 1, the eigenvectors from EVD will be written
#                        to files.  To use this option, use the "evdoutev"
#                        command (which invokes this option).
#
#    EVDmat 0            If set to a non-zero value, this writes EVD data
#                        suitable for fphi or similar procedure to evddata.out.
#                        The correct way to use this option is by using the
#                        "evdout" command which invokes it.
#
#    FPHIMethod 1        This sets the method for which EVD data should be
#                        written.  The correct way to use this option is by
#                        using the "fhpi" command which has a "-method2"
#                        option, however the default Method 1 works better now.
#
#    ResetRandom 0       If this option is set to a non-zero value, the
#                        random number sequence used during maximization will
#                        be set to its initial default value at the beginning
#                        of maximization.
#
#    dontallowsamplechange 0   If option modeltype is evd, and this option is
#                              set to 1, model maximization will terminate
#                              prematurely with an error message if the
#                              the sample changes from the previous evd
#                              model.  If modeltype is evd, and this option
#                              is defaulted at 0, the evd code will simply
#                              create new matrices as required.  If the
#                              option samplesametrustme is 1, this option
#                              has no effect unless the size of a pedigree
#                              changes.  If the option modeltype is not
#                              evd, this option has no effect.
#                         
#    singulartrait 0     If set to 0, a trait having only one non-blank value
#                        in the entire phenotypes file is an error which will
#                        halt maximization, because most of the time when this
#                        happens it is because there is a user mistake which
#                        needs to be corrected.  If set to 1, such a trait
#                        will be considered the "unaffected" value of a
#                        discrete trait and discrete trait maximization will
#                        be performed.  If set to 2, the trait will be
#                        considered a quantitative trait and quantitative
#                        trait maximization will be performed, but then you
#                        must also preset fake upper and lower bounds for
#                        the mean and sd parameters because the normal
#                        algorithm for guessing them doesn't work in this
#                        case.  Use the "parameter" command to preset these
#                        boundaries. The latter two options are generally only
#                        useful in the presence of covariate variation.
#
#    PolyClasses ""      The polyclasses option is modified by the polyclass
#                        command, users should not modify it.  It becomes
#                        a comma separated list of all the classes in a
#                        polyclass model.  This is created so that maximize
#                        can handle discrete trait and mixed trait models by
#                        locating the SD parameters for each trait.
#                        
#    ParameterFormat 16  number of significant digits used for writing
#                        parameter values to model files and queries.
#                        Prior to Version 6.3.6, the default was 10.
#                        16 is a compromise value that displays almost
#                        all the precision in a double precision floating
#                        point number, while avoiding representational issues
#                        that cause a string of 9's to appear at the end of
#                        a number.  The largest useful value is 17, that
#                        always shows all information available, but sometimes
#                        at the expense of being very ugly.
#
#    MatrixNumberFormat 15  number of significant digits used for writing
#                           results of matrix operations.  15 works best.
#
#    ExpNotation 0       1 forces exponential notation (but only for certain
#                        commands, mga is the only one currently).  0 is
#                        auto mode, which typically uses fixed point while
#                        a few digits of precision are shown, then flips to
#                        exponential notation if required to show a nonzero
#                        value.
#
#   ShuffleReseeding 1   1 gives consistent repeatable results by reseeding
#                        random generator at the beginning of each mathmatrix
#                        shuffle to default seed.  Options are:
#
#     1...seeded on every shuffle to 5489u for consistent results (DEFAULT)
#     0...seeded first shuffle to 5489u, then free running
#    -1...seeded every shuffle to time() for purely stochastic results
#    -2...seeded first shuffle to time(), then free running
#     Other values: seed to this value at beginning of each shuffle
#
# IMPORTANT NOTE: the zscore options below pertain to the now obsolescent
# zscore command implementation.  Now it is preferred to use the zscore_
# prefix operator with the define command.
#
#    zscore 0            The default mode has zscore deactivated.  Activation
#                        by setting this to a non-zero number should be done
#                        by the zscore command and only when the additionally
#                        required z options have been set.
#
#    zmean1 0            When zscore is active, zmean1 should be set to the
#                        expected mean value for trait 1.
#
#    zsd1 0              When zscore is active, zsd1 should be set to the
#                        expected standard deviation for trait 1.  This option
#                        must not be zero when zscore is non-zero.
#
#    zmean2 0            When zscore is active, zmean2 should be set to the
#                        expected mean value for trait 2.
#
#    zsd2 0              When zscore is active, zsd2 should be set to the
#                        expected standard deviation value for trait 2.  This
#                        option must not be zero when zscore is non-zero.
#-

# solar::phenotypes --
#
# Purpose:  Load the phenotypes file or display its variables 
# 
# Usage:    load phenotypes [<filename>]+      ;#  sets phenotype file
#           phenotypes                         ;#  returns phenotype names
#                                              ;#    (and filenames)
#           phenotypes -files                  ;#  returns filenames used
#                                              ;#    (useful in scripts)
# 
# Notes:    (1) Pedigree data should be loaded beforehand with the 
#               "load pedigree" command.  You may have pedigree and
#               phenotypes data in the same file if it has all the
#               required fields, but you will still have to load it
#               as phenotypes after loading it as the pedigree.
#
#           (2) The phenotypes file may be in Comma Delimited or PEDSYS format.
#               There must be an ID field (and FAMID field, if ID's are not
#               unique in all the data), and then there may be any number of
#               phenotypes.  Sex is determined by the SEX field in the
#               pedigree file and a SEX field in the phenotypes file is
#               ignored.  If FAMID is present in both pedigree and
#               phenotypes files, it is assumed to be necessary to make ID's
#               unique.  If FAMID is not present in both files, uniqueness
#               of ID's is tested during loading (since this is an often
#               overlooked user error).  If FAMID is present in both files,
#               uniqueness is not tested.  The fieldname EGO may be used
#               in place of ID, and the "field" command may may be used to
#               specify a different field name for ID.  For more discussion
#               about the phenotypes file format, see file-phenotypes.
#
#           (3) Once a phenotypes file is loaded in a particular working
#               directory, it remains loaded until another phenotypes is
#               loaded, even if SOLAR is restarted there at a later time.
#               The current pedigree state is kept in a file named
#               phenotypes.info, which points to the current pedigree file(s)
#               by name.  When SOLAR starts, it check this file, and
#               get the header from the phenotypes file(s) so that the
#               phenotypes available are known.
#
#           (4) During maximization, the pedigree data and phenotypes file
#               are joined, so it is possible some errors will not be apparent
#               until that time.
#
#           (5) Individuals missing phenotypic data are removed from the
#               maximization sample, and need not be included in the
#               phenotypes file, however they should be included in the
#               pedigree file as they may contribute to the pedigree
#               structure and genetic coefficients of those individuals
#               who are included.
#
#           (6) Families in which no non-probands are present are removed
#               from the maximization sample.  Proband status is controlled
#               by a PROBND field in the phenotypes file.  To switch
#               proband detection off, you may rename that field, or
#               use the command "field proband -none".
#-

# solar::trait --
#
# Purpose:  Select the trait (dependent variable)
# 
# Usage:    trait                               ; show current trait info
#           trait <trait1>                      ; selects one trait
#           trait [<traiti> ]+                  ; multivariate (up to 20)
#           trait -noparm [<traiti> ]+          ; don't touch parameters at all
#
#           [define <defname> = <expression>]+  ; Define any expressions as
#           trait [<phenotype>|<defname> ]+     ; traits...see "help define"
#
# Notes:    Solar is case insensitive to variable names.  Thus the command:
#
#                trait foo
#
#           will match a variable named FOO.  Variables can not be
#           distinguished on the basis of case in Solar. A phenotypes file
#           must be loaded before giving the trait command.
#
#           Starting with SOLAR version 4.x, arbitary expressions including
#           one or more phenotypes may be defined with the "define" command
#           and then used as trait(s).  See "help define" for more details.
#
#           If a model has already been created, it is recommended to give
#           the "model new" command to clear it out prior to giving the trait
#           command.  It is only reasonable to skip the "model new" if the
#           new trait has similar parameter estimates to the previous trait.
#           For a change of one trait to another, SOLAR will attempt
#           to accomodate the change by adjusting parameter values (unless
#           the -noparm option is used) as described below.  Any change of
#           trait(s) involving two traits is not permitted (if any
#           trait-specific parameters have been created); you will get an
#           error message and the trait will go into a special error state
#           ("Must_give_command_model_new") which will require you to give
#           the "model new" command to clear before any model can be
#           maximized (however, the model can be examined and saved in this
#           state...you may wish to repair it offline in a text editor).
#           Under no circumstances will the trait command create new
#           parameters or delete old parameters.  Normally the "polygenic"
#           command is given to create and test the standard variance
#           component parameters.
#
#           If changing from one trait to another, the Mean and SD parameters,
#           if present, will be reset to zero to force setting starting values
#           and boundaries during the next maximization.  Covariate betas
#           will boundaries will also be zeroed.
#
# Examples:
#
#           trait bmi
#           trait q1 q2
#
#           define a = 10 * log(q4)
#           trait a q3
#-


# now handled again by C++

# proc trait {args} {
#    set old ""
#    set new ""
#    catch {set old [ctrait]}
#    set rval [eval ctrait $args]
#    catch {set new [ctrait]}
#    if {0!=[string compare $old $new] && [option zscore]!=0} {
#	puts "\nZscore active.  Use zscore -off to deactivate."
#	zscore
#    }
#    return $rval
# }


# solar::constraint --
#
# Purpose:  Create, list, or delete constraints
# 
# Usage:    constraint term [+ <term>...] = <number> | <term>
#             <term> is [<factor>*]<parameter>
#           constraint                            ; display all constraints
#           constraint delete <number>            ; delete constraint by number
#           constraint delete <spec>|<left-spec>  ; delete specified constraint
#           constraint delete_all                 ; delete all constraints
# 
# Example:  constraint e2 + h2r = 1
#           constraint bq1 = bq2
#           constraint delete bq1
#           constraint delete h2r + e2 = 1
#           constraint H2 + 3*H2q1 - 2*H2q2 = 5*E2  ; anything is possible
# 
# Notes:    (1) The constraint numbers are shown when listing constraints.
#
#           (2) If a new constraint matches the right hand "body" of an
#               existing constraint, that existing constraint is replaced by
#               the new constraint (the old one would be invalid anyway).
#
#               solar> constraint sd = 1
#               solar> constraint
#               [1] sd = 1
#               solar> constraint sd = 0
#               solar> constraint
#               [1] sd = 0
#
#           (3) For the "constraint delete <left-spec>" command, if there
#               is a constraint matching the entire left specification of a
#               constraint, it is deleted.  Or, you can specify the entire
#               specification to deleted.  (The "constraint delete <parameter>"
#               version of the constraint command was ambiguous and is now
#               obsolete.)
#
#               [1] e2 + h2r + h2q1 = 1
#               [2] h2q1 = 0
#               constraint delete h2q1              ;# deletes constraint [2]
#               constraint delete h2q1 = 0          ;# deletes constraint [2]
#               constraint delete e2 + h2r + h2q1   ;# deletes constraint [1]
#
#           (4) Instead of constraining covariate beta values to 0, use
#               the "covariate suspend" command instead as this permits
#               greater efficiency.
#
#           (5) If you need to constrain interaction covariates (e.g. age*sex)
#               or parameters whose name begins with a number, or parameters
#               whose name includes other special characters, enclose
#               the parameter name in angle brackets <>.  When deleting the
#               constraint, angle brackets are optional around the parameter
#               name.  Do not include numeric factors in the delete command.
#
#               constraint 3*<bage*sex> = 1
#               constraint delete bage*sex
#
#          (6)  Constaints may only be simple linear equations of terms
#               which include a optional numeric factor and a parameter name.
#               Operating exponents and functions are not supported.
#               If you need to constrain a power of some model feature,
#               consider making the parameter itself contain the required
#               power, then it can be linearly constrained.
#
#          (7)  Numeric constants (such as 1 or 0) should only appear as
#               the right hand term.
#-

# solar::covariate --
#
# Purpose:  Set up covariates (independent variables).
#           It can handle interactions and polynomial terms.
#           For other non-polynomial models, use the 'mu' command.
# 
# Usage:    covariate <variable>[^n | ^1,2[,3...]][*<variable> | #<variable>
#                                                 [([trait])]]*
#                     Creates a new covariate.  See below for examples.
#                                          ;
#           covariate                      ; display all covariate info
#           covariate delete <string>      ; deletes covariate and beta(s)
#           covariate delete_all           ; deletes all covariates and beta(s)
#                                          ;
#           covariate <variable>()         ; Null Covariate: require var in
#                                          ;   sample without covariation
#                                          ;
#                                          ; Covariate Suspension (for
#                                          ;   temporary hypothesis testing).
#           covariate suspend <string>     ; temporarily disable covariate
#           covariate restore <string>     ; re-activate suspended covariate
# 
# Examples: 
#
#   covariate age                       ; Simple covariate Age
#   covariate age*sex                   ; Age by Sex interaction (only)
#   covariate age*diabet*diameds        ; 3-way interaction
#   covariate age^2                     ; Age squared as a simple covariate
#   covariate age^1,2                   ; Shorthand for: age age^2
#   covariate age#diabet                ; Shorthand for the following:
#                                       ;   covariate age diabet age*diabet
#   covariate age^1,2,3#sex             ; Shorthand for all the following:
#       covariate sex age age*sex age^2 age^2*sex age^3 age^3*sex
#
#   covariate sex age(q1) age*sex(q3)   ; Trait-specific Covariates:
#                                       ;   covariate sex applied to all traits
#                                       ;   covariate age applied to trait q1
#                                       ;   covariate age*sex applied to q3
#
#   In a multivariate analysis, trait-specific covariates are only required
#   for the sample of their trait.  See note (7) below.
#
#   covariate q2()                      ; Null-Covariate:
#                                       ;   require q2 in sample of all traits
#
# Notes:    (1) More than one covariate may be specified separated by spaces.
#               Also more than one covariate command may be used.  Adding a
#               new covariate does not remove previous ones.  Spaces are
#               not allowd within the specification of each covariate term.
#           (2) Pound (#) and comma (,) are shorthands allowed ONLY if there
#               are no more than two variables.  Further, only the first
#               variable may have multiple exponents separated by commas.
#               The following are INVALID:
#
#                   covariate age^1,2*diabed*diamed   ;# INVALID
#                   covariate age#diabet*diamed       ;# INVALID
#                   covariate weight*height^1,2       ;# INVALID
#
#               Instead, specify multiple covariates as required:
#
#                   covariate age*diabet*diamed age^2*diabet*diamed
#                   covariate age diabet diamed age*diabet age*diamed 
#                   covariate diabet*diamed age*diabet*diamed
#                   covaraite height^1,2*weight
#
#           (3) N-way interactions are possible to any N.
#
#           (4) Covariate commands create beta parameters automatically.
#               Beta names begin with 'b' followed by the variables
#               and exponents as in the non-shorthand form (e.g. bage^2*sex).
#
#           (5) Quantitative variables are mean-adjusted.  Binary variables
#               are adjusted so that the lowest value is 0 and the highest
#               value is 1.
#
#           (6) Suspended covariate variables are still required in sample.
#
#           (7) In a bivariate analysis "unqualified" covariates are applied
#               to and required by all traits, and trait-specific covariates
#               (with parenthetically named trait) apply only to the named
#               trait and are only required for that trait.  (This was
#               changed in SOLAR version 4.0.3.)
#
#               In a univariate analysis, ALL covariates are are currently
#               required regardless of whether they apply to the trait.
#               (The requirement of covariates specific to a different trait
#               may be removed in a future update.)
#
#               Null covariates (such as ef() ) are not applied to any
#               trait, but are required by all traits.
#
#               Variables not declared as covariates, but used in the mu
#               equation, are (at this time) required by all traits.
#
#           (8) When a trait is changed, covariate beta parameters are
#               and reset to force re-evaluation of boundaries
#               on the next maximization.  But trait changes
#               are not permitted for bivariate models; "model new" is
#               required.  "model new" will remove all covariates.
#
#           (9) When created, beta parameters have starting value 0.0 and
#               no boundaries.  Likely boundaries are set automatically
#               during the first maximization, and expanded if necessary up
#               to an internally determined maximum (and you may further
#               expand them if need be but this is highly unlikely).
#               If desired, you may set boundaries after beta values have
#               been created, and this will bypass automatic boundary
#               setting (but not automatic boundary expansion).
#-


# solar::drand --
#
# Purpose:  Return a random floating-point number between 0 and 1
# 
# Usage:    drand [ <seed> ]
# 
#           If no argument is given, drand returns a floating-point
#           number in the interval [0,1].
#
#           When an argument is given, it is taken to be an integer with
#           which to seed the random number generator.  If a seed value
#           of 0 is specified, the system time is used as the seed.  The
#           random number generator should be seeded prior to its first
#           use in a SOLAR run.  If the random number generator has not
#           been seeded when it is first called, it will be seeded with
#           the system time automatically.
#-


# solar::parameter --
#
# Purpose:  Create, modify, or list parameter(s)
# 
# Usage:    parameter <name>                ; display or create a new parameter
#           parameter <name> start <value> lower <value> upper <value>
#                                           ; set parameter start and 
#                                           ;   boundaries
#           parameter <name> =              ; return current parameter value
#           parameter <name> = <value>      ; set current (start) value
#           parameter <name> start <value>  ; set current (start) value
#           parameter <name> se             ; display last computed std error
#           parameter <name>                ; display all parameter information
#           parameter delete <name>         ; delete parameter
#           parameter                       ; display all parameters
#           parameter -return               ; return parameter info in a list
#           parameter fixupper <value>      ; set upper bound as fixed
#           parameter fixlower <value>      ; set lower bound as fixed
#                                          ; fixed bounds are not auto adjusted
#
# Notes:    (1) The most commonly required standard parameters are created
#               automatically when you give the "covariate" and "polygenic"
#               commands.  Their starting points and boundaries are also
#               set automatically either then or at the start of maximization
#               by fairly reliable heuristics.  Boundaries are set
#               automatically only if both boundaries are set to zero at the
#               beginning of maximization, so if you preset one boundary,
#               be sure to set the other.  The "standard" parameters include:
#
#                   mean (for trait)
#                   SD (standard deviation)
#                   e2
#                   h2r
#                   h2q1
#                   c2
#                   b* (covariate beta)
#
#           (2) "start" and "=" are identical operators.  "=" is simply a
#               more convenient and mnemonic name in most cases.  Once
#               a maximization has been performed, the "start" or "="
#               value will actually be the maximimum likelihood estimate.
#               It will then be the starting value for the NEXT maximization.
#
#               Note that when you are setting the starting value for a
#               parameter, you must surround the "=" used for assignment
#               with spaces.  For example:
#
#                   parameter h2r = 0.1
#
#               If you did not surround the = with spaces, it would appear
#               that you were simply trying to create a new parameter named
#               h2r=0.1.  To prevent this kind of mistake, such names are
#               not allowed.  See note 4 below.
#
#           (3) When a parameter is deleted, any constraint it appears in is
#               deleted also.  This behavior is obsolescent.  In the future,
#               a single TERM may be deleted from the constraint instead.
#               In the meantime, it is recommended to edit constraints to
#               remove parameters slated for deletion, THEN delete the
#               parameter.
#
#           (4) When naming parameters, you are advised to stick to the
#               usual alphabetic, numeral, and underscore characters unless
#               there is a good reason not to.
#
#               However, other special characters may be actually allowed in
#               order to accomdate all automatically created parameter
#               names.  The use of some of these characters will
#               force the requirement that these parameters be quoted in
#               <> when used in constraint, omega, and mu commands so as
#               not to imply math operations.
# -

# solar::matrix --
#
# Purpose:  Set up sample matrix (phi2 etc.) or math matrix
# 
# SOLAR has two kinds of matrix, the original sample matrix variables used
# during maximization (e.g. phi2) and a newer matrix algebra object used
# for high speed math calculations.  For clarity these may be called
# "sample matrix" and "math matrix".
#
# Usage:    matrix load [<option>] <filename> <name1> [<name2>]
#             ;# loads sample matrix variable(s) (one or two) from file}
#
#           set matname [matrix load <filename>]
#             ;# create math matrix from contents of csv file
#             ;# (header line is ignored)
# 
#           set matname [matrix new {{1 2} {3 4}}]
#             ; create math matrix from tcl list of lists
#            
#           matrix                         ; displays sample matrices
#           matrix delete <name>           ; deletes a matrix (either kind)
#           matrix delete_all              ; deletes all sample matrices
#           matrix reset                   ; deletes all math matrices
#           
#           matrix debug                   ; print info about sample matrices
#           matrix -return                 ; return sample matrix commands
# 
#           <option> == -sample            ; remove missing ID's from sample
#           <option> == -allow             ; default missing ID's to diagonal 1
#           <option> ==                    ; error if matrix missing ID
#
# Notes:    A .gz suffix is appended to the filename, if it is not specified.
#           Matrix files are compressed with (GNU) gzip; gunzip must be
#           installed in the user's path.
#
#   CSV Sample Matrix Files (recommended for user written matrix files)
#
#           As of version 7.5.4, matrices can be in CSV format, using user ID
#           (not pedindex) as the key field.  The required fields are
#           id1,id2,matrix1.  The optional fields are famid1,famid2,matrix2.
#           If FAMID is required to disambiguate ID's, famid1 and famid2 must
#           be included in each record since not all matrices are limited to
#           family interactions.  All other fields in a CSV matrix file are
#           ignored.  The mapping from ID to pedindex is obtained from the
#           currently loaded pedigree file.  It is recommended that users
#           create matrix files in the CSV format.  CSV matrix files must
#           be gzipped and a .gz suffix must be at end of filename.  For
#           more information, give the command "file-matrix".
#
#           As of version 7.5.5, matrix files are checked against the model
#           sample during maximization.  By default, if a diagonal matrix
#           entry is missing for anyone who would otherwise be included in the
#           sample, an error occurs and messages displaying the missing
#           individuals will be printed to the terminal.  There are two options
#           which permit missing diagonal entries.  -allow permits such
#           individuals and their diagonal entry will be defaulted to 1.0.
#           -sample removes such individuals from the sample, and a message
#           indicating the number of such removed individuals will be written
#           to the maximization output file.
# 
#           Matrix names beginning with 'ibd' or 'd7' are 'defaultable.'
#           This means that if they have a value of -1, that value is replaced
#           with Phi2 or Delta7 respectively.  By default, Phi2 and Delta7 are
#           calculated internally, but may be overwritten with externally
#           provided matrices.  (This feature is obsolescent and should not
#           be used in new code.)
#
#   Traditional Format SOLAR Sample Matrix Files
#
#           Traditional format SOLAR matrix files are discussed in Sec. 8.3 of
#           the manual.  These are space delimited with semi-fixed
#           format.  The first two columns are for the IBDID1 and IBDID2 and
#           they MUST have the same fixed width throughout the file.  Then
#           there must be one or two data columns, beginning no earlier than
#           column 14 (counting the first position as column 1). The data
#           columns are not fixed format and can be separated by one or more
#           spaces.  The columns for IBDID1 and IBDID2 should be right
#           justified and separated by a space.
#
#           To see an example of the traditional matrix file format, you
#           can give these SOLAR commands:
#
#           example
#           load pedigree gaw10.ped
#           gunzip phi2.gz
#           head phi2
#
#           This will show output that starts like this:
#
#           1     1  0.1268587045 .61377
#           1     1  1.0000000  1.0000000
#           2     2  1.0000000  1.0000000
#           3     3  1.0000000  1.0000000
#           4     4  1.0000000  1.0000000
#           5     5  1.0000000  1.0000000
#           6     6  1.0000000  1.0000000
#           7     7  1.0000000  1.0000000
#           8     8  1.0000000  1.0000000
#           9     9  1.0000000  1.0000000
#       
#           This sample shows the following things:
#
#           IBDID1 starts in the first column (column 1) and is 5 columns
#           wide (allowing IBDID's up to 32000, the max allowed).
#           
#           There is one space separating the IBDID's.  This should not
#           be a tab and you should not use tabs at all in this file.
#
#           IBDID2 starts in column 7 and is allowed 5 columns.
#
#           There are two spaces.
#
#           VALUE1 starts in column 14 and is free format.
#
#           There is one space separating value 1 and 2.
#
#           The very first record in this file is not real data but
#           a checksum for the matching pedigree file.  The second
#           record in the file overwrites the checksum values with
#           the actual data for 1,1.  The optional checksum is
#           checked against the pedigree file during loading to ensure
#           the matrix corresponds to the actual loaded pedigree.
#           You can add this checksum to any gzipped matrix file by
#           running the matcrc command on it, for example:
#
#           matcrc phi2.gz
#
#           This will prepend a checksum for the currently loaded pedigree
#           to the matrix file, deleting a pre-existing checksum if needed.
#           You must be sure the currently loaded pedigree is the correct
#           pedigree for this matrix.  This step is optional but we
#           recommend it.
#
#   Using Matrix Files
#
#           The preferred way to set up linkage models is simply to let
#           the multipoint, twopoint, and bayesavg commands do everything
#           for you.  For more control, you can also use the "linkmod"
#           (multipoint) and "linkmod2p" (twopoint) commands to set up
#           (but not evaluate) linkage models.  linkmod and linkmod2p
#           set up all the required parameters and constraints for you.
#           The command "loadkin" will load the phi2.gz matrix file,
#           bypassing the usual on-the-fly calculations performed by SOLAR.
#
#-

# solar::omega --
#
# Purpose:  Sets the Omega (Covariance) equation directly
#
#           Important: By default, SOLAR (starting with version 2.0.2)
#           sets up the omega automatically when you give the "trait"
#           command, and subsequently modifies it as required during
#           polygenic, multipoint, and other commands.  It is only
#           necessary for the user to use the omega command for
#           special purpose advanced analyses.
# 
# Usage:    omega = <expression>    ; sets Omega equation
#           omega                   ; displays Omega equation
#           omega reset             ; reset default Omega equation
# 
# Notes:    The default omega for a SOLAR model with one linkage element is:
#
#           omega = pvar*(I*e2 + phi2*h2r + mibd1*h2q1)
#
# Notice that each term inside the parentheses has a matrix and a parameter.
# Some of the matrices loaded from external files and some are computed
# internally.
#
# The built-in variables are:
#
#           pvar...........Phenotypic variance.  This is the square of the SD
#                          (trait standard deviation) parameter.
#
#           I.............Identity matrix, which equals 1 when pair of
#                         individuals is the same individual, 0 otherwise.
#
#           phi2..........Two times the kinship coefficient, for quantiatitive
#                          models this is normally computed internally and 
#                          on-the-fly to reduce storage requirements.  
#                          Also found (identically) in the phi2.gz matrix file
#                          created by "load pedigree," SOLAR uses the 
#                          phi2.gz by default for discrete traits, or the usage
#                          of the external file can be forced by using the
#                          "loadkin" command or giving a suitable "load matrix"
#                          command such as "load matrix phi2.gz phi2 delta7".
#
#           delta7........Dominance coefficient, equivalent to Jacquard's
#                         delta7 from the series delta1-delta7 when there
#                         is no inbreeding.  If there is inbreeding, this
#                         should not be used.  As with phi2, this is, by
#                         default, computed internally and on-the-fly for
#                         quantitative models unless "loadkin" or a 
#                         comparable "load matrix" command is given.  The
#                         delta7 matrix is the 2nd matrix within the phi2.gz
#                         file.  This matrix should only be loaded if it is
#                         needed because it is usually not used by SOLAR.
#
#           male_i........1 if individual "i" is male, 0 otherwise.
#           male_j........1 if individual "j" is male, 0 otherwise.
#           female_i......1 if individual "i" is female, 0 otherwise.
#           female_j......1 if individual "j" is female, 0 otherwise.
#           si............index of trait i (1..ntraits)
#           sj............index of trait j (1..ntraits)
#
#           For <phenotype> which is the name of a phenotypic variable:
#
#           x_<phenotype>.....sample mean for <phenotype>.  For example, x_age.
#           min_<phenotype>...minimum sample value of <phenotype>.
#           max_<phenotype>...maximum sample value of <phenotype>.
#           <phenotype>_i.....value of <phenotype> for individual "i".
#           <phenotype>_j.....value of <phenotype> for individual "j".
#
#           For <parameter> which is the base name of a parameter:
#
#           <parameter>(ti)...replace "ti" by name of trait of individual "i".
#                             For example, h2r(ti) may be h2r(weight) in
#                             an analysis of traits height and weight.
#           <parameter>(tj)...replace "tj" by name of trait of individual "j".
#
#           teq...............trait for individuals i and j is the same.
#           tne...............trait for individuals i and j are not the same.
#
#           Matrices may also be used, along with math operators
#           + - * / and also ^ (power) and () (parentheses), and also
#           all math functions defined by the C Programming Language
#           which includes "log" for natural logarithm, trig functions,
#           and hyperbolic functions, among others.  Here is a list:
#           erfc, erf, lgamma, gamma, j1, j0, y1, y0, rint, floor, ceil, 
#           tanh, cosh, sinh, atan, acos, asin, tan, cos, sin, expm1, exp,
#           logb, log1p, log10, log, cbrt, sqrt, and abs.
#
#           Parameter names with special characters should be enquoted using
#           angle brackets so the entire name is enquoted, including any
#           prefix string.  For example, given a variable named age.onset with
#           dot, the mean value of the variable could be indicated with
#           <x_age.onset> in angle brackets as shown.  This is the same as
#           the rule used by the define and constraint commands.
#
#           Beginning with version 3.0.4, the following equalities and
#           inequalities may also be used between two terms.  If the
#           operator is true, 1 is returned, otherwise 0 is returned.
#           This enables you to construct compound conditional expressions
#           having the same effect as could have been done with "if"
#           statements.  The C operators < and > have been replaced with
#           << and >> so as not to be confused with the <> quotation of
#           variable names in SOLAR.
#
#           C Format    Fortran Format    Test
#           --------    --------------    ----
#
#           ==          .eq.              if equal
#           !=          .ne.              if not equal
#           >=          .ge.              if greather than or equal
#           <=          .le.              if less than or equal
#           >>          .gt.              if greater than
#           <<          .lt.              if less than
#
#           Example of use of inequalities:
#
#               omega = pvar * (I*e2 + (h2r >= 0.125)*phi2*h2rc + \
#                 (h2r < 0.125)*phi2*h2rd)
#
#           Beware that comparing the equality or inequality of two floating
#           point numbers sometimes does not work as expected due to
#           numerical representation limitations.  For example, 1/3 might
#           not equal 2/6.
#
#           The precedence of the equality and inequality operators is below
#           that of all other operations, so their expressions should be
#           in parentheses as in the example above.
#
#           There is also a function named "print" which simply prints
#           the value of its argument, which may be any expression, and
#           then returns that value.  For example, in place of the standard
#           univariate omega
#
#               omega = pvar*(phi2*h2r + I*e2)
#
#           You could have:
#
#               omega = pvar*(print(phi2) + I*e2)
#
#           and this would print each phi2 value as it is used.  An
#           expression may include any number of print functions, and
#           they are evaluated in the standard order of evaluation,
#           starting with the innermost subexpression.  If you simply
#           want to print some value without including it in the rest of
#           the expression, you can multiply the print function by zero,
#           for example:
#
#	        omega = pvar*(phi2*h2r + I*e2 + 0*print(delta7))
#
#           At this time, the print function can only print one number,
#           without any identifying string.  After each value is printed,
#           you must press RETURN to procede to the next, or you can hold
#           down RETURN to pass through a lot of prints.
#
# For multivariate models which have 3 or more traits, the automatically
# created standard omega includes generic rho parameters rhoe_ij, rhog_ij,
# and rhoc_ij if household effects, and rhoq1_ij (to rhoq10_ij) for linkage
# models.  When the omega is evaluated, the i and j are replaced with the
# trait indexes.  For example, with traits 1 and 2 rhoe_ij becomes rhoe_12.
# It is possible to write omegas without these generic rhos if desired.
#
# There are also 4 additional generic rho's available for custom usage:
# rhoa_ij, rhob_ij, rhod_ij, and rhof_ij.
#-

# solar::loglike --
#
# Purpose:  Get the log likelihood of the current model
# 
# Usage:    loglike
# 
# Note:     This could be used in a TCL script like this:
#
#           set firstll [loglike]
#
#           All this procedure does is retrieve the stored loglikelihood.
#           The current model must have been maximized first, either with
#           the maximize command, or with a command like twopoint or
#           multipoint which maximizes and compares many models.  If
#           the current model has not been maximized, an error is raised.
#-

# solar::quadratic --
#
# Purpose:  Get the most recent quadratic form after a maximization
# 
# Usage:    quadratic
# 
# Note:     This could be used in a TCL script like this:
#
#           set firstq [quadratic]
# 
#-

# solar::matcrc --
#
# Purpose:  Prepend pedindex checksum (CRC) to the beginning of a matrix file
#
# Usage:    matcrc [<path>/]<filename> [[<pedindexdir>] [-notzipped]]
#
#           If pedindex.out is in a different directory, the optional
#           <pedindexdir> is specified to point to where it is located.
#           It defaults to the current directory "."
#
#           If -notzipped argument is prepended the input matrix is not
#           unzipped first so it must be unzipped already.
#
# Quick Notes:
#
#            Matrix files in SOLAR include phi2.gz, IBD, and MIBD files.
#
#            If you have modified a pedindex file directly and are subsequently
#            having pedigree mismatch errors with your matrix files, but
#            are sure everything is correct, you can simply run matcrc on
#            all your matrix files to fix the problem quickly.  However it is
#            recommended you read all about matcrc first.
#
#            Beginning with version 7.5.0, support for gzipped csv matrix files
#            is added.  The checksum will have id1 and id2 set to "checksum"
#            and the checksum value will be in matrix1 after the initial
#            decimal point.  All other fields are ignored and are simply copied
#            from the first record.
#
# Examples: matcrc phi2.gz
#           matcrc gaw10mibd/mibd.1.10.gz
#
#           The first two lines in an unzipped matrix file created by SOLAR
#           since Version 4.0 will look like this:
#
#               1     1  0.1268587045 .61377
#               1     1  1.0000000  1.0000000
#
#           The first line is a checksum used to verify that the current
#           pedindex file is the same as the one used when the matrix was
#           created.  This checksum is not required when the matrix is read,
#           but if it is present, and it does not match the checksum values
#           from the pedindex file, an error will stop maximization until the
#           problem is fixed.
#
#           The checksum is immediately followed by the actual data for the
#           diagonal identity pair 1,1.
#
#           When the matrix is actually being read, from beginning to end, the
#           second line values will overwrite the first line values in the
#           memory used for matrix storage.  The checksum line will therefore
#           have no effect on maximization results.  The numbers in the
#           checksum line are only used for matrix/pedindex validation in
#           SOLAR versions 4.0 and beyond.
#
#           matcrc is used by all SOLAR procedures that create matrix files
#           since version 4.0.  At this time, it need not be used for user
#           created matrix files, it is optional now.  But we recommend that
#           users writing their own matrix files also use matcrc to
#           postprocess their matrix files so that their matrices are also
#           protected from being used with a changed pedigree.  Even a slight
#           change to a pedigree file can change the numbering of all
#           individuals in the pedindex, and thus make a previously created
#           matrix file entirely incorrect.
#
# Notes:    The relevant pedindex.out file is assumed to be in the current
#           working directory.  The matrix file can be in any other directory
#           so long as the relative or absolute path is given.
#
#           The current user must have write access to the matrix file
#           and to the directory itself, because during matcrc operation
#           the matrix file is unzipped and then rezipped.
#
#           A Posix compliant  "cksum" command is assumed to be available
#           on the system.  It produces a quasi-unique polynomial Cyclic
#           Redundancy Check (CRC) on the pedindex.out file; the chances of
#           any different pedindex.out file having the same CRC is
#           astronomically small.  This includes changes such as swapping
#           characters or lines.
#
#           The CRC and Number of Octets (NOCT) is prepended to the beginning
#           of the matrix in such a way as to be backward compatible with
#           earlier versions of SOLAR.  The first line in the actual matrix
#           is used as a template, but the coefficients are replaced with
#           CRC and NOCT.  Since this is followed by the same line with the
#           actual coefficients, the actual data overwrites the preceding
#           numbers and the checksum in the preceding line will have no
#           ill effect with any version of SOLAR, even those which did not
#           use the checksum.
#
#           Beginning with SOLAR version 4.0, when any matrix is loaded,
#           the CRC and NOCT (if present) are detected and compared with
#           those from the current  pedindex.out file.  When SOLAR
#           version 4.0 loads a matrix with no checksum, no warning is given,
#           but it may cause a warning or error in some future version.  A
#           mismatch in the CRC or NOCT will cause an error when loading.
#           The mismatch can be corrected by running matcrc again, because it
#           detects whether or not the matrix has already been signed, and
#           removes the previous signature if necessary.  However this should
#           only be done after the user has carefully validated the pedigree
#           and matrix match.  It would be safest to reconstruct all matrix
#           file with the current pedigree, if done by SOLAR procedures
#           it would not be necessary for the user to run matcrc since
#           it is done automatically by those procedures.
#
#           User construction of matrix files is discussed in Sec. 8.3 of
#           the manual.  Matrix files are space delimited with semi-fixed
#           format.  The first two columns are for the two IBDID values,
#           followed by one or two data value columns.  The data value
#           columns should  begin in character position 14 or higher
#           (counting the first position as 1).  Once gzipped, the file
#           should be processed with the matcrc command after loading the
#           pedigree file, to generate a safety checksum value.
# -

#
# Note: The "load matrix" code in matrix.cc permits any fixed position
# for the beginning of data values.  However, because of the way the
# checksum is currently written, starting in position 14, that requires
# the rest of the matrix to follow that precedent, with possible deviation
# of one character position (data values could start in column 13, though
# that is not recommended).  In future it may be required to make matcrc
# actually look at the rest of the file to allow for IBDID's higher than
# 99999, which would require making the starting data position higher.
#


proc matcrc {filename {pedindexdir .} {notzipped ""}} {

    ifdebug puts "matcrc $filename $pedindexdir"

    if {![file exists $filename]} {
	error "File $filename not found for matcrc"
    }

    if {![file exists $pedindexdir/pedindex.out]} {
	error "File $pedindexdir/pedindex.out not found for matcrc"
    }

    if {[catch {set record [exec cksum $pedindexdir/pedindex.out]}]} {
	error "Error running cksum on $pedindexdir/pedindex.out"
    }
    set crc [lindex $record 0]
    set noct [lindex $record 1]

    if {$notzipped == ""} {
	if {[catch {exec gunzip $filename}]} {
	    error "Error running gunzip on $filename"
	}
	set unname [file rootname $filename]
    } else {
	if {$notzipped != "-notzipped"} {
	    error "matcrc: Invalid argument $notzipped"
	}
	set unname $filename
	exec rm -rf $filename.gz
    }

# See if $filename already has a signature that needs to be removed

    set cksumfound 0
    set checksum_added 0
    set unfile [open $unname]
    set record1 [gets $unfile]
    set record2 [gets $unfile]    

# if csv format

    if {-1 != [string first , $record1]} {

# allow breakouts in case of csv matrix errors

	while {1} {

# find positions of id1, id2, and matrix1

	    set fields [split [string tolower $record1] ,]
	    set id1pos [lsearch $fields id1]
	    if {$id1pos == -1} {
		puts "Warning! id1 not found in csv matrix"
		puts "Cannot add checksum"
		break
	    }
	    set matrix1pos [lsearch $fields matrix1]
	    if {$matrix1pos == -1} {
		puts "Warning!  matrix1 field not found in csv matrix"
		puts "Cannot add checksum"
		break
	    }
	    set id2pos [lsearch $fields id2]
	    if {$id2pos == -1} {
		puts "Warning! id2 field not found in csv matrix"
		puts "Cannot add checksum"
		break
	    }

# see if second record is already a checksum record

	    set data [split [string tolower $record2] ,]
	    set id1 [lindex $data $id1pos]
	    if {$id1 == "checksum"} {
		ifdebug puts "old checksum found"
		set cksumfound 1
	    }

# open matrix.matcrc temp file and start writing to it

	    set ofile [open $unname.matcrc w]
	    puts $ofile $record1  ;# record1 is csv header
	    if {$cksumfound} {
		ifdebug puts "creating cksum record from previous"
		set newdata [lreplace $data $matrix1pos $matrix1pos .$crc]
		set cksumrecord [join $newdata ,]
		puts $ofile $cksumrecord
	    } else {
		ifdebug puts "creating cksum record using second record for template"
		set newdata1 [lreplace $data $id1pos $id1pos checksum]
		set newdata2 [lreplace $newdata1 $id2pos $id2pos checksum]
		set newdata3 [lreplace $newdata2 $matrix1pos $matrix1pos .$crc]
		set cksumrecord [join $newdata3 ,]
		puts $ofile $cksumrecord
		puts $ofile $record2
	    }
	    set checksum_added 1
	    break
	}
# now write out remainder of csv file and rename as $unname
	
	if {$checksum_added} {
	    while {[gets $unfile record] >= 0} {
		puts $ofile $record
	    }
	    close $ofile
	}
	close $unfile
	if {$checksum_added} {
	    exec mv $unname.matcrc $unname
	}
    } else {

# original tab delimited matrix format
# look for first and second records having same id1 and id2
# then there is existing header, which we must skip

	close $unfile
	if {![string compare [lindex $record1 0] [lindex $record2 0]] && \
		![string compare [lindex $record1 1] [lindex $record2 1]]} {
	    if {"SunOS" == [exec uname]} {
		exec tail +2 $unname >$unname.matcrc
	    } else {
		exec tail -n +2 $unname >$unname.matcrc
	    }
	    exec mv $unname.matcrc $unname
	}

# Catenate new header and original file, then rename back

	set id1 [lindex $record1 0]
	set id2 [lindex $record1 1]
    
	set tfile [open $unname.matcrc1 w]
	if {"" == $id2} {
	    puts $tfile "    $id1     $id2  0.$crc"
	} else {
	    puts $tfile "    $id1     $id2  0.$crc .$noct"
	}
	close $tfile

	exec cat $unname.matcrc1 $unname > $unname.matcrc2
	exec mv $unname.matcrc2 $unname
	file delete $unname.matcrc1
    }

# zip file

    exec gzip $unname

    return ""
}

# solar::matcsv -- private
#
# Purpose: Convert a matrix in csv format using ID's to a SOLAR matrix
#
# Special Note: SOLAR Version 7.5.0 loads csv matrix files natively, and
#               this procedure is no longer used or needed
#                 
# Usage:   matcsv <input> [<output>]
#
#         (1) If the input filename ends in .gz, it is assumed to be gzipped,
#             otherwise not.  No other archive formats are supported.
#
#         (2) Output filename defaults to input filename (less terminating
#             .gz, if present) suffixed with .solarmatrix.gz  The output
#             file will always be gzipped and the first line will be a cksum
#             that is formatted like a regular data line but is not used as
#             data by SOLAR (see help matcrc).
#
#         (3) The pedindex.out and pedindex.cde files (created by load
#             pedigree) must be in the current working directory.
#
#         (4) The input file must have at least 3 columns identified as
#             id1, id2, and matrix1.  Additional columns allowed are
#             matrix2, famid1, and famid2.  famid's should not be used unless
#             needed to disambiguate ID's, in which case they must also be
#             used in pedigree and phenotypes files.  If both famids are
#             always going to be the same, a single field named famid may
#             be used.
# -

proc matcsv {infile {outfile ""}} {

# The need for a conversion procedure has been eliminated.
# Instead, matrix will read both tab and csv delimited files directly
# This procedure is being saved in preliminary untested form

# Unzip input file if needed

    if {[file extension $infile] == ".gz"} {
	if {[catch {exec gunzip $infile}]} {
	    error "Error running gunzip on $infile"
	}
	set infile [file rootname $infile]
    }

    if {"" == $outfile} {
	set outfile $infile.solarmatrix
    }

# open matrix file and set up mandatory fields id1,id2,matrix1

    set tinfile [tablefile open $infile]
    set names [string tolower [tablefile $tinfile names]]

    tablefile $tinfile start_setup
    if {-1 == [lsearch $names id1]} {
	error "matcsv: Matrix file $infile missing field: id1"
    }
    tablefile $tinfile setup id1
    if {-1 == [lsearch $names id2]} {
	error "matcsv: Matrix file $infile missing field: id2"
    }
    tablefile $tinfile setup id2
    if {-1 == [lsearch $names matrix1]} {
	error "matcsv: Matrix file $infile missing field: matrix1"
    }
    tablefile $tinfile setup matrix1

# check for second matrix

    set matrices 1
    if {-1 != [lsearch $names matrix2]} {
	set matrices 2
	tablefile $tinfile setup matrix2
    }

# famid variations in matrix file

    set famid_found 0
    if {-1 != [lsearch $names famid1]} {
	set famid_found 2
	if {-1 == [lsearch $names famid2]} {
	    error "matcsv: matrix has famid1 but not famid2"
	}
	tablefile $tinfile setup famid1
	tablefile $tinfile setup famid2

    } elseif {-1 != [lsearch $names famid]} {
	set famid_found 1
	tablefile $tinfile setup famid
    }

# open and read pedindex into associative array

    set pedfile [tablefile open pedindex.out]
    tablefile $pedfile start_setup
    tablefile $pedfile setup id
    tablefile $pedfile setup ibdid
    if {$famid_found > 0} {
	tablefile $pedfile setup famid
    }

    set highest_ibdid 0

    while {{} != [set record [tablefile $pedfile get]]} {
	set ibdid [lindex $record 1]
	if {$ibdid > $highest_ibdid} {set highest_ibdid $ibdid}
	set Ibdid([lindex $record 0].famid.[lindex $record 2]) $ibdid
    }
    tablefile $pedfile close

# open output file

    set outf [open $outfile w]

# Read each matrix line, convert to ibdid, and output

    while {{} != [set record [tablefile $tinfile get]]} {

	set rc1 [catch {
	    set famindex [expr 2 + $matrices]
	    set famid1 [lindex $record $famindex]
	    set id1 $Ibdid([lindex $record 0].famid.$famid1)
	}]

	set rc2 [catch {
	    set famindex [expr 2 + $matrices]
	    if {$famid_found} {
		set famindex [expr $famindex + ($famid_found-1)]
	    }
	    set famid2 [lindex $record $famindex]
	    set id2 $Ibdid([lindex $record 1].famid.$famid2)
	}]
	if {$rc1} {
    error "missing pedigree info for ID=[lindex $record 0] with famid $famid1"
	}
	if {$rc2} {
    error "missing pedigree info for ID=[lindex $record 1] with famid $famid2"
	}

	if {$matrices<2} {
	    puts $outf "[format %5d $id1] [format %5d $id2]  [lindex $record 2]"
	} else {
	    puts $outf "[format %5d $id1] [format %5d $id2]  [lindex $record 2]  [lindex $record 3]"
	}
    }

# close files

    puts "closing files"
    close $outf
    tablefile $tinfile close

# Re-write matrix file sorted and with full diagonal

    exec [usort] -k1,1n -k2,2n $outfile >$outfile.sorted
    exec rm -f $outfile
    set in [open $outfile.sorted]
    set out [open $outfile w]
    set last 0
    while {-1 != [gets $in record]} {
	set fid1 [lindex $record 0]
	set fid2 [lindex $record 1]
	while {$last < $fid1} {
	    set wd 0
	    if {$last+1 < $fid1} {
		set wd [expr $last+1]
		incr last
	    } elseif {$fid2 > $fid1} {
		set wd $fid1
		set last $fid1
	    } elseif {$fid2 == $fid1} {
		set last $fid1
		break
	    } else {
		break
	    }
	    close $in
	    close $out
	    error "writing diagonal for $wd, id1=$fid1, id2=$fid2"
	    if {$matrices == 1} {
		puts $out "[format %5d $wd] [format %5d $wd]  1.0000000"
	    } else {
		puts $out "[format %5d $wd] [format %5d $wd]  1.0000000  1.0000000"
	    }
	}
	puts $outf $record
    }
    close $in
    close $out
    exec rm -f $outfile.sorted

# Add pedigree checksum

    matcrc $outfile . -notzipped

# Done!

    return "$outfile.gz written"
}


# solar::pedigree --
#
# Purpose:  Process the pedigree data.
#
# Usage:    load pedigree <filename> [-founders]   ; loads pedigree file
#           pedigree show [all | <ped#>]           ; displays pedigree data
#           pedigree classes [-full [-nowarn] [-phi2]] [-model [-meanf]]
#                                          ; displays relative-class counts
#
#           When a pedigree file is loaded, each individual in the file is
#           assigned a unique integer identifier for internal use by SOLAR.
#           The mapping from permanent IDs to integer IDs is stored in the
#           file 'pedindex.out'.  Therefore, loading a pedigree data file
#           named 'pedindex.out' is not allowed since that would result in
#           the pedigree file being overwritten.  Attempting to re-load
#           a previously created 'pedindex.out' under a different name will
#           not work either; see Section 8.2.1 in the Manual for discussion.
#
#           If the pedigree file contains founders only, i.e. a set of
#           unrelated individuals with no parental data, parent ID fields
#           are not required.  In this case, the '-founders' option must
#           be included in the load command.  If this option is specified
#           but the pedigree file does contain parent ID fields, those
#           fields will be ignored
#
#           If the keyword 'all' is given in the 'pedigree show' command,
#           detailed info is displayed for all pedigrees.  If a pedigree
#           number is specified, detailed info is displayed for that
#           pedigree only.  If no argument is given, the show command
#           displays summary information.
#
#           The 'pedigree classes' command displays a tally of the relative
#           classes present in the pedigree data.  By default, the counts
#           for relationships of 3rd degree and higher, as well as some
#           1st and 2nd degree relationships, are combined.  If the '-full'
#           option is included, then the counts for all relative classes
#           are given separately.  In this case, a warning message will be
#           displayed if any of the relative classes cannot be handled by
#           SOLAR's native method for computing multipoint IBDs.  The
#           '-nowarn' option will turn the warning off.  If the '-phi2'
#           option is included with the '-full' option, an additional
#           column will be displayed which contains the kinship coefficient,
#           multiplied by 2, for each relative class.
#
#           If the '-model' option in included in the 'pedigree classes'
#           command, the relative class tallies will include only those
#           pairs of individuals who both enter the polygenic analysis
#           specified in the null0 model for the current trait.  The
#           '-full' and '-phi2' options work as described above, but the
#           '-nowarn' option is superfluous since the warning message
#           described above is never displayed.  When the '-meanf' option
#           is included with the '-full' option, the "Mean F" statistic
#           is calculated and displayed.
#
#           The state of the currently loaded pedigree data is stored in
#           the file 'pedigree.info' in the current working directory.
#           This file persists between SOLAR runs, which means that the
#           pedigree data which is loaded at the end of a session will
#           still be loaded the next time SOLAR is invoked (from within
#           the same working directory.)
#
# Notes:    The pedigree load command creates several files in the current
#           working directory:
#
#              pedindex.out   maps each ego ID to a sequential ID (IBDID)
#                               assigned by SOLAR
#              pedindex.cde   PEDSYS code file for pedindex.out
#              phi2.gz        gzipped file containing the kinship matrix
#                               multiplied by 2
#              house.gz       gzipped file containing the household matrix
#
#           The household matrix file will be created only if a household
#           ID field is present in the pedigree file.
#
#           The files listed above are specifically created for the
#           pedigree file being loaded.  They will be deleted when a new
#           pedigree file is loaded.  Hence, a different working directory
#           must be used for each data set.
#
# For a description of the pedigree file, enter 'file-pedigree'
#-

proc pedigree {args} {

    proc reverse_sort_by_second_num {a b} {
	set a1 [lindex $a 1]
	set b1 [lindex $b 1]
	if {$b1 < $a1} {
	    return -1
	} elseif {$b1 > $a1} {
	    return 1
	}
	return 0
    }

    global env

    set all_founders 0
    set full_disp 0
    set nowarn 0
    set showphi2 0
    set do_model 0
    set do_meanf 0

    set pedargs [read_arglist $args -founders {set all_founders 1} \
                 -full {set full_disp 1} -nowarn {set nowarn 1} \
                 -phi2 {set showphi2 1} -model {set do_model 1} \
                 -meanf {set do_meanf 1} ]

    set arg1 [lindex $pedargs 0]

    if {$arg1 == "load"} {
        if {[lindex $pedargs 1] == "pedindex.out"} {
            error \
    "Loading a file named 'pedindex.out' is not allowed. See 'help pedigree'."
        }
        if {[if_global_exists PedNdxIDList]} {
            purge_global PedNdxIDList
        }
        if {$all_founders} {
            lappend pedargs "all_founders"
        }
        set status [eval cpedigree $pedargs]
        if {[file exists phi2.gz]} {
            matcrc phi2.gz
        }
        if {[file exists house.gz]} {
            matcrc house.gz
        }
        return $status

    } elseif {$arg1 == "classes"} {

        if {$do_meanf && !$do_model} {
           puts "Warning: -model option not specified, so option -meanf is ignored"
           set do_meanf 0
        }

        if {$do_meanf && !$full_disp} {
           puts "Warning: -full option not specified, so option -meanf is ignored"
           set do_meanf 0
        }

        if {$do_model} {
            if {![file exists [full_filename null0.mod]]} {
                error "null0 model not found. Run polygenic command first."
           }
        }

        if {![file exists mibdrel.ped]} {
            status_message "Running mibd relate ..."
            if {[catch {mibd relate} errmsg]} {
                status_message " "
                if {![file exists relate.unk]} {
                    error $errmsg
                } else {
                    puts $errmsg
                }
            }
        }

        if {$do_model} {
            status_message "Testing null0.mod ..."
            set whofilename [full_filename who.out]
            delete_files_forcibly $whofilename

            save model [full_filename before_relatives]
            load model [full_filename null0]
            catch {maximize -who -q}
            load model [full_filename before_relatives]

            if {![file exists $whofilename]} {
                error "Error occurred testing null0 model"
            }

            status_message "Reading who.out ..."
            set wfile [open $whofilename r]
            set highest_id 0
            while {-1 != [gets $wfile line]} {
                if {![is_integer $line]} {
                    error "Non-integer sequential ID in $whofilename: $line"
                }
                if {$line > $highest_id} {
                    set highest_id $line
                }
            }
            seek $wfile 0 start

            set if_included_1 {}
            for {set i 0} {$i <= $highest_id} {incr i} {
                lappend if_included_1 0
            }

            while {-1 != [gets $wfile line]} {
                set if_included_1 [lreplace $if_included_1 $line $line 1]
            }
            close $wfile
        }

        status_message "Reading classes.tab ..."
        set highest_class 0
        set cfilename $env(SOLAR_LIB)/classes.tab
        set cfile [open $cfilename r]
        while {-1 != [gets $cfile line]} {
            if {1 != [scan $line "%d" class]} {
                error "error reading classes.tab"
            }
            set splits [split $line "|"]
            set class_name($class) [lindex $splits 1]
            set constraint($class) [lindex $splits 3]
            set phi2($class) [lindex $splits 4]
            if {$do_meanf} {
                set fract [split [lindex $splits 7] "/"]
                if {[llength $fract] == 1} {
                    set coeff2($class) [lindex $fract 0]
                } else {
                    set coeff2($class) \
                        [expr double([lindex $fract 0])/[lindex $fract 1]]
                }
            }
            if {$class != 99} {
                set fract [split $phi2($class) "/"]
                if {[llength $fract] == 1} {
                    set kin2($class) [lindex $fract 0]
                } else {
                    set kin2($class) \
                        [expr double([lindex $fract 0])/[lindex $fract 1]]
                }
            }
            if {$class > $highest_class} {
                set highest_class $class
            }
        }
        close $cfile

        set pfile [open "|gunzip -c phi2" r]
        gets $pfile line
        if {[lindex $line 2] >= 1} {
            set ibdid1 [lindex $line 0]
            set ibdid2 [lindex $line 1]
            set 2kin($ibdid1,$ibdid2) [lindex $line 2]
        }
        while {-1 != [gets $pfile line]} {
            set ibdid1 [lindex $line 0]
            set ibdid2 [lindex $line 1]
            set 2kin($ibdid1,$ibdid2) [lindex $line 2]
        }
        close $pfile

        set pairs_found {}
        for {set class 0} {$class <= $highest_class} {incr class} {
            lappend pairs_found 0
        }

        if {!$full_disp} {
            for {set deg 1} {$deg <= 20} {incr deg} {
                set degree_pairs($deg) 0
            }
        }

        status_message "Reading mibdrel.ped ..."
        set mtable [tablefile open mibdrel.ped]
        tablefile $mtable start_setup
        tablefile $mtable setup IBDID1
        tablefile $mtable setup IBDID2
        tablefile $mtable setup PHI2
        tablefile $mtable setup CLASS
        while {{} != [set line [tablefile $mtable get]]} {
            set ibdid1 [lindex $line 0]
            set ibdid2 [lindex $line 1]
            if {$do_model} {
                set if_1 [lindex $if_included_1 $ibdid1]
                set if_2 [lindex $if_included_1 $ibdid2]
                if {![llength $if_1] || !$if_1 || ![llength $if_2] || !$if_2} {
                    continue
                }
            }
            set k2 [lindex $line 2]
            set class [lindex $line 3]
            if {$class > 0 && $k2 == 0} {
                set k2 $2kin($ibdid1,$ibdid2)
                if {$class != 99} {
                    set kin2($class) $k2
                }
            }

            if {$full_disp} {
                set found [lindex $pairs_found $class]
                incr found
                set pairs_found [lreplace $pairs_found $class $class $found]
                continue
            }

            if {$ibdid1 == $ibdid2} {
                set class 1
            } elseif {[regexp "^Parent" $class_name($class)]} {
                set class 2
            } elseif {[regexp "^Sibling" $class_name($class)]} {
                set class 3
            } elseif {[regexp "^Grandparent" $class_name($class)]} {
                set class 4
            } elseif {[regexp "^Avuncular" $class_name($class)]} {
                set class 5
            } elseif {[regexp "^Half sib" $class_name($class)]} {
                set class 6
            }
            if {$class < 7 || $class == 23 || $class == 300} {
                set found [lindex $pairs_found $class]
                incr found
                set pairs_found [lreplace $pairs_found $class $class $found]
                continue
            }

            for {set deg 1} {$deg <= 20} {incr deg} {
                if {$k2 >= [expr pow(.5,$deg)]} {
                    incr degree_pairs($deg)
                    break
                }
            }
        }
        tablefile $mtable close

        set kin2ord {}
        for {set class 0} {$class <= $highest_class} {incr class} {
            if {$class == 99} {
                set k2 1e-300
            } elseif {![info exists kin2($class)]} {
                set k2 0
            } else {
                set k2 $kin2($class)
            }
            lappend kin2ord "$class $k2"
        }
	set kin2ord [lsort -command reverse_sort_by_second_num $kin2ord]

        status_message "Writing output file ..."
        set mean_f_numerator 0
        set mean_f_denominator 0
        if {$do_model} {
            set ofile [open [full_filename relatives.out] w]
        } else {
            set ofile [open mibdrel.tab w]
	}
        puts $ofile ""
        if {$full_disp && $showphi2} {
            puts $ofile \
"    NPairs  Phi2          Relationship"
            puts $ofile \
"  ========  ============  ==========================================="
        } elseif {$full_disp} {
            puts $ofile \
"    NPairs  Relationship"
            puts $ofile \
"  ========  ==========================================="
        } else {
            puts $ofile \
"    NPairs  Relationship"
            puts $ofile \
"  ========  ========================"
        }

        if {!$full_disp} {
            set how_many [lindex $pairs_found 1]
            if {$how_many > 0} {
                puts $ofile [format "%10d  %s" $how_many $class_name(1)]
            }
            set how_many [lindex $pairs_found 300]
            if {$how_many > 0} {
                puts $ofile [format "%10d  %s" $how_many $class_name(300)]
            }
            for {set class 2} {$class <= 3} {incr class} {
                set how_many [lindex $pairs_found $class]
                if {$how_many > 0} {
                    puts $ofile [format "%10d  %s" $how_many $class_name($class)]
                }
            }
            if {$degree_pairs(1)} {
                puts $ofile [format "%10d  %s" $degree_pairs(1) "Other 1st degree"]
            }
            for {set class 4} {$class <= 6} {incr class} {
                set how_many [lindex $pairs_found $class]
                if {$how_many > 0} {
                    puts $ofile [format "%10d  %s" $how_many $class_name($class)]
                }
            }
            set how_many [lindex $pairs_found 23]
            if {$how_many > 0} {
                puts $ofile [format "%10d  %s" $how_many $class_name(23)]
            }
            if {$degree_pairs(2)} {
                puts $ofile [format "%10d  %s" $degree_pairs(2) "Other 2nd degree"]
            }
            if {$degree_pairs(3)} {
                puts $ofile [format "%10d  %s" $degree_pairs(3) "3rd degree"]
            }
            for {set deg 4} {$deg <= 20} {incr deg} {
                if {$degree_pairs($deg)} {
                    puts $ofile [format "%10d  %dth degree" $degree_pairs($deg) $deg]
                }
            }
            set how_many [lindex $pairs_found 0]
            if {$how_many > 0} {
                puts $ofile [format "%10d  %s" $how_many $class_name(0)]
            }

        } else {
            set flagged 0
            for {set i 0} {$i <= $highest_class} {incr i} {
                set class [lindex [lindex $kin2ord $i] 0]
                set how_many [lindex $pairs_found $class]
                if {$how_many > 0} {
                    set flag " "
                    if {!$do_model && !$nowarn && $constraint($class) == -1} {
                        set flag "*"
                        set flagged 1
                    }
                    if {$showphi2} {
                        if {$class == 99} {
                            puts $ofile \
        [format "%s%9d                %s" $flag $how_many $class_name($class)]
                        } else {
                            puts $ofile \
        [format "%s%9d  %-12.10g  %s" $flag $how_many $kin2($class) $class_name($class)]
                        }
                    } else {
                        puts $ofile \
        [format "%s%9d  %s" $flag $how_many $class_name($class)]
                    }
                    if {$do_meanf} {
                        set f_i $coeff2($class)
                        if {$f_i} {
                            set f_i [expr double($f_i) * -0.5]
                            set mean_f_numerator [expr $mean_f_numerator + $how_many * $f_i]
                            set mean_f_denominator [expr $mean_f_denominator + $how_many]
                        }
                    }
                }
            }
            if {$flagged} {
                puts $ofile ""
                puts $ofile \
"NOTE: SOLAR cannot compute multipoint IBDs for the relative classes flagged"
                puts $ofile \
"with an asterisk (*). You must simplify your pedigree or use another program"
                puts $ofile \
"to compute multipoint IBDs."
            }
        }

        if {$mean_f_denominator > 0} {
            set mean_f [format %.3f [expr $mean_f_numerator / $mean_f_denominator]]
            puts $ofile ""
            puts $ofile "Mean f is $mean_f"
        }
        puts $ofile ""
        close $ofile
        status_message " "
        if {$do_model} {
            exec >&@stdout <@stdin more [full_filename relatives.out]
            puts "This table also written to [full_filename relatives.out]"
        } else {
            exec >&@stdout <@stdin more mibdrel.tab
        }
        return

    } else {
        return [eval cpedigree $pedargs]
    }
}


# solar::marker --
#
# Purpose:  Process the marker genotype data.
#
# Usage:    load marker [ -xlinked ] <filename>   ; loads marker file
#           marker unload [ -nosave ]       ; unloads marker genotype data
#           marker discrep [<marker> ...]   ; checks for marker discrepancies
#           marker names                    ; displays marker names
#           marker show [<marker> ...]      ; displays summary of marker data
#           marker fname                    ; returns name of marker file
#
#           The '-xlinked' option of the 'marker load' command can be given
#           when loading genotype data for X-linked markers.  Alternatively,
#           the XLinked option can be set with the 'ibdoption' command.
# 
#           Genotype data will not be unloaded for markers for which MLE
#           allele frequencies have been computed but not saved to a file.
#           To save MLE allele frequencies, use the 'freq save' command.
#           To unload markers without saving MLE allele frequencies, give
#           the '-nosave' option in the 'marker unload' command.
#
#           If a marker name is not specified for the 'marker discrep' or
#           the 'marker show' command, the command applies to all markers
#           currently loaded.
#
#           The state of the currently loaded marker data is stored in the
#           file 'marker.info' in the current working directory.  This file
#           persists between SOLAR runs, which means that the markers which
#           are loaded at the end of a session will still be loaded the
#           next time SOLAR is invoked (from within the same working
#           directory.)
#
# Notes:    The marker load command creates a subdirectory in the current
#           working directory for each marker.  Marker subdirectories are
#           named 'd_<marker>', where <marker> is the marker name.  The
#           contents of a subdirectory will depend on the type of marker
#           processing performed, and will include various input, output,
#           and (possibly) error files.  The marker subdirectories are
#           deleted when the marker genotype data is unloaded.
#
#           The loci in the marker file must all be autosomal or all be
#           X-linked.  By default, SOLAR assumes the loci are autosomal.
#
#           The set of markers in the marker file and the set of markers in
#           the freq file do not have to be the same.  Allele frequencies
#           will be computed for markers that do not appear in the freq file
#           at the time these markers are loaded.
#
# For a description of the marker file, enter 'file-marker'
#-

proc marker {args} {
    set xlinked 0
    set nosave 0

    set mrklist [ read_arglist $args -xlinked {set xlinked 1} \
                                     -nosave {set nosave 1} ]
    set arg1 [lindex $mrklist 0]

    if {$arg1 == "load"} {
        if {[llength $mrklist] != 2} {
            error "Invalid marker command"
        }
        if {$xlinked} {
            cmarker load -xlinked [lindex $mrklist 1]
        } else {
            cmarker load [lindex $mrklist 1]
        }
        foreach mrk [cmarker names] {
            if {[cfreq nall $mrk] == 0} {
                puts "Warning: No genotypic data for marker $mrk"
            }
        }
        return
    }

    if {$arg1 == "unload"} {
        if {[llength $mrklist] != 1} {
            error "Invalid marker command"
        }
        if {$nosave} {
            cmarker unload -nosave
        } else {
            cmarker unload
        }
        return
    }

    if {$arg1 == "names"} {
        if {[llength $mrklist] != 1} {
            error "Invalid marker command"
        }
        return [cmarker names]
    }

    if {$arg1 == "show"} {
        set sfile solar-show-text.[pid]
        if {[llength $mrklist] == 1} {
            exec cat << [cmarker show] > $sfile
        } else {
            set nohwetest 1
            for {set i 1} {$i < [llength $mrklist]} {incr i} {
                if {[cfreq chi2_hwe [lindex $mrklist $i]] >= 0} {
                    set nohwetest 0
                }
            }
            exec echo > $sfile
            exec cat << \
                "marker           #typed  %typed  #foutyp  %foutyp" >> $sfile
            if {!$nohwetest} {
                exec cat << "  HWE p-val" >> $sfile
            }
            exec cat << "\n" >> $sfile
            exec cat << \
                "---------------  ------  ------  -------  -------" >> $sfile
            if {!$nohwetest} {
                exec cat << "  ---------" >> $sfile
            }
            exec cat << "\n" >> $sfile
            for {set i 1} {$i < [llength $mrklist]} {incr i} {
                exec cat << [cmarker show [lindex $mrklist $i]] >> $sfile
            }
        }
        exec >&@stdout <@stdin more $sfile
        delete_files_forcibly $sfile
        return
    }

    if {$arg1 == "discrep"} {
        if {[llength $mrklist] == 1} {
            eval lappend mrklist [marker names]
        }
        set err 0
        for {set i 1} {$i < [llength $mrklist]} {incr i} {
            set mrk [lindex $mrklist $i]
            if {[cfreq nall $mrk] == 0} {
                puts "Cannot run allfreq for marker $mrk: no data"
                continue
            }
            if {[catch {cmarker discrep $mrk} errmsg]} {
                set err 1
                if {[llength $errmsg] == 1} {
                    set id [get_id $errmsg]
                    if {[llength $id] == 2} {
                        puts \
"\nMendelian inconsistency found near individual FAMID = [lindex $id 0] ID = [lindex $id 1]"
                    } else {
                        puts \
"\nMendelian inconsistency found near individual ID = [lindex $id 0]"
                    }
                } else {
                    puts $errmsg
                }
            }
        }
        if {$err && [llength $mrklist] > 2} {
            error "Errors were encountered for one or more markers."
        }
        return
    }

    if {$arg1 == "fname"} {
        if {[llength $mrklist] != 1} {
            error "Invalid marker command"
        }
        return [cmarker fname]
    }

    if {$arg1 == "name"} {
        if {[llength $mrklist] != 2} {
            error "Invalid marker command"
        }
        return [cmarker name [lindex $mrklist 1]]
    }

    error "Invalid marker command"
}


# solar::freq --
#
# Purpose:  Process the allele frequency data.
#
# Usage:    load freq [-nosave] <filename>    ; loads freq file
#           freq unload                ; unloads allele frequencies
#           freq mle [-nose] [-hwe] [<marker> ...]
#                                      ; computes MLE allele frequencies
#           freq save <filename>       ; saves allele frequencies to a file
#           freq show [<marker> ...]   ; displays allele frequencies
#
#           The 'freq unload' command will not unload allele frequency
#           information for markers with currently loaded genotype data.
#           Frequency data for such markers is unloaded at the time the
#           genotype data is unloaded with the 'marker unload' command.
#
#           In general, it is not necessary to unload existing frequency
#           data before loading a new freq file; the unloading will be done
#           automatically.  However, the 'freq load' command will not replace
#           previously loaded frequency data if MLE allele frequencies have
#           been computed but not saved for one or more markers.  In that
#           case, the MLEs must be saved to a file, or the -nosave option
#           must be specified.
#
#           MLE allele frequencies are computed only for markers with
#           currently loaded genotype data.  To load genotype data, use
#           the 'load marker' command.  If a marker name is not specified
#           for the 'freq mle' command, MLE allele frequencies will be
#           computed for all markers with currently loaded genotype data.
#           By default, standard errors are computed for the marker allele
#           frequency estimates.  These additional calculations result in
#           increased compute time, but can be avoided with the '-nose'
#           option if standard errors are not required.
#
#           When the '-hwe' option is given, the 'freq mle' command carries
#           out an additional likelihood maximization for each marker, in
#           this case finding MLEs for marker genotype frequencies rather
#           than allele frequencies.  A test of whether the assumption of
#           Hardy-Weinberg equilibrium holds is provided by comparing the
#           likelihood of the genotype frequency-based model with that of
#           the allele frequency-based model (which assumes HWE).  When
#           this test has been conducted, the associated p-values will be
#           displayed by the 'marker show' command.
#           
#           The file created by the 'freq save' command is written in a
#           format suitable for subsequent loading with the 'freq load'
#           command.
#
#           If a marker name is not specified for the 'freq show' command,
#           all currently loaded frequency information will be displayed.
#
#           The currently loaded allele frequency information is stored in
#           the file 'freq.info' in the current working directory.  This
#           file persists between SOLAR runs, which means that the allele
#           frequencies which are in effect at the end of a session will
#           still be in effect the next time SOLAR is invoked (from within
#           the same working directory.)
#
# Notes:    The set of markers in the freq file and the set of markers in
#           the marker file do not have to be the same.  Allele frequencies
#           will be computed for markers that do not appear in the freq file
#           at the time these markers are loaded with the 'load marker'
#           command.
#
# For a description of the freq file, enter 'file-freq'
#-

proc freq {args} {
    set nosave 0
    set nose 0
    set hwe 0

    set mrklist [ read_arglist $args -nosave {set nosave 1} \
                                     -nose {set nose 1} \
                                     -hwe {set hwe 1} ]
    set arg1 [lindex $mrklist 0]

    if {$arg1 == "load"} {
        if {[llength $mrklist] != 2} {
            error "Invalid freq command"
        }
        set frqfname [lindex $mrklist 1]
        if {[catch {set frqfile [open $frqfname r]}]} {
            error "Cannot open frequency data file $frqfname"
        }
        set frqlist {}
        while {-1 != [gets $frqfile record]} {
            set mrk [lindex $record 0]
            if {[lsearch $frqlist $mrk] >= 0} {
                error \
                "Marker $mrk appears more than once in frequency data file"
                close $frqfile
            }
            lappend frqlist $mrk
        }
        close $frqfile
        if {$nosave} {
            cfreq load -nosave $frqfname
        } else {
            cfreq load $frqfname
        }
        return
    }

    if {$arg1 == "unload"} {
        if {[llength $mrklist] != 1} {
            error "Invalid freq command"
        }
        cfreq unload
        return
    }

    if {$arg1 == "save"} {
        if {[llength $mrklist] != 2} {
            error "Invalid freq command"
        }
        cfreq save [lindex $mrklist 1]
        return
    }

    if {$arg1 == "show"} {
        set sfile solar-show-text.[pid]
        if {[llength $mrklist] == 1} {
            exec cat << [cfreq show] > $sfile
        } else {
            exec cat << [cfreq show [lindex $mrklist 1]] > $sfile
            for {set i 2} {$i < [llength $mrklist]} {incr i} {
                exec echo >> $sfile
                exec cat << [cfreq show [lindex $mrklist $i]] >> $sfile
            }
        }
        exec >&@stdout <@stdin more $sfile
        delete_files_forcibly $sfile
        return
    }

    if {$arg1 == "mle"} {
        if {[llength $mrklist] == 1} {
            eval lappend mrklist [marker names]
        }
        set err 0
        for {set i 1} {$i < [llength $mrklist]} {incr i} {
            set mrk [lindex $mrklist $i]
            if {[cfreq nall $mrk] == 0} {
                puts "Cannot run allfreq for marker $mrk: no data"
                continue
            }
            if {$nose && $hwe} {
                set terr [catch {cfreq mle -nose -hwe $mrk} errmsg]
            } elseif {$nose} {
                set terr [catch {cfreq mle -nose $mrk} errmsg]
            } elseif {$hwe} {
                set terr [catch {cfreq mle -hwe $mrk} errmsg]
            } else {
                set terr [catch {cfreq mle $mrk} errmsg]
            }
            if {$terr} {
                set err 1
                if {[llength $errmsg] == 1} {
                    set id [get_id $errmsg]
                    if {[llength $id] == 2} {
                        puts \
"\nMendelian inconsistency found near individual FAMID = [lindex $id 0] ID = [lindex $id 1]"
                    } else {
                        puts \
"\nMendelian inconsistency found near individual ID = [lindex $id 0]"
                    }
                } else {
                    puts $errmsg
                }
            }
        }
        if {$err && [llength $mrklist] > 2} {
            error "Errors were encountered for one or more markers."
        }
        return
    }

    error "Invalid freq command"
}


# solar::map --
#
# Purpose:  Process the map data.
#
# Usage:    load map [-haldane | -kosambi | -basepair] <filename>
#                                           ; loads map file
#           map show [<marker> ...]         ; displays map data
#           map unload                      ; unloads map data
#           map names                       ; displays marker names
#           map fname                       ; returns name of map file
#           map chrnum                      ; returns chromosome identifier
#           map nloci                       ; returns number of loci in map
#           map func                        ; returns mapping function code
#                                             ('b'=basepair, 'h'=Haldane,
#                                              'k'=Kosambi)
#
#           In the map file, marker locations are typically given in cM.
#           When multipoint IBDs are computed, the distances between pairs
#           of markers are converted to recombination fractions by means of
#           a mapping function.  By default, the Kosambi mapping function
#           is assumed.  The Haldane mapping function can also be used by
#           specifying the -haldane option when loading the map file.
#
#           Marker locations can also be specified as integer numbers of
#           basepairs.  This is useful, for example, when the markers are
#           SNPs with known offsets in basepairs from some starting location.
#           When basepair locations are used, the mapping function is called
#           "basepair" rather than Kosambi or Haldane, but in fact there is
#           no mapping provided from basepairs to recombination fractions.
#           Therefore, such maps cannot be used to compute multipoint IBDs.
#
#           In map files which contain cM locations, the first line of the
#           file can optionally include the name of the mapping function,
#           in which case no command line option is required to specify the
#           mapping function.  If the load command specifies a different
#           mapping function from that specified in the map file, the load
#           command option takes precedence.
#
#           Map files which contain basepair locations must either have
#           the basepair mapping function specified on the first line of
#           the file or be loaded using the -basepair option.
#
#           If a marker name is not specified for the 'map show' command,
#           all currently loaded map data will be displayed.
#
#           The name of the currently loaded map file and the mapping
#           function are stored in the file 'map.info' in the current
#           working directory.  This file persists between SOLAR runs,
#           which means that the map file will still be loaded the next
#           time SOLAR is invoked (from within the same working directory.)
#
# For a description of the map file, enter 'file-map'
#-

proc map {args} {
    proc sort_by_loc {a b} {
	set a0 [lindex $a 1]
	set b0 [lindex $b 1]
	if {$b0 > $a0} {
	    return -1
	} elseif {$b0 < $a0} {
	    return 1
	}
	return 0
    }

    set basepair 0
    set haldane 0
    set kosambi 0

    set mrklist [ read_arglist $args -basepair {set basepair 1} \
                                     -haldane {set haldane 1} \
                                     -kosambi {set kosambi 1} ]
    set arg1 [lindex $mrklist 0]

    if {$arg1 == "unload"} {
        cmap unload
        if {[file exists map.info]} {
            file delete map.info
        }
        return
    }

    if {$arg1 == "load"} {
        map unload
        if {[llength $mrklist] != 2} {
            error "Invalid map command"
        }
        set mapfname [lindex $mrklist 1]
        if {[catch {set mapfile [open $mapfname r]}]} {
            error "Unable to open map data file: $mapfname"
        }

        set outfp [open map.file.tmp w]
        gets $mapfile record
        puts $outfp $record
        set loclist {}
        while {-1 != [gets $mapfile record]} {
            lappend loclist "[lindex $record 0] [lindex $record 1]"
        }
        close $mapfile
        set loclist [lsort -command sort_by_loc $loclist]
        for {set i 0} {$i < [llength $loclist]} {incr i} {
            puts $outfp [lindex $loclist $i]
        }
        close $outfp

        if {$basepair} {
            cmap load -basepair $mapfname
        } elseif {$haldane} {
            cmap load -haldane $mapfname
        } elseif {$kosambi} {
            cmap load -kosambi $mapfname
        } else {
            cmap load $mapfname
        }
        delete_files_forcibly map.file.tmp

        set infofile [open map.info w]
        puts $infofile [lindex $mrklist 1]
        if {[cmap func] == "b"} {
            puts $infofile basepair
        } elseif {[cmap func] == "h"} {
            puts $infofile Haldane
        } else {
            puts $infofile Kosambi
        }
        close $infofile
        return
    }

    if {[catch {set infofile [open map.info r]}]} {
        error "Map data have not been loaded."
    }

    gets $infofile mapfname
    gets $infofile mapfunc
    if {[string tolower $mapfunc] == "basepair"} {
        set basepair 1
    } elseif {[string tolower $mapfunc] == "haldane"} {
        set haldane 1
    } else {
        set kosambi 1
    }
    close $infofile

    if {[catch {cmap fname}]} {
        if {$basepair} {
            map load -basepair $mapfname
        } elseif {$haldane} {
            map load -haldane $mapfname
        } else {
            map load -kosambi $mapfname
        }
    } elseif {[cmap fname] != $mapfname || \
              ($basepair && [cmap func] != "b") || \
              ($haldane && [cmap func] != "h") || \
              ($kosambi && [cmap func] != "k")} {
        set infofile [open map.info w]
        puts $infofile [cmap fname]
        if {[cmap func] == "b"} {
            puts $infofile basepair
        } elseif {[cmap func] == "h"} {
            puts $infofile Haldane
        } else {
            puts $infofile Kosambi
        }
        close $infofile
    }

#   This forces map.info to be read and "loaded" if it exists.
    if {$arg1 == "test"} {
        return
    }

    if {$arg1 == "names"} {
        if {[llength $mrklist] != 1} {
            error "Invalid map command"
        }
        return [cmap names]
    }

    if {$arg1 == "show"} {
        set sfile solar-show-text.[pid]
        if {[llength $mrklist] == 1} {
            exec cat << [cmap show] > $sfile
        } else {
            exec echo > $sfile
            if {[cmap func] == "b"} {
                exec cat << "marker          locn(bp)\n" >> $sfile
            } else {
                exec cat << "marker          locn(cM)\n" >> $sfile
            }
            exec cat << "--------------  --------\n" >> $sfile
            for {set i 1} {$i < [llength $mrklist]} {incr i} {
                exec cat << [cmap show [lindex $mrklist $i]] >> $sfile
            }
        }
        exec >&@stdout <@stdin more $sfile
        delete_files_forcibly $sfile
        return
    }

    if {$arg1 == "fname"} {
        if {[llength $mrklist] != 1} {
            error "Usage: map fname"
        }
        return [cmap fname]
    }

    if {$arg1 == "chrnum"} {
        if {[llength $mrklist] != 1} {
            error "Usage: map chrnum"
        }
        return [cmap chrnum]
    }

    if {$arg1 == "nloci"} {
        if {[llength $mrklist] != 1} {
            error "Usage: map nloci"
        }
        return [cmap nloci]
    }

    if {$arg1 == "func"} {
        if {[llength $mrklist] != 1} {
            error "Usage: map func"
        }
        return [cmap func]
    }

    if {$arg1 == "locn"} {
        if {[llength $mrklist] != 2} {
            error "Usage: map locn marker-name"
        }
        return [cmap locn [lindex $mrklist 1]]
    }

    error "Invalid map command"
}


# solar::ibd --
#
# Purpose:  Compute marker-specific IBDs.
#
# Usage:    ibd [-nomle] <marker> [<marker> ...]
#                                     ; computes IBDs for specified markers
#           ibd [-nomle]              ; computes IBDs for all markers
#           ibd mito                  ; computes mitochondrial IBDs
#
#           ibd export [-file <filename>] [-overwrite] [-append]
#                      [-nod7] [-ibdid] [<marker> ...]
#                                     ; writes IBDs for specified markers
#                                     ; to a file in comma delimited format
#
#           ibd import [-file <filename>] [-nod7] [-ibdid] [<marker> ...]
#                                     ; reads IBDs for specified markers
#                                     ; from a file in comma delimited format
#
#           ibd prep <program> [-version 2.82] [<marker> ...]
#                                     ; prepares input files needed to compute
#                                     ; IBDs using <program>, where <program>
#                                     ; is loki, simwalk (sw), merlin, or
#                                     ; genehunter (gh)
#
#           Before any ibd command can be run, the directory in which to
#           store the IBDs must be specified with the 'ibddir' command.  This
#           specification is stored in a file ibddir.info in the working
#           directory, so it need not be repeated in future sessions from
#           the same working directory.
#
#           The first record in all matrix files produced by SOLAR, including
#           IBD matrix files, is a checksum and not real data; see matcrc
#           command for details.  This checksum is optional in user created
#           matrix files. If present, it prevents a using matrix with a 
#           different or changed pedigree.
#           
#           In the absence of prior knowledge of marker allele frequencies,
#           it is recommended that the 'freq mle' command be used to compute
#           maximum likelihood estimates of the allele frequencies.  This
#           will improve the accuracy with which missing genotypes are imputed
#           in the IBD computation process.  IBDs will not be computed for
#           markers with simple-count allele frequencies generated by the
#           'load marker' command, i.e. MLE allele frequencies are required
#           when prior frequency data is not available.  To compute IBDs
#           using the simple-count allele frequencies instead of MLEs,
#           specify the -nomle option.  Alternatively, the NoMLE option can
#           be set using the 'ibdoption' command.
#
#           The method used to compute marker-specific IBDs will depend on
#           the family structure and will be selected automatically.  It
#           is possible to choose the Monte Carlo method regardless of the
#           automatic selection by using the 'ibdoption' command.  For
#           performance reasons, the Monte Carlo method will be used
#           automatically for completely-typed markers.
#
#           Mitochondrial IBDs are a special case.  Each pair of individuals
#           who share a common maternal lineage, i.e. who have inherited the
#           same mitochondrial DNA, will be assigned an IBD value of 1, while
#           all other pairs are assigned an IBD value of 0.  The necessary
#           information is completely contained in the pedigree data.  Hence,
#           there is no mitochondrial marker data to load, nor are allele
#           frequencies required.
#
#           The 'ibd export' command outputs the IBDs for a specified set
#           of markers into a comma delimited file.  The IBDs must be stored
#           in the directory named in the 'ibddir' command.  If no marker
#           names are given, then all IBDs found in the 'ibddir' directory
#           are exported.  By default, the SOLAR indexed IDs (IBDIDs) in the
#           IBD files are translated to permanent IDs, and family IDs are
#           included when present in the pedigree file.  The default name for
#           the output file is "solar-ibd-export.out".  The default fields in
#           the output file are MARKER, [FAMID,] ID1, ID2, IBD, and D7.
#
#           The options for the 'ibd export' command are
#
#               -file (or -f)         Export IBDs to this filename.
#
#               -overwrite (or -ov)   Overwrite existing output file.
#
#               -append (or -a)       Append IBDs to existing output file.
#
#               -nod7                 Don't include D7 field from IBD files.
#
#               -ibdid                Write out SOLAR indexed IDs (IBDIDs)
#                                       rather than permanent IDs.
#
#           The 'ibd import' command inputs the IBDs for a specified set of
#           markers from a comma delimited file.  IBD files are written and
#           stored in the directory named in the 'ibddir' command.  If an
#           IBD file for an imported marker already exists, it is overwritten.
#           By default, the permanent IDs in the input file are translated
#           to SOLAR indexed IDs (IBDIDs).  Family IDs must be included in
#           the input file when they are present in the pedigree file.
#           The default name for the input file is "solar-ibd-import.in".
#           The default fields in the input file are MARKER, [FAMID,] ID1,
#           ID2, and IBD.  If the input file does not contain a D7 field,
#           all D7 values in the IBD files are set to zero.  By default,
#           all IBDs in the input file are imported.  However, if markers
#           are specified on the command line, then IBDs are imported for
#           those markers only.  If one and only one marker is specified on
#           the command line, a MARKER field is not required in the input
#           file.  The order of the markers in the input file is unimportant,
#           but all the lines for a given marker must be adjacent.  Unless
#           there is inbreeding in the pedigree file, checks are made to
#           ensure that imported IBDs for parent-offspring and self pairs
#           are correct (0.5 and 1, respectively).  An option is provided
#           to make the parent-offspring error checking appropriate for
#           X-linked markers.  Checks are also made to ensure that imported
#           IBDs for unrelated individuals are equal to 0.
#
#           The options for the 'ibd import' command are
#
#               -file (or -f)         Import IBDs from this filename.
#
#               -nod7                 Don't take D7 from input file; set D7
#                                       to zero instead.
#
#               -ibdid                Input file contains SOLAR indexed IDs
#                                       (IBDIDs) rather than permanent IDs.
#
#               -xlinked              Use error checking appropriate for
#                                       X-linked markers
#
# Notes:    The computed IBDs are stored in gzipped files with names of the
#           form 'ibd.<marker>.gz', where <marker> is the marker name.  All
#           working files created during the IBD computation process will be
#           stored in the marker-specific subdirectories created by the
#           'marker load' command.
#
#           Mitochondrial IBDs are stored in the gzipped file 'ibd.mito.gz'.
#           If a marker exists in the marker data that is named 'mito', the
#           same file name will be used to store the IBDs for that marker.
#           Hence, the marker name 'mito' should not be used if you intend
#           to use the 'ibd mito' command.
#-

proc ibd {args} {
    set inform 0
    set nomle 0
    set fname ""
    set append 0
    set overwrite 0
    set nod7 0
    set use_ibdid 0
    set xlinked 0

    set mrklist [ read_arglist $args \
                      -inform {set inform 1} \
                      -nomle {set nomle 1} \
                      -file fname -f fname \
                      -append {set append 1} -a {set append 1} \
                      -overwrite {set overwrite 1} -ov {set overwrite 1} \
                      -nod7 {set nod7 1} \
                      -ibdid {set use_ibdid 1} \
                      -xlinked {set xlinked 1} ]

    if {[llength $mrklist] && [lindex $mrklist 0] == "import"} {
        if {[catch {pedigree_loaded} errmsg]} {
            error $errmsg
        }
        if {[catch {ibddir -session} errmsg]} {
            error $errmsg
        }

        if {[llength $mrklist] > 1} {
            if {[string tolower [lindex $mrklist 1]] == "loki"} {
                set ibdfmt "loki"
                error "Loki IBD computation not yet supported."
            } elseif {[string tolower [lindex $mrklist 1]] == "merlin"} {
                set ibdfmt "merlin"
            } elseif {[string tolower [lindex $mrklist 1]] == "genehunter" || \
                  [string tolower [lindex $mrklist 1]] == "genehunter2" || \
                  [string tolower [lindex $mrklist 1]] == "gh" || \
                  [string tolower [lindex $mrklist 1]] == "gh2"} {
                set ibdfmt "gh"
                error "Genehunter IBD computation not yet supported."
            } elseif {[string tolower [lindex $mrklist 1]] == "simwalk" || \
                      [string tolower [lindex $mrklist 1]] == "simwalk2" || \
                      [string tolower [lindex $mrklist 1]] == "sw" || \
                      [string tolower [lindex $mrklist 1]] == "sw2"} {
                set ibdfmt "sw"
                error "SimWalk2 IBD computation not yet supported."
            } else {
                set ibdfmt "cdf"
            }
        } else {
            set ibdfmt "cdf"
        }

        if {$ibdfmt == "cdf"} {
            if {$fname == ""} {
                set fname solar-ibd-import.in
            }
            if {[catch {set impfile [tablefile open $fname]}]} {
                error "Cannot open file $fname"
            }
        } elseif {$ibdfmt == "loki"} {
            if {$fname == ""} {
                set fname loki.ibd
            }
            if {[file exists $fname]} {
                if {[file extension $fname] == ".gz"} {
                    set impfile [open "|gunzip -c $fname" r]
                } else {
                    set impfile [open $fname r]
                }
            } elseif {[file exists $fname.gz]} {
                set impfile [open "|gunzip -c $fname" r]
            } else {
                error "Cannot open file $fname"
            }
        } elseif {$ibdfmt == "merlin"} {
            if {$fname == ""} {
                set fname merlin.ibd
            }
            if {[file exists $fname]} {
                if {[file extension $fname] == ".gz"} {
                    set impfile [open \
                "|gunzip -c $fname | [usort] -k 4,4 -k 1,1n -k 2,2n -k 3,3n" r]
                } else {
                    set impfile [open \
                        "|[usort] -k 4,4 -k 1,1n -k 2,2n -k 3,3n $fname" r]
                }
            } elseif {[file exists $fname.gz]} {
                set impfile [open \
                "|gunzip -c $fname | [usort] -k 4,4 -k 1,1n -k 2,2n -k 3,3n" r]
            } else {
                error "Cannot open file $fname"
            }
            puts -nonewline "Sorting records from $fname ..."
            flush stdout
        } elseif {$ibdfmt == "gh"} {
            if {$fname == ""} {
                set fname ghmibd.ibd
            }
            if {[file exists $fname]} {
                if {[file extension $fname] == ".gz"} {
                    set impfile [open "|gunzip -c $fname | [usort] -k 1,1n" r]
                } else {
                    set impfile [open "|[usort] -k 1,1n $fname" r]
                }
            } elseif {[file exists $fname.gz]} {
                set impfile [open "|gunzip -c $fname | [usort] -k 1,1n" r]
            } else {
                error "Cannot open file $fname"
            }
            puts -nonewline "Sorting records from $fname ..."
            flush stdout
        } elseif {$ibdfmt == "sw"} {
            if {$fname == ""} {
                set fname IBD-01.001
            }
            if {[file exists $fname]} {
                if {[file extension $fname] == ".gz"} {
                    set impfile [open "|gunzip -c $fname" r]
                } else {
                    set impfile [open $fname r]
                }
            } elseif {[file exists $fname.gz]} {
                set impfile [open "|gunzip -c $fname" r]
            } else {
                error "Cannot open file $fname"
            }
        }

        set infofile [open pedigree.info r]
        gets $infofile record
        gets $infofile record
        gets $infofile record
        scan $record "%d %d %d %d" nped nfamt nindt nfout
        set is_inbred 0
        for {set i 1} {$i <= $nped} {incr i} {
            set typed($i) 0
            gets $infofile record
            scan $record "%d %d %d %d %s" \
                 nfam($i) nind($i) nfou($i) nlbrk($i) inbred($i)
            if {$inbred($i) == "y"} {
                set is_inbred 1
            }
        }
        close $infofile

        set pedfile [tablefile open pedindex.out]
        tablefile $pedfile start_setup
        tablefile $pedfile setup FIBDID
        tablefile $pedfile setup MIBDID
        tablefile $pedfile setup SEX
        tablefile $pedfile setup MZTWIN
        tablefile $pedfile setup PEDNO
        for {set i 1} {$i <= $nindt} {incr i} {
            set record [tablefile $pedfile get]
            set fibdid($i) [lindex $record 0]
            set mibdid($i) [lindex $record 1]
            set sex($i) [lindex $record 2]
            set mztwin($i) [lindex $record 3]
            if {$mztwin($i) != 0} {
                if {![info exists twinid($mztwin($i))]} {
                    set twinid($mztwin($i)) $i
                } else {
                    lappend twinid($mztwin($i)) $i
                }
            }
            set pedno($i) [lindex $record 4]
        }
        tablefile $pedfile close

        set pfile [open "|gunzip -c phi2" r]
        gets $pfile line
        if {[lindex $line 2] >= 1} {
            set ibdid1 [lindex $line 0]
            set ibdid2 [lindex $line 1]
            if {[lindex $line 2] != 0} {
                set rel($ibdid1,$ibdid2) 1
            }
        }
        while {-1 != [gets $pfile line]} {
            set ibdid1 [lindex $line 0]
            set ibdid2 [lindex $line 1]
            if {[lindex $line 2] != 0} {
                set rel($ibdid1,$ibdid2) 1
            }
        }
        close $pfile

        if {$ibdfmt == "cdf"} {
            set pedfile [tablefile open pedindex.out]
            set need_famid 0
            if {[tablefile $pedfile test_name FAMID]} {
                set need_famid 1
            }
            tablefile $pedfile close
            if {!$use_ibdid} {
                if {!$need_famid && [tablefile $impfile test_name FAMID]} {
                    tablefile $impfile close
                    error \
                    "FAMID field is present in $fname but not in pedindex.out."
                }
            }

            set nomrknam 1
            if {[tablefile $impfile test_name MARKER]} {
                set nomrknam 0
            }
            if {[llength $mrklist] != 2 && $nomrknam} {
                tablefile $impfile close
                error "A MARKER field is required in $fname."
            }

            tablefile $impfile start_setup
            if {!$nomrknam} {
                tablefile $impfile setup MARKER
            }
            if {$use_ibdid} {
                if {[catch {tablefile $impfile setup IBDID1}]} {
                    tablefile $impfile close
                    error "An IBDID1 field is required in $fname."
                }
                if {[catch {tablefile $impfile setup IBDID2}]} {
                    tablefile $impfile close
                    error "An IBDID2 field is required in $fname."
                }
            } else {
                if {$need_famid} {
                    if {[catch {tablefile $impfile setup FAMID}]} {
                        tablefile $impfile close
                        error "A FAMID field is required in $fname."
                    }
                }
                if {[catch {tablefile $impfile setup ID1}]} {
                    tablefile $impfile close
                    error "An ID1 field is required in $fname."
                }
                if {[catch {tablefile $impfile setup ID2}]} {
                    tablefile $impfile close
                    error "An ID2 field is required in $fname."
                }
            }
            if {[catch {tablefile $impfile setup IBD}]} {
                tablefile $impfile close
                error "An IBD field is required in $fname."
            }
            if {[tablefile $impfile test_name D7]} {
                tablefile $impfile setup D7
            } else {
                set nod7 1
            }

            if {$nomrknam} {
                set mrkname [lindex $mrklist 1]
                set ibdfname [format "%s/ibd.%s" [ibddir] $mrkname]
                delete_files_forcibly $ibdfname
                set ibdfile \
                    [open "|[usort] -k 1,1n -k 2,2n -u | gzip > $ibdfname.gz" w]
                puts -nonewline "Importing IBDs for $mrkname ..."
                flush stdout
            } else {
                set mrkname ""
            }

            set errfile [open solar-ibd-import.err w]
            set skip 0
            while {{} != [set record [tablefile $impfile get]]} {
                set fld 0
                if {!$nomrknam && [lindex $record 0] != $mrkname} {
                    if {$mrkname != "" && !$skip} {
                        for {set i 1} {$i <= $nindt} {incr i} {
                            if {!$typed($pedno($i))} {
                                puts $ibdfile \
                                [format "%5d %5d %10.7f %10.7f" $i $i -1 -1]
                            }
                        }
                        close $ibdfile
                        matcrc $ibdfname.gz
                        for {set i 1} {$i <= $nped} {incr i} {
                            set typed($i) 0
                        }
                        puts ""
                    }
                    set mrkname [lindex $record 0]
                    set skip 0
                    if {[llength $mrklist] > 1} {
                        set skip 1
                        for {set i 1} {$i < [llength $mrklist]} {incr i} {
                            if {[lindex $mrklist $i] == $mrkname} {
                                set skip 0
                            }
                        }
                    }
                    if {!$skip} {
                        set ibdfname [format "%s/ibd.%s" [ibddir] $mrkname]
                        delete_files_forcibly $ibdfname
                        set ibdfile [open \
                        "|[usort] -k 1,1n -k 2,2n -u | gzip > $ibdfname.gz" w]
                        puts -nonewline "Importing IBDs for $mrkname ..."
                        flush stdout
                    }
                }
                if {$skip} {
                    continue
                }

                if {!$nomrknam} {
                    incr fld
                }
                if {$use_ibdid} {
                    set ibdid1 [lindex $record $fld]
                    incr fld
                    set ibdid2 [lindex $record $fld]
                    incr fld
                } else {
                    if {$need_famid} {
                        set famid [lindex $record $fld]
                        incr fld
                        set ibdid1 [get_ibdid $famid [lindex $record $fld]]
                        incr fld
                        set ibdid2 [get_ibdid $famid [lindex $record $fld]]
                        incr fld
                    } else {
                        set ibdid1 [get_ibdid [lindex $record $fld]]
                        incr fld
                        set ibdid2 [get_ibdid [lindex $record $fld]]
                        incr fld
                    }
                }
                if {$ibdid1 < $ibdid2} {
                    set tmp $ibdid1
                    set ibdid1 $ibdid2
                    set ibdid2 $tmp
                }
                set ibd [lindex $record $fld]
                incr fld
                set d7 0
                if {!$nod7} {
                    set d7 [lindex $record $fld]
                }
                puts $ibdfile \
                [format "%5d %5d %10.7f %10.7f" $ibdid1 $ibdid2 $ibd $d7]

                if {[info exists twinid($mztwin($ibdid1))]} {
                    set ids $twinid($mztwin($ibdid1))
                    for {set i 0} {$i < [llength $ids]} {incr i} {
                        if {[lindex $ids $i] >= $ibdid2} {
                            puts $ibdfile [format "%5d %5d %10.7f %10.7f" \
                                [lindex $ids $i] $ibdid2 $ibd $d7]
                        } else {
                            puts $ibdfile [format "%5d %5d %10.7f %10.7f" \
                                $ibdid2 [lindex $ids $i] $ibd $d7]
                        }
                    }
                }
                if {[info exists twinid($mztwin($ibdid2))]} {
                    set ids $twinid($mztwin($ibdid2))
                    for {set i 0} {$i < [llength $ids]} {incr i} {
                        if {[lindex $ids $i] >= $ibdid1} {
                            puts $ibdfile [format "%5d %5d %10.7f %10.7f" \
                                [lindex $ids $i] $ibdid1 $ibd $d7]
                        } else {
                            puts $ibdfile [format "%5d %5d %10.7f %10.7f" \
                                $ibdid1 [lindex $ids $i] $ibd $d7]
                        }
                    }
                }

                set typed($pedno($ibdid1)) 1
                if {![info exists rel($ibdid1,$ibdid2)]} {
                    if {$ibd} {
                        puts $errfile \
"$mrkname: IBD for unrelateds [get_id $ibdid1] and [get_id $ibdid2] is $ibd instead of 0"
                    }
                } elseif {$inbred($pedno($ibdid1)) != "y"} {
                    if {$xlinked} {
                        if {$ibdid2 == $fibdid($ibdid1)} {
                            if {$sex($ibdid1) == 1 && $ibd != 0} {
                                puts $errfile \
"$mrkname: IBD for male [get_id $ibdid1] and father [get_id $ibdid2] is $ibd instead of 0"
                            } elseif {$sex($ibdid1) == 2 && $ibd != 0.5} {
"$mrkname: IBD for female [get_id $ibdid1] and father [get_id $ibdid2] is $ibd instead of 0.5"
                            }
                        } elseif {$ibdid2 == $mibdid($ibdid1) && $ibd != 0.5} {
                            puts $errfile \
"$mrkname: IBD for [get_id $ibdid1] and mother [get_id $ibdid2] is $ibd instead of 0.5"
                        }
                    } else {
                        if {($ibdid2 == $fibdid($ibdid1) || \
                             $ibdid2 == $mibdid($ibdid1)) && $ibd != 0.5} {
                            puts $errfile \
"$mrkname: IBD for [get_id $ibdid1] and parent [get_id $ibdid2] is $ibd instead of 0.5"
                        }
                    }
                    if {$ibdid1 == $ibdid2 && $ibd != 1 && $ibd != -1} {
                        puts $errfile \
"$mrkname: IBD for [get_id $ibdid1] with self is $ibd instead of 1"
                    }
                }
            }

            if {!$skip && [info exists ibdfile]} {
                for {set i 1} {$i <= $nindt} {incr i} {
                    if {!$typed($pedno($i))} {
                        puts $ibdfile [format "%5d %5d %10.7f %10.7f" $i $i -1 -1]
                    }
                }
                close $ibdfile
                matcrc $ibdfname.gz
                puts ""
            }

            close $errfile
            if {[file size solar-ibd-import.err] == 0} {
                file delete solar-ibd-import.err
            } else {
                error "Errors were detected. See file solar-ibd-import.err"
            }
            tablefile $impfile close

        } elseif {$ibdfmt == "merlin"} {
            set errfile [open solar-ibd-import.err w]

            set currmrk ""
            while {-1 != [gets $impfile record]} {
                if {[lindex $record 0] == "FAMILY"} {
                    continue
                }
                set mrkname [lindex $record 3]
                if {$mrkname != $currmrk} {
                    if {$currmrk != ""} {
                        for {set i 1} {$i <= $nindt} {incr i} {
                            if {!$typed($pedno($i))} {
                                puts $ibdfile \
                                [format "%5d %5d %10.7f %10.7f" $i $i -1 -1]
                            }
                        }
                        close $ibdfile
                        matcrc $ibdfname.gz
                        for {set i 1} {$i <= $nped} {incr i} {
                            set typed($i) 0
                        }
                        puts ""
                    } else {
                        puts ""
                    }
                    set ibdfname [format "%s/ibd.%s" [ibddir] $mrkname]
                    delete_files_forcibly $ibdfname
                    set ibdfile [open \
                        "|[usort] -k 1,1n -k 2,2n | gzip > $ibdfname.gz" w]
                    puts -nonewline "Importing IBDs for marker $mrkname ..."
                    flush stdout
                    set currmrk $mrkname
                }

                set ibdid1 [lindex $record 1]
                set ibdid2 [lindex $record 2]
                if {$ibdid1 < $ibdid2} {
                    set tmp $ibdid1
                    set ibdid1 $ibdid2
                    set ibdid2 $tmp
                }
                set ibd [format %.8g \
                         [expr .5*[lindex $record 5] + [lindex $record 6]]]
                if {$ibd == 0} {
                    continue
                }
                set d7 [format %.8g [lindex $record 6]]
                if {$d7 < 1e-307} {
                    set d7 0
                }
                puts $ibdfile [format "%5d %5d %10.7f %10.7f" $ibdid1 \
                               $ibdid2 $ibd $d7]

                if {$mztwin($ibdid1) && \
                        $ibdid1 == [lindex $twinid($mztwin($ibdid1)) 0]} {
                    set ids $twinid($mztwin($ibdid1))
                    for {set i 1} {$i < [llength $ids]} {incr i} {
                        if {[lindex $ids $i] >= $ibdid2} {
                            puts $ibdfile [format "%5d %5d %10.7f %10.7f" \
                                [lindex $ids $i] $ibdid2 $ibd $d7]
                        } else {
                            puts $ibdfile [format "%5d %5d %10.7f %10.7f" \
                                $ibdid2 [lindex $ids $i] $ibd $d7]
                        }
                    }
                    if {$ibdid1 == $ibdid2} {
                        for {set i 1} {$i < [llength $ids]} {incr i} {
                            set idi [lindex $ids $i]
                            for {set j 1} {$j <= $i} {incr j} {
                                set idj [lindex $ids $j]
                                if {$idj <= $idi} {
                                    puts $ibdfile [format \
                                        "%5d %5d %10.7f %10.7f" $idi $idj \
                                        $ibd $d7]
                                } else {
                                    puts $ibdfile [format \
                                        "%5d %5d %10.7f %10.7f" $idj $idi \
                                        $ibd $d7]
                                }
                            }
                        }
                    }
                }
                if {$mztwin($ibdid2) && \
                        $ibdid2 == [lindex $twinid($mztwin($ibdid2)) 0]} {
                    set ids $twinid($mztwin($ibdid2))
                    for {set i 1} {$i < [llength $ids]} {incr i} {
                        if {[lindex $ids $i] >= $ibdid1} {
                            puts $ibdfile [format "%5d %5d %10.7f %10.7f" \
                                [lindex $ids $i] $ibdid1 $ibd $d7]
                        } else {
                            puts $ibdfile [format "%5d %5d %10.7f %10.7f" \
                                $ibdid1 [lindex $ids $i] $ibd $d7]
                        }
                    }
                }

                set typed($pedno($ibdid1)) 1
                if {![info exists rel($ibdid1,$ibdid2)]} {
                    if {$ibd} {
                        puts $errfile \
"$mrkname: IBD for unrelateds [get_id $ibdid1] and [get_id $ibdid2] is $ibd instead of 0"
                    }
                } elseif {$inbred($pedno($ibdid1)) != "y"} {
                    if {($ibdid2 == $fibdid($ibdid1) || \
                         $ibdid2 == $mibdid($ibdid1)) && $ibd != 0.5} {
                        puts $errfile \
"$mrkname: IBD for [get_id $ibdid1] and parent [get_id $ibdid2] is $ibd instead of 0.5"
                    }
                    if {$ibdid1 == $ibdid2 && $ibd != 1 && $ibd != -1} {
                        puts $errfile \
"$mrkname: IBD for [get_id $ibdid1] with self is $ibd instead of 1"
                    }
                }
            }
            close $impfile

            for {set i 1} {$i <= $nindt} {incr i} {
                if {!$typed($pedno($i))} {
                    puts $ibdfile [format "%5d %5d %10.7f %10.7f" $i $i -1 -1]
                }
            }
            close $ibdfile
            matcrc $ibdfname.gz
            puts ""

            close $errfile
            if {[file size solar-ibd-import.err] == 0} {
                file delete solar-ibd-import.err
            } else {
                error "Errors were detected. See file solar-ibd-import.err"
            }

        }

        return
    }

    if {[llength $mrklist] && [lindex $mrklist 0] == "export"} {
        if {[catch {pedigree_loaded} errmsg]} {
            error $errmsg
        }
        if {[catch {ibddir} errmsg]} {
            error $errmsg
        }
        set pedfile [tablefile open pedindex.out]
        set need_famid 0
        if {[tablefile $pedfile test_name FAMID]} {
            set need_famid 1
        }
        tablefile $pedfile close
        if {$fname == ""} {
            set fname solar-ibd-export.out
        }
        if {[file exists $fname] && !$overwrite && !$append} {
            error "File $fname already exists. Use -overwrite or -append option."
        }

        if {$append} {
            set expfile [tablefile open $fname]
            if {![tablefile $expfile test_name MARKER]} {
                tablefile $expfile close
                error "A MARKER field is required in $fname."
            }
            if {$use_ibdid} {
                if {![tablefile $expfile test_name IBDID1]} {
                    tablefile $expfile close
                    error "An IBDID1 field is required in $fname."
                }
                if {![tablefile $expfile test_name IBDID2]} {
                    tablefile $expfile close
                    error "An IBDID2 field is required in $fname."
                }
            } else {
                if {![tablefile $expfile test_name ID1]} {
                    tablefile $expfile close
                    error "An ID1 field is required in $fname."
                }
                if {![tablefile $expfile test_name ID2]} {
                    tablefile $expfile close
                    error "An ID2 field is required in $fname."
                }
                if {$need_famid && ![tablefile $expfile test_name FAMID]} {
                    tablefile $expfile close
                    error "A FAMID field is required in $fname."
                }
                if {!$need_famid && [tablefile $expfile test_name FAMID]} {
                    tablefile $expfile close
                    error \
                    "A FAMID field is present in $fname but not in pedindex.out."
                }
            }
            if {![tablefile $expfile test_name IBD]} {
                tablefile $expfile close
                error "An IBD field is required in $fname."
            }
            if {!$nod7 && ![tablefile $expfile test_name D7]} {
                tablefile $expfile close
                error "A D7 field is required in $fname."
            }
            if {$nod7 && [tablefile $expfile test_name D7]} {
                tablefile $expfile close
                error "A D7 field is not to be included but is present in $fname."
            }
            tablefile $expfile close
            set expfile [open $fname a]
        } else {
            set expfile [open $fname w]
            puts -nonewline $expfile "MARKER,"
            if {$use_ibdid} {
                if {$nod7} {
                    puts $expfile "IBDID1,IBDID2,IBD"
                } else {
                    puts $expfile "IBDID1,IBDID2,IBD,D7"
                }
            } else {
                if {$need_famid} {
                    puts -nonewline $expfile "FAMID,"
                }
                if {$nod7} {
                    puts $expfile "ID1,ID2,IBD"
                } else {
                    puts $expfile "ID1,ID2,IBD,D7"
                }
            }
        }

        if {[llength $mrklist] > 1} {
            set ibdlist {}
            for {set i 1} {$i < [llength $mrklist]} {incr i} {
                lappend ibdlist "[ibddir]/ibd.[lindex $mrklist $i].gz"
            }
        } else {
            set ibdlist [glob [ibddir]/ibd.*.gz]
        }

        set filerr 0
        for {set i 0} {$i < [llength $ibdlist]} {incr i} {
            set ibdfile [lindex $ibdlist $i]
            if {[file exists $ibdfile]} {
                set mrkname [string range [file tail $ibdfile] \
                    4 [expr [string length [file tail $ibdfile]] - 4]]
                puts -nonewline "Exporting IBDs for $mrkname ..."
                flush stdout
                set ifile [open "|gunzip -c $ibdfile" r]
                while {-1 != [gets $ifile line]} {
                    if {[lindex $line 0] == 1 && [lindex $line 1] == 1 && \
                            [lindex $line 2] < 1} {
                        continue
                    }
                    if {$use_ibdid} {
                        set id1 [lindex $line 0]
                        set id2 [lindex $line 1]
                        set famid ""
                    } else {
                        set id1 [get_id [lindex $line 0]]
                        if {$id1 == ""} {
#                            close $ifile
                            close $expfile
                            error "Can't find an ID for IBDID [lindex $line 0]"
                        }
                        if {$need_famid} {
                            set famid [lindex $id1 0]
                            set id1 [lindex $id1 1]
                        } else {
                            set famid ""
                        }
                        set id2 [get_id [lindex $line 1]]
                        if {$id2 == ""} {
#                            close $ifile
                            close $expfile
                            error "Can't find an ID for IBDID [lindex $line 1]"
                        }
                        if {$need_famid} {
                            if {[lindex $id2 0] != $famid} {
#                                close $ifile
                                close $expfile
                                error \
        "IBDIDs [lindex $line 0] and [lindex $line 1] have different FAMIDs"
                            }
                            set id2 [lindex $id2 1]
                        }
                    }
                    if {$famid != ""} {
                        if {$nod7} {
                            puts $expfile \
        "$mrkname,$famid,$id1,$id2,[format %.7g [lindex $line 2]]"
                        } else {
                            puts $expfile \
        "$mrkname,$famid,$id1,$id2,[format %.7g \ [lindex $line 2]],[format \
         %.7g [lindex $line 3]]"
                        }
                    } else {
                        if {$nod7} {
                            puts $expfile \
        "$mrkname,$id1,$id2,[format %.7g [lindex $line 2]]"
                        } else {
                            puts $expfile \
        "$mrkname,$id1,$id2,[format %.7g \ [lindex $line 2]],[format %.7g \
         [lindex $line 3]]"
                        }
                    }
                }
                close $ifile
                puts ""
            } else {
                puts "Cannot open file $ibdfile"
                set filerr 1
            }
        }
        close $expfile
        if {$filerr} {
            puts \
"Check marker name spelling. This command treats marker names as case-sensitive."
        }
        return
    }

    if {[llength $mrklist] && [lindex $mrklist 0] == "prep"} {
        if {[llength $mrklist] < 2} {
            error "Usage: ibd prep <package> \[<marker> ...\]"
        }

        if {[catch {pedigree_loaded} errmsg]} {
            error $errmsg
        }

        set pkg [lindex $mrklist 1]
        if {[string tolower $pkg] == "loki"} {
            set ibdfmt "loki"
            puts "Loki IBD computation not yet supported."
#            puts "Preparing input files for Loki IBD computation ..."
        } elseif {[string tolower $pkg] == "merlin"} {
            set ibdfmt "merlin"
            puts "Preparing input files for Merlin IBD computation ..."
        } elseif {[string tolower $pkg] == "simwalk" || \
                  [string tolower $pkg] == "simwalk2" || \
                  [string tolower $pkg] == "sw" || \
                  [string tolower $pkg] == "sw2"} {
            set ibdfmt "sw"
            puts "SimWalk2 IBD computation not yet supported."
#            puts "Preparing input files for SimWalk2 IBD computation ..."
        } elseif {[string tolower $pkg] == "genehunter" || \
                  [string tolower $pkg] == "genehunter2" || \
                  [string tolower $pkg] == "gh" || \
                  [string tolower $pkg] == "gh2"} {
            set ibdfmt "gh"
            puts "GeneHunter IBD computation not yet supported."
#            puts "Preparing input files for GeneHunter IBD computation ..."
        } else {
            error "IBD computation using $pkg is not supported."
        }

        set infofile [open pedigree.info r]
        gets $infofile record
        gets $infofile record
        gets $infofile record
        scan $record "%d %d %d %d" nped nfamt nindt nfout
        set is_inbred 0
        set max_nbits 0
        for {set i 1} {$i <= $nped} {incr i} {
            set typed($i) 0
            gets $infofile record
            scan $record "%d %d %d %d %s" \
                 nfam($i) nind($i) nfou($i) nlbrk($i) inbred($i)
            if {$inbred($i) == "y"} {
                set is_inbred 1
            }
            set nbits($i) [expr 2*$nind($i) - 3*$nfou($i)]
            if {$nbits($i) > $max_nbits} {
                set max_nbits $nbits($i)
            }
        }
        close $infofile

        if {$is_inbred} {
            if {$ibdfmt == "merlin"} {
#                error \
#"Computing IBDs for inbred pedigrees using Merlin is not supported by SOLAR."
            } elseif {$ibdfmt == "gh"} {
                error \
"Computing IBDs for inbred pedigrees using GeneHunter is not supported by SOLAR."
            } elseif {$ibdfmt == "sw" && $version == "2.82"} {
                error \
"Computing IBDs for inbred pedigrees using SimWalk2 v2.82 is not supported by SOLAR."
            }
        }

        set pedfile [tablefile open pedindex.out]
        tablefile $pedfile start_setup
        tablefile $pedfile setup FIBDID
        tablefile $pedfile setup MIBDID
        tablefile $pedfile setup SEX
        tablefile $pedfile setup MZTWIN
        tablefile $pedfile setup PEDNO
        for {set i 1} {$i <= $nindt} {incr i} {
            set record [tablefile $pedfile get]
            set fibdid($i) [lindex $record 0]
            set mibdid($i) [lindex $record 1]
            set sex($i) [lindex $record 2]
            set mztwin($i) [lindex $record 3]
            if {$mztwin($i) != 0} {
                if {![info exists twinid($mztwin($i))]} {
                    set twinid($mztwin($i)) $i
                } else {
                    lappend twinid($mztwin($i)) $i
                }
            }
            set pedno($i) [lindex $record 4]
            set gtype($i) {}
        }
        tablefile $pedfile close

        set frqfile [open freq.info r]
        gets $frqfile record
        while {-1 != [gets $frqfile record]} {
            set mrk [lindex $record 0]
            set alllist($mrk) {}
            set frqlist($mrk) {}
            for {set i 4} {$i < [llength $record]} {incr i} {
                set allname [lindex $record $i]
                if {$allname == "se=" || $allname == "hwe="} {
                    incr i
                    continue
                }
                lappend alllist($mrk) $allname
                incr i
                lappend frqlist($mrk) [lindex $record $i]
            }
        }
        close $frqfile

        if {[llength $mrklist] == 2} {
            eval lappend mrklist [marker names]
        }
        for {set i 2} {$i < [llength $mrklist]} {incr i} {
            set mrk [cmarker name [lindex $mrklist $i]]
            set genfname d_$mrk/translat.tab
            if {[catch {set genfile [open $genfname r]}]} {
                error "Cannot open file $genfname"
            }
            gets $genfile genrec
            gets $genfile genrec
            set ibdid 0
            while {-1 != [gets $genfile genrec]} {
                set n [lindex $genrec 0]
                for {set j 0} {$j < $n} {incr j} {
                    incr ibdid
                    gets $genfile genrec
                    set a1 [string range $genrec 19 21]
                    set a2 [string range $genrec 22 24]
                    if {$a1 != "   "} {
                        if {$ibdfmt == "merlin"} {
                            eval lappend gtype($ibdid) \
                                [format "%d %d" $a1 $a2]
                        } elseif {$ibdfmt == "gh"} {
                            eval lappend gtype($ibdid,$mrk) \
                                [format "%d %d" $a1 $a2]
                        } else {
                            lappend gtype($ibdid) [format %d/%d $a1 $a2]
                        }
                    } else {
                        if {$ibdfmt == "merlin"} {
                            eval lappend gtype($ibdid) "0 0"
                        } elseif {$ibdfmt == "gh"} {
                            eval lappend gtype($ibdid,$mrk) "0 0"
                        } else {
                            lappend gtype($ibdid) "*"
                        }
                    }
                }
            }
            close $genfile
        }

        if {$ibdfmt == "merlin"} {
            set outf [open mlibd.frq w]
            set outf2 [open mlibd.dat w]
            set outf3 [open mlibd.map w]
            for {set i 2} {$i < [llength $mrklist]} {incr i} {
                set mrk [cmarker name [lindex $mrklist $i]]
                puts $outf "M $mrk"
                puts $outf2 "M $mrk"
                for {set j 0} {$j < [llength $alllist($mrk)]} {incr j} {
                    puts $outf "A [expr $j + 1] [lindex $frqlist($mrk) $j]"
                }
                puts $outf3 "0 $mrk 0"
            }
            close $outf3
            close $outf2
            close $outf

            set outf [open mlibd.ped w]
            for {set i 1} {$i <= $nindt} {incr i} {
                if {$mztwin($i) && $i != [lindex $twinid($mztwin($i)) 0]} {
                    continue
                }
                set fa $fibdid($i)
                if {$fa != 0 && $mztwin($fa)} {
                    set fa [lindex $twinid($mztwin($fa)) 0]
                }
                set mo $mibdid($i)
                if {$mo != 0 && $mztwin($mo)} {
                    set mo [lindex $twinid($mztwin($mo)) 0]
                }
                puts $outf "$pedno($i) $i $fa $mo $sex($i) $gtype($i)"
            }
            close $outf

            set outf [open mlibd.cmd w]
            puts $outf \
"merlin -dmlibd.dat -pmlibd.ped -fmlibd.frq -mmlibd.map --ibd --singlepoint \
--bits $max_nbits"
            close $outf

            puts "The following files have been created:"
            puts "    mlibd.cmd 		- Merlin IBD command"
            puts "    mlibd.dat 		- data description file"
            puts "    mlibd.ped 		- pedigree/genotype data"
            puts "    mlibd.frq 		- allele frequency file"
            puts "    mlibd.map 		- map file"
        }

        return
    }

    if {$inform} {
        if {[llength $args] != 3} {
            error "Invalid ibd command"
        } else {
            eval cibd -inform $mrklist
            return
        }
    }

    if {$mrklist == ""} {
        eval cibd $args
    } else {
        foreach mrk $mrklist {
            if {$nomle && $mrk != "mito"} {
                cibd -nomle $mrk
            } else {
                cibd $mrk
            }
        }
    }
}


# solar::ibs --
#
# Purpose:  Compute marker-specific IBS matrices.
#
# Usage:    ibs <marker> [<marker> ...]
#                                     ; computes IBSs for specified markers
#           ibs                       ; computes IBSs for all markers
#
#           Before the ibs command can be run, the directory in which to
#           store the IBSs must be specified with the 'ibddir' command.
#
# Notes:    The computed IBSs are stored in gzipped files with names of the
#           form 'ibs.<marker>.gz', where <marker> is the marker name.
#-

proc ibs {args} {
    if {$args == ""} {
        cibs
    } else {
        foreach mrk $args {
            cibs $mrk
        }
    }
}


# solar::ibdoption --
#
# Purpose:  Set or display IBD processing options.
#
# Options:  XLinked   select this option to load X-linked marker data
#           NoMLE     if this option is chosen, MLE allele frequencies are not
#                     required for IBD calculation
#           MCarlo    if this option is chosen, the Monte Carlo method will be
#                     used to calculate IBDs
#           MibdWin   size (in cM) of the multipoint IBD window - the MIBDs at
#                     a given chromosome location depend only on markers inside
#                     or on the boundary of the window centered at that location
#
# Usage:    ibdoption                   ; displays current IBD options
#
#           ibdoption xlinked           ; toggles the XLinked option
#           ibdoption xlinked <y/n>     ; sets the XLinked option
#           ibdoption xlinked ?         ; displays the current setting of XLinked
#
#           ibdoption nomle             ; toggles the NoMLE option
#           ibdoption nomle <y/n>       ; sets the NoMLE option
#           ibdoption nomle ?           ; displays the current setting of NoMLE
#
#           ibdoption mcarlo            ; toggles the MCarlo option
#           ibdoption mcarlo <y/n>      ; sets the MCarlo option
#           ibdoption mcarlo ?          ; displays the current setting of MCarlo
#           ibdoption mcarlo # <num>    ; sets number of imputations
#           ibdoption mcarlo #          ; displays number of imputations
#           ibdoption mcarlo max <y/n>  ; choose max risk for first imputation?
#           ibdoption mcarlo max ?      ; displays max risk option
#
#           ibdoption mibdwin           ; displays the multipoint IBD window size
#           ibdoption mibdwin <size>    ; sets the multipoint IBD window size
#-


# solar::mibd --
#
# Purpose:  Compute multipoint IBDs.
#
# Usage:    mibd relate [-mxnrel <n>]   ; creates relative-class file
#           mibd merge                  ; merges marker IBDs
#           mibd means [-typed | -all]  ; computes mean IBD by relative-class
#           mibd [<from> <to>] <incr>   ; computes multipoint IBDs
#
#           mibd export [-file <filename>] [-overwrite] [-append]
#                       [-nod7] [-ibdid] [-byloc] [<chromo> ...]
#                                     ; writes MIBDs for specified chromosomes
#                                     ; to a file in comma delimited format
#
#           mibd import [-file <filename>] [-nod7] [-ibdid] [<chromo> ...]
#                                     ; reads MIBDs for specified chromosomes
#                                     ; from a file in comma delimited format
#
#           mibd prep <program> [-version 2.82] [-usefreq] [-qter]
#                                     ; prepares input files needed to compute
#                                     ; MIBDs using <program>, where <program>
#                                     ; is loki, simwalk (sw), merlin, or
#                                     ; genehunter (gh)
#
#           mibd import <program> [-file <filename>] [-version 2.82]
#                                     ; imports MIBDs from an output file
#                                     ; computed by <program>, where <program>
#                                     ; is loki, simwalk (sw), merlin, or
#                                     ; genehunter (gh)
#
#           Before any mibd command can be run, the directory in which to
#           store the mIBDs must be specified with the 'mibddir' command.  This
#           specification is stored in a file mibddir.info in the working
#           directory, so it need not be repeated in future sessions from
#           the same working directory.
#
#           The first record in all matrix files produced by SOLAR, including
#           mIBD matrix files, is a checksum and not real data; see the matcrc
#           command for details.  This checksum is optional in user created
#           matrix files. If present, it prevents a using matrix with a 
#           different or changed pedigree.
#           
#           The 'mibd relate' command can be run after the pedigree
#           file has been loaded, and only needs to be run once per data set.
#           A tally of the relative classes present in the data set can then
#           be displayed with the 'pedigree classes' command.
#
#           A pair of individuals may be related in multiple ways, e.g. as
#           1st cousins and as 2nd cousins.  To conserve memory, there is a
#           default limit on the number of ways any two individuals may be
#           related.  For some complex pedigrees, it may be necessary to
#           specify a higher limit using the '-mxnrel' option.
#
#           The remaining commands in the first group of mibd commands shown
#           above must be run once for each chromosome.  The 'mibd merge'
#           command must be run first, followed by the 'mibd means' command.
#
#           The 'mibd means' can take one of two options: -typed or -all.
#           If the -typed option is specified, only the IBDs for pairs of
#           individuals who are both genotyped will be used to compute mean
#           IBDs by relative class.  If the -all option is specified, the
#           IBDs for all pairs of individuals are used.  The default option
#           is -all.
#
#           The following steps are required before computing multipoint IBDs
#           for chromosome N:
#
#             1. Compute the marker-specific IBDs for all markers on
#                chromosome N.  For more information on computing marker-
#                specific IBDs, enter 'help ibd'.
#             2. Load the map file for chromosome N.
#             3. Use the 'ibddir' command to specify the directory where
#                the marker-specific IBDs are stored.
#             4. Use the 'mibddir' command to specify the directory where
#                the multipoint IBDs are to be written.
#
#           Only the last of the first four mibd commands shown above need
#           be entered (for a particular chromosome.)  If the merged IBD
#           file does not exist, 'mibd merge' will automatically be run to
#           create it.  If the mean IBD file does not exist or is older than
#           the merged IBD file, 'mibd means' will be run.  Note that when
#           'mibd means' is run automatically, the default option, -all,
#           will be used.  The 'mibd means' command must be issued directly
#           in order to use the -typed option.  If any of the marker IBD
#           files is newer than the merged IBD file, a warning message will
#           be displayed.  In order to update the merged IBD file, the
#           'mibd merge' command must be issued directly - this will not be
#           done automatically.
#
#           The 'mibd export' command outputs the multipoint IBDs for a
#           specified set of chromosomes into a comma delimited file.
#           The MIBDs must be stored in the directory named in the 'mibddir'
#           command.  If no chromosomes are specified, then all multipoint
#           IBDs found in the 'mibddir' directory are exported.  By default,
#           the SOLAR indexed IDs (IBDIDs) in the MIBD files are translated
#           to permanent IDs, and family IDs are included when present in
#           the pedigree file.  The default name for the output file is
#           "solar-mibd-export.out".  The default fields in the output file
#           are CHROMO, LOCATION, [FAMID,] ID1, ID2, IBD, and D7, where
#           LOCATION is the chromosomal location in cM.
#
#           WARNING: The file to which MIBDs are exported can become very
#           large. To keep export files to a manageable size, it may be best
#           to export MIBDs on a per-chromosome basis, i.e. one export file
#           per chromosome, or on a per-location basis by using the -byloc
#           option.
#
#           The options for the 'mibd export' command are
#
#               -file (or -f)         Export MIBDs to this filename.
#
#               -overwrite (or -ov)   Overwrite existing output file.
#
#               -append (or -a)       Append MIBDs to existing output file.
#
#               -nod7                 Don't include D7 field from MIBD files.
#
#               -ibdid                Write out SOLAR indexed IDs (IBDIDs)
#                                       rather than permanent IDs.
#
#               -byloc                Export MIBDs on a per-location basis,
#                                     i.e. one export file per location. The
#                                     export files are given unique names by
#                                     appending the chromosome number and the
#                                     location to the filename given by the
#                                     -file option.
#
#           The 'mibd import' command inputs the multipoint IBDs for a
#           specified set of chromosomes from a comma delimited file.
#           MIBD files are written and stored in the directory named in the
#           'mibddir' command.  If an MIBD file for an imported chromosomal
#           location already exists, it is overwritten.  By default, the
#           permanent IDs in the input file are translated to SOLAR indexed
#           IDs (IBDIDs).  Family IDs must be included in the input file
#           when they are present in the pedigree file.  The default name
#           for the input file is "solar-mibd-import.in".  The default
#           fields in the input file are CHROMO, LOCATION, [FAMID,] ID1,
#           ID2, and IBD.  If the input file does not contain a D7 field,
#           all D7 values in the MIBD files are set to zero.  By default,
#           all MIBDs in the input file are imported.  If chromosomes are
#           specified on the command line, however, MIBDs are imported for
#           those chromosomes only.
#
#           NOTE: The order of the chromosomes and chromosomal locations
#           in the input file is unimportant, but all the lines for a given
#           chromosomal location MUST BE ADJACENT.  To be safe, you may want
#           to sort the input file by chromosome and chromosomal location
#           to ensure that the input file is ordered correctly.
#
#           The options for the 'mibd import' command are
#
#               -file (or -f)         Import MIBDs from this filename.
#
#               -nod7                 Don't take D7 from input file; set D7
#                                       to zero instead.
#
#               -ibdid                Input file contains SOLAR indexed IDs
#                                       (IBDIDs) rather than permanent IDs.
#
#
#           The 'mibd prep' command generates the input files needed to
#           compute multipoint IBDs using a program other than SOLAR.
#           The programs currently supported are Loki, SimWalk2, Merlin and
#           GeneHunter.  Before this command can be run, marker data and a
#           map file must have been loaded.  The input files are generated
#           from various files created by SOLAR when pedigree and marker
#           data are loaded and so contain SOLAR indexed IDs (IBDIDs).
#
#           The marker locations written to an input file will be in Haldane
#           cM.  If the user has loaded a Kosambi map file, the necessary
#           conversion to Haldane is made automatically.  By default, IBDs
#           will be calculated at every integer cM location from 0 to the
#           last marker in the map file.  The '-qter' option extends the range
#           of locations to the end of the chromosome.  For each chromosome,
#           SOLAR defines qter as the nearest integer location greater than
#           or equal to the position (in Haldane cM) of the last marker on
#           that chromosome in the deCODE map.
#
#           The allele frequencies in effect, whether read from a file or
#           computed by SOLAR, are passed to the multipoint IBD calculation
#           program, except in the case of Loki, for which the default action
#           is to let Loki estimate the allele frequencies.  The '-usefreq'
#           option can be used to force Loki to use the current allele
#           frequencies.
#
#           NOTE: After the input files have been created, the user must exit
#           SOLAR and run the external program to compute the multipoint IBDs.
#           Once the IBD calculations are complete, the resulting output file
#           can be imported into SOLAR using the 'mibd import <program>'
#           command.
#
#           The 'mibd import <program>' command reads an output file which
#           contains the multipoint IBDs computed by a program other than
#           SOLAR, and imports those IBDs to create SOLAR-format MIBD files.
#           The programs currently supported are Loki, SimWalk2, Merlin
#           and GeneHunter.  This command is designed to work with the
#           'mibd prep' command, so the output file is assumed to contain
#           SOLAR indexed IDs (IBDIDs), not the real IDs from the pedigree
#           data file.  Before this command can be run, the 'mibddir'
#           command must have been given to specify the directory in which
#           the MIBD files are to be stored.
#
#           NOTE: For both the 'mibd prep' and 'mibd import' commands, if
#           SimWalk2 is the program chosen, then it is assumed that version
#           2.91 or a newer version of SimWalk2 will be used to compute the
#           multipoint IBDs. In previous versions of SOLAR, SimWalk2 version
#           2.82 was assumed. Due to a backward incompatibility in file
#           formats that was introduced in later SimWalk2 versions, if you
#           wish to use the earlier version of SimWalk2, it is now necessary
#           to include the '-version 2.82' option.
#
# Notes:    The computed multipoint IBDs are stored in gzipped files with
#           names of the form 'mibd.<chromo>.<loc>.gz', where <chrom> is the
#           chromosome number and <loc> is the chromosomal location.
#
#           Several additional files are created and used for multipoint
#           IBD calculation:
#
#              mibdrel.ped       relative-class and kinship information
#              mibdchrN.loc      marker locations on chromosome N
#              mibdchrN.mrg.gz   merged marker-specific IBDs for chromosome N
#              mibdchrN.mean     mean IBD by relative class for chromosome N
#-

proc mibd {args} {

    global env

    set mxnrel ""
    set append 0
    set overwrite 0
    set fname ""
    set nod7 0
    set use_ibdid 0
    set byloc 0
    set byped 0
    set usefreq 0
    set qter 0
    set version 0

    set chrlist [ read_arglist $args \
                      -file fname -f fname \
                      -mxnrel mxnrel \
                      -append {set append 1} -a {set append 1} \
                      -overwrite {set overwrite 1} -ov {set overwrite 1} \
                      -nod7 {set nod7 1} \
                      -ibdid {set use_ibdid 1} \
                      -byloc {set byloc 1} \
                      -byped {set byped 1} \
                      -usefreq {set usefreq 1} \
                      -qter {set qter 1} \
                      -version version -v version ]

    if {[llength $chrlist] && [lindex $chrlist 0] == "import"} {
        if {[catch {pedigree_loaded} errmsg]} {
            error $errmsg
        }
        if {[catch {mibddir -session} errmsg]} {
            error $errmsg
        }

        if {[llength $chrlist] > 1} {
            if {[string tolower [lindex $chrlist 1]] == "loki"} {
                set ibdfmt "loki"
            } elseif {[string tolower [lindex $chrlist 1]] == "merlin"} {
                set ibdfmt "merlin"
            } elseif {[string tolower [lindex $chrlist 1]] == "genehunter" || \
                  [string tolower [lindex $chrlist 1]] == "genehunter2" || \
                  [string tolower [lindex $chrlist 1]] == "gh" || \
                  [string tolower [lindex $chrlist 1]] == "gh2"} {
                set ibdfmt "gh"
            } elseif {[string tolower [lindex $chrlist 1]] == "simwalk" || \
                      [string tolower [lindex $chrlist 1]] == "simwalk2" || \
                      [string tolower [lindex $chrlist 1]] == "sw" || \
                      [string tolower [lindex $chrlist 1]] == "sw2"} {
                set ibdfmt "sw"
            } else {
                set ibdfmt "cdf"
            }
        } else {
            set ibdfmt "cdf"
        }

        if {$ibdfmt == "merlin" || $ibdfmt == "gh" || $ibdfmt == "sw"} {
            if {[llength $chrlist] != 3} {
                error "A single chromosome must be specified."
            }
            set chr [lindex $chrlist 2]
        }

        if {$ibdfmt == "cdf"} {
            if {$fname == ""} {
                set fname solar-mibd-import.in
            }
            if {[catch {set impfile [tablefile open $fname]}]} {
                error "Cannot open file $fname"
            }
        } elseif {$ibdfmt == "loki"} {
            if {$fname == ""} {
                set fname loki.ibd
            }
            if {[file exists $fname]} {
                if {[file extension $fname] == ".gz"} {
                    set impfile [open "|gunzip -c $fname" r]
                } else {
                    set impfile [open $fname r]
                }
            } elseif {[file exists $fname.gz]} {
                set impfile [open "|gunzip -c $fname" r]
            } else {
                error "Cannot open file $fname"
            }
        } elseif {$ibdfmt == "merlin"} {
            if {$fname == ""} {
                set fname merlin.ibd
            }
            if {[file exists $fname]} {
                if {[file extension $fname] == ".gz"} {
                    set impfile [open \
                "|gunzip -c $fname | [usort] -k 4,4 -k 1,1n -k 2,2n -k 3,3n" r]
                } else {
                    set impfile [open \
                        "|[usort] -k 4,4 -k 1,1n -k 2,2n -k 3,3n $fname" r]
                }
            } elseif {[file exists $fname.gz]} {
                set impfile [open \
                "|gunzip -c $fname | [usort] -k 4,4 -k 1,1n -k 2,2n -k 3,3n" r]
            } else {
                error "Cannot open file $fname"
            }
            puts -nonewline "Sorting records from $fname ..."
            flush stdout
        } elseif {$ibdfmt == "gh"} {
            if {$fname == ""} {
                set fname ghmibd.ibd
            }
            if {[file exists $fname]} {
                if {[file extension $fname] == ".gz"} {
                    set impfile [open "|gunzip -c $fname | [usort] -k 1,1n" r]
                } else {
                    set impfile [open "|[usort] -k 1,1n $fname" r]
                }
            } elseif {[file exists $fname.gz]} {
                set impfile [open "|gunzip -c $fname | [usort] -k 1,1n" r]
            } else {
                error "Cannot open file $fname"
            }
            puts -nonewline "Sorting records from $fname ..."
            flush stdout
        } elseif {$ibdfmt == "sw"} {
            if {$fname == ""} {
                set fname IBD-01.001
            }
            if {[file exists $fname]} {
                if {[file extension $fname] == ".gz"} {
                    set impfile [open "|gunzip -c $fname" r]
                } else {
                    set impfile [open $fname r]
                }
            } elseif {[file exists $fname.gz]} {
                set impfile [open "|gunzip -c $fname" r]
            } else {
                error "Cannot open file $fname"
            }
        }

        set infofile [open pedigree.info r]
        gets $infofile record
        gets $infofile record
        gets $infofile record
        scan $record "%d %d %d %d" nped nfamt nindt nfout
        set is_inbred 0
        for {set i 1} {$i <= $nped} {incr i} {
            set typed($i) 0
            gets $infofile record
            scan $record "%d %d %d %d %s" \
                 nfam($i) nind($i) nfou($i) nlbrk($i) inbred($i)
            if {$inbred($i) == "y"} {
                set is_inbred 1
            }
        }
        close $infofile

        set pedfile [tablefile open pedindex.out]
        tablefile $pedfile start_setup
        tablefile $pedfile setup FIBDID
        tablefile $pedfile setup MIBDID
        tablefile $pedfile setup SEX
        tablefile $pedfile setup MZTWIN
        tablefile $pedfile setup PEDNO
        for {set i 1} {$i <= $nindt} {incr i} {
            set record [tablefile $pedfile get]
            set fibdid($i) [lindex $record 0]

            set mibdid($i) [lindex $record 1]
            set sex($i) [lindex $record 2]
            set mztwin($i) [lindex $record 3]
            if {$mztwin($i) != 0} {
                if {![info exists twinid($mztwin($i))]} {
                    set twinid($mztwin($i)) $i
                } else {
                    lappend twinid($mztwin($i)) $i
                }
            }
            set pedno($i) [lindex $record 4]
        }
        tablefile $pedfile close

        set pfile [open "|gunzip -c phi2" r]
        gets $pfile line
        if {[lindex $line 2] >= 1} {
            set ibdid1 [lindex $line 0]
            set ibdid2 [lindex $line 1]
            if {[lindex $line 2] != 0} {
                set rel($ibdid1,$ibdid2) 1
            }
        }
        while {-1 != [gets $pfile line]} {
            set ibdid1 [lindex $line 0]
            set ibdid2 [lindex $line 1]
            if {[lindex $line 2] != 0} {
                set rel($ibdid1,$ibdid2) 1
            }
        }
        close $pfile

        if {$ibdfmt == "cdf"} {
            set pedfile [tablefile open pedindex.out]
            set need_famid 0
            if {[tablefile $pedfile test_name FAMID]} {
                set need_famid 1
            }
            tablefile $pedfile close
            if {!$use_ibdid} {
                if {!$need_famid && [tablefile $impfile test_name FAMID]} {
                    tablefile $impfile close
                    error \
                    "FAMID field is present in $fname but not in pedindex.out."
                }
            }

            tablefile $impfile start_setup
            if {[catch {tablefile $impfile setup CHROMO}]} {
                tablefile $impfile close
                error "An CHROMO field is required in $fname."
            }
            if {[catch {tablefile $impfile setup LOCATION}]} {
                tablefile $impfile close
                error "An LOCATION field is required in $fname."
            }
            if {$use_ibdid} {
                if {[catch {tablefile $impfile setup IBDID1}]} {
                    tablefile $impfile close
                    error "An IBDID1 field is required in $fname."
                }
                if {[catch {tablefile $impfile setup IBDID2}]} {
                    tablefile $impfile close
                    error "An IBDID2 field is required in $fname."
                }
            } else {
                if {$need_famid} {
                    if {[catch {tablefile $impfile setup FAMID}]} {
                        tablefile $impfile close
                        error "A FAMID field is required in $fname."
                    }
                }
                if {[catch {tablefile $impfile setup ID1}]} {
                    tablefile $impfile close
                    error "An ID1 field is required in $fname."
                }
                if {[catch {tablefile $impfile setup ID2}]} {
                    tablefile $impfile close
                    error "An ID2 field is required in $fname."
                }
            }
            if {[catch {tablefile $impfile setup IBD}]} {
                tablefile $impfile close
                error "An IBD field is required in $fname."
            }
            if {[tablefile $impfile test_name D7]} {
                tablefile $impfile setup D7
            } else {
                set nod7 1
            }

            set errfile [open solar-mibd-import.err w]
            set chr ""
            set locn ""
            set skip 0
            while {{} != [set record [tablefile $impfile get]]} {
                if {[lindex $record 0] != $chr || [lindex $record 1] != $locn} {
                    if {$chr != "" && !$skip} {
                        for {set i 1} {$i <= $nindt} {incr i} {
                            if {!$typed($pedno($i))} {
                                puts $ibdfile \
                                [format "%5d %5d %10.7f %10.7f" $i $i -1 -1]
                            }
                        }
                        close $ibdfile
                        matcrc $ibdfname.gz
                        for {set i 1} {$i <= $nped} {incr i} {
                            set typed($i) 0
                        }
                        puts ""
                    }
                    set chr [lindex $record 0]
                    set locn [lindex $record 1]
                    set skip 0
                    if {[llength $chrlist] > 1} {
                        set skip 1
                        for {set i 1} {$i < [llength $chrlist]} {incr i} {
                            if {[lindex $chrlist $i] == $chr} {
                                set skip 0
                            }
                        }
                    }
                    if {!$skip} {
                        set ibdfname \
                            [format "%s/mibd.%s.%s" [mibddir] $chr $locn]
                        delete_files_forcibly $ibdfname
                        set ibdfile [open \
                        "|[usort] -k 1,1n -k 2,2n -u | gzip > $ibdfname.gz" w]
                        puts -nonewline \
                        "Importing IBDs for chromosome $chr location $locn ..."
                        flush stdout
                    }
                }
                if {$skip} {
                    continue
                }

                set fld 2
                if {$use_ibdid} {
                    set ibdid1 [lindex $record $fld]
                    incr fld
                    set ibdid2 [lindex $record $fld]
                    incr fld
                } else {
                    if {$need_famid} {
                        set famid [lindex $record $fld]
                        incr fld
                        set ibdid1 [get_ibdid $famid [lindex $record $fld]]
                        incr fld
                        set ibdid2 [get_ibdid $famid [lindex $record $fld]]
                        incr fld
                    } else {
                        set ibdid1 [get_ibdid [lindex $record $fld]]
                        incr fld
                        set ibdid2 [get_ibdid [lindex $record $fld]]
                        incr fld
                    }
                }
                if {$ibdid1 < $ibdid2} {
                    set tmp $ibdid1
                    set ibdid1 $ibdid2
                    set ibdid2 $tmp
                }
                set ibd [lindex $record $fld]
                incr fld
                set d7 0
                if {!$nod7} {
                    set d7 [lindex $record $fld]
                }
                puts $ibdfile \
                [format "%5d %5d %10.7f %10.7f" $ibdid1 $ibdid2 $ibd $d7]

                if {$mztwin($ibdid1) && \
                        $ibdid1 == [lindex $twinid($mztwin($ibdid1)) 0]} {
                    set ids $twinid($mztwin($ibdid1))
                    for {set i 1} {$i < [llength $ids]} {incr i} {
                        if {[lindex $ids $i] >= $ibdid2} {
                            puts $ibdfile [format "%5d %5d %10.7f %10.7f" \
                                [lindex $ids $i] $ibdid2 $ibd $d7]
                        } else {
                            puts $ibdfile [format "%5d %5d %10.7f %10.7f" \
                                $ibdid2 [lindex $ids $i] $ibd $d7]
                        }
                    }
                    if {$ibdid1 == $ibdid2} {
                        for {set i 2} {$i < [llength $ids]} {incr i} {
                            set idi [lindex $ids $i]
                            for {set j 1} {$j <= $i} {incr j} {
                                set idj [lindex $ids $j]
                                if {$idj <= $idi} {
                                    puts $ibdfile [format \
                                        "%5d %5d %10.7f %10.7f" $idi $idj \
                                        $ibd $d7]
                                } else {
                                    puts $ibdfile [format \
                                        "%5d %5d %10.7f %10.7f" $idj $idi \
                                        $ibd $d7]
                                }
                            }
                        }
                    }
                }
                if {$mztwin($ibdid2) && \
                        $ibdid2 == [lindex $twinid($mztwin($ibdid2)) 0]} {
                    set ids $twinid($mztwin($ibdid2))
                    for {set i 1} {$i < [llength $ids]} {incr i} {
                        if {[lindex $ids $i] >= $ibdid1} {
                            puts $ibdfile [format "%5d %5d %10.7f %10.7f" \
                                [lindex $ids $i] $ibdid1 $ibd $d7]
                        } else {
                            puts $ibdfile [format "%5d %5d %10.7f %10.7f" \
                                $ibdid1 [lindex $ids $i] $ibd $d7]
                        }
                    }
                }

                set typed($pedno($ibdid1)) 1
                if {![info exists rel($ibdid1,$ibdid2)]} {
                    if {$ibd} {
                        puts $errfile \
"$chr/$locn: IBD for unrelateds [get_id $ibdid1] and [get_id $ibdid2] is $ibd instead of 0"
                    }
                } elseif {$inbred($pedno($ibdid1)) != "y"} {
                    if {($ibdid2 == $fibdid($ibdid1) || \
                         $ibdid2 == $mibdid($ibdid1)) && $ibd != 0.5} {
                        puts $errfile \
"$chr/$locn: IBD for [get_id $ibdid1] and parent [get_id $ibdid2] is $ibd instead of 0.5"
                    }
                    if {$ibdid1 == $ibdid2 && $ibd != 1 && $ibd != -1} {
                        puts $errfile \
"$chr/$locn: IBD for [get_id $ibdid1] with self is $ibd instead of 1"
                    }
                }
            }
            tablefile $impfile close

            if {!$skip && [info exists ibdfile]} {
                for {set i 1} {$i <= $nindt} {incr i} {
                    if {!$typed($pedno($i))} {
                        puts $ibdfile [format "%5d %5d %10.7f %10.7f" $i $i -1 -1]
                    }
                }
                close $ibdfile
                matcrc $ibdfname.gz
                puts ""
            }

            close $errfile
            if {[file size solar-mibd-import.err] == 0} {
                file delete solar-mibd-import.err
            } else {
                error "Errors were detected. See file solar-mibd-import.err"
            }

        } elseif {$ibdfmt == "loki"} {
            set errfile [open solar-mibd-import.err w]
            set chr ""
            set skip 0
            while {-1 != [gets $impfile record]} {
                if {[llength $record] == 0} {
                    continue
                }
                if {[lindex $record 0] == "**Linkage"} {
                    if {$chr != "" && !$skip} {
                        for {set i 1} {$i <= $nindt} {incr i} {
                            if {!$typed($pedno($i))} {
                                puts $ibdfile \
                                [format "%5d %5d %10.7f %10.7f" $i $i -1 -1]
                            }
                        }
                        close $ibdfile
                        matcrc $ibdfname.gz
                        for {set i 1} {$i <= $nped} {incr i} {
                            set typed($i) 0
                        }
                        puts ""
                    }

                    set chr [string range [lindex $record 2] 0 \
                        [expr [string length [lindex $record 2]] - 2]]
                    set locn ""
                    set skip 0
                    if {[llength $chrlist] > 2} {
                        set skip 1
                        for {set i 2} {$i < [llength $chrlist]} {incr i} {
                            if {[lindex $chrlist $i] == $chr} {
                                set skip 0
                            }
                        }
                    }
                } elseif {[lindex $record 0] == "**Position"} {
                    if {$locn != "" && !$skip} {
                        for {set i 1} {$i <= $nindt} {incr i} {
                            if {!$typed($pedno($i))} {
                                puts $ibdfile \
                                [format "%5d %5d %10.7f %10.7f" $i $i -1 -1]
                            }
                        }
                        close $ibdfile
                        matcrc $ibdfname.gz
                        for {set i 1} {$i <= $nped} {incr i} {
                            set typed($i) 0
                        }
                        puts ""
                    }

                    set locn [lindex $record 2]
                    set ibdfname [format "%s/mibd.%s.%s" [mibddir] $chr $locn]
                    delete_files_forcibly $ibdfname
                    set ibdfile [open \
                        "|[usort] -k 1,1n -k 2,2n -u | gzip > $ibdfname.gz" w]
                    puts -nonewline \
                        "Importing IBDs for chromosome $chr location $locn ..."
                    flush stdout
                } elseif {[lindex $record 0] != "Iteration"} {
                    if {$skip} {
                        continue
                    }
                    set ibdid1 [lindex $record 0]
                    set ibdid2 [lindex $record 1]
                    if {$ibdid1 < $ibdid2} {
                        set tmp $ibdid1
                        set ibdid1 $ibdid2
                        set ibdid2 $tmp
                    }
                    set ibd [lindex $record 2]
                    set d7 [lindex $record 3]
                    if {$d7 < 1e-307} {
                        set d7 0
                    }
                    puts $ibdfile [format "%5d %5d %10.7f %10.7f" $ibdid1 \
                                   $ibdid2 $ibd $d7]

                    if {$mztwin($ibdid1) && \
                            $ibdid1 == [lindex $twinid($mztwin($ibdid1)) 0]} {
                        set ids $twinid($mztwin($ibdid1))
                        for {set i 1} {$i < [llength $ids]} {incr i} {
                            if {[lindex $ids $i] >= $ibdid2} {
                                puts $ibdfile [format "%5d %5d %10.7f %10.7f" \
                                    [lindex $ids $i] $ibdid2 $ibd $d7]
                            } else {
                                puts $ibdfile [format "%5d %5d %10.7f %10.7f" \
                                    $ibdid2 [lindex $ids $i] $ibd $d7]
                            }
                        }
                        if {$ibdid1 == $ibdid2} {
                            for {set i 1} {$i < [llength $ids]} {incr i} {
                                set idi [lindex $ids $i]
                                for {set j 1} {$j <= $i} {incr j} {
                                    set idj [lindex $ids $j]
                                    if {$idj <= $idi} {
                                        puts $ibdfile [format \
                                            "%5d %5d %10.7f %10.7f" $idi $idj \
                                            $ibd $d7]
                                    } else {
                                        puts $ibdfile [format \
                                            "%5d %5d %10.7f %10.7f" $idj $idi \
                                            $ibd $d7]
                                    }
                                }
                            }
                        }
                    }
                    if {$mztwin($ibdid2) && \
                            $ibdid2 == [lindex $twinid($mztwin($ibdid2)) 0]} {
                        set ids $twinid($mztwin($ibdid2))
                        for {set i 1} {$i < [llength $ids]} {incr i} {
                            if {[lindex $ids $i] >= $ibdid1} {
                                puts $ibdfile [format "%5d %5d %10.7f %10.7f" \
                                    [lindex $ids $i] $ibdid1 $ibd $d7]
                            } else {
                                puts $ibdfile [format "%5d %5d %10.7f %10.7f" \
                                    $ibdid1 [lindex $ids $i] $ibd $d7]
                            }
                        }
                    }

                    set typed($pedno($ibdid1)) 1
                    if {![info exists rel($ibdid1,$ibdid2)]} {
                        if {$ibd} {
                            puts $errfile \
"$chr/$locn: IBD for unrelateds [get_id $ibdid1] and [get_id $ibdid2] is $ibd instead of 0"
                        }
                    } elseif {$inbred($pedno($ibdid1)) != "y"} {
                        if {($ibdid2 == $fibdid($ibdid1) || \
                             $ibdid2 == $mibdid($ibdid1)) && $ibd != 0.5} {
                            puts $errfile \
"$chr/$locn: IBD for [get_id $ibdid1] and parent [get_id $ibdid2] is $ibd instead of 0.5"
                        }
                        if {$ibdid1 == $ibdid2 && $ibd != 1 && $ibd != -1} {
                            puts $errfile \
"$chr/$locn: IBD for [get_id $ibdid1] with self is $ibd instead of 1"
                        }
                    }
                }
            }
            close $impfile

            if {!$skip && [info exists ibdfile]} {
                for {set i 1} {$i <= $nindt} {incr i} {
                    if {!$typed($pedno($i))} {
                        puts $ibdfile [format "%5d %5d %10.7f %10.7f" $i $i -1 -1]
                    }
                }
                close $ibdfile
                matcrc $ibdfname.gz
                puts ""
            }

            close $errfile
            if {[file size solar-mibd-import.err] == 0} {
                file delete solar-mibd-import.err
            } else {
                error "Errors were detected. See file solar-mibd-import.err"
            }

        } elseif {$ibdfmt == "merlin"} {
            set errfile [open solar-mibd-import.err w]

            set currloc ""
            while {-1 != [gets $impfile record]} {
                if {[lindex $record 0] == "FAMILY"} {
                    continue
                }
                set locn [format %.10g [lindex $record 3]]
                if {$locn != $currloc} {
                    if {$currloc != ""} {
                        for {set i 1} {$i <= $nindt} {incr i} {
                            if {!$typed($pedno($i))} {
                                puts $ibdfile \
                                [format "%5d %5d %10.7f %10.7f" $i $i -1 -1]
                            }
                        }
                        close $ibdfile
                        matcrc $ibdfname.gz
                        for {set i 1} {$i <= $nped} {incr i} {
                            set typed($i) 0
                        }
                        puts ""
                    } else {
                        puts ""
                    }
                    set ibdfname [format "%s/mibd.%s.%s" [mibddir] $chr $locn]
                    delete_files_forcibly $ibdfname
                    set ibdfile [open \
                        "|[usort] -k 1,1n -k 2,2n -u | gzip > $ibdfname.gz" w]
                    puts -nonewline \
                        "Importing IBDs for chromosome $chr location $locn ..."
                    flush stdout
                    set currloc $locn
                }

                set ibdid1 [lindex $record 1]
                set ibdid2 [lindex $record 2]
                if {$ibdid1 < $ibdid2} {
                    set tmp $ibdid1
                    set ibdid1 $ibdid2
                    set ibdid2 $tmp
                }
                set ibd [format %.8g \
                         [expr .5*[lindex $record 5] + [lindex $record 6]]]
                if {$ibd == 0} {
                    continue
                }
                set d7 [format %.8g [lindex $record 6]]
                if {$d7 < 1e-307} {
                    set d7 0
                }
                puts $ibdfile [format "%5d %5d %10.7f %10.7f" $ibdid1 \
                               $ibdid2 $ibd $d7]

                if {$mztwin($ibdid1) && \
                        $ibdid1 == [lindex $twinid($mztwin($ibdid1)) 0]} {
                    set ids $twinid($mztwin($ibdid1))
                    for {set i 1} {$i < [llength $ids]} {incr i} {
                        if {[lindex $ids $i] >= $ibdid2} {
                            puts $ibdfile [format "%5d %5d %10.7f %10.7f" \
                                [lindex $ids $i] $ibdid2 $ibd $d7]
                        } else {
                            puts $ibdfile [format "%5d %5d %10.7f %10.7f" \
                                $ibdid2 [lindex $ids $i] $ibd $d7]
                        }
                    }
                    if {$ibdid1 == $ibdid2} {
                        for {set i 1} {$i < [llength $ids]} {incr i} {
                            set idi [lindex $ids $i]
                            for {set j 1} {$j <= $i} {incr j} {
                                set idj [lindex $ids $j]
                                if {$idj <= $idi} {
                                    puts $ibdfile [format \
                                        "%5d %5d %10.7f %10.7f" $idi $idj \
                                        $ibd $d7]
                                } else {
                                    puts $ibdfile [format \
                                        "%5d %5d %10.7f %10.7f" $idj $idi \
                                        $ibd $d7]
                                }
                            }
                        }
                    }
                }
                if {$mztwin($ibdid2) && \
                        $ibdid2 == [lindex $twinid($mztwin($ibdid2)) 0]} {
                    set ids $twinid($mztwin($ibdid2))
                    for {set i 1} {$i < [llength $ids]} {incr i} {
                        if {[lindex $ids $i] >= $ibdid1} {
                            puts $ibdfile [format "%5d %5d %10.7f %10.7f" \
                                [lindex $ids $i] $ibdid1 $ibd $d7]
                        } else {
                            puts $ibdfile [format "%5d %5d %10.7f %10.7f" \
                                $ibdid1 [lindex $ids $i] $ibd $d7]
                        }
                    }
                }

                set typed($pedno($ibdid1)) 1
                if {![info exists rel($ibdid1,$ibdid2)]} {
                    if {$ibd} {
                        puts $errfile \
"$chr/$locn: IBD for unrelateds [get_id $ibdid1] and [get_id $ibdid2] is $ibd instead of 0"
                    }
                } elseif {$inbred($pedno($ibdid1)) != "y"} {
                    if {($ibdid2 == $fibdid($ibdid1) || \
                         $ibdid2 == $mibdid($ibdid1)) && $ibd != 0.5} {
                        puts $errfile \
"$chr/$locn: IBD for [get_id $ibdid1] and parent [get_id $ibdid2] is $ibd instead of 0.5"
                    }
                    if {$ibdid1 == $ibdid2 && $ibd != 1 && $ibd != -1} {
                        puts $errfile \
"$chr/$locn: IBD for [get_id $ibdid1] with self is $ibd instead of 1"
                    }
                }
            }
            close $impfile

            for {set i 1} {$i <= $nindt} {incr i} {
                if {!$typed($pedno($i))} {
                    puts $ibdfile [format "%5d %5d %10.7f %10.7f" $i $i -1 -1]
                }
            }
            close $ibdfile
            matcrc $ibdfname.gz
            puts ""

            close $errfile
            if {[file size solar-mibd-import.err] == 0} {
                file delete solar-mibd-import.err
            } else {
                error "Errors were detected. See file solar-mibd-import.err"
            }

        } elseif {$ibdfmt == "gh"} {
            set errfile [open solar-mibd-import.err w]
            gets $impfile record

            set currloc ""
            while {-1 != [gets $impfile record]} {
                if {[lindex $record 0] == "pos"} {
                    continue
                }
                set locn [format %.10g [lindex $record 0]]
                if {$locn == -0} {
                    set locn 0
                }
                if {$locn != $currloc} {
                    if {$currloc != ""} {
                        for {set i 1} {$i <= $nindt} {incr i} {
                            if {!$typed($pedno($i))} {
                                puts $ibdfile \
                                [format "%5d %5d %10.7f %10.7f" $i $i -1 -1]
                            } else {
                                puts $ibdfile \
                                [format "%5d %5d %10.7f %10.7f" $i $i 1 1]
                            }
                        }
                        close $ibdfile
                        matcrc $ibdfname.gz
                        for {set i 1} {$i <= $nped} {incr i} {
                            set typed($i) 0
                        }
                        puts ""
                    } else {
                        puts ""
                    }
                    set ibdfname [format "%s/mibd.%s.%s" [mibddir] $chr $locn]
                    delete_files_forcibly $ibdfname
                    set ibdfile [open \
                        "|[usort] -k 1,1n -k 2,2n -u | gzip > $ibdfname.gz" w]
                    puts -nonewline \
                        "Importing IBDs for chromosome $chr location $locn ..."
                    flush stdout
                    set currloc $locn
                }

                scan [lindex $record 2] "%d,%d" ibdid1 ibdid2
                if {$ibdid1 < $ibdid2} {
                    set tmp $ibdid1
                    set ibdid1 $ibdid2
                    set ibdid2 $tmp
                }
                set ibd [format %.8g \
                         [expr .5*[lindex $record 7] + [lindex $record 8]]]
                if {$ibd == 0} {
                    continue
                }
                set d7 [format %.8g [lindex $record 8]]
                if {$d7 < 1e-307} {
                    set d7 0
                }
                puts $ibdfile [format "%5d %5d %10.7f %10.7f" $ibdid1 \
                               $ibdid2 $ibd $d7]

                if {$mztwin($ibdid1) && \
                        $ibdid1 == [lindex $twinid($mztwin($ibdid1)) 0]} {
                    set ids $twinid($mztwin($ibdid1))
                    for {set i 1} {$i < [llength $ids]} {incr i} {
                        if {[lindex $ids $i] >= $ibdid2} {
                            puts $ibdfile [format "%5d %5d %10.7f %10.7f" \
                                [lindex $ids $i] $ibdid2 $ibd $d7]
                        } else {
                            puts $ibdfile [format "%5d %5d %10.7f %10.7f" \
                                $ibdid2 [lindex $ids $i] $ibd $d7]
                        }
                    }
                    if {$ibdid1 == $ibdid2} {
                        for {set i 2} {$i < [llength $ids]} {incr i} {
                            set idi [lindex $ids $i]
                            for {set j 1} {$j <= $i} {incr j} {
                                set idj [lindex $ids $j]
                                if {$idj <= $idi} {
                                    puts $ibdfile [format \
                                        "%5d %5d %10.7f %10.7f" $idi $idj \
                                        $ibd $d7]
                                } else {
                                    puts $ibdfile [format \
                                        "%5d %5d %10.7f %10.7f" $idj $idi \
                                        $ibd $d7]
                                }
                            }
                        }
                    }
                }
                if {$mztwin($ibdid2) && \
                        $ibdid2 == [lindex $twinid($mztwin($ibdid2)) 0]} {
                    set ids $twinid($mztwin($ibdid2))
                    for {set i 1} {$i < [llength $ids]} {incr i} {
                        if {[lindex $ids $i] >= $ibdid1} {
                            puts $ibdfile [format "%5d %5d %10.7f %10.7f" \
                                [lindex $ids $i] $ibdid1 $ibd $d7]
                        } else {
                            puts $ibdfile [format "%5d %5d %10.7f %10.7f" \
                                $ibdid1 [lindex $ids $i] $ibd $d7]
                        }
                    }
                }

                set typed($pedno($ibdid1)) 1
                if {![info exists rel($ibdid1,$ibdid2)]} {
                    if {$ibd} {
                        puts $errfile \
"$chr/$locn: IBD for unrelateds [get_id $ibdid1] and [get_id $ibdid2] is $ibd instead of 0"
                    }
                } elseif {$inbred($pedno($ibdid1)) != "y"} {
                    if {($ibdid2 == $fibdid($ibdid1) || \
                         $ibdid2 == $mibdid($ibdid1)) && $ibd != 0.5} {
                        puts $errfile \
"$chr/$locn: IBD for [get_id $ibdid1] and parent [get_id $ibdid2] is $ibd instead of 0.5"
                    }
                    if {$ibdid1 == $ibdid2 && $ibd != 1 && $ibd != -1} {
                        puts $errfile \
"$chr/$locn: IBD for [get_id $ibdid1] with self is $ibd instead of 1"
                    }
                }
            }
            close $impfile

            for {set i 1} {$i <= $nindt} {incr i} {
                if {!$typed($pedno($i))} {
                    puts $ibdfile [format "%5d %5d %10.7f %10.7f" $i $i -1 -1]
                } else {
                    puts $ibdfile [format "%5d %5d %10.7f %10.7f" $i $i 1 1]
                }
            }
            close $ibdfile
            matcrc $ibdfname.gz
            puts ""

            close $errfile
            if {[file size solar-mibd-import.err] == 0} {
                file delete solar-mibd-import.err
            } else {
                error "Errors were detected. See file solar-mibd-import.err"
            }

        } elseif {$ibdfmt == "sw"} {
            puts -nonewline "Processing SimWalk2 output file ..."
            flush stdout
            set firstloc 99999
            set lastloc 0
            set outf [open "|[usort] > solar-mibd-import.tmp" w]
            while {-1 != [gets $impfile record]} {
                if {[string match *pedigree* $record] || \
                        [string match "*number of*" $record]} {
                    continue
                }
                if {[regexp {.*[0-9].*} $record] && $version == "2.82"} {
                   if {[llength $record] == 8} {
                        set locn [format %.10g [lindex $record 3]]
                        set ibdid1 [lindex $record 0]
                        set ibdid2 [lindex $record 1]
                        if {$ibdid2 > $ibdid1} {
                            set tmp $ibdid1
                            set ibdid1 $ibdid2
                            set ibdid2 $tmp
                        }
                        set ibd [format %.8g [expr 2*[lindex $record 7]]]
                        if {$ibd == 0} {
                            continue
                        }
                        if {$is_inbred} {
                            puts $outf "$chr,$locn,$ibdid1,$ibdid2,$ibd"
                        } else {
                            set d7 [format %.8g [lindex $record 6]]
                            puts $outf "$chr,$locn,$ibdid1,$ibdid2,$ibd,$d7"
                        }
                    } elseif {[llength $record] == 6} {
                        set locn [format %.10g [expr int([lindex $record 1])]]
                        if {$locn == [lindex $record 1]} {
                            set ibd [format %.8g [expr 2*[lindex $record 5]]]
                            if {$ibd == 0} {
                                continue
                            }
                            if {$is_inbred} {
                                puts $outf "$chr,$locn,$ibdid1,$ibdid2,$ibd"
                            } else {
                                set d7 [format %.8g [lindex $record 4]]
                                puts $outf "$chr,$locn,$ibdid1,$ibdid2,$ibd,$d7"
                            }
                        }
                    } elseif {[llength $record] == 2} {
                        set locn [format %.10g [lindex $record 0]]
                        set ibd [format %.8g [expr 2*[lindex $record 1]]]
                        if {$ibd == 0} {
                            continue
                        }
                        puts $outf "$chr,$locn,$ibdid1,$ibdid2,$ibd"
                    } elseif {[llength $record] == 5} {
                        set locn [format %.10g [lindex $record 0]]
                        set ibd [format %.8g [expr 2*[lindex $record 4]]]
                        if {$ibd == 0} {
                            continue
                        }
                        if {$is_inbred} {
                            puts $outf "$chr,$locn,$ibdid1,$ibdid2,$ibd"
                        } else {
                            set d7 [format %.8g [lindex $record 3]]
                            puts $outf "$chr,$locn,$ibdid1,$ibdid2,$ibd,$d7"
                        }
                    } else {
                        continue
                    }
                    if {$locn < $firstloc} {
                        set firstloc $locn
                    } elseif {$locn > $lastloc} {
                        set lastloc $locn
                    }
                } elseif {[regexp {.*[0-9].*} $record]} {
                    if {[llength $record] == 3} {
                        set ibdid1 [lindex $record 0]
                        set ibdid2 [lindex $record 1]
                        if {$ibdid2 > $ibdid1} {
                            set tmp $ibdid1
                            set ibdid1 $ibdid2
                            set ibdid2 $tmp
                        }
                        continue
                    } elseif {[llength $record] == 6} {
                        set locn [format %.10g [expr int([lindex $record 1])]]
                        if {$locn == [lindex $record 1]} {
                            set ibd [format %.8g [expr 2*[lindex $record 5]]]
                            if {$ibd == 0} {
                                continue
                            }
                            if {$is_inbred} {
                                puts $outf "$chr,$locn,$ibdid1,$ibdid2,$ibd"
                            } else {
                                set d7 [format %.8g [lindex $record 4]]
                                puts $outf "$chr,$locn,$ibdid1,$ibdid2,$ibd,$d7"
                            }
                        }
                    } else {
                        continue
                    }
                    if {$locn < $firstloc} {
                        set firstloc $locn
                    } elseif {$locn > $lastloc} {
                        set lastloc $locn
                    }
                }
            }

            if {!$is_inbred} {
                for {set i 1} {$i <= $nindt} {incr i} {
                    for {set j $firstloc} {$j <= $lastloc} {incr j} {
                        set locn [format %.10g $j]
                        puts $outf "$chr,$locn,$i,$i,1,1"
                    }
                }
            }
            close $outf
            close $impfile
            puts ""

            set outf [open solar-mibd-import.in w]
            if {$is_inbred} {
                puts $outf "CHROMO,LOCATION,IBDID1,IBDID2,IBD"
            } else {
                puts $outf "CHROMO,LOCATION,IBDID1,IBDID2,IBD,D7"
            }
            close $outf
            exec cat solar-mibd-import.tmp >> solar-mibd-import.in
            file delete solar-mibd-import.tmp
            mibd import -ibdid
            file delete solar-mibd-import.in
        }

        return
    }

    if {[llength $chrlist] && [lindex $chrlist 0] == "export"} {
        if {[catch {pedigree_loaded} errmsg]} {
            error $errmsg
        }
        if {[catch {mibddir} errmsg]} {
            error $errmsg
        }
        set pedfile [tablefile open pedindex.out]
        set need_famid 0
        if {[tablefile $pedfile test_name FAMID]} {
            set need_famid 1
        }
        tablefile $pedfile close

        if {[llength $chrlist] > 1} {
            set ibdlist {}
            for {set i 1} {$i < [llength $chrlist]} {incr i} {
                eval lappend ibdlist \
                     [glob [mibddir]/mibd.[lindex $chrlist $i].*.gz]
            }
        } else {
            set ibdlist [glob [mibddir]/mibd.*.gz]
        }

        for {set i 0} {$i < [llength $ibdlist]} {incr i} {
            set ibdfile [lindex $ibdlist $i]
            if {[file exists $ibdfile]} {
                set chr [string range [file tail $ibdfile] 5 \
                    [expr [string length [file tail $ibdfile]] - 4]]
                set locn [string range $chr [expr [string first . $chr] + 1] end]
                set chr [string range $chr 0 [expr [string first . $chr] - 1]]
                puts -nonewline \
                     "Exporting IBDs for chromosome $chr location $locn ..."
                flush stdout

                if {$byloc || !$i} {
                    if {$fname == ""} {
                        set fname solar-mibd-export.out
                    }
                    set tfname $fname
                    if {$byloc} {
                        if {$i} {
                            close $expfile
                        }
                        set tfname $fname.$chr.$locn
                    }
                    if {[file exists $tfname] && !$overwrite && !$append} {
        error "\nFile $tfname already exists. Use -overwrite or -append option."
                    }
                    if {$append} {
                        set expfile [tablefile open $tfname]
                        if {![tablefile $expfile test_name CHROMO]} {
                            tablefile $expfile close
                            error "\nA CHROMO field is required in $tfname."
                        }
                        if {![tablefile $expfile test_name LOCATION]} {
                            tablefile $expfile close
                            error "\nA LOCATION field is required in $tfname."
                        }
                        if {$use_ibdid} {
                            if {![tablefile $expfile test_name IBDID1]} {
                                tablefile $expfile close
                                error "\nAn IBDID1 field is required in $tfname."
                            }
                            if {![tablefile $expfile test_name IBDID2]} {
                                tablefile $expfile close
                                error "\nAn IBDID2 field is required in $tfname."
                            }
                        } else {
                            if {![tablefile $expfile test_name ID1]} {
                                tablefile $expfile close
                                error "\nAn ID1 field is required in $tfname."
                            }
                            if {![tablefile $expfile test_name ID2]} {
                                tablefile $expfile close
                                error "\nAn ID2 field is required in $tfname."
                            }
                            if {$need_famid && \
                                    ![tablefile $expfile test_name FAMID]} {
                                tablefile $expfile close
                                error "\nA FAMID field is required in $tfname."
                            }
                            if {!$need_famid && \
                                    [tablefile $expfile test_name FAMID]} {
                                tablefile $expfile close
        error "\nA FAMID field is present in $tfname but not in pedindex.out."
                            }
                        }
                        if {![tablefile $expfile test_name IBD]} {
                            tablefile $expfile close
                            error "\nAn IBD field is required in $tfname."
                        }
                        if {!$nod7 && ![tablefile $expfile test_name D7]} {
                            tablefile $expfile close
                            error "\nA D7 field is required in $tfname."
                        }
                        if {$nod7 && [tablefile $expfile test_name D7]} {
                            tablefile $expfile close
        error "\nA D7 field is not to be included but is present in $tfname."
                        }
                        tablefile $expfile close
                        set expfile [open $tfname a]
                    } else {
                        set expfile [open $tfname w]
                        puts -nonewline $expfile "CHROMO,LOCATION,"
                        if {$use_ibdid} {
                            if {$nod7} {
                                puts $expfile "IBDID1,IBDID2,IBD"
                            } else {
                                puts $expfile "IBDID1,IBDID2,IBD,D7"
                            }
                        } else {
                            if {$need_famid} {
                                puts -nonewline $expfile "FAMID,"
                            }
                            if {$nod7} {
                                puts $expfile "ID1,ID2,IBD"
                            } else {
                                puts $expfile "ID1,ID2,IBD,D7"
                            }
                        }
                    }
                }

                set ifile [open "|gunzip -c $ibdfile" r]
                while {-1 != [gets $ifile line]} {
                    if {[lindex $line 0] == 1 && [lindex $line 1] == 1 && \
                            [lindex $line 2] < 1} {
                        continue
                    }
                    if {$use_ibdid} {
                        set id1 [lindex $line 0]
                        set id2 [lindex $line 1]
                        set famid ""
                    } else {
                        set id1 [get_id [lindex $line 0]]
                        if {$id1 == ""} {
                            close $ifile
                            close $expfile
                            error "\nCan't find an ID for IBDID [lindex $line 0]"
                        }
                        if {$need_famid} {
                            set famid [lindex $id1 0]
                            set id1 [lindex $id1 1]
                        } else {
                            set famid ""
                        }
                        set id2 [get_id [lindex $line 1]]
                        if {$id2 == ""} {
                            close $ifile
                            close $expfile
                            error "\nCan't find an ID for IBDID [lindex $line 1]"
                        }
                        if {$need_famid} {
                            if {[lindex $id2 0] != $famid} {
                                close $ifile
                                close $expfile
                                error \
        "\nIBDIDs [lindex $line 0] and [lindex $line 1] have different FAMIDs"
                            }
                            set id2 [lindex $id2 1]
                        }
                    }
                    if {$famid != ""} {
                        if {$nod7} {
                            puts $expfile \
        "$chr,$locn,$famid,$id1,$id2,[format %.8g [lindex $line 2]]"
                        } else {
                            puts $expfile \
        "$chr,$locn,$famid,$id1,$id2,[format %.8g \ [lindex $line 2]],[format \
         %.8g [lindex $line 3]]"
                        }
                    } else {
                        if {$nod7} {
                            puts $expfile \
        "$chr,$locn,$id1,$id2,[format %.8g [lindex $line 2]]"
                        } else {
                            puts $expfile \
        "$chr,$locn,$id1,$id2,[format %.8g \ [lindex $line 2]],[format %.8g \
         [lindex $line 3]]"
                        }
                    }
                }
                close $ifile
                puts ""
            } else {
                puts "Cannot open $ibdfile"
            }
        }
        close $expfile
        return
    }

    if {[llength $chrlist] && [lindex $chrlist 0] == "prep"} {
        if {[llength $chrlist] != 2} {
error "You must specify the analysis package for which to prepare input files."
        }
        if {[string tolower [lindex $chrlist 1]] == "loki"} {
            set ibdfmt "loki"
            puts "Preparing input files for Loki multipoint IBD computation ..."
        } elseif {[string tolower [lindex $chrlist 1]] == "merlin"} {
            set ibdfmt "merlin"
            puts "Preparing input files for Merlin multipoint IBD computation ..."
        } elseif {[string tolower [lindex $chrlist 1]] == "simwalk" || \
                  [string tolower [lindex $chrlist 1]] == "simwalk2" || \
                  [string tolower [lindex $chrlist 1]] == "sw" || \
                  [string tolower [lindex $chrlist 1]] == "sw2"} {
            set ibdfmt "sw"
            puts "Preparing input files for SimWalk2 multipoint IBD computation ..."
        } elseif {[string tolower [lindex $chrlist 1]] == "genehunter" || \
                  [string tolower [lindex $chrlist 1]] == "genehunter2" || \
                  [string tolower [lindex $chrlist 1]] == "gh" || \
                  [string tolower [lindex $chrlist 1]] == "gh2"} {
            set ibdfmt "gh"
            puts "Preparing input files for GeneHunter multipoint IBD computation ..."
        } else {
            error "IBD computation using [lindex $chrlist 1] is not supported."
        }

        if {[catch {pedigree_loaded} errmsg]} {
            error $errmsg
        }

        set infofile [open pedigree.info r]
        gets $infofile record
        gets $infofile record
        gets $infofile record
        scan $record "%d %d %d %d" nped nfamt nindt nfout
        set is_inbred 0
        set max_nbits 0
        for {set i 1} {$i <= $nped} {incr i} {
            set typed($i) 0
            gets $infofile record
            scan $record "%d %d %d %d %s" \
                 nfam($i) nind($i) nfou($i) nlbrk($i) inbred($i)
            if {$inbred($i) == "y"} {
                set is_inbred 1
            }
            set nbits($i) [expr 2*$nind($i) - 3*$nfou($i)]
            if {$nbits($i) > $max_nbits} {
                set max_nbits $nbits($i)
            }
        }
        close $infofile

        if {$is_inbred} {
            if {$ibdfmt == "merlin"} {
#                error \
#"Computing IBDs for inbred pedigrees using Merlin is not supported by SOLAR."
            } elseif {$ibdfmt == "gh"} {
                error \
"Computing IBDs for inbred pedigrees using GeneHunter is not supported by SOLAR."
            } elseif {$ibdfmt == "sw" && $version == "2.82"} {
                error \
"Computing IBDs for inbred pedigrees using SimWalk2 v2.82 is not supported by SOLAR."
            }
        }

        set pedfile [tablefile open pedindex.out]
        tablefile $pedfile start_setup
        tablefile $pedfile setup FIBDID
        tablefile $pedfile setup MIBDID
        tablefile $pedfile setup SEX
        tablefile $pedfile setup MZTWIN
        tablefile $pedfile setup PEDNO
        for {set i 1} {$i <= $nindt} {incr i} {
            set record [tablefile $pedfile get]
            set fibdid($i) [lindex $record 0]
            set mibdid($i) [lindex $record 1]
            set sex($i) [lindex $record 2]
            set mztwin($i) [lindex $record 3]
            if {$mztwin($i) != 0} {
                if {![info exists twinid($mztwin($i))]} {
                    set twinid($mztwin($i)) $i
                } else {
                    lappend twinid($mztwin($i)) $i
                }
            }
            set pedno($i) [lindex $record 4]
            set gtype($i) {}
        }
        tablefile $pedfile close

        if {[catch {set frqfile [open freq.info r]}]} {
            error "Marker data have not been loaded."
        }
        gets $frqfile record
        while {-1 != [gets $frqfile record]} {
            set mrk [lindex $record 0]
            set alllist($mrk) {}
            set frqlist($mrk) {}
            for {set i 4} {$i < [llength $record]} {incr i} {

                set allname [lindex $record $i]
                if {$allname == "se=" || $allname == "hwe="} {
                    incr i
                    continue
                }
                lappend alllist($mrk) $allname
                incr i
                lappend frqlist($mrk) [lindex $record $i]
            }
        }
        close $frqfile

        set mrklist [cmarker names]
        foreach mrk $mrklist {
            set genfname d_$mrk/translat.tab
            if {[catch {set genfile [open $genfname r]}]} {
                error "Cannot open file $genfname"
            }
            gets $genfile genrec
            gets $genfile genrec
            set ibdid 0
            while {-1 != [gets $genfile genrec]} {
                set n [lindex $genrec 0]
                for {set i 0} {$i < $n} {incr i} {
                    incr ibdid
                    gets $genfile genrec
                    set a1 [string range $genrec 19 21]
                    set a2 [string range $genrec 22 24]
                    if {$a1 != "   "} {
                        if {$ibdfmt == "merlin"} {
                            eval lappend gtype($ibdid) \
                                [format "%d %d" $a1 $a2]
                        } elseif {$ibdfmt == "gh"} {
                            eval lappend gtype($ibdid,$mrk) \
                                [format "%d %d" $a1 $a2]
                        } else {
                            lappend gtype($ibdid) [format %d/%d $a1 $a2]
                        }
                    } else {
                        if {$ibdfmt == "merlin"} {
                            eval lappend gtype($ibdid) "0 0"
                        } elseif {$ibdfmt == "gh"} {
                            eval lappend gtype($ibdid,$mrk) "0 0"
                        } else {
                            lappend gtype($ibdid) "*"
                        }
                    }
                }
            }
            close $genfile
        }

#  These qter positions are based on the deCODE map.  For each chromosome,
#  the map was converted from Kosambi to Haldane cM and qter was set to the
#  nearest integer location >= the position of the last marker.

        if {$qter} {
            set qterpos(1) 279
            set qterpos(2) 264
            set qterpos(3) 226
            set qterpos(4) 209
            set qterpos(5) 210
            set qterpos(6) 194
            set qterpos(7) 187
            set qterpos(8) 170
            set qterpos(9) 164
            set qterpos(10) 181
            set qterpos(11) 155
            set qterpos(12) 175
            set qterpos(13) 133
            set qterpos(14) 126
            set qterpos(15) 138
            set qterpos(16) 134
            set qterpos(17) 141
            set qterpos(18) 124
            set qterpos(19) 113
            set qterpos(20) 101
            set qterpos(21) 68
            set qterpos(22) 72
            set qterpos(23) 194
            set qterpos(X) 194
        }

        if {$ibdfmt == "loki"} {
            if {[map func] == "b"} {
                error \
"Map contains basepair locations so can't be used to compute multipoint IBDs."
            }
            set chrnum [cmap chrnum]
            set outf [open mibdchr$chrnum.loc w]
            puts $outf Haldane
            set kosambi 0
            if {[cmap func] == "k"} {
                set kosambi 1
            }
            set maplist [cmap names]
            set mrk [lindex $maplist 0]
            if {[lsearch $mrklist $mrk] == -1} {
                close $outf
                error "Marker $mrk is in map but not in marker file."
            }
            set locn($mrk) [cmap locn $mrk]
            if {$kosambi} {
                if {$locn($mrk) != 0} {
                    set x [expr double($locn($mrk))/100]
                    set theta [expr .5*(exp(4*$x) - 1)/(exp(4*$x) + 1)]
                    set hloc [expr (-.5*log(1 - 2*$theta))*100]
                    puts $outf "$mrk $hloc"
                } else {
                    set hloc 0
                    puts $outf "$mrk 0"
                }
                set hlocn($mrk) $hloc
                set firsthloc $hloc
                set lasthloc $hloc
                set lastkloc $locn($mrk)
            } else {
                puts $outf "$mrk $locn($mrk)"
                set hlocn($mrk) $locn($mrk)
                set firsthloc $locn($mrk)
                set lasthloc $locn($mrk)
            }
            for {set i 1} {$i < [cmap nloci]} {incr i} {
                set mrk [lindex $maplist $i]
                if {[lsearch $mrklist $mrk] == -1} {
                    close $outf
                    error "Marker $mrk is in map but not in marker file."
                }
                set locn($mrk) [cmap locn $mrk]
                if {$kosambi} {
                    set x [expr double($locn($mrk) - $lastkloc)/100]
                    set theta [expr .5*(exp(4*$x) - 1)/(exp(4*$x) + 1)]
                    set hloc [expr (-.5*log(1 - 2*$theta))*100 + $lasthloc]
                    puts $outf "$mrk [format %.10g $hloc]"
                    if {[format %.10g $hloc] == [format %.10g $lasthloc]} {
                        close $outf
                        error \
    "Loki does not allow zero recombination between markers. Check map file."
                    }
                    set hlocn($mrk) $hloc
                    set lasthloc $hloc
                    set lastkloc $locn($mrk)
                } else {
                    puts $outf "$mrk $locn($mrk)"
                    if {$locn($mrk) == $lasthloc} {
                        close $outf
                        error \
    "Loki does not allow zero recombination between markers. Check map file."
                    }
                    set hlocn($mrk) $locn($mrk)
                    set lasthloc $locn($mrk)
                }
            }
            set firstpos [expr int($firsthloc)]
            set lastpos [expr int($lasthloc + 1)]
            close $outf

            if {$byped} {
                set currped 0
            } else {
                set outf [open lkmibd.data w]
            }
            for {set i 1} {$i <= $nindt} {incr i} {
                if {$mztwin($i) && $i != [lindex $twinid($mztwin($i)) 0]} {
                    continue
                }
                if {$byped && $pedno($i) != $currped} {
                    if {$currped} {
                        close $outf
                    }
                    set currped $pedno($i)
                    set outf [open lkmibd.data.$currped w]
                }
                set fa "*"
                if {$fibdid($i) != 0} {
                    set fa $fibdid($i)
                    if {$mztwin($fa)} {
                        set fa [lindex $twinid($mztwin($fa)) 0]
                    }
                }
                set mo "*"
                if {$mibdid($i) != 0} {
                    set mo $mibdid($i)
                    if {$mztwin($mo)} {
                        set mo [lindex $twinid($mztwin($mo)) 0]
                    }
                }
                puts $outf "$i $fa $mo $gtype($i)"
            }
            close $outf

            set outf [open lkmibd.prep w]
            puts $outf "missing \"*\""
            puts $outf "log \"log\""
            puts -nonewline $outf "file \[GS = \"/\"\] \"lkmibd.data\",id,fa,mo"
            for {set i 0} {$i < [llength $mrklist]} {incr i} {
                puts -nonewline $outf ",[lindex $mrklist $i]"
            }
            puts $outf ""
            puts $outf "pedigree id,fa,mo"
            puts $outf "set prune_option 0"
            puts -nonewline $outf "marker locus [lindex $maplist 0]"
            for {set i 1} {$i < [llength $maplist]} {incr i} {
                puts -nonewline $outf ",[lindex $maplist $i]"
            }
            puts $outf ""
            puts -nonewline $outf "link \"$chrnum\""
            for {set i 0} {$i < [llength $maplist]} {incr i} {
                puts -nonewline $outf ",[lindex $maplist $i]"
            }
            puts $outf ""
            close $outf

            set outf [open lkmibd.loki w]
            puts $outf "iterations 100000"
            puts $outf "start output 10"
            puts $outf "output frequency 10"
            puts $outf "total map 3500"
            for {set i 0} {$i < [llength $maplist]} {incr i} {
                set mrk [lindex $maplist $i]
                puts $outf "position $mrk [format %.10g $hlocn($mrk)]"
            }
            if {$usefreq} {
                for {set i 0} {$i < [llength $maplist]} {incr i} {
                    set mrk [lindex $maplist $i]
                    puts -nonewline $outf "frequency $mrk"
                    for {set j 0} {$j < [llength $alllist($mrk)]} {incr j} {
                        puts -nonewline $outf \
                            " [expr $j + 1],[lindex $frqlist($mrk) $j]"
                    }
                    puts $outf ""
                }
            }
            puts $outf "compress ibd output"
            puts $outf "set lm_ratio 0.5"
            if {!$qter || ![info exists qterpos($chrnum)]} {
                puts $outf "estimate ibd grid 0,$lastpos,1"
            } else {
                if {$lastpos > $qterpos($chrnum)} {
                    puts \
"Your last marker location on chromosome $chrnum is past qter. MIBDs will end at qter."
                }
                puts $outf "estimate ibd grid 0,$qterpos($chrnum),1"
            }
            close $outf

            puts "The following files have been created:"
            puts "    lkmibd.data		- pedigree/genotype data"
            puts "    lkmibd.prep		- prep parameter file"
            puts "    lkmibd.loki		- loki parameter file"
            puts "    mibdchr$chrnum.loc	- map file for SOLAR plots"
            puts \
"Move the file mibdchr$chrnum.loc to the directory where the Loki-computed IBDs"
            puts "will be stored."

        } elseif {$ibdfmt == "merlin"} {
            if {[map func] == "b"} {
                error \
"Map contains basepair locations so can't be used to compute multipoint IBDs."
            }
            set chrnum [cmap chrnum]
            set outf [open mibdchr$chrnum.loc w]
            set outf2 [open mlmibd.map w]
            puts $outf Haldane
            set kosambi 0
            if {[cmap func] == "k"} {
                set kosambi 1
            }
            set maplist [cmap names]
            set mrk [lindex $maplist 0]
            if {[lsearch $mrklist $mrk] == -1} {
                close $outf2
                close $outf
                error "Marker $mrk is in map but not in marker file."
            }
            set locn($mrk) [cmap locn $mrk]
            if {$kosambi} {
                if {$locn($mrk) != 0} {
                    set x [expr double($locn($mrk))/100]
                    set theta [expr .5*(exp(4*$x) - 1)/(exp(4*$x) + 1)]
                    set hloc [expr (-.5*log(1 - 2*$theta))*100]
                    puts $outf "$mrk [format %.2f $hloc]"
                    puts $outf2 "$chrnum $mrk [format %.2f $hloc]"
                } else {
                    set hloc 0
                    puts $outf "$mrk 0"
                    puts $outf2 "$chrnum $mrk 0"
                }
                set firsthloc $hloc
                set lasthloc $hloc
                set lastkloc $locn($mrk)
            } else {
                puts $outf "$mrk $locn($mrk)"
                puts $outf2 "$chrnum $mrk $locn($mrk)"
                set firsthloc $locn($mrk)
                set lasthloc $locn($mrk)
            }
            for {set i 1} {$i < [cmap nloci]} {incr i} {
                set mrk [lindex $maplist $i]
                if {[lsearch $mrklist $mrk] == -1} {
                    close $outf2
                    close $outf
                    error "Marker $mrk is in map but not in marker file."
                }
                set locn($mrk) [cmap locn $mrk]
                if {$kosambi} {
                    set x [expr double($locn($mrk) - $lastkloc)/100]
                    set theta [expr .5*(exp(4*$x) - 1)/(exp(4*$x) + 1)]
                    set hloc [expr (-.5*log(1 - 2*$theta))*100 + $lasthloc]
                    puts $outf "$mrk [format %.2f $hloc]"
                    puts $outf2 "$chrnum $mrk [format %.2f $hloc]"
                    set lasthloc $hloc
                    set lastkloc $locn($mrk)
                } else {
                    puts $outf "$mrk $locn($mrk)"
                    puts $outf2 "$chrnum $mrk $locn($mrk)"
                    set lasthloc $locn($mrk)
                }
            }
            set firstpos [expr int($firsthloc)]
            set lastpos [expr int($lasthloc + 1)]
            close $outf2
            close $outf

            set outf [open mlmibd.frq w]
            set outf2 [open mlmibd.dat w]
            for {set i 0} {$i < [llength $mrklist]} {incr i} {
                set mrk [lindex $mrklist $i]
                puts $outf "M $mrk"
                puts $outf2 "M $mrk"
                for {set j 0} {$j < [llength $alllist($mrk)]} {incr j} {
                    puts $outf "A [expr $j + 1] [lindex $frqlist($mrk) $j]"
                }
            }
            close $outf2
            close $outf

            if {$byped} {
                set currped 0
            } else {
                set outf [open mlmibd.ped w]
            }
            for {set i 1} {$i <= $nindt} {incr i} {
                if {$mztwin($i) && $i != [lindex $twinid($mztwin($i)) 0]} {
                    continue
                }
                if {$byped && $pedno($i) != $currped} {
                    if {$currped} {
                        close $outf
                    }
                    set currped $pedno($i)
                    set outf [open mlmibd.ped.$currped w]
                }
                set fa $fibdid($i)
                if {$fa != 0 && $mztwin($fa)} {
                    set fa [lindex $twinid($mztwin($fa)) 0]
                }
                set mo $mibdid($i)
                if {$mo != 0 && $mztwin($mo)} {
                    set mo [lindex $twinid($mztwin($mo)) 0]
                }
                puts $outf "$pedno($i) $i $fa $mo $sex($i) $gtype($i)"
            }
            close $outf

            set outf [open mlmibd.cmd w]
            puts $outf \
"merlin -dmlmibd.dat -pmlmibd.ped -fmlmibd.frq -mmlmibd.map --ibd \
--grid 1 --start 0 --stop $lastpos --bits $max_nbits"
            close $outf

            puts "The following files have been created:"
            puts "    mlmibd.cmd 		- Merlin IBD command"
            puts "    mlmibd.dat 		- data description file"
            puts "    mlmibd.ped 		- pedigree/genotype data"
            puts "    mlmibd.frq 		- allele frequency file"
            puts "    mlmibd.map 		- map file"
            puts "    mibdchr$chrnum.loc	- map file for SOLAR plots"
            puts \
"Move the file mibdchr$chrnum.loc to the directory where the Merlin-computed IBDs"
            puts "will be stored."

        } elseif {$ibdfmt == "gh"} {
            if {[map func] == "b"} {
                error \
"Map contains basepair locations so can't be used to compute multipoint IBDs."
            }
            set chrnum [cmap chrnum]
            set outf [open mibdchr$chrnum.loc w]
            puts $outf Haldane
            set kosambi 0
            if {[cmap func] == "k"} {
                set kosambi 1
            }
            set maplist [cmap names]
            set mrk [lindex $maplist 0]
            if {[lsearch $mrklist $mrk] == -1} {
                close $outf
                error "Marker $mrk is in map but not in marker file."
            }
            set locn($mrk) [cmap locn $mrk]
            if {$kosambi} {
                if {$locn($mrk) != 0} {
                    set x [expr double($locn($mrk))/100]
                    set theta [expr .5*(exp(4*$x) - 1)/(exp(4*$x) + 1)]
                    set hloc [expr (-.5*log(1 - 2*$theta))*100]
                    puts $outf "$mrk [format %.2f $hloc]"
                } else {
                    set hloc 0
                    puts $outf "$mrk 0"
                }
                set firsthloc $hloc
                set lasthloc $hloc
                set lastkloc $locn($mrk)
            } else {
                puts $outf "$mrk $locn($mrk)"
                set firsthloc $locn($mrk)
                set lasthloc $locn($mrk)
            }
            for {set i 1} {$i < [cmap nloci]} {incr i} {
                set mrk [lindex $maplist $i]
                if {[lsearch $mrklist $mrk] == -1} {
                    close $outf
                    error "Marker $mrk is in map but not in marker file."
                }
                set locn($mrk) [cmap locn $mrk]
                if {$kosambi} {
                    set x [expr double($locn($mrk) - $lastkloc)/100]
                    set theta [expr .5*(exp(4*$x) - 1)/(exp(4*$x) + 1)]
                    set hloc [expr (-.5*log(1 - 2*$theta))*100 + $lasthloc]
                    puts $outf "$mrk [format %.2f $hloc]"
                    set lasthloc $hloc
                    set lastkloc $locn($mrk)
                } else {
                    puts $outf "$mrk $locn($mrk)"
                    set lasthloc $locn($mrk)
                }
            }
            set firstpos [expr int($firsthloc)]
            set lastpos [expr int($lasthloc + 1)]
            close $outf

            set nmrk [cmap nloci]
            set outf [open ghmibd.loc w]
            puts $outf "[expr $nmrk + 1] 0 0 5"
            puts $outf "0 0.0 0.0 0"
            puts -nonewline $outf "1"
            for {set i 2} {$i <= [expr $nmrk + 1]} {incr i} {
                puts -nonewline $outf " $i"
            }
            puts $outf ""
            puts $outf "1  2"
            puts $outf "0.99 0.01"
            puts $outf "1"
            puts $outf "0.001 0.999 0.999"
            for {set i 0} {$i < $nmrk} {incr i} {
                set mrk [lindex $maplist $i]
                set nall [llength $alllist($mrk)]
                puts $outf "3 $nall"
                puts -nonewline $outf [lindex $frqlist($mrk) 0]
                for {set j 1} {$j < $nall} {incr j} {
                    puts -nonewline $outf " [lindex $frqlist($mrk) $j]"
                }
                puts $outf ""
            }
            puts $outf "0 0"
            puts -nonewline $outf "0"
            set lastloc $locn([lindex $maplist 0])
            for {set i 1} {$i < $nmrk} {incr i} {
                set mrk [lindex $maplist $i]
                set x [expr double($locn($mrk) - $lastloc)/100]
                if {$kosambi} {
                    set theta [expr .5*(exp(4*$x) - 1)/(exp(4*$x) + 1)]
                } else {
                    set theta [expr .5*(1 - exp(-2*$x))]
                }
                puts -nonewline $outf " [format %8.6f $theta]"
                set lastloc $locn($mrk)
            }
            puts $outf ""
            puts $outf "1 0.1 0.45"
            close $outf

            if {$byped} {
                set currped 0
            } else {
                set outf [open ghmibd.ped w]
            }
            for {set i 1} {$i <= $nindt} {incr i} {
                if {$mztwin($i) && $i != [lindex $twinid($mztwin($i)) 0]} {
                    continue
                }
                if {$byped && $pedno($i) != $currped} {
                    if {$currped} {
                        close $outf
                    }
                    set currped $pedno($i)
                    set outf [open ghmibd.ped.$currped w]
                }
                set fa $fibdid($i)
                if {$fa != 0 && $mztwin($fa)} {
                    set fa [lindex $twinid($mztwin($fa)) 0]
                }
                set mo $mibdid($i)
                if {$mo != 0 && $mztwin($mo)} {
                    set mo [lindex $twinid($mztwin($mo)) 0]
                }
                puts -nonewline $outf "$pedno($i) $i $fa $mo $sex($i) 2"
                for {set j 0} {$j < $nmrk} {incr j} {
                    puts -nonewline $outf " $gtype($i,[lindex $maplist $j])"
                }
                puts $outf ""
            }
            close $outf

            set outf [open ghmibd.cmd w]
            if {$kosambi} {
                puts $outf "map function kosambi"
            }
            puts $outf "load ghmibd.loc"
            puts $outf "max bits $max_nbits"
            puts $outf "increment distance 1"
            puts $outf "scan ghmibd.ped"
            puts $outf "dump ibd"
            puts $outf "ghmibd.ibd"
            puts $outf "q"
            close $outf

            puts "The following files have been created:"
            puts "    ghmibd.cmd 		- control file"
            puts "    ghmibd.ped 		- pedigree/genotype data"
            puts "    ghmibd.loc 		- locus file"
            puts "    mibdchr$chrnum.loc	- map file for SOLAR plots"
            puts \
"Move the file mibdchr$chrnum.loc to the directory where the GeneHunter-computed IBDs"
            puts "will be stored."

        } elseif {$ibdfmt == "sw"} {
            if {[map func] == "b"} {
                error \
"Map contains basepair locations so can't be used to compute multipoint IBDs."
            }
            set chrnum [cmap chrnum]
            set outf [open mibdchr$chrnum.loc w]
            puts $outf Haldane
            set kosambi 0
            if {[cmap func] == "k"} {
                set kosambi 1
            }
            set maplist ""
            foreach mrk [cmap names] {
               if {[llength $alllist($mrk)] < 2} {
                   puts "Skipping marker $mrk: less than 2 alleles"
               } else {
                   lappend maplist $mrk
               }
            }
            set mrk [lindex $maplist 0]
            if {[lsearch $mrklist $mrk] == -1} {
                close $outf
                error "Marker $mrk is in map but not in marker file."
            }
            set maplist8 [string range $mrk 0 7]
            set locn($mrk) [cmap locn $mrk]
            if {$kosambi} {
                puts $outf "$mrk 0"
                set lasthloc 0
                set lastkloc $locn($mrk)
            } else {
                puts $outf "$mrk $locn($mrk)"
                set lasthloc $locn($mrk)
            }
            for {set i 1} {$i < [llength $maplist]} {incr i} {
                set mrk [lindex $maplist $i]
                if {[lsearch $mrklist $mrk] == -1} {
                    close $outf
                    error "Marker $mrk is in map but not in marker file."
                }
                lappend maplist8 [string range $mrk 0 7]
                set locn($mrk) [cmap locn $mrk]
                if {$kosambi} {
                    set x [expr double($locn($mrk) - $lastkloc)/100]
                    set theta [expr .5*(exp(4*$x) - 1)/(exp(4*$x) + 1)]
                    set hloc [expr (-.5*log(1 - 2*$theta))*100 + $lasthloc]
                    puts $outf "$mrk [format %.2f $hloc]"
                    set lasthloc $hloc
                    set lastkloc $locn($mrk)
                } else {
                    puts $outf "$mrk $locn($mrk)"
                    set lasthloc $locn($mrk)
                }
            }
            set lastpos [expr int($lasthloc)]
            close $outf

            set maplist8 [lsort $maplist8]
            set mrk [lindex $maplist8 0]
            for {set i 1} {$i < [llength $maplist8]} {incr i} {
                if {[lindex $maplist8 $i] == $mrk} {
                    error \
            "For SimWalk2, marker names must be unique in first 8 characters."
                }
                set mrk [lindex $maplist8 $i]
            }

            set outf [open BATCH2.DAT w]
            puts $outf ""
            puts $outf "01"
            puts $outf "4"
            puts $outf ""
            puts $outf "02"
            puts $outf "1"
            puts $outf ""
            puts $outf "03"
            puts $outf "Multipoint IBD Calculation"
            puts $outf ""
            puts $outf "09"
            puts $outf "swmibd.map"
            puts $outf ""
            puts $outf "10"
            puts $outf "swmibd.loc"
            puts $outf ""
            puts $outf "11"
            puts $outf "swmibd.ped"
            puts $outf ""
            puts $outf "12"
            puts $outf "2"
            puts $outf "1"
            puts $outf ""
            puts $outf "13"
            puts $outf "y"
            puts $outf ""
            if {$version == "2.82"} {
                puts $outf "49"
                puts $outf "0"
            } else {
                puts $outf "39"
                puts $outf "ibd"
                puts $outf ""
                puts $outf "49"
                puts $outf "-1"
                puts $outf "1"
            }
            close $outf

            set outf [open swmibd.map w]
            puts $outf AFFECTED
            puts $outf [format "        %8.5f" 0.5]
            puts $outf [lindex $maplist 0]
            set lastloc $locn([lindex $maplist 0])
            for {set i 1} {$i < [llength $maplist]} {incr i} {
                set mrk [lindex $maplist $i]
                set x [expr double($locn($mrk) - $lastloc)/100]
                if {$kosambi} {
                    set theta [expr .5*(exp(4*$x) - 1)/(exp(4*$x) + 1)]
                } else {
                    set theta [expr .5*(1 - exp(-2*$x))]
                }
                puts $outf [format "        %8.5f" $theta]
                puts $outf $mrk
                set lastloc $locn($mrk)
            }
            close $outf

            set outf [open swmibd.loc w]
            puts $outf "AFFECTEDAUTOSOME 2 2"
            puts $outf "       1 0.99990"
            puts $outf "       2 0.00010"
            puts $outf " 1       1"
            puts $outf "  1/  1"
            puts $outf " 2       1"
            puts $outf "  1/  2"
            for {set i 0} {$i < [llength $maplist]} {incr i} {
                set mrk [lindex $maplist $i]
                puts $outf \
                    [format "%-8.8sAUTOSOME%2d 0" $mrk [llength $alllist($mrk)]]
                for {set j 0} {$j < [llength $alllist($mrk)]} {incr j} {
                    puts $outf [format "%5d   %8.5f" [expr $j + 1] \
                                [lindex $frqlist($mrk) $j]]
                }
            }
            close $outf

            if {!$byped} {
                set outf [open swmibd.ped w]
                puts $outf "(I6,2X,A8)"
                puts $outf [format "(3A5,A1,A3,A1,%dA5)" [llength $maplist]]
            }
            set ibdid 0
            for {set i 1} {$i <= $nped} {incr i} {
                if {$byped} {
                    if {$i > 1} {
                        close $outf
                    }
                    set outf [open swmibd.ped.$i w]
                    puts $outf "(I6,2X,A8)"
                    puts $outf [format "(3A5,A1,A3,A1,%dA5)" [llength $maplist]]
                }
                set n $nind($i)
                set ii $ibdid
                for {set j 0} {$j < $n} {incr j} {
                    incr ii
                    if {$mztwin($ii) && \
                            $ii != [lindex $twinid($mztwin($ii)) 0]} {
                        set n [expr $n - 1]
                    }
                }
                puts $outf [format "%6d  FAM%05d" $n $i]
                for {set j 0} {$j < $nind($i)} {incr j} {
                    incr ibdid
                    if {$mztwin($ibdid) && \
                            $ibdid != [lindex $twinid($mztwin($ibdid)) 0]} {
                        continue
                    }
                    set fa ""
                    if {$fibdid($ibdid) != 0} {
                        set fa $fibdid($ibdid)
                        if {$mztwin($fa)} {
                            set fa [lindex $twinid($mztwin($fa)) 0]
                        }
                    }
                    set mo ""
                    if {$mibdid($ibdid) != 0} {
                        set mo $mibdid($ibdid)
                        if {$mztwin($mo)} {
                            set mo [lindex $twinid($mztwin($mo)) 0]
                        }
                    }
                    set twin ""
                    puts -nonewline $outf [format \
                        "%5s%5s%5s%1s%3s2" $ibdid $fa $mo $sex($ibdid) $twin]
                    for {set k 0} {$k < [llength $maplist]} {incr k} {
                        set mrk [lindex $maplist $k]
                        set kk [lsearch $mrklist $mrk]
                        if {[lindex $gtype($ibdid) $kk] == "*"} {
                            set geno ""
                        } else {
                            scan [lindex $gtype($ibdid) $kk] "%d/%d" a1 a2
                            set geno [format "%2d/%2d" $a1 $a2]
                        }
                        puts -nonewline $outf [format "%5s" $geno]
                    }
                    puts $outf ""
                }
            }
            close $outf

            puts "The following files have been created:"
            puts "    BATCH2.DAT		- control file"
            puts "    swmibd.map		- map file"
            puts "    swmibd.loc		- locus file"
            puts "    swmibd.ped		- pedigree/genotype data"
            puts "    mibdchr$chrnum.loc	- map file for SOLAR plots"
            puts \
"Move the file mibdchr$chrnum.loc to the directory where the SimWalk2-computed IBDs"
            puts "will be stored."
        }

        return
    }

    if {[llength $chrlist] && [lindex $chrlist 0] == "relate"} {
        if {$mxnrel == ""} {
            set mxnrel 10
        }
        if {[catch {exec $env(SOLAR_BIN)/relate $mxnrel > relate.out} errmsg]} {
            if {[file exists relate.out]} {
                if {[file size relate.out]} {
#                    exec >&@stdout <@stdin more relate.out
                    set errmsg [exec cat relate.out]
                    delete_files_forcibly mibdrel.ped relate.unk relate.out
                    error $errmsg
                } else {
                    delete_files_forcibly relate.out
                    if {[file exists relate.unk] && [file size relate.unk]} {
                        error \
"Relationships were found that are not included in SOLAR's relative classes table.\
\nSOLAR cannot compute multipoint IBDs for this pedigree structure. A detailed list\
\nof these relationships can be found in the file \"relate.unk\"."
                    }
                    error $errmsg
                }
            } else {
                error $errmsg
            }
        }
        delete_files_forcibly relate.out
        return
    }

    if {[llength $chrlist]} {
# A map file containing cM locations must be loaded for other mibd commands
        map test
        if {[cmap func] == "b"} {
            error \
    "Map contains basepair locations so can't be used to compute multipoint IBDs."
        }
    }

    eval cmibd $args
}


# solar::field --
#
# Purpose:  Allow non-standard user data field names
# 
# Usage:    field                              ; This shows all mappings
#           field <default name> <user name>   ; Create one mapping 
#           field <default name>               ; Show one mapping
#           field <default name> -none         ; Ignore this field (see notes)
#           field <default name> -default      ; Restore full default
#
# Examples: field ID Subject
#           field FA Father
#           field MO Mother
#
# Notes:
#
# The default names are ID, FA, MO, PROBND, MZTWIN, FAMID, SEX, and HHID.
# EGO, SIRE, and DAM are also permitted in place of ID, FA, MO by default.
# However, unlike the default, you can only specify one name to be
# mapped.  However, you can restore the full default for any field
# using the -default argument.
#
# The -none argument declares a field to be ignored whether it exists or
# not.  This is useful, for example, if you want the optional PROBND
# field to be ignored:
#
#     field PROBND -none
# 
# This would signify that there is no PROBND field, i.e. there are no
# probands.  PROBND, MZTWIN, and HHID are optional fields for which the
# -none argument may be used.  Most other fields are mandatory and -none
# cannot be used for them.
# 
# Your field selections are saved for future SOLAR sessions to a file
# named field.info in the working directory.  Once you have entered
# your field selections, you need not enter them again (starting with
# version 2.0.6) when you are running SOLAR from the same working
# directory.  However, if you followed our previous recommendation to
# put field commands in a .solar file, note that the settings in the
# .solar file take precedence over the settings in field.info.
#
# If you would like to remove an entire old set of field assignments,
# you can delete the field.info file BEFORE starting SOLAR.  (SOLAR
# reads the field.info file when starting.)
#
# FAMID field may or may not be required depending on whether your ID's
# are unique in the entire dataset.  If your ID's are unique, you do
# not need FAMID.  However, if your ID's are sequential within each
# family, you need a FAMID field in both your pedigree and phenotypes
# files, otherwise they are ambiguous.  SOLAR now catches this mistake
# when either pedigree or phenotypes files are loaded.
#-

# Procedure to update field.info file

proc field_info_update {internal user} {
    if {[file exists field.info]} {

# Copy old file, but deleting old field assignment, if any

	file rename field.info field.info.being_updated
	set infile [open field.info.being_updated r]
	set outfile [open field.info w]
	while {-1 != [gets $infile line]} {
	    if {"\#" == [string range $line 0 0]} {
		puts $outfile $line
	    } else {
		if {[string tolower $internal] == \
			[string tolower [lindex $line 0]]} {
		} else {
		    puts $outfile $line
		}
	    }
	}

# Add new assignment to file

        if {"-default" != [string tolower $user]} {
	    puts $outfile "$internal $user"
	}
	close $outfile
	close $infile
	file delete field.info.being_updated

    } elseif {"-default" != $user} {
	set outfile [open field.info w]
	puts $outfile "$internal $user"
	close $outfile
    }
    return ""
}

# solar::help
#
# Purpose:  Display instructional messages
#
# Usage:    help                                ; Displays list of commands
#           help <command>                      ; Displays doc for one command
#           help -output <filename>             ; Write list to a file
#           help <command> -output <filename>   ; Write doc to a file
#           help -user                          ; Display user commands
#
# Notes:    See also the doc and usage commands.
#
#           The help for any particular command is quite detailed and may
#           cover several pages.  The usage command provides a very brief
#           summary of command options, and does not invoke the 'more' pager
#           so it will stay on your window while you enter the next command.
#
#           See help for "helpadd" to see how you can add your own help
#           messages to the help system.  To display all the user commands,
#           give the command "help -user".  To display the help for any
#           particular user command, the ordinary "help <command>" will
#           work, because it searches user tcl files if <command> is not
#           found in SOLAR tcl files.
#-

# solar::helpadd
#
# Purpose:  Explain how to add more help messages to SOLAR
#
#           SOLAR first looks for a help message for a particular
#           commands in the main SOLAR tcl package file solar.tcl.
#           Next, it looks for help messages in all *.tcl files in the
#           current tcl search paths:
# 
#               .            (The current working directory)
#               ~/lib        (A lib subdirectory of your home directory)
#               $SOLAR_LIB   (the lib subdirectory of the SOLAR installation.
#                             This symbol is created by SOLAR.)
#
#           All help messages are commented text blocks beginning with a
#           command header and ending with a line containing "#-" or any
#           line with no leading "#".
#
#               # <packagename>::<commandname> [--] [private|public]
#
#           The packagename is taken from the filename: it is the segment
#           of the filename preceeding the first period (.).  For example,
#           the packagename for solar.tcl is "solar" and the packagename
#           for john.solar.tcl is "john".
#
#           Each help block should contain at minimum a purpose and usage
#           section as shown in example below.
#
#           The help display of all commands available is taken from all
#           the commands defined in solar.tcl and other files in $SOLAR_LIB.
#           The "help -user" command shows all commands for which help is
#           available defined in . and ~/lib.  To block display of this
#           command in "help -user" include the word "private" following
#           the command name.
#
#           Example:
#
#           File named multiproc.tcl defines a command named mmultipoint.  The
#           file is placed in ~/lib and has a help header that looks like this:
#
#           # multiproc::mmultipoint -- public
#           #
#           # Purpose:   Spawn multipoint jobs per chromosome
#           #
#           # Usage:     mmultipoint <minlod>
#           #-
#           
#           mmultipoint uses a private procedure mspawn:
#
#           # multiproc::mspawn -- private
#           #
#           # Purpose:   Launch job on cluster machine
#           #
#           # Usage:     mspawn <scriptfile>
#           #-
#-
proc helpadd {} {
	helpscript helpadd
}


# solar::verbosity --
#
# Purpose:  Set the output verbosity.
# 
# Usage:    verbosity default          ;  Set default verbosity
#           verbosity plus             ;  Set above average verbosity
#           verbosity min              ;  Set minimum verbosity
#           verbosity max              ;  Set maximum verbosity (debugging)
#           verbosity                  ;  Display current verbosity
#
#           verbosity -number          ;  Show verbosity hex bit-coded number
#           verbosity <number>         ;  Set verbosity with bit-coded number
#
#  Notes:   During analysis scripts such as polygenic and multipoint, the
#           default verbosity supresses all the usual maximization output
#           (such as you would see with the 'maximize' command run by
#           itself).
#
#           The maximization output can be turned on for analysis scripts
#           using the 'plus' verbosity level.  'plus' is above default,
#           but below 'max.'
#
#           The bit coded numbers used for various verbosity levels are
#           subject to change.  User scripts should use the name (such
#           as 'default') to be consistent with future releases.
#
#           There are now a few reports which are so verbose that they
#           are not even included in verbosity max.  They may be specified
#           by using hex coded numbers (starting with "0x").  These are
#           subject to change in future releases.
#
#               0x4ffff    Max verbosity with per-pedigree likelihoods for
#                          each interation.
#
#           Hex-coded verbosity numbers were not supported prior to version
#           2.0.2.
#-

# solar::key --
#
# Purpose:  Make user key
#
# Usage:    key <make> <username>
# 
# Note:     This is intended for use only by authorized personnel.
#-


# solar::chi --
#
# Purpose:  Compute probability for a chi-square value
# 
# Usage:    chi <value> <degrees>
#           chi -number <value> <degrees>      ; return only the number
#           chi -inverse <pvalue> <degrees>
#
# Notes:    Without the -number argument, the result is returned as a string
#           like this:
#
#           p = 0.0012345
#
#           (The sign will be "<" if below available accuracy.)
#
#           With the -inverse argument, the chi-square value corresponding
#           to a given p-value is returned. The -number argument does not
#           apply when the inverse is computed and should not be given.
#
#           chi will raise an error for certain out of bound conditions
#           You may use a catch {} block to prevent this from ending scripts:
# 
#           set test [catch {set p [chi $val $deg]}]
#           if {$test != 0} {set p 1.0}
# 
#-

# solar::tclgr --
#
# Purpose:  Create xmgr session with pipe connection to SOLAR
# 
#   Note:   This is a low-level plot interface used by other commands.
#           Most users will use the higher level interfaces such as
#           'plot' or 'multipoint -plot.'
#
#   Usage:  tclgr open                          ;# Start xmgr session
#           tclgr send <xmgr command line>      ;# send command and wait now
#           tclgr buffer <xmgr command line>    ;# add xmgr command to buffer
#           tclgr flush                         ;# flush buffer of commands
#           tclgr close                         ;# end xmgr  session
#           tclgr syscommand <syscommand>       ;# Setup sys command for XMGR
#                                               ;#   'xmgr' is the default
#           <XMGR command line>               ;# defined in Ace/gr docs 
#
#  The tclgr open command has a '-buffersize <number>' option.  The default
#  buffersize is 1000.
#
#  If the user closes the XMGR session remotely, the 'tclgr close' command
#  must be used to officially close it before it can be re-opened.
#-

# solar::tablefile --
#
# Purpose:  Read data file in comma delimited or PEDSYS format
# 
# Usage:    set tablenum [tablefile open <filename>]
#           tablefile $tablenum names               ; return field names
#           tablefile $tablenum short_names         ; return short names
#           tablefile $tablenum widths              ; return field widths
#           tablefile $tablenum start_setup         ; start user record
#           tablefile $tablenum setup <name>        ; add field to user record
#           tablefile $tablenum get                 ; get user record
#           tablefile $tablenum rewind              ; rewind file
#           tablefile $tablenum close               ; close file
#           tablefile $tablenum test_name <name>    ; test for named field
#           tablefile $tablenum get_position        ; get current position
#           tablefile $tablenum set_position <pos>  ; set position
# 
# Notes:    Intended for use in scripts.
#
#           The get command will return data elements in a proper list.
#           This means that if a data element includes spaces, it will be
#           enclosed in braces.  For best results, data records should be
#           read using lindex, which removes the braces.
# 
#           On End of File, get will return an empty list.  This should be
#           tested for.  Other file errors will raise Tcl exceptions.
#
# See Also: solarfile
#-

# solar::solarfile --
#
# Purpose:  Read data file applying "field" name mapping
#
# Usage:    Same as tablefile (see) but using "solarfile" command name, plus:
#
#           solarfile $tablenum establish_name <generic-name>
#
#           establish_name returns the actual field name applied or mapped
#           the the generic name.  For example, the generic-name "id" might
#           actually be "ego" in the file, or a name mapped to "id" using
#           the field command.  The generic-names are listed by the field
#           command.
#
# Notes:    Intended for use in scripts.
#
#           This command extends the "tablefile" command by allowing for
#           user-supplied field name mapping using the "field" command.
#           This also supports both default field names for basic identifiers:
#           id,fa,mo and ego,sire,dam.
# -

# solar::scale --
#
# Purpose:  scale a covariate variable, or disable default scaling
#
# Usage:    scale <var> <center>  ; scale this variable to this center value
#           noscale <var>         ; use 0 as center value disabling default
#           scale                 ; show all non-default scaling in effect
#           scale <var>           ; show scaling for this variable
#           scale default <var>   ; return to default scaling for var
#           scale default_all     ; return to default for all vars
#
#           <var>     any covariate variable, might be used in interaction too
#           <center>  real number
#
# Notes:
#
#     By default, SOLAR adjusts all covariate variables to the sample mean.
#     Using the scale command, you can adjust any covariate variable to
#     any other fixed value, or disable adjustment altogether (by adjusting
#     to zero).
#
#     The adjustment applies to the variable whether it appears in a simple
#     covariate (such as "age") or an interaction covariate (such as
#     "age*sex") or both.
#
#     There is currently no way of scaling the trait variable, or scaling
#     any variable by a factor.  Those features could be added in a future
#     release.
#
#     Scaling is saved with the model, and is superceded by whatever scaling
#     is in effect with a new model.
#-

proc noscale {variable} {
    scale $variable 0
}


# solar::solarversion --
#
# Purpose:  Display the SOLAR program version
# 
# Usage:    solarversion
#-

# solar::alnorm --
#
# Purpose:  Evaluates the tail of normal curve
#
# Usage:    alnorm <x> [t | f]
#
#           If "t", curve is evaluated from X to infinity
#           If "f", curve is evaluated from minus infinity to X
#-

#
# End of section documenting commands implemented in C++
#


# solar::ascertainment --
#
# Purpose:  Describe ascertainment correction using proband(s)
#
# Ascertainment correction is available through use of the proband field in
# the phenotypes file.  Ascertainment correction by conditioning on
# probands is automatically performed if there is a field named 'proband',
# 'probnd', or 'prband' (in upper or lower case) in the phenotypes file.  
# Probands are those individuals through whom the pedigree has been 
# ascertained.
#
# In a proband field, blank ( ) or zero (0) signifies non-proband (normal)
# status, and anything else signifies proband status.  No decimal point is
# permitted after the zero.
#
# If your proband field is named something else, the safest approach is to
# modify your data files accordingly.  If that is not possible, you can use
# the SOLAR 'field' command to map your name.  For example, if your proband
# field is actually named 'Affected', you would use the following command:
#
#     solar> field probnd Affected
#
# (Note that the name 'probnd' is used as a field type selector because that
#  is the PEDSYS standard mnemonic.)
#
# For routine use, such a field command should be included in a .solar
# startup file or user script.  Field mappings are not included in model
# files.  For that reason, it may be safest to modify the code or data file
# if possible.
#
# Conversely, if your file has a probnd field but you wish it to be ignored,
# you can rename the probnd field or give a command like the following:
#
#     solar> field probnd -none
#
# Proband individuals are required to have all the quantitative
# variables required of other individuals to be included (as probands) in
# the analysis.  Probands who are missing any quantitative variables are
# not included in the Proband Count and except for defining the pedigree
# structure do not enter into any calculations.
#
# Unlike the program Fisher, SOLAR does not require probands to be at the
# beginning of pedigrees, and does not require you to provide a "proband
# count."  Other than that, SOLAR uses the ascertainment correction
# algorithm built-in to Fisher.
#
# SOLAR prints a Proband Count in the summary statistics, which are written
# to maximization output files and to the terminal unless the verbosity
# level is set low.
# -

proc ascertainment {} {
    return [helpscript ascertainment]
}

# solar::boundary --
#
# Purpose:  Change artificial boundary heuristics
#
# Usage:   boundary                                     ; show settings
#          boundary wide [start|off]                    ; wide boundaries
#          boundary null [on|off]                       ; use boundaries from
#                                                       ;  null model
#          boundary start upper <term> [<term> ...]     ; Initial upper bounds
#            <term> ::== <number> or <number>*h2r
#          boundary float upper <number>                ; Later upper bounds
#          boundary change <number>                     ; Amount to change by
#          boundary crunch <number>                     ; Crunch bounds +/-
#          boundary quadratic tol <number>              ; quadratic tolerance
#          boundary max crunch <number>                 ; Maximum crunches
#          boundary h2r factor <number>                 ; Bound h2r
#          boundary e2 squeeze <number>                 ; Bound e2
#          boundary trace [off]                         ; Trace upper bounds
#          boundary hints                               ; More discussion
#          boundary cov retries <number>                ; Max covar retries
#          boundary cov incr <number>                   ; On each retry,
#                                                       ;   increase cov bound
#                                                       ;   by this factor
# Examples:
#          boundary start upper .2 .1 .05
#          boundary float upper .05
#
# Notes:   To function properly, the maximization algorithm used by SOLAR
#          needs a little bit of help in the form of artificial boundaries.
#          In general, any variance component can assume a any value from
#          0.0-1.0, but in any particular case the range is more limited, and
#          artificially limiting the range helps SOLAR maximize successfully.
#
#          A set of heuristics and retry algorithms has been developed for
#          SOLAR to set and adjust artificial boundaries.  The heuristics
#          should not normally require adjustment.  If they do, please send
#          a message to solar@txbiomedgenetics.org so we can improve SOLAR.
#
#          You will know if you are having trouble with the boundary
#          heuristics becase you will get 'Boundary' or 'Convergence'
#          errors.  Beginning with SOLAR version 1.4.0, you will not get
#          'Boundary' errors for variance components because SOLAR will
#          automatically increase the boundaries up to the theoretic limits
#          (0.0-1.0) as required.  If you get 'Convergence' errors, you
#          should try setting some of the heuristics to lower values than
#          they have already.  In addition to these heuristics, there are
#          now also built-in retry mechanisms which automatically increase
#          bounds if they are found to be too small, or decrease bounds if
#          they are too big (being too being can cause convergence
#          problems).  SOLAR will always discover if bounds are set too
#          small to find the correct result and increase them, but it may
#          not be able to deal automatically with bounds that need to be
#          set very close to the correct result.
#
#          If you get Boundary errors for covariates, you can deal with
#          them in one of two ways.  For one, you can simply set the
#          covariate upper and lower bounds in the starting model to
#          reasonable values using the 'parameter' command and then re-run
#          the analysis.  Or, you can use the 'boundary cov retries' or
#          'boundary cov incr' commands to adjust the covariate boundary
#          retry mechanism (which is separate from the mechanism for
#          variance component boundaries).  Covariate bounds do not
#          have obvious theoretic limits, so it is impossible to
#          automatically prevent all covariate boundary errors.
#
#
#    boundary wide on    ... set wide boundaries for variance components (N/A)
#    boundary wide start ... set wide boundaries at start of chromosome
#    boundary wide off   ... use standard boundary heuristics
#
#          "boundary wide on" causes the boundaries for future linkage
#          models to be set to the full natural range (lower 0 upper 1).
#          This has no effect on the model currently in memory, but will be
#          applied to future linkage models created by the multipoint,
#          twopoint, linkmod, and linkmod2p commands.  This supercedes the
#          standard variance component heuristics ("boundary start upper,"
#          "boundary float upper," "boundary h2r factor," and "boundary e2
#          squeeze") and also sets "boundary null off."  THIS OPTION IS NOT
#          YET AVAILABLE (use "boundary wide start" instead).
#
#          "boundary wide start" causes the boundaries for future linkage
#          models to be set to the full natural range for the first QTL on
#          each chromosome.  After the first QTL, the standard heuristics
#          are applied.  (For twopoint linkage, this is the same as
#          "boundary wide on")
#
#          Both "boundary wide on" and "boundary wide start" turn off
#          "boundary null on," as the options are incompatible.
#
#          "boundary wide off" restores the usual variance component
#          boundary heuristics for future linkage models.  "boundary wide
#          off" does not necessarily restore the exact boundaries
#          previously in use, and it does not restore "boundary null on" if
#          that had previously been in effect.
#
#    boundary null on  ... set boundaries according to null model
#    boundary null off ... back to standard boundary heuristics
#
#          "boundary null on" causes the boundaries for future linkage
#          models to be taken from the current null model.  In pass 1 of
#          multipoint, for example, the boundaries would be taken from
#          null0.mod, and in pass 2, they would be taken from null1.mod.
#          In cases of persistent convergence failure, you can edit the
#          boundaries in the null model and use "multipoint -restart"
#          to attempt to resolve the jam.
#
#          "boundary null on" turns off "boundary wide on" and
#          "boundary wide start," if they had been operative, because the
#          options are incompatible.
#
#          In the case of h2q* parameters not defined in the null model
#          (for example, h2q2 will not be defined in null1.mod, though it
#          is required for all two-linkage models), the default is to
#          use the boundaries for the previous h2q parameter.  SOLAR
#          always defines h2q1 in null0.mod.
#
#         "boundary null off" restores the usual boundary heuristics for
#          future linkage models.  It does not restore "boundary wide start"
#          or "boundary wide on" if those had been in effect previously.
#          
#    boundary start upper ... set upper bound starting point for h2q's
#    boundary float upper ... set upper bound based on previous h2q value
#
#          These commands apply to the upper bounds of h2q* parameters
#          (e.g. h2q1).  The default values are deliberately chosen to
#          be quite low because they are automatically raised as required
#          by a retry mechanism.  If the starting values were set to high,
#          convergence errors could occur, and the mechanism for handling
#          convergence errors is not as robust because it doesn't know which
#          boundaries to adjust.
#
#          'boundary start upper' sets the starting value for the upper
#          bound of each new h2q parameter at the beginning of each
#          chromosome.  This can be set as a single number (0.0 - 1.0) or
#          as a term including 'h2r' (such as 0.8*h2r, which is the default).
#          (H2r will be taken from the preceding null model if one is found.
#          For example, if there is one linkage component, the null model is
#          null0.out, which contains no linkage components.  If there are
#          two linkage components, the null model is null1.out which contains
#          one linkage component.)  Multiple values can be specified, one
#          for each multipoint scan.  The last value specified applies to
#          all remaining scans.  The default value of 0.8*h2r means that
#          the upper bound for each new linkage component is set allowing
#          for 80% of the current residual heritability to be accounted for
#          by the first locus.
#
#          'boundary float upper' sets the value for the upper bound of
#          the newest h2q parameter after the beginning of each chromosome.
#          The upper bound floats above each previously maximized h2q
#          value by this amount, which defaults to 0.1.
#
#    boundary change
#
#          'boundary change' sets the value by which a bound changes
#          after a boundary condition is detected.  Upper bounds will be
#          raised and lower bounds will be lowered by this amount.  The
#          default value is 0.1.
#
#    boundary crunch
#
#          'boundary crunch' sets the boundaries around each variance
#          component if a convergence error occurs and then invokes a retry.
#          The default value is 0.1.  For example, if the previous value for
#          h2r was 0.3, the new boundaries will be set at 0.2 and 0.4.
#          Boundary crunch is only applied after convergence errors, after
#          which the boundaries can expand again through the retry mechanism.
# 
#    boundary max crunch
#
#          'boundary max crunch' sets the limit on the number of crunch
#          attempts for each locus.  Any given crunch may be followed by a
#          series of boundary expansions, so multiple crunches may be
#          required.  The default is 10, to give a large reasonable chance
#          of success (if success is going to be achievable).  Two crunches
#          in a row are never permitted (that would be meaningless).
#
#    boundary quadratic tol
#
#          The normalized quadratic (for quantitative traits only) is
#          normally required to be between 0.999 and 1.001.  For some
#          problems, this is unrealistic.  To change the tolerance to
#          +/- 0.01 (0.99-1.01), you would give the command:
#
#               boundary quadratic tol 0.01
#
#          The allowed range is 0 - 1.
#
#    boundary h2r factor
#
#          'boundary h2r factor' sets an upper bound for h2r based on the h2r
#          value in the null model.  The default value of 1.1 means that
#          h2r is allowed to grow to 1.1x the size it had in the null model.
#          So far as I know, this has never needed adjustment.  In any case,
#          if it is too small, the automatic retry system will handle it.
#
#    boundary e2 squeeze
#
#          'boundary e2 squeeze' sets boundaries for e2 based on the previous
#          e2 value.  The default value of 0.1 means that e2 is allowed to
#          deviate +/- 0.1 from the preceeding value.
#
#    boundary trace [off]
#
#          'boundary trace' enables a trace of the upper bound applied to the
#          newest h2q for each locus, and shows all retries and perturbations.
#          This feature may be shut off with 'boundary trace off'.
#
#    boundary cov retries <integer>
#
#          'boundary cov retries' sets the maximum number of retries during
#          which the  covariate boundaries are increased.  The default is
#          10.
#
#    boundary cov incr <number>
#
#          'boundary cov incr' sets the factor controlling the amount by 
#          which a covariate boundary is increased during a retry.  The
#          default is 5, which results in at least a five-fold increase
#          on each retry.  (The actual increase depends on the difference
#          between both boundaries, and so will be larger than 5 in the
#          beginning.  This is subject to change.)
# -          


proc boundary {args} {
    if {{} == $args} {
	if {[boundarynull]} {
	    puts "boundary null on (supercedes some other heuristics)"
	} else {
	    puts "boundary null off"
	    if {[boundarywide]} {
		puts "boundary wide start"
	    } else {
		puts "boundary wide off"
	    }
	    puts "boundary start upper [h2qsupper]"
	    puts "boundary float upper [h2q_float]"
	    puts "boundary h2r factor [h2rf]"
	    puts "boundary e2 squeeze [e2squeeze]"
	}
	puts "boundary crunch [bcrunch]"
	puts "boundary max crunch [maxcrunch]"
	puts "boundary change [boundary_change]"
	puts "boundary quadratic tol [boundary_quadratic]"
	puts "boundary cov retries [covboundretries]"
	puts "boundary cov incr [covboundincr]"
	if {[trace_boundaries]} {
	    puts "boundary trace on"
	} else {
	    puts "boundary trace off"
	}
	return ""
    }
    set nargs [llength $args]

#   puts "nargs is $nargs"
#   puts "args is $args"

    if {$nargs == 2 && \
	    0!=[string_imatch "null" [lindex $args 0]]} {
	if {[string_imatch on [lindex $args 1]]} {
	    boundarynull -on
	    boundarywide -off
	} elseif {[string_imatch off [lindex $args 1]]} {
	    boundarynull -off
	} else {
	    error "Invalid boundary null option"
	}
	return ""
    }

    if {$nargs == 2 && \
	    0!=[string_imatch "wide" [lindex $args 0]]} {
	if {[string_imatch start [lindex $args 1]]} {
	    boundarywide -on
	    boundarynull -off
	} elseif {[string_imatch off [lindex $args 1]]} {
	    boundarywide -off
	} elseif {[string_imatch on [lindex $args 1]]} {
	    error "Use 'boundary wide start' instead"
	} else {
	    error "Invalid boundary wide option"
	}
	return ""
    }

    if {$nargs >= 3 && \
	    0!=[string_imatch "start" [lindex $args 0]] && \
	    0!=[string_imatch "upper" [lindex $args 1]]} {
	eval h2qsupper [lrange $args 2 end]
	return ""
    }

    if {$nargs == 3 && \
	    0!=[string_imatch "float" [lindex $args 0]] && \
	    0!=[string_imatch "upper" [lindex $args 1]]} {
	h2q_float [lindex $args 2]
	return ""
    }

    if {$nargs == 2 && \
	    0!=[string_imatch "change" [lindex $args 0]]} {
	boundary_change [lindex $args 1]
	return ""
    }

    if {$nargs == 2 && \
	    0!=[string_imatch "quadratic" [lindex $args 0]] && \
	    0!=[string_imatch "tol" [lindex $args 1]]} {
	return [boundary_quadratic]
    }
    if {$nargs == 3 && \
	    0!=[string_imatch "quadratic" [lindex $args 0]] && \
	    0!=[string_imatch "tol" [lindex $args 1]]} {
	boundary_quadratic [lindex $args 2]
	return ""
    }

    if {$nargs == 3 && \
	    0!=[string_imatch "h2r" [lindex $args 0]] && \
	    0!=[string_imatch "factor" [lindex $args 1]]} {
	h2rf [lindex $args 2]
	return ""
    }

    if {$nargs == 3 && \
	    0!=[string_imatch "e2" [lindex $args 0]] && \
	    0!=[string_imatch "squeeze" [lindex $args 1]]} {
	e2squeeze [lindex $args 2]
	return ""
    }

    if {($nargs == 1 || $nargs == 2) && \
	    0!=[string_imatch "trace" [lindex $args 0]]} {
	if {$nargs == 1} {
	    trace_boundaries on
	} else {
	    trace_boundaries [lindex $args 1]
	}
	return ""
    }

    if {$nargs == 1 && \
	    0!=[string_imatch "hints" [lindex $args 0]]} {
	helpscript boundary-notes
	return ""
    }

    if {$nargs == 2 && \
	    0!=[string_imatch "crunch" [lindex $args 0]]} {
	bcrunch [lindex $args 1]
	return ""
    }

    if {$nargs == 3 && \
	    0!=[string_imatch "max" [lindex $args 0]] && \
	    0!=[string_imatch "crunch" [lindex $args 1]]} {
	maxcrunch [lindex $args 2]
	return ""
    }

    if {$nargs == 3 && \
	    0 != [string_imatch "cov" [lindex $args 0]] && \
	    0 != [string_imatch "retries" [lindex $args 1]]} {
	covboundretries [lindex $args 2]
	return ""
    }

    if {$nargs == 3 && \
	    0 != [string_imatch "cov" [lindex $args 0]] && \
	    0 != [string_imatch "incr" [lindex $args 1]]} {
	covboundincr [lindex $args 2]
	return ""
    }

    error "Invalid boundary command"
}

proc boundary-notes {} {
    return [helpscript boundary-notes]
}

# solar::boundary-notes --
#
# Purpose:  Discuss boundary error resolution strategies
#
# This is an extension of the help provided for the 'boundary' command, which
# you should read first.
#
# When convergence errors occur during a multipoint scan, scanning will
# terminate at the end of the scan regardless of whether some LOD scores
# reached criterion levels or not, and a message like the following will be
# displayed on the terminal (and printed to the multipoint.out file):
#
#    *** Exiting because convergence errors occurred in last pass
#
# Also, to the terminal and the applicable output file for the scan, an
# error code will be appended to the end of each line on which an error
# occurred, for example:
#
# chrom 18  loc     0      0.0000    -2203.917  0.022568  0.268372 ConvrgErr
#
# The code "ConvrgErr" indicates that a Convergence Error occurred such
# that it was impossible to find a good maximum likelihood estimation.
#
# Beginning with version 1.4.0, SOLAR now uses a retry mechanism so that
# boundary errors (related to variance components) will not occur.
# Boundaries will be increased incrementally until their theoretic limits
# are reached.  It is still possible that convergence errors might occur,
# and those may be controlled with the boundary command.
#
# Boundary errors related to covariates are also handled with a retry
# mechanism controlled by the 'boundary cov retries' command and the
# 'boundary cov incr' command.  The default values should work in almost
# every case, but it is not possible to say they will always work because
# with covariates there are no theoretic limits.
#
# If convergence errors occur, you should use the 'boundary' command to
# lower the applicable artificial boundary setting heuristic.  For example,
# if the error is at the beginning of a chromosome, you should use the
# 'boundary start upper' command to set a lower value than the default.
#
# All you need to do when these errors occur during a multipoint scan is
# to "restart" the scan after resetting the applicable heuristic.  The
# restart will detect those models for which an error occurred, and
# redo them with the new heuristics.  For example:
#
# solar> boundary start upper 0.1 0.05
# solar> boundary float upper 0.05
# solar> boundary change 0.05
# solar> multipoint 3 -restart
#
# (In earlier releases, you had to edit out the models for which errors
#  occurred in the multipoint1.out file and then restart.  Now SOLAR
#  recognizes models for which errors occurred and will redo them by
#  default.)
#
# -

proc bcrunch {args} {
    if {{} == $args} {
	return [use_global_if_defined Solar_Boundary_Crunch 0.1]
    }
    ensure_float $args
    global Solar_Boundary_Crunch
    set Solar_Boundary_Crunch $args
    return ""
}

proc maxcrunch {args} {
    if {{} == $args} {
	return [use_global_if_defined Solar_Boundary_Max_Crunch 2]
    }
    ensure_float $args
    global Solar_Boundary_Max_Crunch
    set Solar_Boundary_Max_Crunch $args
    return ""
}

# solar::boundarynull -- private
#
# Purpose: Wide open boundaries for all variance components
#
# Usage:   boundarynull -on      ; Set boundaries wide open for future
#                                ;   linkage models
#          boundarynull -off     ; Back to regular heuristics
#          boundarynull          ; Show current status
#          boundarynull -apply   ; Apply current setting (ON or OFF) to
#                                ; current model, which is presumed to be
#                                ; a linkage model (else you are in trouble)
#
# Note: This is applied to linkage models created during multipoint and
#       twopoint.
# -


proc boundarynull {args} {

    global Solar_Boundary_Null  ;# Global reserved for boundarynull

# Read arguments

    set on 0
    set off 0
    set apply 0
    set badargs [read_arglist $args -on {set on 1} -off {set off 1} \
	    -apply {set apply 1}]
    if {"" != $badargs} {
	error "boundarynull: invalid argument: $badargs"
    }
    if {$off && $apply} {
	error "boundarynull: -off and -apply may not be combined"
    }
    if {$off && $on} {
	error "boundarynull: -off and -on may not be combined"
    }

# Turn off boundarynull state and return

    if {$off} {
	set Solar_Boundary_Null 0
	return 0
    }

# Turn on boundarynull state

    if {$on} {
	set Solar_Boundary_Null 1
    }

    set current_status [use_global_if_defined Solar_Boundary_Null 0]

# Apply boundarynull to current model

    if {$apply && $current_status} {
	set vclist [get_variance_components]

# Get name of applicable null model

	set nullnumber [h2qcount]
	if {$nullnumber > 0} {
	    incr nullnumber -1
	}
	set nullname null$nullnumber

# Apply to each variance component parameter

	set last_h2q_lower 100
	set last_h2q_upper -100

	foreach vc $vclist {

# Use null values only if they can be found and are workable

	    set lower [parameter $vc lower]
	    set upper [parameter $vc upper]
	    set value [parameter $vc =]
	    set newlower 100
	    set newupper -100
	    catch {set newlower [oldmodel $nullname $vc -lower]}
	    catch {set newupper [oldmodel $nullname $vc -upper]}

# If this is an H2Q* parameter, and we couldn't read its value,
#   use previously saved H2*

	    if {[string_imatch h2 [string range $vc 0 1]]} {
		if {$newlower == 100} {
		    set newlower $last_h2q_lower
		}
		if {$newupper == -100} {
		    set newupper $last_h2q_upper
		}

# If this is an H2Q* parameter, and it has a value, save it for next time

		if {$newlower != 100} {
		    set last_h2q_lower $newlower
		}
		if {$newupper != -100} {
		    set last_h2q_upper $newupper
		}
	    }

	    if {$value > $newlower && $newlower >= 0.0} {
		set lower $newlower
	    }
	    if {$value < $newupper} {
		set upper $newupper
	    }
	    parameter $vc lower $lower upper $upper
	}
    }

# Return current status

    return $current_status
}



# solar::boundarywide -- private
#
# Purpose: Wide open boundaries for all variance components
#
# Usage:   boundarywide -on      ; Set boundaries wide open for future
#                                ;   linkage models
#          boundarywide -off     ; Back to regular heuristics
#          boundarywide          ; Show current status
#          boundarywide -apply   ; Apply current setting (ON or OFF) to
#                                ; current model, which is presumed to be
#                                ; a linkage model (else you are in trouble)
#
# Note: This is applied to linkage models created during multipoint and
#       twopoint.
# -


proc boundarywide {args} {

    global Solar_Boundary_Wide  ;# Global reserved for boundarywide

# Read arguments

    set on 0
    set off 0
    set apply 0
    set badargs [read_arglist $args -on {set on 1} -off {set off 1} \
	    -apply {set apply 1}]
    if {"" != $badargs} {
	error "boundarywide: invalid argument: $badargs"
    }
    if {$off && $apply} {
	error "boundarywide: -off and -apply may not be combined"
    }
    if {$off && $on} {
	error "boundarywide: -off and -on may not be combined"
    }

# Turn off boundarywide state and return

    if {$off} {
	set Solar_Boundary_Wide 0
	return 0
    }

# Turn on boundarywide state

    if {$on} {
	set Solar_Boundary_Wide 1
    }

    set current_status [use_global_if_defined Solar_Boundary_Wide 0]

# Apply boundarywide to current model

    if {$apply && $current_status} {
	set vclist [get_variance_components]
	foreach vc $vclist {
	    parameter $vc lower 0 upper 1
	}
    }

# Return current status

    return $current_status
}


# Return active variance components in a list
#
# Depends on h2qcount, check_house, and check_epistasis
#
# Usage: get_variance_components [traits]
#
proc get_variance_components {args} {

    if {"" == $args} {
	set traits [trait]
    } else {
	set traits $args
    }
    set h2qindex [h2qcount]
    set ifhouse [check_house]
    set ifepistasis [check_epistasis]

    set parvector ""
    foreach t $traits {
	set suffix ""
	if {1 < [llength [trait]]} {
	    set suffix \($t\)
	}
	if {[if_parameter_exists e2$suffix]} {
	    lappend parvector e2$suffix
	}
	if {[if_parameter_exists h2r$suffix]} {
	    lappend parvector h2r$suffix
	}
	for {set i 1} {$i <= $h2qindex} {incr i} {
	    lappend parvector h2q$i$suffix
	}
	if {$ifhouse} {
	    lappend parvector c2$suffix
	}
	if {$ifepistasis} {
	    lappend parvector h2qe1$suffix
	}
    }
    return $parvector
}
    


# solar::e2lower -- private
#
# Purpose:  Set lower bound for e2 in analyses (prevent h2r going to 1.0)
# 
# Usage:    e2lower <number>
#
# Notes:    The default value is 0.03, which prevents certain convergence
#           problems, notably h2r going to nearly 1.0.  There are other
#           mechanisms (e2squeeze, soft_lower_bound, and the maximize
#           retry mechanism) at play here too.
#
#           Prior to version 2.0.5, the default value was 0.01, but it was
#           found this did not correct a tendency in some cases for h2r
#           to improperly go to .99...
# -

proc e2lower {args} {
    if {{} == $args} {
	if {0 == [llength [info globals Solar_E2_Lower_Bound]]} {
	    return 0.03
	}
	global Solar_E2_Lower_Bound
	return "$Solar_E2_Lower_Bound"
    }
    global Solar_E2_Lower_Bound
    ensure_float $args
    if {$args < 0 || $args >= 1} {
	error "Invalid number for e2squeeze, must be >=0 and <1"
    }
    set Solar_E2_Lower_Bound $args
    return ""
}

# solar::h2rf -- private
#
# Purpose:  Set factor for limiting upper bound for h2r
# 
# Usage:    h2rf <number>
#
# THIS COMMAND IS NOW OBSOLESCENT.  See 'help boundary' for the new
# commands.
#
# Notes: This number, when multiplied by the null model's h2r, gives
#        upper bounds for h2r in linkage models.  This may aid convergence
#        in some cases.
#
#        There is a related factor, h2qf, for h2q's.
#
#        The default value of 1.1 means that h2r is only allowed to
#        allowed to grow to a number 10% higher than the value of h2r
#        in the null0 model, (or absolute 1-[e2lower], whichever is lower).
#
#        To allow h2* to grow to the normal bound of 1, specify a value of 0.
#
#        This command may be removed when better convergence control
#        techniques are developed.
# -

proc h2rf {args} {
    if {$args == {}} {
	if {0 == [llength [info globals Solar_H2R_Upper_Factor]]} {
	    return 1.1
	}
	global Solar_H2R_Upper_Factor
	return "$Solar_H2R_Upper_Factor"
    }
    ensure_float $args
    if {$args < 0} {
	error "Invalid h2rf"
    }
    global Solar_H2R_Upper_Factor
    set Solar_H2R_Upper_Factor $args
    return ""
}


proc trace_boundaries {args} {
    global Solar_Trace_Boundaries
    if {[string_imatch $args on] || [string_imatch $args 1]} {
	set Solar_Trace_Boundaries 1
	return ""
    }
    if {[string_imatch $args off] || [string_imatch $args 0]} {
	set Solar_Trace_Boundaries 0
	return ""
    }
    if {{} == $args}  {
	return [use_global_if_defined Solar_Trace_Boundaries 0]
    }
    error "trace_boundaries: Invalid argument"
}

proc countmax {} {
    global SOLAR_Maximizes
    set SOLAR_Maximizes 0
    return ""
}

proc h2qsupper {args} {
    set getindex 0
    set startlist [read_arglist $args \
	    -get getindex]

# Get value, applying null h2r if specified

    if {$getindex > 0} {
	set hlist [h2qsupper]
	set hindex1 [lowest [llength $hlist] $getindex]
	set hindex0 [expr $hindex1 - 1]
	set term [lindex $hlist $hindex0]
	if {-1!=[set tpos [string first * $term]]} {
	    set number [string range $term 0 [expr $tpos-1]]

# If null[getindex-1] h2r not available, we get highest nulln h2r that is
# Otherwise default to 0.8

	    set nullh2r 0.8
	    for {set i [expr $getindex-1]} {$i >= 0} {set i [expr $i - 1]} {
		if {0==[catch {set nullh2r [nulln [expr $i] h2r]}]} break
	    }

# Don't let calculated value exceed 0.98 or drop lower than 0.1

	    set bound [highest 0.1 [lowest 0.98 [expr $number * $nullh2r]]]
	    return $bound
	} else {
	    return [lowest 1.0 $term]
	}
    }

# Set value

    if {[llength $args]} {
	set use_h2r 0
	set terms {}
	foreach term $args {
	    if {-1!=[set tpos [string first * $term]]} {
		set use_h2r 1
		set prefix [string range $term 0 [expr $tpos-1]]
		set suffix [string range $term [expr $tpos+1] end]
		if {[string_imatch $prefix h2r]} {
		    set number $suffix
		} elseif {[string_imatch $suffix h2r]} {
		    set number $prefix
		} else {
		    error "Invalid term $term; only h2r is permitted"
		}
	    } else {
		set number $term
	    }
	    if {[catch {ensure_float $number}]} {
		error "Invalid term $term"
	    }
	    if {$use_h2r} {
		set term "$number*h2r"
	    }
	    lappend terms $term
	}
	global Solar_H2q_Start_Upper_Bounds
	set Solar_H2q_Start_Upper_Bounds $terms
	return ""
    }

# Echo current list to user or proc

    return [use_global_if_defined Solar_H2q_Start_Upper_Bounds 0.8*h2r]
}



# solar::h2qf -- private
#
# Purpose:  THIS IS NOW OBSOLETE.  See 'help boundary'
#
# Usage:    To avoid breaking scripts, this command is silently ignored.
# 
# -

proc h2qf_i {index} {
    set harray [h2qf]
    if {[llength $harray] <= $index} {
	set i [expr [llength $harray] - 1]
    } else {
	set i [expr $index - 1]
    }
    return [lindex $harray $i]
}

proc h2qf {args} {
    if {$args == {}} {
	if {0 == [llength [info globals Solar_H2Q_Upper_Factor]]} {
	    return 1.25
	}
	global Solar_H2Q_Upper_Factor
	return "$Solar_H2Q_Upper_Factor"
    }
    foreach arg $args {
	ensure_float $arg
	if {$arg < 0} {
	    error "Invalid h2qf factor $arg"
	}
    }
    global Solar_H2Q_Upper_Factor
    set Solar_H2Q_Upper_Factor $args
    return ""
}

proc h2q_float {args} {
    global Solar_H2Q_FLOAT
    if {{} == $args} {
	if {![if_global_exists Solar_H2Q_FLOAT]} {
	    return 0.1
	} else {
	    return $Solar_H2Q_FLOAT
	}
    }
    ensure_float $args
    set Solar_H2Q_FLOAT $args
    return ""
}

proc boundary_change {args} {
    global Solar_Boundary_Change
    if {{} == $args} {
	if {![if_global_exists Solar_Boundary_Change]} {
	    return 0.1
	} else {
	    return $Solar_Boundary_Change
	}
    }
    ensure_float $args
    set Solar_Boundary_Change $args
    return ""
}

proc boundary_quadratic {args} {
    global Solar_Boundary_Quadratic
    if {{} == $args} {
	if {![if_global_exists Solar_Boundary_Quadratic]} {
	    return 0.001
	} else {
	    return $Solar_Boundary_Quadratic
	}
    }
    ensure_float $args
    if {$args > 1.0 || $args < 0.0} {
	error "boundary quadratic tol must be between 0 and 1"
    }
    set Solar_Boundary_Quadratic $args
    return ""
}

proc covboundretries {args} {
    global Solar_Cov_Boundary_Retries
    if {{} == $args} {
	if {![if_global_exists Solar_Cov_Boundary_Retries]} {
	    return 10
	} else {
	    return $Solar_Cov_Boundary_Retries
	}
    }
    ensure_integer $args
    set Solar_Cov_Boundary_Retries $args
    return ""
}

proc covboundincr {args} {
    global Solar_Cov_Boundary_Incr
    if {{} == $args} {
	if {![if_global_exists Solar_Cov_Boundary_Incr]} {
	    return 5
	} else {
	    return $Solar_Cov_Boundary_Incr
	}
    }
    ensure_float $args
    if {$args < 1.0} {
	error "Increase factor must be greater than 1"
    }
    set Solar_Cov_Boundary_Incr $args
    return ""
}

	

# solar::updatechanges -- private
#
# Purpose:  Generate manual Appendix 4 (Change Notes)
#
# Notes:  filename is 94.appendix_4.html
#         also creates README.news with same content
#
# See also updatedoc
#
#-

proc updatechanges {} {
    set outfile [help-on-command change-notes-data]
    set headerfile [open appendix_header w]
    puts $headerfile "<html>"
    puts $headerfile "<head>"
    puts $headerfile "<title>Solar Manual Appendix 4</title>"
    puts $headerfile "</head>"
    puts $headerfile "<body bgcolor=ffffff text=#000000>"
    puts $headerfile "<a href=index.html>Go to Main Index</a>"
    puts $headerfile "<br><a href=00.contents.html>Go to Table of Contents</a>"
    puts $headerfile "<h1><center>Appendix 4</center></h1>"
    puts $headerfile "<h1><center>Summary of Changes to SOLAR</center></h1>"
    puts $headerfile "<p>"
    puts $headerfile ""
    puts $headerfile "This appendix is a concatenation of all the change notes from the"
    puts $headerfile "<b>SOLAR</b> change-notes command.  More recent changes appear earlier."
    puts $headerfile "</p>"
    puts $headerfile "<pre>"
    puts $headerfile ""
    close $headerfile
    set trailerfile [open appendix_trailer w]
    puts $trailerfile "</pre>"
    puts $trailerfile "</body>"
    puts $trailerfile "</html>"
    exec cat appendix_header $outfile appendix_trailer >94.appendix_4.html
    exec mv $outfile README.news
    delete_files_forcibly appendix_header
}

# solar::change-notes --
#
# Purpose:  Display recent changes
#
# Usage:    change-notes
#-

# solar::change-notes-data-- private
#
# ****   New in version 8.1.1
#
# 1. Speed improvements in all SOLAR operations thanks to improved
#    compiler optimization.  Polygenic runs 1.73 times faster compared
#    with previous official version 7.6.4.  The biggest improvement is
#    in C++ intensive mathmatrix operations based on Eigen; as a
#    result of this and other changes "fphi -p" is now 25 times faster
#    than in version 8.0.6.
#
# 2. The implentation of fphi p value estimation, working since 8.0.6,
#    is made cleaner and slightly faster.  The use of a new mathmatrix
#    operation PermuteY which eliminates the need to create an
#    intermediate shuffled vector for each column of the permuted
#    matrix (as had been done in 8.0.6).  Far fewer temporary matrices
#    are created.  These changes had only a small effect on speed
#    compared with the improved compiler optimizations.
#
# 3. fphi p value estimation now uses better random numbers to shuffle the
#    Y matrix: Mersenne Twister mt19937 which has a period of 2^19937.
#    AFAIK this is the best quality fast random generator available, and
#    it is included in the recent C++11 standard so is widely available.
#
# 4. fphi p value calculation numerator was off by one.  Now p values
#    from "fphi -p" are slightly smaller (better).
#
# 5. Other changes to fphi: Tcl code is cleaned up with much junk removed,
#    variables given meaningful names, obsolete -method argument
#    removed, -testonly is now called -indicator.
#
# 6. option ShuffleReseeding controls how the mt19937 random number
#    generator used for matrix shuffling is seeded.  The default value
#    is 1 which means the generator is reseeded to the default value
#    5489u at the beginning of each matrix shuffle operation.  This
#    gives consistent p value results every time, and the most
#    comparable results when the same sample is used across a range of
#    models.  The options are:
#
#     1...seeded on every shuffle to 5489u for consistent results (DEFAULT)
#     0...seeded first shuffle to 5489u, then free running
#    -1...seeded every shuffle to time() for purely stochastic results
#    -2...seeded first shuffle to time(), then free running
#     Other values: seed to this value at beginning of each shuffle
#
# 7. Errors in test models in mga (either the regular test model or the
#    interaction test model) would cause mga to exit.  Now such errors are
#    reported to the screen and a line with SNP name but blank results is
#    written to the output file.
#
# 8. option MatrixNumberFormat controls the formatting precision of numbers
#    output by matrix operations such as "show".  The default of 15 eliminates
#    long unrounded numbers such as 0.4999999999999999.
#
# 9. This version is the first ever to have a public source code
#    release, along with the usual binary releases for Linux, Mac, and
#    Microsoft Windows.  The binary releases are fully tested and
#    replace Official Version 7.6.4 on the general download links at
#    http://solar.txbiomedgenetics.org.  Due to the now distributed
#    nature of SOLAR Eclipse development, this release is not called
#    Official but instead General as in intended-for-general-use.  An
#    Official version with coordinated updates from all developers may
#    be available soon.
#
# **** Skipped versions 8.0.7 to 8.1.0
#
# These versions were experiments in how to increase speed of fphi p
# value estimation.  Sadly it is far easier to make it slower than
# faster, and so relatively code changes were chosen for 8.1.1 that
# help a bit but don't attempt to tackle the biggest bottleneck.  The
# biggest bottleneck is the big matrix multiply with dimensions (nS x
# nS) times (nS x nP).  For typical 1000 person sample and 5000 nP,
# this would require 15 billion floating point calculations.  Eigen
# already optimizes this by a factor of 5.  Additional optimizations
# specially applicable to a permuted matrix could also improve
# performance by a factor of 5.  But to be better than Eigen already
# is, we would need to combine their optimization with the special
# ones, which is very difficult.  So until that can be done we are
# just letting Eigen optimize the multiplication as it already does.
#
# ****   New in version 8.0.6
#
# 1.  fphi -p has been corrected and appears to give reasonable p
#     values, BUT this is still Highly Experimental. (1) Speed is an
#     issue here, but 95% of the time is spent doing a single large
#     matrix multiplication in C++/Eigen.  This will be addressed with
#     a virtual-shuffle-matrix-multiplication in the next version(s),
#     and also by adding a completely different implementation of fphi
#     to SOLAR.  (2) We are also concerned about the p values possibly
#     not being accurate, and it is true they do not match another
#     implementation of fphi.  We are continuing to investigate.  The
#     h2r values in both implementations match and we believe they are
#     "correct" as far as an fphi approximation can do.
#
# 2.  MathMatrix objects (such as used by fphi) can be written to files
#     using the new "output" command. Matrices are written without csv header.
#
#         output $m <filename>        ;# write out matrix as csv file
#
#     This can be combined with "evdout" to write out the X, Y, and Z
#     matrices computed by SOLAR EVD:
#
#         model new
#         trait q4
#         covar age
#         evdout
#         set X [evdinx]
#         set Y [evdiny]
#         set Z [evdinz]
#         output $X Xmatrix.csv
#         output $Y Ymatrix.csv
#         output $Z Zmatrix.csv
#         
# 3.  evdinz command to obtain Z matrix from evddata.out.
#
# 4.  shuffle command added which can shuffle a vector into a vector,
#     or shuffle a vector into n columns of a matrix (MathMatrix).
#
#         shuffle $v         ;# shuffle the elements of vector v
#         shuffle $v n       ;# shuffle the elements of vector v into n-1 cols
#                            ;# retaining first column unshuffled
#
# 5.  identity <rows> creates an identity matrix with the specified number
#     of rows.
#
# 6.  "mathmatrix debug" shows lots of info and especially prints before and
#     after each multiplication.  "fphi -debug -p" turns on this feature to
#     highlight the time taken by multiplication of the shuffled matrix.
#.  
# ****   New in version 8.0.5
#
# 1.  fphi now computes p value for h2r (but not yet correct in 8.0.5).
#
# 2.  concatenate command concatenates two matrices, either vertically
#     or horizontally.
#
# 3.  power command still works as before to perform power calculations, 
#     but has been extended to also do elementwise power (square, cube,
#     inverse, etc) operations on MathMatrix objects.  This occurs if the
#     first argument to the power command is a MathMatrix, the second
#     argument will then be interpreted as the power to which each element
#     of the matrix should individually be raised or lowered, for example:
#
#         power $M 2
#
# 4.  max command has been extended to perform an elementwise maxing operation
#     which returns a matrix with each element either the original value
#     the second argument to the command.  For example the following command
#     would truncate all the negative values in a matrix to zero:
#
#         max $M 0
#
#     If there is no element in the matrix which needs to be changed, the
#     original matrix is returned.  Otherwise a new matrix is created.
#
# 5.  Documented traditional sample matrix format better for "help matrix"
#
# ****   New in version 8.0.4
#
# 1.  fphi is now producing reasonable approximations of heritability.
#     Several implementation errors from 8.0.3 had to be corrected to make
#     this possible, and the procedure now uses "Method 1" consistently.
#     See "help fphi" for more information on FPHI methods 1 and 2.
#
# 2.  "option ResetRandom 1" will reset the random number sequence used during
#     maximization to its initial starting value at the beginning of
#     maximization for the current model.  This is for experimental use in
#     solving convergence inconsistencies.
#
# 3.  fphi has a -method2 option which attempts to use "Method 2".  This
#     does not appear to be working yet.
#
# 4.  The first value returned by fphi is now correctly labeled an
#     "indicator variable."  The actual test-statistic is still being
#     implemented.  The primary significance of the indicator variable is
#     that heritability estimation is impossible if it is non-positive.
#
# 5.  evdinx now loads the X matrix without a leading column of 1's as
#     as required for Method 1.  To get the column of 1's as required for
#     Method 2, give the command "evdinx -method2", or simply use the
#     command "fphi -method2" which takes care of this.
#
# 6.  fphi now allows the direct specification of X, Y, and Z matrices.
#     If all three are specified, it is not necessary to specify trait and
#     covariates, and "evdout" is not invoked.
#
# ****   New in version 8.0.3
#
# 1.  New procedure fphi calculates a test statistic and fast estimated h2r.
#     The model trait and covariates must have already been chosen.  If the
#     -testonly option is specified, only the test-statistic is returned and
#     h2r is not estimated.
#
# 2.  Matrix operations "ols" and "solve" now have X and Y arguments reversed
#     as is common practice (octave, matlab, R). So now the commands are
#     "ols y x" and "solve y x" corresponding to the standard description
#     "regress y on x" where Y is the dependent variable and X is the design
#     matrix.
#
# 3.  The diagonal command now does one of two things: it returns a vector
#     when given a rectangular matrix, or (new) it returns a "diagonal
#     matrix" when given a vector.  A "diagonal matrix" is a square matrix
#     with non-zero values only on the diagonal.  A "rectangular matrix" has
#     more than 1 rows and more than 1 columns.  A vector has either 1 row or
#     1 column.  SOLAR does not keep track of which matrices are diagonal.
#
# 4.  New dinverse command does a fast matrix inversion on a diagonal matrix.
#     The matrix must be a diagonal matrix and dinverse does not check this.
#
# 5.  The plus and minus commands now permit one or two scalar arguments
#     so you can add or subtract: 1) matrix and matrix, 2) scalar and matrix,
#     and 3) scalar and scalar.  The times command already allowed scalars like
#     this.
#     
# ****   New in version 8.0.2
#
# 1.  evdout is entirely redesigned to output the just the EVD transformed
#     variables, unless the -evectors option is used.  The transformed variables
#     are writted to evddata.out.
#
# 2.  New procedures evdinx (reads in X matrix), evdiny (reads in Y matrix), and
#     evdinev (reads in eigenvectors matrix) added to read EVD data written
#     by evdout into matrixes.
#
# 3.  evdmat is obsoleted.  Use evdout then evdinx, evdiny, and/or evdinev.
#     Memory-to-matrix commands evdmatx, evdmaty, and evdmatev were designed
#     but not finished in time for version 8.0.2.
#
# ****   New in version 8.0.1
#
# 1.  Interactive and scriptable matrix algebra is now supported.  Commands
#     include new, load, show, row, col, diagonal, rows, cols, times, plus
#     minus, transpose, inverse, ols, solve, evalues, evectors, mean, min
#     and max.  See "help mathmatrix" for details.  MathMatrix objects are
#     different from the sample relationship matrixes used during maximization.
#     They are implemented in C++ using Eigen.
#
# 2.  evdout will output phi2 eigenvectors and eigenvalues as
#     used by SOLAR for EVD2 maximization.  See 'help evdout' for details
#     on evdout, evdin, and evdmat.
#
# 3.  evdin will read in the matrix file(s) created by evdout and
#     create MathMatrix object(s) representing them.
#
# 4.  evdmat creates MathMatrix object(s) directly from the
#     current phi2 eigenvectors without writing or reading a file.  
#     See  'help evdmat' for details.
#     
# 5.  zscoring using a command like "define zt = zscore_trait" generated
#     spurious messages about zscores being deleted.  These messages had no
#     useful meaning and have been removed.  The zscoring feature works fine.
#
# ****   New in version 8.0.0
#
# 1.  Version 8.0.0 is compiled with 64 bit memory model (linux releases
#     only) to enable handling larger numbers of traits, parameters, and/or
#     individuals per pedigree.  However, memory beyond 2 Gb can not usually
#     be allocated because contiguous memory is currently required.  That may
#     be addressed in future updates.  However 64 bit compilation is a
#     necessary first step, it sometimes helps, and also failed memory
#     allocation is now usually reported with an error message rather than
#     causing an unexplained crash as usually happened before.
#     Maximization is about 2% faster for all model types.
#
# ****   New in version 7.6.6
#
# 1.  polygenic -residinor fixed, was mistakenly using previous null output.
#
# ****   New in version 7.6.5
#
# 1.  polygenic -residinor computes a residual trait from the final
#     sporadic model and inormalizes it as the final trait to determine
#     heritability.
#
# 2.  restore_phen restores the original phenotypes file after running
#     polygenic -residinor
#
# ****   New in version 7.6.4
#
# 1.  residual now works correctly with long SNP names (snp_*) when the
#     snp variable only has two consecutive values.
#
# 2.  mga now permits having snp covariates in the null model while defaulting
#     the list of snps to all those in the phenotypes files.  Previously this
#     would cause either the snps to be double included or erronously removed.
#     Previously users were expected to use -snps or -snplists options to
#     restrict the list of snps to be tested to those not in the null model in
#     cases like this, but that requirement was too easily overlooked.
#
# ****   New in version 7.6.3
#
# 1.    p value produced by polygenic for C2 parameter no longer truncated
#       at 0.0000001 due to obsolete formatting code.
#
# ****   New in version 7.6.2
#
# 1.    Additional results have been added to mga.out:
#           est_maf (estimated minor allele frequency: mean/2)
#           est_mac (estimated minor allele copies: mean*samplesize)
#           dosage_sd (SD of the SNP dosage variable)
#           
# ****   New in version 7.6.1
#
# 1.    The formula used to compute standard errors for mga has been
#       corrected.  The correct formula is sqrt($beta*$beta/$chi).
#       The variable "chi" is actually "chi-square" and does not
#       need to be squared.
#
# ****   New in version 7.6.0
#
# 1.   vcfselect extracts genotype per sample data for a single SNP from
#      vcf files.  vcfinfo extracts genotype information, with or
#      without the per sample data.  These are very experimental and
#      feedback is requested.
#
# 2.   32000 mztwins and 32000 individuals now supported (was 20000 mztwins).
#
# 3.   400 simultaneous traits now permitted (was 20).  Standard polygenic
#      models with >50 traits may cause memory exhaustion during maximization
#      due to the large number of rho*_ij parameters (ntraits*ntraits).  This
#      memory exhaustion problem will be addressed in future versions.  With
#      standard parameterization, memory required during maximization is a
#      cubic function of the number of traits multiplied by the square of the
#      size of largest family.
#
# 4.   Memory leak related to use of the zscore_ operator in defined
#      expressions now fixed.
#
# 5.   Now linked with 2014 update to imaging libraries.
#
# 6.   Improperly formatted mibd file names now cause error message rather
#      than crashing multipoint with no explanation, or skipping name.
#
# 7.   fakedata generates large pedigree/phenotypes files with small
#      families and random data.
#
# ****   New in version 7.5.9
#
# 1.   Up to 20000 mztwins are now supported.
#
# 2.   "load pedigree" will use 5 columns for the mztwin id in the pedindex
#      if more than 999 mztwin groups are found in the pedigree.
#      Otherwise, it will use only 3 columns, to ensure compatibility with
#      previous checksums stored in matrixes.
# 
# ****   New in version 7.5.8
#
# 1.   Up to 15000 mztwins are now supported (7500 pairs of twins).
#
# 2.   CSV matrixes can now be loaded even if they have ID's not in the
#      current pedigree.  Warnings are displayed and written to a file
#      named matrix.load.err, but in the end the matrix is loaded and
#      may be useable.
#
# ****   New in version 7.5.7
#
# 1.   Commands "house" and "polygenic" now preserve an existing loaded
#      house matrix filename and options.  So they save the new "-sample"
#      and "-allow" options.
#
# ****   New in version 7.5.6
#
# 1.   Matrix options -allow and -sample are saved in model files, as
#      needed for correct operation with many commands.
#
# ****   New in version 7.5.5
#
# 1.   Matrixes are now checked for completeness during maximization.  All
#      individuals in the sample must have at least a diagonal matrix entry.
#      If not, an error occurs and the missing individuals are printed. There
#      are two options to modify this.  "load matrix -allow" permits missing
#      individuals and defaults their diagonal to 1.0.  "load matrix -sample"
#      removes the individuals from the sample, and a count of individuals
#      removed for not being in matrix is written to the maximization output
#      file.
#
# 2.   Documentation for CSV matrix files is added to "help matrix".
#
# 3.   mga -ixsnp now computes SE's for the bIX by default.  Previously
#      they could only be estimated if the now obsolescent -evdse or -slowse
#      options were used.
#
# ****   New in version 7.5.4
#
# 1.   stats now has -sample option, which calculates stats for the current
#      model sample.
#
# ****   New in version 7.5.3
#
# 1.   SOLAR is now called SOLAR Eclipse in the startup message, in honor of
#      its use in imaging research where the Eclipse name is used.
#
# 2.   A private option is added to the key command to help determine
#      the linux system compatibility on different linux distributions.
#      This is intended to make the SOLAR installer more reliably select
#      the best binary version on a particular system.
#
# ****   New in version 7.5.2
#
# 1.  "matrix debug" now reads matrixes in memory, rather than relying on
#     statistics accumulated as the matrix file was being read.  This way it
#     can find values that are defaulted to 0 or -1.  Matrixes are traversed
#     from 1,1 to max,max.
#
# ****   New in version 7.5.1
#
# 1.  "matrix debug" now shows the minimum and maximum values for both
#     on and off the diagonal.  For each such minimum or maximum, it shows
#     the first pair of IBDID's found having that value.
#
# ****   New in version 7.5.0
#
# 1.  Matrices can now be read in CSV format, using user ID (not pedindex)
#     as the index.  If FAMID is required to disambiguate ID's, famid
#     should be included for each individual (famid1, famid2) since not
#     all matrices are limited to family interactions.  The required fields
#     are id1,id2,matrix1.  The optional fields are matrix2,famid1,famid2.
#     All other fields in a csv matrix file are ignored.  The mapping from
#     ID to pedindex is obtained from the currently loaded pedigree
#     pedindex.out.
#
# 2.  CSV matrices must be gzipped just like original format matrixes and
#     have filenames ending in ".gz".  The matrix commands are identical, the
#     actual type of matrix file is autodetected.
#
# 3.  Matrix reading code has been largely rewritten to attain much greater
#     speed than before, despite now also having to detect and process two
#     different kinds of matrix files and also having to translate ID's to
#     pedindex, which might have made it far slower.  Notably matrix files
#     are now usually read in one pass whereas it used to require two passes.
#     Also, the association from ID's to pedindex is now done using an
#     advanced C++ object known as unordered_map, which has only officially
#     become part of C++ in recent years.  This is a hashtable equivalent in
#     function to the associative array in Tcl, whose speed of access does
#     not decline exponentially with pedigree size, nor does it require
#     NxN memory increase.  This may be applied to other machinery inside
#     SOLAR in the future to obtain further speed increases and better
#     functionality.
#
# 4.  Though matrix handling is only a small part of MGA, matrix reading has
#     gotten sufficiently faster that MGA overall runs about 3% faster.  (This
#     test was done using original format matrices.  The speed increases should
#     apply to both types, but original format will generally be faster.)
#
# 5.  A CSV matrix may have a checksum field comparable to the one used for
#     original format.  The checksum field is optional (not required). The
#     matcrc command will prepend a checksum to a CSV matrix just
#     as it does for an original format matrix.  The checksum must be in the
#     first data record of the file, and it has id1 named "checksum" and id2
#     named checksum.  The actual checksum value is in the matrix1 field,
#     preceded by decimal point.  The checksum comes from running cksum on
#     the pedindex.out file, so that any changes to the pedindex following
#     creation of the matrix will give an error.  This is useful as often
#     people forget to update their matrices after updating a pedigree.
#
# 6.  "matrix debug" now shows the sum of all matrix values, as taken from the
#     matrix file.
#
#
# ****   Versions 7.4.6 through 7.4.9 reserved
#
# ****   New in version 7.4.5
#
# 1.   matrix debug now shows the actual minimum matrix value for both
#      one data column and two data column matrix files.  Previously, for
#      one column matrix files, it showed the minimum value that was greater
#      than zero.
#
# ****   New in version 7.4.4
#
# 1.   polygenic now has option -testcovar, to test a single covariate.
#      All other covariates are fixed and untested.  The tested covariate
#      is not removed from the final model in any case.  The default
#      probability level for declared significance is changed to 0.05.
#      The proportion of variance is reported only for the tested covariate.
#
# ****   New in version 7.4.3
#
# 1.  mga now starts covariate beta parameters at 0.001 instead of 0.0.
#     This has not changed results in any regression tests, but might help
#     increase sensitivity for troublesome models.
#
# 2.  solar -niskey <key> is now available for use on clusters or clouds where
#     a fixed username and home directory are not available.  -niskey uses
#     the nisdomainname shell command to obtain the nis identity, for which
#     a key should be requested. If the name is dotted, only the second
#     to last dotted portion is used for identification.  For example on
#     solar.txbiomedgenetics.org only txbiomedgenetics is used as identity.
#
# ****   New in version 7.4.2
#
# 1.  mga now by default outputs standard errors for the snp beta
#     that are calculated from the chi and the beta value
#     (sqrt(beta^2/chi^2)).  You can still use the -evdse and
#     -slowse options for estimated standard errors as before.
#     If the standard error cannot be computed because chi is exactly zero,
#     which should never happen, the SE will be reported as 10e20.
#
# 2.  polyclass -maxsnp will now output computed standard errors as is
#     the default for mga.  (Note: as a result, -maxsnp will use EVD1
#     fast maximization without standard error estimation.  Previously
#     -maxsnp had invoked -slowse.)
#
# ****   New in version 7.4.1
#
# 1.  A new session option ExpNotation has been added, to force output in
#     exponential notation (such as -1.004e-4) in specific cases where needed.
#     Normally, many commands such as mga output in fixed point notation as
#     long as there are a few digits shown, and then they switch to exponential
#     for tiny values which would otherwise be reported as zero.  So if a value
#     is shown as 0.00000 by mga it must actually be zero.  But sometimes
#     people wonder if that is actually correct.  So the ExpNotation
#     option has been added to force output of all numbers in exponential
#     notation, so there can be no doubt.  Currently this option only affects
#     the output of mga, but it may be applied to other commands in the future.
#     It defaults to 0 (zero) meaning to use the default auto mode, and is set
#     to 1 to force exponential:
#
#         option ExpNotation 1
#
#     This option remains in effect during a single session of SOLAR.  It is
#     not saved to models, and will return to default for subsequent or
#     concurrent sessions of SOLAR.
#
# ****   New in version 7.4.0
#
# 1.  When parameters are constrained to zero, it is necessary (per the design
#     of the Fisher/Mendel Search program) to set the lower boundary slightly
#     below zero.  Previously that adjustment had been -0.01.  This has been
#     changed to -0.001.  A new global variable SOLAR_constraint_tol is
#     set to the absolute value (0.001) of the adjustment when SOLAR starts.
#     A user can change that global variable before running polygenic to change
#     the automatic boundary adjustment, which cannot be made smaller than
#     0.00011 without changing Fisher code.  Generally the automatic boundary
#     adjustment for constraints can now be bypassed by setting the boundary to
#     a non-zero value before calling polygenic or maximize.
#
# ****   New in version 7.3.9
#
# 1.  Sometimes e2 would get lower boundary set incorrectly to -0.01.  This
#     has been fixed.  It would especially happen when household effect was
#     being added to a polygenic model in which e2 was previously estimated
#     to be zero.
#
# ****   New in version 7.3.8
#
# 1.  Some maximization errors that could occur during twopoint could cause
#     the scan to stop.  Errors are now caught, reported, and the twopoint
#     scan continues.
#
# ****   New in version 7.3.7
#
# 1.  polyclass -maxsnp now writes results to file named polyclass.snpname.out
#     in addition to the usual polyclass.out, where snpname is the name of
#     the snp with the leading snp_ removed.  If the -append option is used,
#     previous contents of the polyclass.snp are retained, so one can accumulate
#     results from a large number of SNP tests when polyclass -maxsnp -append is
#     used.  Files are written to the output directory (usually named after the
#     trait).
#
# ****   New in version 7.3.6
#
# 1.  polyclass -maxsnp (without -comb) now does not include the SNP
#     covariate in the initially estimated model or residuals.  SNP covariates
#     are added later when running mga on the residuals.
#
# 2.  polyclass -maxsnp now checks to ensure the snp name is prefixed with
#     snp_ or SNP_ since that is required by mga.
#
# ****   New in version 7.3.5
#
# 1.  polyclass -maxsnp (without -comb) has been fundamentally changed.  Now
#     it produces residuals for a fully loaded model, then uses mga to
#     evaluate each snp for each class with other classes blanked.
#
# ****   New in version 7.3.4
#
# 1.  polyclass -comb -maxsnp <snp> was broken by the changes in 7.3.3 but
#     is now fixed.
#     
# ****   New in version 7.3.3
#
# 1.  polyclass -maxsnp now produces residuals in a file named
#     polyclass.residuals.out in the output directory.  The file
#     includes fields ID, residual, trait, and all covariate variables.
#
# 2.  twopoint could fail if some particular model failed in a particular way
#     when testing the log likelihood.  That is fixed now.
#
# 3.  intraitclass command added.
#
# ****   New in version 7.3.2
#
# 1.  Under unusual circumstances, SOLAR was exhausting available
#     logical file units because of leaving many copies of
#     polygenic.logs.out open.  This is now fixed.
#
#     The problem only occurred with thousands of back-to-back
#     runs of "polygenic" with very small sample size.  In the end,
#     when no additional logical file units were available, SOLAR would
#     crash.  The exact cause of the problem was unclear, and was not
#     fixed by creating an exception handling wrapper around polygenic
#     to ensure that close is invoked on the polygenic.logs.out logical
#     unit, but instead has been resolved by creating a new set of file
#     writing procedures (see below) to be used for polygenic.logs.out.
#
# 2.  A new set of file writing procedures (putsnew, putsa, and putsat)
#     is available which obviates the need for the user to open and close
#     an output file, or to write to both terminal and output file at the
#     same time.  These procedures are intended to replace the old, tricky,
#     and inefficient "putsout" procedure that many users found too
#     convenient not to use.  They are not a replacement for the built-in
#     Tcl procedures open, puts, and close, which should still be used in
#     cases where efficiency is paramount and a tight writing loop is
#     possible without intervening maximizations.
#
# ****   New in version 7.3.1
#
# 1.  polyclass -maxsnp has been changed so that when not in -comb
#     mode, only one snp p value is calculated and only snp
#     related statistics are reported.  All classwise snp betas
#     are constrained to be the same value, and the p value is
#     calculated by constraining all of them to zero, a one degree
#     test.
#
# 2.  polyclass now reports the class number that fails when attempting
#     to initialized parameters.  Users may get an error that says there
#     are no individuals available for analysis which may seem unbelievable
#     unless the class number is also reported.  It is easy to overlook
#     the possibility that some variable is not defined for a particular
#     class.
#
# ****   New in version 7.3.0
#
# 1.  polyclass -intrait is corrected.  Previously it summed the first
#     specified class twice, resulting in SD of 2.  The defect had
#     been present since version 7.1.4.
#
# ****   New in version 7.2.9
#
# 1.  polyclass -maxsnp now works with -comb.
#
# 2.  the formatting of chi and varexp for polyclass -maxsnp has been
#     corrected to the formatting used for other outputs.
#
# ****   New in version 7.2.8
#
# 1.  polyclass now has option -maxsnp <snpname> which will do a
#     classwise association analysis of one snp.  Results reported
#     include the snp beta value, chi square, p value, and variance
#     explained.  The residual heritabilities are shown with and without
#     the snp.
#
# ****   New in version 7.2.7
#
# 1.  This version uses the new C++0x (2011) C++ standard libraries, so
#     that unordered_map (hash table) can be used in a future version.
#
# ****   New in version 7.2.6
#
# 1.  polyclass now works with discrete traits, and for bivariate models
#     with one or two discrete traits.
#
# ****   New in version 7.2.5
#
# 1.  New option RicVolOffset allows adjustment to the volume numbers in
#     the phenotype file, and it defaults to 1 instead of the previous
#     fixed zero adjustment.  This means that if the volume "1" is specified
#     in the phen file, 1 is subracted to get the actual array index of "0" in
#     the RicVolumeSet image data file.  This prevents segmentation violation
#     in test file split_csv_0029.csv which has 859 volumes and the last one
#     listed is 859.  With the new default the range of array indexes used is
#     0-858 which is typical C programming. In addition, it produces a
#     non-zero heritability for the trait FA_0029.
#
# 2.  RicVolumeSet volume specifications in the phenotypes file are now tested
#     (after being adjusted by the new RicVolOffset) to make sure they do not
#     exceed the range actually found in the image file.  This replaces the
#     previous segmentation violation with an error message.
#  
# ****   New in version 7.2.4
#
# 1.  polyvoxel command does routine polygenic analysis on voxel
#     data.
#
# ****   New in version 7.2.3
#
# 1.  polyclass -resmax -comb now works.
#
# ****   New in version 7.2.2
#
# 1.  polyclass -resmax option maximizes the polyclass model using residuals
#     which it computes first.  If -intrait is also specified, it is the
#     residuals which are inormalized (inormalization is not done on the
#     initial model from which the residuals are derived).
#
# 2.  residual now works on polyclass models if the -class option is specified.
#
# 3.  polyclass -maxi option is renamed to -max for consistency with -resmax.
#     The original -maxi will continue to work.  The maximization output file
#     is renamed to polyclassmax.out from polyclass.all.out and the model
#     saved is now called polyclassmax.mod.
#     
# ****   New in version 7.2.1
#
# 1.  polyclass -g -intrait is now working correctly for univariate and
#     bivariate.  Although declared working for univariate in 7.1.4, it was
#     not including all classes in the inormalized traits.
#
# ****   New in version 7.2.0
#
# 1.  Boundaries used by polyclass -maxi have been corrected to fix
#     convergence issues.  When paremeter h2r is constrained to zero,
#     parameter e2 has to have an upper bound of 1.01 to permit
#     numerical imprecision.
# 
# ****   New in version 7.1.9
#
# 1.  polyclass -maxi now writes results to file named polyclass.out in the
#     output directory defined by trait name or outdir command.
#
# ****   New in version 7.1.8
#
# 1.  mga -ixsnp has been fixed, the null model always has the bixsnp,
#     the test model has the two snps and their interaction, and the
#     interaction test model just has the two snps (and no interaction).
#     Previously it was all wrong.
#
# ****   New in version 7.1.7
#
# 1.  polyclass -maxi now computes p value for each h2r by class.
#
# ****   New in version 7.1.6
#
# 1.  mga -ixsnp <snp> option added for interaction analysis.  An interaction
#     covariate is added to the test model, and a 3rd model in run in which
#     the SNP is included but not the interaction.  Additional chi, p value,
#     ix beta, ix beta se, and variance are added to output file.
#
# ****   New in version 7.1.5
#
# 1.  polyclass with the -intrait and -g options at the same time is now
#     fixed for univariate models but not yet bivariate models.
#
# 2.  imout polygenic volume assignments have changed, allowing 8 reserved
#     volumes for linkage or association use.  Covariate information now
#     starts at volume 20.  See 'help polyimout' for the assignments and
#     more information.
#
# 3.  imout now uses the dimensions of the loaded mask if available.  The
#     mask must be loaded first with the mask command.  If a mask is loaded,
#     the only arguments the user need specify are the output filename and
#     the number of volumes:
#
#         imout image2 -nvol 40
#
#      If the dimensions are defaulted to the mask, all required arguments
#      must be given in one imout command like the one above (since
#      that creates the imout object).  Within that same command line, it is
#      possible to override the mask dimensions individually.  It is also
#      possible to ignore the current mask file with the -ignoremask argument,
#      in which case all the dimensions must be given but multiple lines could
#      be used as when there is no loaded mask.
#
# 4.  imout -ncovar allows the number of volumes to be specified as what
#     would be required for that number of covariates.  For example, the
#     above imout command could also be given like this:
#
#         imout image2 -ncovar 5
#
#     which would allow for 5 covariates (which requires 40 volumes).  There
#     is also a -ntrait argument but it defaults to 1 and any number other than
#     1 is currently an error, as the polygenic multivariate volume
#     assignments have not yet been made.
#
# 5.  Attempting to use voxel coordinates outside of the range of the imout
#     now returns an error message rather than crashing SOLAR.
#
#
# ****   New in version 7.1.4
#
# 1.  polyclass -intrait is now working correctly with univariate and
#     multivariate traits.  Previously it derived all inverse normals from
#     Class 1 only and therefore generated hugely incorrect models.  One
#     remaining problem is that the combination of -intrait and -g doesn't
#     yet work, it causes an error.  This will be fixed asap.
#
# 2.  The equation parser used for omega, mu, and defines has been enhanced
#     to permit unavailable variables if they are multiplied by zero (the zero
#     must come first) similar to the way this works in C++.  For example
#     the following definition permits the class based inormal functions
#     to be used in a definition so that it has a value for both classes:
#
#         define i_q4 = (class==1)*inormalc_1_q4 + (class==2)*inormalc_2_q4
#
#      Depending on whether class is 1 or 2, only inormalc_1_q4 or
#      inormalc_2_q4 need be defined (and only one is defined for any
#      individual, depending on their class).  If the class is 1, the
#      term (class==2) becomes zero, so the following multiplicand 
#      inormalc_2_q4 is ignored.
#
# 3.   polyclass -comb and -maxi options added.  -comb creates a combined
#      class model using the class-specific traits (if -intrait is used).
#      -maxi performs ordinary maximization after the polyclass model
#      is created.
#
# 4.   polyclass now gives error message when classes are not specified.
#      Previously, if no classes were specified, polyclass would crash
#      SOLAR.
#       
# ****   New in version 7.1.3
#
# 1.  Linked with new version of RicVolumed.a libraries.
#
# ****   New in version 7.1.2
#
# 1.  EVD2 models now initialize beta variable boundaries in the same way as
#     is done for standard models.  Previously, boundaries were set by the
#     default mechanism as applied to the Stage 2 model, with EVD
#     transformed variables, which generally resulted in larger boundaries
#     than desirable.  Boundary setting for EVD2 models is now done in Tcl to
#     avoid having to go through the internal data packing required for
#     "maximize -initpar" which  would be simpler to program but less
#     efficient.  This is the first time the boundary algorithm has been
#     coded in Tcl.
#
# 2.  polyclass and sporclass now have arguments -intrait and -incovar to
#     inormalize all trait and class variables respectively.  -incovar is
#     not currently working due to mu command limitations that will be
#     corrected in the next version.
#
# 3.  Previously, polyclass simply omitted all covariates if the -g option
#     was used.  This has been fixed.
#
# 4.  inormal now has a -class option, which restricts the sample and the
#     means to individuals with the specified class number.
#
# ****   New in version 7.1.1
#
# 1.  Old style text file line termination is now detected when phenotypes
#     files are opened, and an error message is given:
#
#         File old.txt has unsupported text line terminators
#         Use retext command to fix file before using
#
#     It is not possible to handle this issue automatically as is done
#     with Windows style text files.  But now a translation program is
#     provided...which is really just a call to the system program tr.
#     Previously using files like these would produce this useless message:
#
#         Short record in input file
#
# 2.  New command "retext" translates files in old Mac style which is still
#     used by some Mac programs to modern Mac and Unix/Linux style.
#
# ****   New in version 7.1.0
#
# 1.  imout command added to select and enable binary image output.  See
#     "help imout" for command description and options.
#
# 2.  The fixupper and fixlower parameter options introduced in version
#     7.0.7 is  extended to multivariate polygenic models, and also
#     to a few other obscure cases that were not working as intended.
#
# 3.  imout now enables image output for the polygenic command.  See 
#     "help polyimout" for how this would be used and what volumes are used
#     for each type of polygenic analysis results.  Currently only univariate
#     polygenic models are supported.
#
# 4.  polymod no longer forces all models to start with h2r set to 0.1.
#     Instead, if a model already has valid e2 and h2r, they are left alone.
#     This does not change the operation of the polygenic command because it
#     always starts with a sporadic model anyway.  However, a future update
#     to polygenic will save the initial variance component parameter values,
#     if any, in order to use them to start the polygenic model
#
# ****   New in version 7.0.9
#
# 1.  Some bivariate multipoint runs would fail with an error:
#     "No parameter H2r has been created."  This was because one
#     section of the retry code had not been generalized to
#     multivariate parameter names, such as H2r(q4).
#
# ****   New in version 7.0.8
#
# 1.  Parameters now have optional fixed upper and/or lower boundaries,
#     specified with the "fixupper" or "fixlower" identifier.  For
#     example:  parameter h2r fixlower 0.1 fixupper 0.5
#
#     If a fixed boundary is set, it will not be changed by automatic
#     boundary adjustment procedures.  However, it can be reset to
#     another fixed or non-fixed bound by the user.  Regardless of whether
#     a fixed or adjustable boundary is in effect, the regular boundary query
#     returns the boundary value.  However, the "fixlower" or "fixupper"
#     queries return null ("") if the current boundary is not fixed.  Fixed
#     boundaries are saved to and read from models, so long as the solar
#     version is 7.0.8 or later.
#
#     [this feature is not yet fully supported for mulitivariate models when
#      running polygenic or polymod]
#
# 2.  mga now has -fixupper and -fixlower arguments which control the
#     boundaries of the snp beta parameters.
#     
# ****   New in version 7.0.7
#
# 1.  An error in accessing the RicVolumeSet object while reading
#     binary data has been fixed.
#
# ****   New in version 7.0.6
#
# 1.  The mask command is created to read binary image files as masks and
#     use them to set the current voxel value, which is then used in the
#     reading of binary image phenotypes.
#
# 2.  The voxel command can be used to set or read the current voxel value.
#
# 3.  The format for specifying binary image files within CSV files has
#     changed.  Now all fields which point to binary files must identify
#     themselves as type "nifti" in the header line following a colon
#     delimiter from the field name.  For example, a valid CSV header
#     could look like this:
#
#         ID,age,count:nifti
#
#     As with the field names, the field type is not case sensitive.  Other
#     types are currently ignored.  Once a header like the above has been
#     read, all the non-blank fields identified as type NIFTI must have fields
#     which are the filenames of RicVolumeSet files in NIFTI format.  Following
#     the filename there must be a colon separator followed by the number of
#     the Volume Set (volset) corresponding to the individual in that record,
#     for example:
#
#         A001,19,images.gz:1
#
# ****   Version 7.0.5 was preliminary to 7.0.6.
# 
# ****   New in version 7.0.4
#
# 1.  RicVolumeSet is now accepted for phenotypic data.  To use this feature,
#     the regular text phenotypes file has alphanumeric field(s) which specify
#     the filename of the RicVolumeSet and the 4 arguments required to
#     identify the specific voxel.  The leading delimiter to specify this
#     kind of data is "<" preceding the filename and ":" for delimiting each
#     numeric index field required.  For example, to specify file named
#     input.gz, volume 1 and 2 for ID's 1 and 2, and x,y,z coordinates 2,3,4,
#     you would have a phenotypes file like this:
#
#       id,qtrait,voxel
#       1,5.0,<input.gz:1:2:3:4
#       2,3.5,<input.gz:2:2:3:4
#
# ****   New in version 7.0.3
#
# 1.  Comma Delimited Files (aka csv) may now have an unlimited record size
#     and an unlimited number of fields in every record, limited only by
#     machine architecture and memory.  (Currently, memory limits may prevent
#     having more than 50,000,000 fields per record--a future update may
#     increase memory efficiency to push this higher.)
#
#     To this end, the Tablefile object which parses these files has been
#     redesigned not to have to fit a record into a single buffer.  It has
#     also been made faster by eliminating data copying, and made more memory
#     efficient by using data buffers more flexibly.  These improvements
#     will apply to pedigree and phenotypes files when they use csv format,
#     and other csv files processed by file operations such as joinfiles,
#     selectfields, and selectrecords.  The transpose operation does not use
#     tablefile and was already unlimited by design.  Pedsys file handling
#     has not been changed and continues with the previous limits of 800,000
#     characters per record (or file header) and 40,000 fields.  Note that
#     much smaller limits apply to the number of traits and covariates
#     which may be used in a model, and models are fundamentally limited
#     not to have more estimated parameters than there are independent
#     observations.
#
# 2.  A new option csvbufsize enables automatic buffer handling when defaulted
#     to zero, or a fixed value set by user.  The automatic handling uses
#     10,000 character buffers but switches to 200,000 character buffers
#     for more efficiently handling very large records.  A record may use
#     an unlimited number of buffers.  Changing the default is not recommended.
#
# 3.  The invert procedure has been renamed "transpose".
#
# 4.  The "memory" command now works properly on linux and Mac OS X.
#     Previously it had only worked on Solaris.  It shows the amount of
#     memory in use by SOLAR.
#
# ****   Versions 7.0.0 through 7.0.2 were intermediate to 7.0.3
#
# ****   New in version 6.6.2
#
# 1.  EVD2 model covariates now function properly and automatically,
#     allowing normal use of polygenic and residual commands.  Previously
#     there were issues with all kinds of covariates in EVD2 models.
#
#     All covariates in EVD2 models are now converted to definitions.  For
#     scalar covariates, this permits subtracting the sample mean correctly,
#     resulting in correctly maximized trait mean parameter(s).  Previously
#     scalar covariates did get correct betas and likelihood estimates, but
#     because of incorrect scaling, the mean parameter value(s) were incorrect.
#
#     For interaction and squared covariates each term must be scaled, then
#     formed into a compound covariate within the definition.  Previously
#     incorrect likelihoods, betas, and mean parameter estimates would occur
#     if there were any interaction or squared covariates because of failure to
#     properly scale each term.
#
#     If a defined covariate is used, it must be pre-scaled within the
#     definition itself if used as a scalar covariate.  If defined term
#     is used within an interaction or squared covariate, an error is
#     raised with the suggestion that the interaction be pre-formed by the
#     user within a definition.  This is because at this time it is not
#     possible to reference definitions from within other definitions.
#
# ****   New in version 6.6.1
#
# 1.  New command "blank" permits the blanking of individuals from a sample
#     according to variable data conditions.  If the expression is non-zero,
#     the person will be blanked.  For example:
#
#         blank age<<55               ;# blank age less than 55
#         blank (class!=1)*(class!=2) ;# blank class not 1 or 2
#
#     this defines a blanking expression and inserts it as a null covariate
#     into the model.  See "help blank" for more information.  See also
#     new feature (3) below.
#
# 2   New command "polyclass" sets up a polygenic model with a set of
#     estimated parameters for each class as defined by a class variable
#     in the phenotypes file.  See "help polyclass" for additional
#     information.
#
# 3.  For covariate definitions, a new named constant called "blank" is
#     available.  When a definition expression returns this value,
#     the person is not included in the sample.  It is intended that this
#     feature be used mainly through the "blank" command which sets up
#     the required expressions correctly for intuitive subset blanking commands.
#
#     However, the underlying mechanism is the new blank constant, which has actually
#     always been present but now has a name for easy use.
#
#     One correct way of using this feature is with a conditional operator
#     like this:
#
#         define sample = blank*(age>>40)
#         covariate sample()
#
#     This would have the effect of blanking records for which the age
#     value is greater than 40.  Since the "sample()" covariate is
#     declared as having null trait, it is not actually included in
#     maximization, but helps to delimit the allowed sample.  There is
#     nothing special about the definition name "sample", any other name
#     could be used.
#
#     The blank constant must be multiplied only by 0, 1, or a conditional
#     expression as shown or multiple conditional expressions.  If blank is
#     operated on numerically, it will just become a small number with no
#     special meaning.  The actual blank value is -1e-20 which has always had
#     this special meaning in Fisher, Mendel, and other classic programs.
#
# 4.  read_output now has a -d option which reads whether a variable was
#     determined to be discrete by maximize or maximize -initpar
#
# ****   New in version 6.6.0
#
# 1.  The Pearson Residual has been corrected by subtracting from the 0,1 
#     scaled discrete trait value, so it is actually a residual now rather
#     than the predicted value.  You get Pearson Residuals automatically
#     when you run residual on a discrete trait model.
#
# 2.  New command "maximize -initpar" does not maximize but reads the
#     phenotype variables, determines the sample, and initializes
#     parameter starts and bounds exactly as a normal maximize does.
#
# 3.  If an attempt is made to maximize a discrete trait model for which
#     the omega has not been defined, the user gets a more helpful message
#     as with quantitative trait models that commands polygenic or polymod
#     will do the required setup.  Previously, the error message complained
#     about the lack of SD constraint, which was not helpful because typically
#     without calling polygenic or polymod there is no SD parameter to be
#     constrained.
#
# 4.  If an attempt is made to maximize a quantitative trait model for which
#     the omega has not been defined, the message used to suggest using
#     the polygenic command but now also suggests polymod, the command which
#     only does setup rather than a full test.
#
# ****   New in version 6.5.9
#
# 1.  Within EVD2 phase 2 models (using evd2 transformed variables) option
#     MergeAllPeds, if present in the original model, is not included
#     because it is not applicable to the phase 2 models which are all
#     unrelateds.
#
# 2.  Within EVD2 phase 2 models, all the transformed covariates are scaled
#     to zero, as required by the mathematics.
#
# 3.  Within EVD2 phase 2 models, the phony phi2.gz matrix containing all
#     unrelateds is no longer loaded as it is not needed.  This makes the
#     models more efficient (matrix loading skipped) but does not change
#     the results.
#
# ****   New in version 6.5.8
#
# 1.  mga (mgassoc) no longer requires the -noevd option to
#     evaluate an evd2 model.  Simply give the command
#     "option modeltype evd2" before running mga.  In this
#     case, standard errors may be specified with either -evdse
#     or -slowse arguments, either way it is the evd2 standard
#     errors for an evd2 model.
#
# ****   New in version 6.5.7
#
# 1.  mga (mgassoc) now handles multivariate models.  There is
#     a beta value, a beta se, and a varexp value for each
#     trait in the output file.  The chi square is evaluated
#     as the number of parameters, and there is one beta
#     parameter for each trait, so the p value is computed
#     accordingly.  To use evd2 give the "option modeltype evd2"
#     prior to running mga, and specify the -noevd or -slowse
#     option to suppress the evd1 default.
#
# 2.  mga (mgassoc) output data columns are reordered so that
#     NAv (number of available individuals in sample) now follows the
#     SNP.  Otherwise the columns are unchanged, except that the last 3
#     (beta, betase, and Varexp) are repeated for each trait if there
#     are multiple traits.  In the comma delimited output file, the
#     trait names are suffixed, for example, Varexp(q4).  In the fortran
#     style tab delimited output file, a dot suffix indicates the trait index
#     if there are multiple traits, for example, Varexp.2 for trait #2
#     (because otherwise the columns could be too wide to be readable).
#
# 3.  A small problem with snphap fixed.
#
# ****   New in version 6.5.6
#
# 1.  EVD2 model translation from the EVD domain back to the original user
#     model has been simplified for better speed, reliability, and 
#     maintainability.  Now it is simple: the original user model is
#     re-loaded, then the maximized values from the phase2 model are
#     assigned to model parameters, and rather than parsing the phase 2
#     model file to get the parameter values, they are now put into a Tcl list
#     after maximization.  The mapping between the original user
#     parameter names and the phase 2 names is remembered through an
#     associative array.  Previously the translation code attempted to parse
#     and convert the entire phase 2 model file back to the user's model,
#     which was very complicated and could fail in some unusual cases, and
#     in all cases the parameters would get re-ordered (which no longer
#     happens).
#
# 2.  parameter command has new option "parameter -return" that returns all
#     current parameter information as a Tcl list.
#
# 3.  matrix command has new option "matrix -return" that returns all current
#     matrix information as a Tcl list.
#
# 4.  Trait specific covariates (such as "covariate age(q4)") are now
#     translated correctly from the original user model with _evd applied
#     both to the covariate variable name and the trait qualifier.
#
# ****   New in version 6.5.5
#
# 1.  EVD2 trivariate omega fixed.
#
# ****   New in version 6.5.4
#
# 1.  EVD2 now correctly preserves matrices, constraints, and options, and
#     correctly translates them to the EVD2 parameterization and back.
#     As a result, sporadic models are now analyzed as sporadic, and so
#     on.  Previously all EVD2 models were treated as polygenic, and
#     pre-existing constraints were ignored and eventually lost.
#
# 2.  As a result of #1, some EVD2 models that converged incorrectly will now
#     not converge.  Models including squared covariates like age^2 appear
#     to have difficulty converging.
#
# 3.  EVD2 now correctly preserves starting values and bounds for covariate
#     parameters.  As a result, retry operations which move these bounds now
#     work as intended.
#
# ****   New in version 6.5.3
#
# 1.  The bivariate omega used for EVD2 now applies abs() to all parameters
#     for which a sqrt() is taken, to prevent domain errors from negative
#     numbers very close to zero.
#
# 
# ****   New in version 6.5.2
#
# 1.  EVD2 now handles interaction covariates and covariates with exponents
#     correctly.
#
# ****   New in version 6.5.1
#
# 1.  Users of Condor parallel system can specify -condor option when
#     starting SOLAR.  Then the userid will be obtained through Condor.
#     The key should then be specified with a -key <key> argument, which
#     must follow the -condor argument.  For example:
#
#        solar -condor -key userkey
#
#     Condor users should not change the current directory (using cd) from
#     the default either before or while running solar.
#
# ****   New in version 6.5.0
#
# 1.  The EVD2 omegas have been simplified, reducing the need for sqrt
#     operators and eliminating sqrt entirely in the univariate case.
#
# ****   New in version 6.4.9
#
# 1.  EVD2 maximization is now seamlessly integrated so that it seems
#     just like regular maximization.  You just set "option modeltype evd2"
#     and run "maximize", "polygenic" or other model maximizing command
#     and you end up with a normal looking model, with the original
#     phenotypes and pedigree files loaded, hiding the way that temporary
#     transformed variables and special parameters were actually used during
#     the second phase of maximization.  Results from the EVD domain get
#     translated back into the original model with the original pedigree and
#     phenotypes.
#
# 2.  The EVD2 transformation of the sex variable was incorrect in the
#     same way as other transformations were prior to version 6.4.8.
#     That was fixed, but it was still not correct until changing the
#     interpretation of male,female as 0,1.  Now it works.
#
# ****   New in version 6.4.8
#
# 1.  EVD2 now appears to produce correct phenotype transformations, and
#     EVD2 maximization is now a reality.  However it is still very clunky
#     to use (see "help evd2").  That that it actually works, EVD2
#     maximization will be streamlined in the next version.
#
# 2.  "mga" is the new official name of the mgassoc command.  Files such as
#     mgassoc.out are renamed mga.out.  The old command can continue to be
#     used.  Output file is mga.out for "mga" command and mgassoc.out for
#     "mgassoc".  The command mgassoc_topedsys is renamed mg_topedsys.
#
# 3.  If the user key is found to be invalid (not matching username) the error
#     message now prints the username, actual key filename, and key string.
#     When using batch queing systems, sometimes a different username or
#     home directory is used and this will help sort out problems more quickly.
#
# ****   New in version 6.4.7
#
# 1.  tmean values corrected in evddata.out (see help evd2).  Both the
#     tmean values and the lambdas produced match the reference data.
#     This shows the eigenvalues and vectors are being computed correctly.
#     But the transformed phenotypes are still not computed correctly.
#
# 2.  The stats command now accepts files with Fortran D style exponents.
#
# 3.  d2e and d2e2 commands handle conversion of D style exponents to the E
#     form more generally understood.  Generally this is not needed anymore
#     since SOLAR understands D style exponents in phenotypes files.
#
# ****   New in version 6.4.6
#
# 1.  invert inverts the rows and columns of a comma delimited file.
#
# 2.  timediff returns the difference between two system time strings.
#     startclock and stopclock conveniently measure time using timediff.
#
# 3.  countfields shows the largest and smallest record sizes in a file,
#     helping determine if it is internally consistent.
#
# ****   New in version 6.4.5
#
# 1.  The command line editing feature no longer requires the creation of
#     a temporary "starting" file when starting solar, eliminating the danger
#     of these files accumulating.
#
# 2.  A recent update could lead to multipoint failing with "invalid command
#     name last_maximize_goodlod".  This is fixed.
#
# ****   New in version 6.4.4
#
# 1.  Additional changes to the command line editing feature added in version
#     6.4.1.  It is now possible to turn it off in 3 different ways.  This is
#     in case it causes problems or inefficiencies in parallel or batch
#     operations (though no such problems or inefficiences have yet been
#     found).  1.  You may invoke solar with the leading argument -noce.
#     2.  You may define a shell variable SOLAR_noce to any non-null value.
#     3.  You may run solar with arguments (solar commands) which is one
#     variation of batch mode (non-interactive) operation.
#
# 2.  Parallel margin is increased to 3500 to prevent problems with current
#     parallel operation on medusa2 when too many machines are started at
#     once.
#
# ****   New in version 6.4.3
#
# 1.  Fixes to the command line editing feature added in version 6.4.2 in
#     certain obscure cases.  For example, when TERM=dumb as inside the
#     emacs shell mode, in which case command editing is impossible anyway,
#     rlwrap is no longer run, avoiding a useless error message.
#
# ****   New in version 6.4.2
#
# 1.  Command line editing is now available.  This means you can recall
#     previous commands with up arrow, and use right and left arrows to
#     position cursor and edit them, just as with most command shells.
#     A program called rlwrap is used to provide this capability, and it
#     uses the readline library.
#
# ****   New in version 6.4.1
#
# 1.  An improvement has been made to solar file input routines
#     for better speed with large files.
#
# 2.  Use of matrix checksums created with matcrc is now documented for
#     the ibd and mibd commands.  Previously it was only mentioned within
#     the documentation for the checksum in section 8.3 of the
#     manual which actually concerns the phi2.gz matrix file.  The
#     documentation for matcrc itself has also been clarified.
#
# 3.  If a user has not registered, the "please register" message now
#     prints the short form of the user's login name which is needed
#     in order to obtain a working SOLAR key.  Many systems now hide this
#     login name behind the user's full given name.
#
# ****   New in version 6.4.0
#
# 1.  Negative LOD scores during some multipoint runs have been fixed by
#     adding a new set of retry strategies if the likelihood is smaller than
#     the null model likelihood.  The actual retry strategies and means of
#     controlling them is described by "help maximize_goodlod".  This is a
#     private procedure not normally documented in the manual.
#
#
# ****   New in version 6.3.9
#
# 1.  The "load phenotypes" command now only shows the first two lines of
#     phenotypes for each file.  If there are more phenotypes, the list is
#     ended with an ellipsis (...) and a comment is given that you may show
#     all the phenotypes with the "phenotypes" command.  Previously, for
#     files with thousands of phenotypes, the mere act of listing all the
#     phenotypes could take a long time, and such a long list would not be
#     ordinarily read anyway.  This makes the "load phenotypes" command 
#     much faster in these cases.  It also keeps the your terminal session
#     from scrolling off the window.  A phenotypes file may have up to
#     40,000 phenotypes.
#
# 2.  The method EVD2 uses for ensuring IBDID's are in increasing order has
#     been greatly improved in efficiency for large pedigrees.  It used to
#     require NxN data shuffling, which is time consuming for large
#     pedigrees.  Now the data ordering requires less than N operations.
#
# 3.  The testing of duplicate individuals for EVD2 is no longer an NxN test,
#     but a N test based on the assumption that pedindex.out is in sorted
#     order, as it always is when created by the "load pedigree" command.
#     Also, unnecessary pedigree tests during maximization are skipped for
#     EVD2.
#
# ****   New in version 6.3.8
#
# 1.  IBDID's in evddata.out are now in consistently increasing order, and
#     as a result the IBDID and lambda's match those in the reference dataset.
#
# ****   New in version 6.3.7
#
# 1.  When a specified phenotype name is found in several loaded phenotype
#     files, the error message now correctly identifies the phenotype name
#     rather than calling it (null).
#
# ****   New in version 6.3.6
#
# 1.  Precision for output of parameter values to model files and script
#     queries is increased from 10 to 16 digits.  This fixes some model
#     problems identified with mgassoc.  The precision can be controlled
#     with a new option ParameterFormat which defaults to 16.
#
# ****   New in version 6.3.5
#
# 1.  mgassoc now handles discrete models which had not previously been
#     maximized.  Previously you would get an error about SD not being
#     constrained.
#
# 2.  mgassoc now updates the "mgassoc_start" model to reflect changes needed,
#     such as polygenic omega and constraint of SD for discrete models.
#     This makes it faster to create new "mgassoc_null" models when needed,
#     avoiding incomplete maximizations, and resulting in a 1% overall
#     increase in speed in a large test.
#
# ****   New in version 6.3.4
#
# 1.  mgassoc now tests sample regardless of whether using EVD or not.  So in
#     non-evd cases, it need not re-run null models for each SNP *unless*
#     sample has changed.  Previously it was only able to test the sample
#     if evd was used, and therefore was forced to run null models for each
#     SNP with discrete traits.
#
# 2.  -runwho is a new option for "maximize" which does the maximization and
#     produces a who.out file listing all the individuals in the analysis.
#
# ****   New in version 6.3.3
#
# 1.  If option EnableDiscrete is 0, an otherwise discrete trait univariate
#     model can now be handled by EVD.  This also means that such models
#     can be handled by the default EVD mode of mgassoc.  HOWEVER, a
#     preferred way of handling discrete traits as if they were quantitative
#     is to multiply by a useful factor to increase the standard deviation
#     above 0.5, such as:  define qt = dt * 5.
#
# ****   New in version 6.3.2
#
# 1.  mgassoc will detect model types which do not permit EVD1 processing
#     (discrete and multivariate models) and revert to -noevd mode
#     automatically.  This prevents errors resulting from sample
#     differences when user forgets to specify -noevd.
#
# 2.  By default, mgassoc no longer runs standard errors, and the standard
#     error field in the output file will be all zeros in this case.  There
#     are two mutually exclusive standard error options, -slowse and -evdse.
#     -evdse runs EVD if possible, and gives a warning that EVD1 computation
#     of standard errors may be inaccurate in some cases.  -slowse runs
#     standard maximization for the model being analyzed.
#
# 3.  "option samplesametrustme 1" may be used to bypass null models for each
#     SNP when EVD is not being used (either because -noevd is specified or
#     not applicable).  This allows faster processing for non-EVD models.
#     EVD models always do automatic fast sample checking, and new null models
#     are forced by sample changes.  The -notsame switch to mgassoc overrides
#     option samplesametrustme.  Previously some of these features were
#     documented but not working.
#
# 4.  mgassoc output for null models is streamlined and better formatted to
#     make it easier to follow null models and SNP results.
#
# 5.  If EVD1 is not used during maximization because model is discrete or
#     multivariate, this is noted in the solar.out file.  Previously this
#     information would only appear in terminal output.
#
# ****   New in version 6.3.1
#
# 1.  bayesavg changed so that will run with any N value rather than crashing
#     with insufficient memory for N > 25.  It now only computes the first
#     few million combinations upfront, adding more as needed.  The -stop
#     rule set with -qtn will hopefully be satisfied before needing to
#     compute and maximize more than the 30 million combinations (which
#     would take years anyway) which would exhaust memory with too many
#     combinations.
#
# 2.  mgassoc p values and EVD1 (option modeltype evd) likelihoods fixed for
#     certain difficult examples by making using the proper quantitative conv
#     value.  Previously conv(discrete) was being used even though these
#     models are actually quantitative.
#
# 3.  Added -saveall option to mgassoc so that all solar.out files are saved.
#
# 4.  Changed occurrances of sfbrgenetics to txbiomedgenetics.
#
# 5.  Changed matrix file CRC to start with "0" so that matrix writers
#     can line up decimal point of CRC with their data
#
# 6.  Changed the display format of the omega (print ) operator to remove
#     trailing imprecision 99999's with matrix single precision data.
#
# ****   New in version 6.3.0
#
# 1.  The Pearson residuals (which you get when running "residual" with a
#     discrete trait) have been corrected.
#
# 2.  EVD2 now writes transformed phenotypes file correctly when a defined
#     trait is used.  Previously, it messed up the file header.  Thus you can
#     now use inormalized traits.
#
# 3.  "help evd2" is now available though it doesn't show up in command list
#     because this is still being debugged.
#
# 4.  If the sample includes no individuals of any kind, you no longer get
#     the confusing error message "No non-probands have complete data" which
#     is especially confusing if you have no probands in the first place.
#     Instead you get this message:
#
#  No individuals have complete data.  Check that ped and phen IDs match.
#
#     It mentions the possibility of ped and phen mismatch because that is
#     one error that can cause this, say, if the formatting of ID's is
#     different, as sometimes happens.  The situation where there ARE
#     individuals in the sample, but they are ALL probands, gets a different
#     message:
#
#  No non-probands have complete data.  Every ped must have one non-proband.
#
# 5.  Updated internal contact addresses to solar@txbiomedgenetics.org.
#
# ****   New in version 6.2.9
#
# New features which assume use of Grid Engine software as at Texas Biomedical
# Research Institute.
#
# 1.  A new parallel option "ignore" is added to deal with users who submit
#     jobs running in automatic fallback mode.  Such jobs previously restricted
#     machines available for "stepup -par" and "stepfor -par" which could not
#     compensate for automatic fallback jobs.  Other parallel parameter
#     defaults have also been adjusted a bit.
#
# 2.  New command "howmanyuser" shows how many jobs a particular user is
#     currently running on parallel resources.  To see jobs run by everyone
#     use existing command "whoranch".
#
# 3.  Online documentation of whoranch and howmanyuser added.
#
# ****   New in version 6.2.8
#
# 1.  Sometimes, usually because of some mistake in datafile preparation,
#     a trait ends up having just one non-blank value in the phenotypes file.
#     When that happened in previous versions, you used to get a very
#     confusing error message (terminating maximization) complaining about
#     discrete trait coding which many people could not understand because
#     they weren't intending to have a discrete trait anyway.
#
#     Now if a trait has just one value, you get error that a trait has just
#     one value.  The error message also suggests a new option SingularTrait
#     which you can set to 1 to have such trait(s) considered discrete, and 2
#     to have such trait(s) considered quantitative (as now documented in 
#     'help option').  It was never possible to maximize such models before.
#
# 2.  The determination of whether a trait is considered discrete or
#     or quantitative is now tightened up to the documented rule, and it
#     is possible to maximize models when a quantitative trait only has two
#     non-blank values.  Now to be considered "discrete" the two non-blank
#     values do actually have to be integer values (as documented) which
#     means having no non-zero fractional part.  (This is not affected
#     by the presence or absence of decimal points in the data file or
#     "data types" in a code file.)  If there are just two values in the
#     file, but either they are not integers or they are not consecutive
#     integers you will just get a warning and the trait will be considered
#     quantitative.  (Previously if there were just two values, integer or 
#     not, but they did not have a difference of 1, you got an error about
#     discrete trait coding and maximization could not be done.)
#
# 3.  The doc command now simply returns URL's where the solar manual may
#     be found locally and at the Texas Biomed SOLAR website.  It no longer
#     tries to run Netscape.
#
# ****   New in version 6.2.7
#
# 1.  Very rare internal error in 'chi' command fixed.  This only appeared
#     on Linux system we currently use, appears to be GNU Fortran compiler
#     optimization bug.
#
# ****   New in version 6.2.6
#
# 1.  EVD2 has been completely rewritten for efficiency and transparency in
#     Fortran, attached directly to the main phenotype input and sample
#     sizing routine pinput and called immediately after sample is determined.
#
# 2.  Phenotypes files are now permitted to have fortran "D" style floating
#     point numbers, so the new Fortran-written evddata.out file can be
#     read
#
# 3.  EVD2 now creates fake pedindex.out and pedindex.cde and phi2.gz.  (It
#     also saves the old files for later restoration via evd2_restore_phen.)
#     Thus the evddata.out file does not need to be "loaded," a potentially
#     very time consuming operation for large pedigrees.
#
# ****   New in version 6.2.5
#
# 1.  Fixes pedselect for multiple pedigrees on linux.  Fixes "model new"
#     not clearing out entire list of pedselects.
#
# 2.  Bypasses writing unbalanced trait individuals to evddata.out.
#
# ****   New in version 6.2.4
#
# 1.  Additional changes for EVD2.  lambda variable is added to evddata.out.
#     Model file required for second phase is automatically generated.
#
# ****   New in version 6.2.3
#
# 1.  Additional corrections to evddata.out files for EVD2.
#
# ****   New in version 6.2.2
#
# 1.  Highly experimental EVD2 now generates evddata.out files for
#     multivariate models.
#
# ****   New in version 6.2.1
#
# 1.  Highly experimental EVD2 is now available by specifying
#     "option evdphase 1" in addition to "option modeltype evd".  This
#     is intended for internal development only at this time.
#
# ****   New in version 6.2.0
#
# 1.  mgassoc has been rewritten to meet the needs of more users.  nGtypes is
#     no longer used (wasn't always useful) to outline the sample.  Instead,
#     the first SNP is used as a null covariate in the null model to accomplish
#     this.  If all SNP's have the same sample, everything works as before.
#     However if any SNP has a different sample, this is now detected, and
#     it forces a new null model for each such SNP.  The user can also
#     specify -notsame as an option.  Note: automatic sample validation is
#     not available with -noevd option, so -notsame mode is assumed unless
#     you set option SampleSameTrustMe 1.  Because of reduced memory usage,
#     the MergeAllPeds option is no longer used by default.
#
# 2.  EVD model processing has been improved in a few ways.  The ID validation
#     no longer uses phi2, it simply compares IBDID's, which is far faster and
#     uses less storage (an NxN comparison of floats has been replaced with an
#     N comparison of ints).  Thus in many cases there is no significant
#     improvement by using the SampleSameTrustMe option anymore.  A new
#     option, DontAllowSampleChange forces an error during EVD processing
#     if the sample *has* changed.  That error is now caught and handled by
#     mgassoc automatically (as described above).
#
# 3.  evd flush is a new command that flushes out all memory used to store
#     EVD matrices.  This should not be needed generally, except it is
#     also invoked by "load pedigree" to allow for the fact that IBDID's
#     could change rendering stored values incorrect.
#
# ****   Versions 6.1.1 through 6.1.9 skipped
#
# ****   New in version 6.1.0
#
# 1.     The selectrecords command now permits the use of Tcl variables with
#        $$ operator.  The single dollar sign $ operator is used for file
#        fields.
#
# 2.     The documentation for using "snp effnum liji" has been clarified.
#
# ****   New in version 6.0.9
#
# 1.     The option pedselect can be added to using a new + modifier.
#        For example, to include 2 pedigrees 1 and 2 you can do this:
#
#            option pedselect 1
#            option pedselect + 2
#
#        To clear list of pedigrees selected this way, you can select 0 again:
#
#             option pedselect 0
#
#        If option pedselect is currently 0, you can start pedselecting either
#        with or without plus sign, instead of the first example shown above,
#        you could now do this:
#
#            option pedselect + 1
#            option pedselect + 2
#
#        The list of pedselects is written to file when model is saved, and
#        restored when the model is loaded.  Like most other options,
#        model new will restore the default value 0 which means all pedigrees
#        are included.
#
# ****   Version 6.0.8 skipped
#
# ****   New in version 6.0.7
#
# 1.   Discrete and mixed-trait models cannot currently be analyzed by EVD.
#      If the "option modeltype evd" command is given for a discrete or
#      mixed trait model, it will now have no effect, other than to generate
#      a warning message in the maximization output file.
#
# 2.   Mainly intended for testing purposes, a new option "SampleSameTrustMe"
#      disables the phi2 comparisons normally done during EVD processing.
#      This could make a series of EVD maximizations faster, but only if the
#      user is absolutely sure that the underlying sample of individuals has
#      not changed.  Note that the selection of a different set of covariates
#      often changes a sample.  Testing has shown that bypassing the phi2
#      comparisons during large scale mgassoc processing improves performance
#      by about 1%, which is not considered worthwhile in view of the added
#      risk of user error, except inside well written scripts.
#
# 3.   If "premax" verbosity (0x800ff) is selected, the size for each EVD
#      matrix will be reported.
#
# 4.   residual now completes successfully when some individuals are missing
#      the variable sex and sex is a covariate.  Previously it would abort on
#      encountering the first such individual because previously missing
#      sex was not allowed.
#
# 5.   residual now computes residuals for discrete traits (previously you
#      would get an error message).  These residuals are adjusted to be
#      "Pearson Residuals."
#
# 6.   Added documentation of options for "string plot".
#
# ****   New in version 6.0.6
#
# 1.   mgassoc command added for measured genotype association analysis.
#
# 2.   "z" format added to fformat to permit output files having identical
#      precision in fixed and csv formats.
#
# 3.   ped2csv command added to convert pedsys files to comma delimited format.
#
# 4.   selectrecords now ignores BLANK fields and elides them from the
#      comma delimited output.  Previously BLANK fields would cause an error.
#      This fix makes it possible to use selectrecords to reformat pedsys
#      files to comma delimited, as in the new ped2csv command.
#
# 5.   mgassoc_topedsys converts mgassoc.out from comma delimited format
#      to pedsys format for those more comfortable with handling pedsys
#      files.  This can be used after the mgassoc analysis has been run.
#
# ****   New in version 6.0.5
#
# 1.   "fastmod" sets up polygenic model with evd for fastest maximization
#      using evd.
#
# 2.   iteration information is not output for normal EVD models to improve
#      speed.  To restore iteration information, use "evdd" (EVD Debug)
#      instead of "evd" in "option modeltype".
#      
# ****   New in version 6.0.4
#
# 1.   Omega equation only evaluated once for each likelihood evaluation
#      for normal diagonals (phi2=1), and for all non-normal diagonals.
#
# ****   New in version 6.0.3
#
# 1.  Omega equation only evaluated once, unless sample has diagonal
#     members with self phi2 is not 1.0.  (Evaluation only, replaced by
#     approach used in 6.0.4.)
#
# ****   New in version 6.0.2
#
# 1.  EVD model maximization available for sporadic models.
#
# 2.  Omega equation only evaluated for diagonal elements where i and j
#     are equal.
#
# ****   New in version 6.0.1
#
# 1.  A redundant internal copy of the phi2 matrix created during EVD model
#     maximization is eliminated.  This reduces memory requirements greatly
#     for very large pedigrees and may even prevent a SOLAR crash on some
#     machines.  Performance improved by 2x over 6.0.0 version.
# 
# ****   New in version 6.0.0
#
# 1.  EVD modeltype with greatly enhanced speed for polygenic models.
#     To enable this experimental feature, give the command:
#
#     option modeltype evd
#
#     The options is only applicable to polygenic models, for all other
#     models this option will be ignored.
#
# 2.  .solar_model_new file may be created by user, if present this file
#     will be executed at the beginning of a SOLAR session and whenever
#     the "model new" command is given.  This file must only contain the
#     following types of commands:
#
#         option trait parameter covariate constraint omega define
#
#     Any other command, or any invalid command, will cause the entire
#     .solar_model_new file to be rejected and an error returned.
#
# ****   New in version 5.0.0
#
# 1.   Version 5 was developed to improve maximization performance by
#      used pre-compiled FORTRAN code for the polygenic omega.  This
#      was successful in boosting performance about 3x.  However, the
#      EVD modifications used in version 6 boost performance by about 60x.
#
#      Version 5 was only used for certain internal projects.
#
# ****   New in version 4.4.0
#
# 1.   zscore_ prefix now available in define statements to specify
#      zscoring of prefixed phenotype.  Such definitions can be used
#      as traits and covariates.  Mean and Standard Deviation are
#      obtained from the actual maximization sample.  This is meant
#      to replace the old and clunky zscore command (though that
#      command is still available).  "help zscore" describes the new
#      prefix operator.
#
# 2.   -rhopse option added to polygenic to compute and save SE of
#      rhop to SOLAR_RhoP_SE.
#
# [Versions 4.3.4 through 4.3.9 were reserved for development of 4.4.0]
# 
# ****   New in version 4.3.3
#
# 1.   If a rho or other variance component is constrained, it will
#      not trigger a retry if is also at a real boundary.
#
# ****   New in version 4.3.2
#
# 1.   Use of x_variables (variable means) in mu and omega commands for
#      discrete and mixed models is now fixed (never worked before).
#
# ****   New in version 4.3.1
#
# 1.   qtld now writes the output file qtld.out with a one line header
#      identifying the field columns.  Existing scripts which read this
#      file should either skip one line or skip the line which begins
#      with "Trait" (the first column).
#
# ****   New in version 4.3.0
#
# 1.   snp ld -plot is fixed so that it reflects the latest updates to the
#      map file (as loaded with the load map command).  Note that it is
#      still not necessary even to load a map file, but if you do it will
#      control the horizontal axis of the plot.
#
# 2.   joinfiles was supposed to ignore fieldnames duplicated in a single file
#      such as BLANK which can't be selected anyway.  However, if such a
#      field occurred an odd number of times, it didn't get ignored, leading
#      to problems.  This has been fixed.
#
# 3.   The version of selectfields from version 4.2.9 (which allows
#      multiple files including the loaded phenotypes file) could not
#      handle a few cases that the old version could: files in which
#      there is no actual ID field.  So a -noid option has been added
#      to selectfields which makes it work like the old version.  Also,
#      selectfiles no longer complains when loaded phenotype files are
#      also listed explicitly.
#
# 4.   Clarify the rules regarding the user-construction of matrix files
#      in the manual and documentation for matrix and matcrc commands.  The
#      data values should begin in character position 14 or higher.
#
# 5.   Added option -d to polymod.
#
# ****   New in version 4.2.9
#
# 1.   stepfor -par parallel option for stepfor, similar in design to
#      parallel option for stepup.  Only available on SFBR compute ranch
#      using GridEngine.
#
# 2.   Useless parameter rhoe is removed from the rhop test model created if
#      the -testrhop option of polygenic is selected.  Otherwise, this would
#      cause errors in standard error estimation if later attempted by user.
#
# 3.   multipoint -restart was not updating models that had previously
#      terminated with convergence error, but maximized correctly this
#      time.  This has now been fixed.
#
# 4.   At the beginning of each pass through the selected genome, multipoint
#      calls a user script named multipoint_user_start_pass which takes one
#      argument, the pass number (which starts at 1 for the first pass).
#      Within this routine, the user can change the selected chromosomes or
#      interval.
#
# 5.   Now you get an error message if you try to compute residuals for
#      other than a univariate quantitative model.  That was the original
#      design of the residual command, but it was inadvertently broken by
#      the addition of new model types.  You also now get an intelligible
#      error message if you try to compute residuals for a non-univariate
#      model, or any model that does not have a mean parameter.
#
# 6.   Documented the fields in qtld.out and mgeno.out
#
# ****   New in version 4.2.8  ****
#
# 1.  selectfields can now work with multiple input files, and includes the
#     currently loaded phenotypes file(s) by default.  New option -np
#     excludes the phenotypes file, option -list allows you to specify a
#     list of fields from a file, and -sample limits the output to
#     individuals with complete data.
#
# 2.  solar can be started with -key <key> argument to specify user key.
#     This is required for some job queing systems when .solar_reg file
#     cannot be accessed.  This feature was originally introduced in version
#     2.0.4 but was broken by a later update.
#
# 3.   New command showspace uses new command option "doranch finduser -all"
#      to create a sorted list of all /tmp space on the ranch.
#
# 4.   New options -dash and -linestyle for stringplot (plot -string).
#
# 5.   Fixed using .CDE files in parallel stepfor.  Previously code files
#      had to have .cde extension in lower case for this procedure.
#
# 6.   To keep standard errors turned off during the polygenic command, you
#      can now simply give the command "option standerr 0" before running
#      polygenic.
# 
# ****   New in version 4.2.7  ****
#
# 1.  New option -finishlogn for stepup allows completion of previous stepup
#     run which failed due to inability to compute logn.
#
# 2.  stepup now reports log(n) actually used in stepup.avg as well as the
#     incredibly verbose stepup.history.
#
# 3.  New retry method in multipoint prevents many convergence errors.
#
# 4.  "snp ld -plot" is now working again, having been broken by snp update
#     in version 4.2.6.
#
# 5.  Changes have been made to "plot -all" to make it useable again.
#     plot -all now produces and saves all the individual chromosome plots,
#     and a page of "miniplots" if there is more than one chromosome plot.
#     It does not try to display the postscript files, but instead lists
#     the names of the postscript files produced, and suggests using lp to
#     print them.  It also suggests the alternative genome plot command
#     "plot -string" which most people have been using instead.
#
# 6.  stepup -par now handles problem of running out of space in
#     /var/tmp (using by Unix sort) by using tmp subdirectory of home
#     directory instead, if necessary.
#
# ****   New in version 4.2.6  ****
#
# 1.  If snp.genocov is created outside of SOLAR, the maximum number of
#     markers allowed by "snp ld" and "snp qtld" is now increased from 3000
#     to 10000.  The limit is now tested rather than causing error.
#
# 2.  Changed the default method for computing the effective number of
#     SNPs to that by Moskvina & Schmidt, which is more conservative.  The
#     previous method due to Li & Ji may now be selected by option.  And even
#     results from Li & Ji method would have previously been incorrect because
#     of an error in reading snp.ld.dat file.  Note that all these "effective
#     number" calculations are highly experimental at this time.
#
# 3.  snp.ld.pos was previously written when genotypic correlations are
#     computed.  Now it is written whenever LD plot is created, so that
#     any changes to the SNP locations in the map file will show up.
#
# 4.  A check has been added to be sure that there is no SNP in snp.genocov
#     that is not also in the loaded map file, if any.  Previously a missing
#     SNP in the map file would cause a crash.
#
#
# ****   New in version 4.2.5  ****
#
# 1.  For qtld, mean results will be output to the measured genotype file
#     mgeno.out even if the standard errors cannot be calculated.
#     Individual standard errors which cannot be calculated will be
#     reported as zero.  Slightly negative values for h2m will be
#     trancated to zero.  Other error handling has been improved.
#
# 2.  "snp ld" and "snp qtld" commands can now be used even if the SNP
#     genotypes file is not loaded.  All that is required is the snp.genocov
#     file produced by "snp covar".
#
# 3.  "snp ld -plot" can now be done without loading a map file.  Without a
#     map file, however, dummy sequential integer basepair locations will be
#     used.
#
# 4.  "snp ld" now has a window to limit the number of correlations that
#     will be computed.  By default, the window is 1 million basepairs wide
#     so that only those pairs of SNPs separated by no more than 1 million
#     basepairs will be considered.  The window size can be adjusted with
#     a new -window option.  A map file with basepair locations must be
#     loaded to use this option.
#
# 5.  New command "snp effnum" will estimate the effective number of SNPs
#     based on the genotypic correlations among the SNPs.  This is an estimate
#     of the number of independent statistical tests performed using these
#     SNPs and can be used to determine an appropriate significance level
#     for the analysis.  At this time the only method implemented is the
#     method due to Li and Ji.
#
# 6.  Derived estimate of RhoP from version 4.2.4 was incorrect.  It was
#     fixed just before Feb 19 so any result before Feb 19 should be
#     rerun.
#
# 7.  Documentation added for define command so that it is
#     understood how to enquote phenotype names with special characters in
#     angle brackets <>.  Documentation for constraint and omega commands
#     also added.
#
# 8.  Under some circumstances, the snp command would produce an erroneous
#     error message about "Name not found: snp_d".  That has now been fixed.
#
# ****   New in version 4.2.4  ****
#
# 1.  polygenic now computes a derived estimate of RhoP, the phenotypic
#     correlation, for bivariate polygenic models only.  An additional
#     -testrhop option determines the p value for rhop being different from
#     zero by maximizing models with a rhop parameter (rhop.mod) and
#     a rhop parameter constrained to zero (rhop0.mod).  Global variables
#     SOLAR_RhoP and SOLAR_RhoP_P are set to access the rhop and p values
#     after running polygenic, and variable SOLAR_RhoP_OK should be
#     checked to ensure the likelihood of the rhop parameterized model
#     is correct.
#
# ****   New in version 4.2.3  ****
#
# 1.  Under very unusual circumstances relating to retries with multivariate
#     models, or loading unmaximized multivariate models, the SD parameter
#     upper bound could get initialized incorrectly to 1.  This could cause
#     multivariate models to maximize incorrectly to a likelihood which is
#     too small, however there are no reports of this actually happening.
#     Now the standard SD parameter bound initialization will be applied
#     whenever the current SD upper bound is obviously wrong.
#
# 2.  When given an ambiguous command abbreviation, "help" was not helpful
#     in listing all the possible completions.  Now it does.
#
# ****   New in version 4.2.2  ****
#
# 1.  Redesign of overlapping commands "pedigree classes", "relatives", and
#     "relpairs" provides information in a more useful way.  Now classes are
#     listed in descending order of phi2, and, by default, the counts
#     for relationships of 3rd degree or higher, as well as some 1st and
#     2nd degree relationships, are combined, except for "relpairs" and
#     if the -full option is used.  When using -full, an additional option
#     -phi2 adds a column containing the phi2 value for each class.
#     A -model option restricts the scope of the tally to those individuals
#     who have data for the null0 model of current trait.  There is also a
#     -meanf option for computing "Mean f".  The "relatives" and "relpairs"
#     commands now simply invoke "pedigree classes" with options (-model
#     for both and -meanf and -full for "relpairs").  An error in the
#     calculation of "Mean f" has been fixed.  If the -full option is used,
#     a warning is displayed if any of the relative classes cannot be
#     handled by SOLAR's native method for computing MIBD's.
#
# 2.  Quite a few new relationship classes have been added, and missing
#     phi2 values inserted.
#
# 3.  The array size limits in "mibd relate" (also used by "pedigree
#     classes" and related commands described above) are eliminated,
#     except for one limit which may be increased by the user with the
#     new -mxnrel option.
#
# 4.  Error messages are now properly provided for "maximize -who" and
#     "maximize -sampledata", such as for missing covariate or missing
#     omega.  And since these commands are also used by other commands
#     (such as "relatives" and "stepfor") those commands now show proper
#     error messages rather than terminating mysteriously on user errors.
#
# 5.  "selectfields" command added select some fields from a file and
#     copy them to a new file.  In addition to the previously existing
#     "selectrecords" and "joinfiles" command, this permits the user to
#     do basic database operations on comma delimited and Pedsys files
#     from within SOLAR.
#
# 6.  "multipoint -restart" lost records in multipoint1.out under some
#     circumstances.  That is now fixed.
#
# 7.  "stats" can now compute statistics for variables created with a
#     "define" command.  This also fixes the use of some other commands
#     with defined variables.
#     
#
# ****   New in version 4.2.1  ****
#
# 1.  Fix for a multipoint -restart problem.
#
# ****   New in version 4.2.0  ****
#
# 1.   New -max option for stepfor sets the maximum dimension.  stepfor
#      now displays Chi^2 and p value for each model tested.
#
# ****   New in version 4.1.9  ****
#
# 1.   New Official version.
#
# 2.   Cleanup routines for /tmp directories on SFBR compute ranch have been
#      added.  See"help doranch".  These are a set of utilities which can
#      also be used to create custom cleanup procedures, such as
#      the new "stepup -parclean" for cleaning up after an emergency
#      shutdown of stepup -par.
#
# 3.   ID names in pedigree files were unnecessarily limited in size on
#      linux and mac distributions.  Combination of ID and FAMID is now
#      correctly limited to 36 characters on all distributions.
#
# ****   New in version 4.1.8  ****
#
# 1.   The residual command now properly accounts for snp_ and hap_ variables
#      (which are "noscaled" by default) and for the use of the "scale"
#      and "noscale" commands.  Previously, residuals from models which
#      used snp_ and _hap variables or scale/noscale commands would
#      be wrong by a constant, which would be of little consequence for most
#      users (for example, it would not affect the kurtosis).
#
# 2.   The file snp.ld.dat now always contains signed genotypic correlations
#      rather than the previous options (absolute value, squared).  The
#      "snp plot" command now has options to display either the absolute
#      value or square of the correlations.
#
# 3.   "pedigree show" now displays a new column, labeled #bit, which gives
#      the number of bits of complexity for each pedigree.  This number is
#      equal to 2 x #non-founders - #founders.
#
# 4.   Discrete traits were broken in experimental version  4.1.7 (only)
#      because the mean boundaries were not initialized correctly.  That
#      is now fixed.
#
# 5.   When snp names are misspelled in the list file, stepup used to fail
#      with only a "file missing" message.  Now better messages are given, and
#      if a covariate variable is missing that message is shown.  Also, all
#      names in list files (read with the "listfile" command) are trimmed of
#      extra spaces on the left and right sides.
#      
# 6.   A fix has been added for a potential divde-by-zero bug in "snp ld"
#      which would occur when everybody is missing one or the other of a
#      pair of SNPs.
#
# 7.   "stepup -par" had two problems that are now fixed.  If any snp name
#      in the snp list is misspelled, a cryptic error would occur.  Now
#      that error is detected and reported.  Also, for some user
#      environments, parallel stepup would not run because the program
#      name couldn't be determined from the usual convention, so a symbol
#      is used instead.  The symbol, SOLAR_PRGRAM_NAME, had been created some
#      time ago in the "solar" startup script.
#
# 8.   The "snp covar" and "snp qtld" commands now produces files
#      snp.geno-list and snp.haplo-list (respectively) which list
#      all the SNPs processed.  These list files are useful for bayesavg
#      and stepup.  They may be used as-is or edited to list the SNPs
#      desired in a particular case.  Previously it was tricky to get
#      such a list.
#
# 9.   The "snp ld" command now automatically computes and displayes the
#      "effective number" of SNPs, which is useful in correcting
#      association-test p-values for multiple testing.
#
# 10.  New cleanup routines for removing /tmp junk from SFBR Ranch machines
#      have been added.  See "help doranch" for details.  Also "stepup -parclean".
#      doranch is an open-ended tool that permits users to easily write their
#      own cleanup routines.
#      
#
# ****   New in version 4.1.7  ****
#
# 1.  Trait and covariate names up to 80 characters can now be handled
#     without any problem.  Also, the trait command now tests to ensure
#     names are not any longer than 80 characters to prevent problems
#     later.  Previously the limit was 18 characters.
#
# 2.  If a standard parameter (mean, sd, e2, h2r, h2q1) is given a starting
#     value by the user, but the bounds are not initialized, SOLAR will
#     initialize the bounds.  This was supposed to happen in previous
#     versions but didn't in many cases.
#
# 3.  If option maxiter is set to 1, which only provides the likelihood for
#     the starting values, no retries are done for boundary errors,
#     misconvergence, incorrect quadratic, or any other such issue in
#     maximization.  This is generally what you would expect for having
#     maxiter set to one, and in this case all retries would be futile anyway
#     since the starting values are never changed no matter how many retries
#     are done.
#
# 4.  Problem with freq command on linux fixed.
#
# 5.  All the remaining error messages that prompted the user to report
#     problems to "solar@darwin.sfbr.org" have been changed to prompt
#     the user to report problems to "solar@txbiomedgenetics.org".  These
#     were for very unlikely scenarios that have probably never occurred.
#     This change had already been made in all the documentation.
# 
# 6.  If restarting a multipoint analysis in which there were convergence,
#     boundary, or other errors, the results with errors will be removed
#     and replaced by the new results, rather than being left in as
#     duplicates.
#
# 7.  The -logn option now works properly in stepup.
#     
# ****   New in version 4.1.6  ****
#
# 1.  Pedigree files need no longer include mother and father ID fields if
#     the "load pedigree" command is used with the "-founders" option.
#
# 2.  Missing value is now allowed for sex.  The missing value may be coded
#     as 0, U, u, or blank.  If a parent is found with missing or incorrect
#     sex code, e.g. female for a father, the code is automatically fixed
#     in the pedindex.out file and a warning is given instead of stopping
#     with an error.
#
# 3.  If a parent record is missing, a founder record is automatically
#     created in pedindex.out and a warning is given instead of stopping
#     with an error.
#
# 4.  More robust handling of SNPs or markers for which there is no genotypic
#     data in the genotypes file.
#
# 5.  The "snp ld" command now offers 3 options for correlations: rho,
#     abs(rho), and rho^2.  rho^2 is now the default.  The -locn option
#     is removed.
#
# 6.  Maximum size of phenotypes (and tablefile) records is increased to
#     800,000 characters, appropriate to recently increased maximum of
#     40,000 fields.
#
# 7.  It is now possible to specify terms having parentheses in constraints
#     without using angle brackets.  For example, the constraint:
#
#     constraint <e2(q4)> + <h2r(q4)> = 1
#
#     may now be specified much more nicely as:
#
#     constraint e2(q4) + h2r(q4) = 1
#
#     However, this is still not true for the omega specification, and
#     the polygenic and other commands still use the angle brackets by
#     default.  Those qualifications will be addressed in future updates.
#   
# ****   New in version 4.1.5  ****
#
# 1.  multipoint -cparm is now much less likely to cause convergence errors
#     because the prototype model is reloaded at the beginning of each
#     chromosome, and whenever there is a gap greater than 11cm.
#
# 2.  The linux edition version of mlink (used during ibd file creation) had
#     the default fastlink limits instead of the bigger SOLAR limits.  This
#     has now been fixed. (For example, in the linux edition you used to be
#     limited to 25 alleles per marker instead of 100, and 1000 individuals
#     instead of 10000.)
#
# ****   New in version 4.1.4  ****
# 
# 1.  power now works for discrete traits, and a -prev option
#     has been added to specify the prevalence of the trait.  A new
#     more accurate method of estimating power as a function of
#     observed h2r is used.  The method for smoothing the power curve
#     has also been improved.
#
# 2.  h2power now catches the error in which "simqtl.phn" appears to be
#     the loaded phenotypes file after an earlier h2power run was aborted.
#
# 3.  chinc now catches errors that could occur in the case of a very
#     large non-centrality parameter (lambda > ~740).
#
# ****   New in version 4.1.3  ****
#
# 1.  polygsd is now fixed for multivariate models.  The problem was that the
#     external kinship matrix was not being loaded, which is required for the
#     standard handling of unbalanced traits.
#
# ****   New in version 4.1.2  ****
#
# 1.  There is now a parallel stepup procedure (stepup -par) designed
#     for use at SFBR's gridware based GCC "ranch".  This is highly
#     EXPERIMENTAL and not supported elsewhere.
#
# 2.  Further improvements have been made to stepup so it produces the
#     same results as bayesavg in even larger cases.  The "strict" rule
#     is now suspended until the very end.  This keeps models that may
#     seed higher dimension models that appear in the final window.
#
# 3.  The maximum numbers of phenotypes, parameters, and covariates
#     have been increased to 40000.
#
# 4.  h2power now works for discrete traits, and a -prev option
#     has been added to specify the prevalence of the trait.  A new
#     more accurate method of estimating power as a function of
#     observed h2r is used.  The method for smoothing the power curve
#     has also been improved.
#
# 5.  Added -qter option to "mibd prep" which extends the range of MIBD
#     estimates to the end of the chromosome when using imported MIBD's.
#     "mibd prep" no longer requires the map file to be ordered by
#     marker location.
#
# 6.  Added "snp unload" command to unload both marker and map files.  Error
#     handling is improved for the "snp load" command.
# 
# 7.  Added "map names" command which displays the names of the markers in
#     the map as a list on a single line.  Added "map chrnum" command which
#     returns the chromosome identifier from the map file.
#
# 8.  Previously only 2000 markers were allowed for MIBD computation.  This
#     has been increased to 3000 markers.
#
# ****   New in version 4.1.1  ****
#
# 1.  The EXPERIMENTAL stepup algorithm has been improved to capture
#     the same models as bayesavg, and it now creates window and average
#     files just like bayesavg.  The new algorithm applies a looser selection
#     rule, including all models better than the null model, up to the
#     3rd degree of freedom, which can be set to other df's with the
#     -cornerdf option.  THIS IS STILL EXPERIMENTAL.
#
# 2.  Both stepfor and stepup now retain the fully_typed() covariate upon
#     successful completion, however it may be removed using the new
#     "stepclean" command, which also unloads the fully_typed.out phenotypes
#     file.
#
# 3.  There was a problem during the "sort" phase of bayesavg on some linux
#     systems.  This was supposed to be fixed in 4.1.0 but wasn't completely
#     fixed.  Now it is.
#
# 4.  Multiple runs of stepup or stepfor could cause a fatal error related to
#     the multiple loading of the fully_typed.out phenotypes files which is
#     now fixed.
#
# 5.  polygenic and residual commands had problem with ID names containing
#     an embedded space.  That is now fixed.
#
# 6.  SOLAR now works on some 64-bit and other linux systems on which it
#     previously crashed during user registration.
#
# ****   New in version 4.1.0  ****
#
# 1.  Important fix made for multivariate use of traits created with the
#     define command.  Under certain circumstances, maximization would
#     fail with a cryptic message, or trait values would get replaced with
#     covariate values during maximization.  The latter would typically
#     result in convergence errors, but could also cause extremely strange
#     results.  
#
#     (Two basic circumstances would elicit this problem: 1) First trait
#     defined, second trait ordinary phenotype, combined with two or more
#     phenotypes files loaded simultaneously in which second trait is NOT
#     found in first phenotypes file but in a later one, 2) First trait
#     defined, combined with two or more other traits (trivariate or higher)
#     at least one of which is an ordinary phenotypic value.)
#
# 2.  If zscore is in effect and the trait is changed without invoking
#     "model new", zscore will be canceled.  This is considered more
#     intuitive than having "zscore" apply to subsequent traits by
#     default, but only if "model new" is not given.  Note that it is
#     recommended to give the "model new" command before changing traits
#     anyway.  (Note: more changes to zscore were made in version 4.0.9.)
#
# 3.  On some of the newest versions of linux, bayesavg would not sort its
#     output file, resulting in incorrect operation.  This has been fixed
#     by using the correct "sort" arguments depending on what system is being
#     used.
#
# 4.  Full plotting support is now provided for Intel Macs.
#
# 5.  The linux edition of SOLAR is now provided in two binary forms: static
#     and dynamic linked.  It appears that some linux distributions work with
#     one better than the other, and vice versa.
#
# 6.  Sun's newest compilers are used for the Sun/SPARC edition of SOLAR.
#     It has been tested, but keep your fingers crossed.
#
# ****   New in version 4.0.9   ****
#
# 1.  zscore has been changed.  The mean and SD used are taken from
#     the actual sample that will be used in maximization, instead of
#     from all trait values in the phenotypes file.  Often the actual
#     sample is a smaller set, restricted by the availability of
#     covariates.  zscore is also much less verbose.  Now, only one
#     line is displayed for each trait showing the trait name, mean
#     and standard deviation.  Even that line is not displayed if zscore
#     is called in a script, or if the "zs" abbreviation is used to
#     invoke the command.  The warnings about zscore being obsolescent have
#     been removed.
#
# 2.  stepfor command added to do forward stepwise covariate screening.
#
# ****   New in version 4.0.8   ****
#
# 1.  Residual fixed when there are many many phenotypes files loaded
#     (total characters in filenames exceeding 1000).  Residual now uses
#     currently active phenotypes files rather than list found in maximization
#     output file because the latter gets truncated at 1000 characters.
#
# 2.  joinfiles -add and -list options added which permit the use of
#     wildcards, directories, and a file containing names of files to be
#     joined.  These options also overcome the system limit on open files
#     so you can join thousands of files at a time.  For example:
#
#         joinfiles -all phen.*
#
# 3.  polygsd and linkqsd commands have been extended to support multivariate
#     models with the esd, gsd, qsd1 parameterization.  In this version,
#     however, linkqsd doesn't work with trivariate and above...that will
#     require changes to the omega command in a future version.
#
# ****   New in version 4.0.7   ****
#
# 1.  Version 4.0.7 is now an Official public release, the first
#     Official release since 2.1.4.  All incremental updates that
#     went into 4.0.6 over several months are included.  Early
#     beta copies of 4.0.6 did not include all of these updates,
#     so a new version number is now used.
#
# 2.  Added warning about using the zscore command.  See "help zscore".
#     
# ****   New in version 4.0.6   *****
#
# 1.  Definitions may now be used as covariates as well as traits.
#     This includes the ability to apply the inverse normal
#     transformation to variables used as covariates.
#
# 2.  The variable "sex" may be used in all definitions.
#
# 3.  Problems with multivariate discrete and mixed quantitative and
#     discrete models have been resolved though a refinement in the
#     conditioning step of the discrete likelihood evaluation code.
#
# 4.  lodadj is now permitted for multivariate models.
#
# 5.  multipoint -cparm now permits multivariate models
#
# 6.  bayesavg -qtn and -cov now report h2q1 results if h2q1 is
#     present and not constrained to zero.
#
# 7.  Maximization errors during qtld with some snps don't prevent it
#     from going on to other snps.
#
# 8.  On Mac OS X, all forms of plotting are now supported.
#     See notes in README.Mac in Mac distributions dated
#     December 22, 2006 or later.
#
# 9.  plot -title is now supported for regular multipoint plots.  Also
#     plot -subtitle is available to set subtitle.  For regular and
#     string plots either "" or " " may be used to specify a blank title.
#     
# 10. For "plot -max <pos> -overlay", test for plot width is done correctly
#     to allow overlay of same or narrower width.  Incorrect error message
#     for attempted wider overlay is corrected and reworded for clarity.
#
# ****   New in version 4.0.5   *****
#
# 1.  Very large pedigrees now require much less memory for loading because
#     unrelated individual pairs are no longer included in the phi2.gz matrix
#     file.
#
# 2.  simqtl no longer requires that user provided genotype files be in
#     exactly the same order as pedindex.out.
#
# 3.  The command "ibs" had unknowingly been broken in Version 4.0.0, it is
#     now fixed.  Probably nobody noticed.
#
# 4.  Show actual file error when reading phenotypes file.  Previously,
#     the "missing FAMID" error message was shown incorrectly in many cases.
#
# 5.  toscript command is nicer.  Command numbers and script name and -ov
#     argument can be in any order (see updated documentation for details).
#     Invalid argument formatting checked in advance.  If there is an error,
#     a bad script (which prevents SOLAR from starting) does not get created.
#
# 6.  pedlike now works for discrete and mixed models
#
# ****   New in version 4.0.4   *****
#
# 1. Mac edition of SOLAR 4 is now available (on request).
#
# 2. "newmod" command takes place of "model new", performing
#    "outdir -default" automatically, and optionally setting trait.
#    (model new command itself remains unchanged.)
#
# 3. The linux edition of SOLAR is now statically linked, in the hope of
#    resolving library conflicts that sometimes occur with linux.
#
# 4. max # markers increased to 3000, max # in a pedigree increased to 30000
#
# 5. "snp qtld" now 2x faster and -xlinked option allows X-linked SNPs.
#
# 6. "mibd prep loki" now defaults to letting Loki estimate allele freqs,
#    but -usefreq option forces use of currently loaded freqs.
#
# 7. map file need not be sorted by user, it is automatically sorted during
#    loading.
#
# 8. New deputy registration feature allows select sites to make
#    solar keys for local use only.  See "deputy" command for more
#    details.
#
# 9. register command is more user friendly.  You can overwrite invalid
#    key immediately, or choose to overwrite one believed to be valid.
#    Key is validated immediately.  Lengthy "registration help" message
#    is only displayed if there is no .solar_reg file so that error
#    message can be seen easily.
#
# 10. Trait qualification for covariates now checked for redundancy and
#     inconsistency.
#
# 11. bayesavg defaults to polygenic model type if the omega is undefined.
#     Previously, bayesavg gave a confusing error message unless the
#     user knew to run the command polymod first.
#
# 12. It is now no longer necessary to give the command "verbosity min"
#     when running commands to compute freq mle's, ibd's, or mibd's.  The
#     normal verbose output is automatically suspended during the operation
#     of these commands.  Also, load freq and load marker have improved
#     error handling.
#
# 13. plotqtld command added to plot various qtld results.
#
# ****   New in version 4.0.3   *****
#
# 1.  unbalanced trait-specific covariates now permitted in sample
#
# Trait-specific covariate variables are now only required in the sample
# of the trait to which they apply, so you get the maximum sample size
# possible for every trait in a multivariate analysis even when you have
# covariate variables that are only defined when one of the traits is
# defined (i.e., unbalanced covariates).
#
# Though they were handled as documented, there was a serious flaw in the
# practical usage of the previous implementation of trait-specific covariates
# which some might have overlooked.  Sample sizes might have been
# unnecessarily reduced because trait-specific covariate variables were
# required even in the sample of traits to which they did not apply.  For
# example, the following did not work well if there were some drinkers who
# never started smoking:
#
#     trait drink smoke
#     covar age_started_smoking(smoke)
#
# Even though the "age_started_smoking" variable would never be used in
# estimating the mean for "drink", it used to be required in the sample
# for "drink" anyway, thus rendering moot the normal ability of SOLAR
# to handle unbalanced traits.  Now "age_started_smoking" would not
# be required in the sample for "drink".
#
# As is further described in Note 7 in the documentation for the covariate
# command, in a UNIVARIATE analysis ALL covariate variables are still
# required in the sample even if they are specific to a different
# trait.  This behavior may be changed in a future update, and for that
# reason the use of trait-specific covariates is not recommended in a
# univariate analysis. There is no real need for them anyway.  Also,
# variables which appear in a user-specified mu or omega command are (at
# this time) required for all traits.
#
# "null" covariates (such as clinic() ) will always be required for all
# traits because "null" covariates were designed for the specific purpose of
# restricting the sample to those having the null covariate variable.
#
# 2.  unbalanced defined traits now permitted
#
# SOLAR now only requires the phenotypic variables actually used in the
# definition of any trait (using the "define" command) in the sample
# of that trait. It determines the variables used in the the definition
# of any trait, and specifically requires those variables in the sample
# of that trait, but not necessarily any other trait in a multivariate
# analysis. Missing any of the variable(s) used in the definition of
# a trait is equivalent to missing that defined trait.
#
# Previously, all the variables in all definitions were required in the
# sample of every trait, even if those definitions were unused.
#
# 3. h2power
#
# A new command "h2power" is added to compute the power to detect a trait
# with a given heritability (h2r) rather the power to detect a given effect
# size (h2q1). Currently this is restricted to a single trait (univariate).
# There is also a new plot option, "plot -h2power" to plot the results.
# See the h2power documentation for more details.
#
# 4. Upgrade to Tcl 8.4.12
#
# For the first time in 6 years, the version of Tcl used by SOLAR has been
# upgraded to version 8.4.12.  The motivation for this was that the newer
# TCL now supports 64 bit integers, which may be useful in dealing with
# numbers and lists larger than 2 billion.  But there are also many other
# new features in the latest Tcl which we may in time come to appreciate.
# Fortunately, performance and reliability enhancements in Tcl 8.4.x have
# eliminated problems that arose in intermediate versions 8.1.x through 8.3.x
# (some of those versions were notoriously slow, for example).
#
#
# 5.  Fix embedded blanks in ID for inormal
#
# Previous 4.x versions could not handle ID or FAMID fields with embedded
# blanks using the "inormal" command or "inormal_" in trait definitions.
# That is now fixed.
#
# ****   New in version 4.0.2   *****
#
# 1.  NEW DISCRETE TRAIT CODE
#
# The discrete trait code has been extensively rewritten and is far better
# than before.  The "mean" parameter now accurately tracks the "threshold"
# as intended, and can range from -8 to 8 in standard deviation units
# (which is about all that can be done in double precision).  Operation of
# the new discrete trait modeling code has been verified through extensive
# simulation.  Heritabilities and likelihoods may differ significantly from
# previous versions when discrete traits are involved.  The "mean" parameter
# is now correctly initialized (when maximization starts) to the inverse
# normal of the difference from 1 of the prevalence.
#
# Unfortunately, the new code does not appear to handle multivariate mixed trait
# (quantitative and discrete) or multivariate discrete models correctly.
# Trait correlations appear to go too high.  So in these cases, use an earlier
# (or later) version of SOLAR.
#
#
# 2. "print" function for debugging omega, define, and mu
#
# You can now use a "print" function in SOLAR omega, mu, and define statements.
# This function prints and then returns the current value of its argument, which
# can be any expression valid for the command.  For example, in place of the
# standard omega:
#
# For example, in place of the standard omega:
#
#  omega = pvar*(phi2*h2r + I*e2)
#
# You could have:
#
#  omega = pvar*(print(phi2)*h2r + I*e2)
#
# and this would print each phi2 value as it used.  An omega,
# mu, or define statement can include any number of print functions.
# They are evaluated in the order of evaluation, starting with the
# innermost subexpression.  If you simply want to print some value
# without including it in the rest of the expression, you can multiply
# the print function by zero, for example:
#
#  omega = pvar*(phi2*h2r + I*e2 + 0*print(delta7))
#
# As each print function is evaluated, it is printed in your terminal,
# and you may press RETURN to continue to the next.  To skip past a lot
# of prints, simply hold down the RETURN key.
#
#
# 3. globals for accessing polygenic results
#
# The polygenic command creates 4 global variables which may
# be accessed later (which is often useful in scripts).  The
# variables are:
#
#   SOLAR_H2r_P        p value for h2r
#   SOLAR_Kurtosis     residual trait kurtosis
#   SOLAR_Covlist_P    list of p values for covariates
#   SOLAR_Covlist_Chi  list of chi values for covariates
# 
# The covariate lists are created only if the -screen option
# is used.  All screened variables are included, regardless of
# whether they were retained in the final model.  Before you
# can access any of these variables in a script, you must
# use a "global" command.  For example:
# 
#   global SOLAR_Kurtosis
#   if {$SOLAR_Kurtosis > 4} {puts "Very bad kurtosis!"}
# 
# 
# 4. chromosome command options: all * show showm
#
# It is now possible to select "all" available chromosomes for multipoint
# scanning using the command "chromosome all" or "chromosome *".  This will
# include numeric as well as alphanumeric (such as 11p) chromosomes.  Also,
# a new command "chromosome show" will display all available chromosomes,
# and a command "chromosome showm" will show all mibd files selected by
# the current "chromsome" and "interval" commands.
#
# 5. custom omegas for trivariate and above models
#
# It is now possible to create custom trivariate and above models without
# including the previously mandatory rho parameters such as rhoe_12, rhog_12,
# rhoc_12, rhoq1_12.  Sometimes it is convenient to write custom omegas where
# such rho's are implicitly assumed to be 0, 1, -1 or some other value.
# Now, only if the particular rho appears in the omega, or if the
# corresponding generic parameter (such as rhoe_ij) appears in the omega,
# are the rho parameters required.  Also, if a parameter required by the
# omega is missing, an error message specifically identifies the
# missing parameter by name (such as mean(q1).
#
# Also, there are now extra generic rho's available for custom use.  The
# available generic rhos are now rhoa_ij to rhog_ij and rhoq1_ij to rhoq10_ij.
# Thus, there are generic parameters rhoa_ij, rhob_ij, rhod_ij and rhof_ij
# which have no previously reserved function.
#
# Also, there are now special "si" and "sj" variables which return the indexes
# of traits i and j (1..ntraits).
#
# 6. normal -inverse
#
# Not to be confused with the "inormal" command and "inormal_" define command
# operator (described below as "new in version 4.0.0") there is also a new
# "normal" command which currently has only one option: -inverse.  This will
# return the inverse normal for any single p value.
#
# 7. maximize -out
#
# Previously "maximize" only permitted "-o" and "-output" options to identify
# the output filename.  This was inconsistent with most other SOLAR commands,
# which use "-out".  So, maximize now allows "-out" also.
#
# 8. option BounDiff
#
# One small part of the improvement in handling discrete traits is how
# maximizing is done near parameter boundaries.  Part of the improved
# method may be altered for unusual "hard" boundaries using the
# "option boundiff" command.  See "help option" for details.
#
# 9. Missing mean parameter message
#
# A missing mean parameter is now identified by name, such as
# "Missing parameter mean(q1)" instead of by the confusing phrase
# "Missing mean parameter for trait q1".
#
# 10.  New mlink
#
# A new version of mlink from the latest fastlink release is used during
# mibd creation.  This was necessary to run on recent linux releases, but other
# than that, we are not aware of any operational difference.
#
# 11.  qtld
#
# A problem with snp names longer than 10 characters was fixed on Dec 13 2005.
#
# 12.  define inormal_
#
# inormal_ prefixed variables in define statements can now be used in expressions such
# as:
#
#       define a = inormal_q4 * 10
#
# or
#
#       define a = inormal_q4 >= 1
#
# 13.  "ibd prep merlin" and "ibd import merlin"
#
# Added commands for using merlin to compute IBD files.
#
# 14.  "mibd prep" starting location 0
#
# "mibd prep" now requests starting MIBDs with location 0 instead of
# first marker.
#
# ****   New in version 4.0.0   *****
#
# 1. The "define" command
# 
# The "define" command enables you to define an arithmetic expression to use
# as a trait.  The expression may include phenotypic variables, math operators,
# functions, and constants.  For example, give a quantitative phenotype named
# "q4" you could define an expression named "alpha" based on a normalized log
# of that phenotype:
# 
# define alpha = 100*log(q4/11.43)
# trait alpha
# covariate age sex age*sex
# polygenic -s
# 
# Now you can apply any desired transformation to a trait within SOLAR itself.
# Definitions become part of models, and are saved to them and loaded from
# them.  Eventually, the use of definitions will be extended to covariates.
# 
# The math functions available include everything available in C++.  See
# "help define" in solar4 for a list.
# 
# By default, all definitions result in "quantitative" traits, unless
# the top level operator is a comparison operator such as >=.
# Comparison operators return 0 (false) and 1 (true) and so produce
# "discrete" traits.  The comparison operators are available in two
# varieties: Fortran and C-like:
# 
# FORTRAN    C-like
# .eq.       ==
# .ne.       !=
# .gt.       >>
# .lt.       <<
# .ge.       >=
# .le.       <=
# 
# (Note the use of >> for > and << for < in the C-like operators.  This was
# necessary to be compatible with other parts of SOLAR syntax: < and > are
# already used.)
# 
# 
# 2.  Comparison operators in mu and omega
#
# The comparison operators described above are now also available for
# use in the "mu" and "omega" commands.
# 
# 3.   Inverse Normal Transformations
#
# Within the "define" command, there is a now a built-in "inormal_" (inverse normal)
# transformation which has quickly become quite popular since it makes
# traits with problem distributions (such as extremely high kurtosis)
# behave as if having something close to a standard normal distribution.
#
#  define a = inormal_q4
#  trait a
#
# See "help inormal" for a complete description of this transformation.
# It is also possible to do relational tests such as >= on expressions
# to produce expressions which are like discrete phenotypes.
#
# There is also a separate command "inormal" which performs the inverse
# normal transformation on a variable in a file and writes out a new file.
# For traits, it is more convenient to use the "inormal_" operator in
# a define command, but if you need to use this transformation for
# covariates, you must currently use the "inormal" command to write the
# transformed values to a file first.  This should not be confused with
# obtaining the inverse normal for a single p value, which is done by
# the "normal -inverse" command described below.
#
# 3.   plotqtn (and qtnm) handle huge X values better
#
#      The qtnm and plotqtn commands now handle large numbers (up to 2B
#      anyway) nicely.  Tick labels are now kept in "integer" format as
#      long as possible (up to 2 billion), and extra spacing is allowed for
#      large X tick labels so they don't run into each other.
#
# 4.   Phenotypes file change detected and handled automatically
#
#      Changes that might be done to the "phenotypes" file(s) after they are
#      first loaded are now detected and "fixed" at the beginning of
#      maximization. If necessary, the phenotypes file is automatically
#      re-indexed.  Also, change in the field mapping of FAMID is detected
#      and handled automatically.  Previously changing a phenotypes file
#      outside SOLAR or changing the field mapping of FAMID could have cause
#      SOLAR to crash or give incorrect results.
#
# 5.   FAMID tested more consistently in multiple phenotypes files
#
#      Previously you might get inconsistent and incorrect results and even
#      cause SOLAR to crash if one phenotypes file has a field named "FAMID"
#      in it but the others don't.
#
#      Now the presence or absense of a FAMID field is handled more
#      consistently across all phenotype files as a group.
#      If any phenotypes file lacks the FAMID field, and FAMID is required,
#      that is properly detected as an error.  If one or more phenotypes
#      files have a field named FAMID, but the ID's are unique anyway, that
#      will not be a problem.
#
# 6.  maximize -sampledata
# 
# There is also a new option to the maximize command called
# "-sampledata" and it causes maximize to write out the sample data,
# including computed trait values (if applicable), to a file called
# sampledata.out in the current maximization output directory.  IT DOES
# NOT MAXIMIZE, IT ONLY WRITES OUT THE SAMPLE DATA THAT WOULD BE
# MAXIMIZED.  This was needed in order to seamlessly compute the
# "residual kurtosis" for the polygenic command when definitions are
# used, but you might also find it useful for other purposes.  It is a
# comma delimited file that can be read by "solarfile" and "tablefile"
# and could be used as another phenotypes file by SOLAR.  This file
# includes only the individuals included that would actually included in
# the analysis.
# 
# 8.  Stringplot overlays
#
# The "plot -string" (and "stringplot") command now accepts options
# which facilitate overlaying multiple LOD plots.  The -name option lets
# you save any stringplot for later overlay usage.  The -layers option lets
# you add previous plots to any new plot.  The -replay option lets you
# display any combination of previous plots.  Also, a -color option is
# available for stringplots now (replacing the previous -colorname)
# and accepting X11 defined color names.  The specification of layers
# may default color specification, use previously stored color specifications,
# or make new color specifications.
#
# 9.  Pedigree Matrix Checking
#
# Matrix files are now automatically checked to be sure that they were created
# using the currently loaded pedigree.  To get the benefit of checking, you
# must consistently use SOLAR version 4.x for both creating and using matrix
# files.  Otherwise, to allow both forward and backward compatibility, no
# checking is done.
#
# A polynomial CRC (Cyclic Redundancy Check), sometimes called "checksum"
# but much harder to fake than a simple sum, is added to the beginning of
# each matrix file as the 1,1 (or first) element.  Since it is immediately
# followed by the real 1,1 value, it has no effect on matrix operations
# and is backwards compatible with previous versions of SOLAR.  Currently,
# the lack of a CRC is ignored to allow older matrix files to be used, but
# if the CRC is present it must match. You can also bypass checking
# by editing out the CRC line at the top of the matrix. The private
# "matcrc" command is used to add CRC's to matrix files.  The possibility
# of the CRC not detecting a pedigree/matrix mismatch is astronomically small.
# Since the checking is based on the actual data content, it is
# unaffected by file copying and creation dates.
#
# ****   New in version 3.0.4   *****
#
# 1.  Household effects are now supported for multivariate models with
#     more than 2 traits.  (Attempt to run houshold analysis with
#     more than 2 traits in previous versions of SOLAR 3.x would have
#     yielded invalid results.)
#
# 2.  When exporting ibd or mibd files, it is no longer necessary to
#     re-enter the ibddir or mibddir in the current session.
#
# 3.  mibd export is now compatible with SimWalk2 Version 2.91.  An option
#     "-version 2.82" supports the earlier version.  Also, marker names
#     are checked for uniqueness in 8 characters as required by SimWalk2.
#
# 4.  The snp and snphap commands no longer limit SNP allele names to only
#     one character.  The snphap command checks for uniqueness in 8 characters
#     as required for SimWalk2.
#
# 5.  Multivariate household models are now supported.
#
# 6.  The "multipoint" command now works for multivariate models without
#     using the -cparm option.
#
# 7.  The "qtld" command now produces a measured genotype file, mgeno.out.
#     The format of the output produced by qtld has been greatly improved.
#     Analyses run with SOLAR versions prior to July 19 2005 should be re-run.
#
#     An additional problem for big qtld runs was fixed on November 18 2005.
#     An error message "Unable to open simple output file" could appear.
#
#     A problem with snp names containing hyphen was fixed on Nov 28 2005.
#
# 8.  mibd import/prep
#
#     "mibd import" now checks unrelateds for for non-zero MIBD;
#      Kosambi->Haldate conversion in SOLAR since Loki didn't do it correctly
#
# 9.  ibd import
#
#     "ibd import" now checks unrelateds for non-zero IBD; -xlinked
#     option added to do appropriate checks for that case.
#
# 10. plot -min
#
#     The "plot -min" option now works properly, adjusting the scale and
#     removing markers that are out of range.  You can use the -min and -max
#     options to zoom in on a particular part of the plot.  Using the xmgr
#     menus to do this is likely to mess up the display of marker positions.
#
#
# 11. Mixing ID and EGO in multiple files
#
#     Loading multiple pedigree files where one used "ID" as the indentifier
#     and another used "EGO" previously caused a problem with "residual"
#     because the redundant EGO field was not removed from an intermediate
#     file by the joinfiles command.
#
# 12. joinfiles fixes
#
#     In addition to the above EGO/ID fix for joinfiles, another problem with
#     multiple occurrances of the same named field in the same file was also
#     fixed.  This commonly occurs when PEDSYS files have a field named BLANK.
#
# 13.  dominance
#
#     The command "dominance-notes" gives pointers on how to do analysis of
#     dominance (it's described in Section 9.4 of the documentation).
#
# 14.  fformat
#
#     fformat is a replacement for the Tcl "format" command which enforces
#     field widths for floating point numbers better than "format".  The
#     modified %f and %e formats do a better job enforcing field widths
#     by reducing precision slightly when required, and a new %y format
#     is added which works much  %g format, except it reverts to exponential
#     format and reduces precision to maintain column width much better.
#     Even while maintaining column width as well as possible, fformat
#     prevents non-zero value from being printed as zero (except for the
#     true fixed width %f format).  Also, a center justification feature
#     is added.  fformat is used by qtld to make for nicer printing of
#     results.
#
# 15.  allsnp hint
#
# A hint is added to qtnm to use the allsnp command first.
# 
# ****   New in version 3.0.3   *****
#
# 1.  loaded "map" file used in qtnm and plotqtn
#
# solar3 now uses a loaded map file for most stages of QTN analysis,
# including the qtnm and plotqtn commands.  In earlier SOLAR versions,
# you would encode the SNP location into the snp name, but those names
# did not match up with the common SNP names.  So now the name and the
# location are entirely separate, but related through a map file, which
# is now a standard map file (though the "basepair" function would
# typically, if not necessarily, be specified instead of Kosambi or
# Haldane).  
#
# If you are using the old-style SNP names, and don't have a
# map file for them, you can invoke the qtnm command with the "-nomap"
# option.
#
# 2.  qtld command does association analysis for SNPs
#
# 3.  plot -max and -min arguments fixed
# 
# The -min and -max options to plot were not working as intended, but now they
# are fixed in version 3.0.3.  Previously, if you specified -max or -min,
# several bad things might happen.  Now:
# 
#   a) Marker labels will NOT get pushed to the right of the plot window, and
#   possibly off the page.
# 
#   b) Marker ticks outside the specified range will not be included.
#   Previously they could go on and on and outside the plot window.
# 
#   c) The LOD curve will be truncated at the exact -max and -min points
#   specified.  Now there is a 5% blank margin at the beginning and end of the
#   plot window, as intended and as there is when -max and -min are not
#   specified.  This give you complete control over what the displayed range
#   is, and makes plots look neater.
# 
# 4.  allsnp
# 
# The "allsnp" command includes all covariates which are SNPs in the
# current model.  This makes it easier to run qtnm.
# 
# 5.  Plotting for qtnm (plotqtn) now scales the X axis automatically.
# Previously the X axis was simply set by the qtn.gr file, which often
# caused range and "too many tics" problems because people didn't bother
# to edit the file as needed.  Note: If you have a custom qtn.gr file,
# it will override automatic scaling.  I recommend you fetch a new
# qtn.gr file from /opt/appl/solar/3.0.3/lib/qtn.gr and add your
# customizations to that, or comment out the following lines by
# prepending # in front of them:
# 
#     @xaxis tick major 2500
#     @xaxis tick minor 500
#     @xaxis ticklabel start type spec
#     @xaxis ticklabel start 0
# 
# 6. Memory leaks in simqtl and tablefile fixed.
# 
# 7. Maximum number of MZTWINs per pedigree increased from 1 to 10.
# 
# 8. Save Hardy-Weinberg Equilibrium (HWE) and allele freq Standard Errors
#    (SE) in external freq files created with 'freq save' which are
#    loaded when freq files are loaded.
#
# 9. simqtl gets kinship coefficients from phi2.gz rather than computing
#    on-the-fly to allow user-supplied values.
#
# 10. Fix bug in 'snp ld -plot' which caused command to fail in some cases.
#
# 11. (11-Apr-05) Fix error in implementation of algorithm for computing
#     QTLD covariates.  Analyses using these covariates should be rerun
#     from scratch because this will affect results.  Create QTLD covariates
#     file from SNP genotype covariates file rather than directly from SNP
#     data for efficiency.
#
# 12. Change snp plot x-axis label to "Nucleotide Position".
#
# ****   New in version 3.0.2   *****
#
# 1.  Experimental support for bivariate models with "mixed" quantitative
#     and discrete traits or multiple discrete traits.  Trait types should
#     be autodetected correctly now, so no option is required for simple
#     mixed trait models.
#
# 2.  The "mean parameter" in a discrete trait model has changed so that
#     the sign is now reversed from before.  The "mean parameter" in a
#     discrete trait model is now the distance of the threshold from
#     the mean, whereas previously it was the distance of the mean from
#     the threshold.
#
# 3.  Multiple phenotypes files supported.  Simply list them all in
#     "load phenotypes" command.
#
# 4.  SNP processing is now available with snp and snphap commands.
#
# 5.  -nose (skip standard error calculations) and -hwe (test Hardy-Weinberg
#     Equilibrium) options have been added to "freq mle" command.  When -hwe
#     option is chosen, the test results are shown by the "marker show"
#     command.
#     
# 6.  "lod" command can compute multivariate LODs corrected to 1df for any
#     number of traits.  "lod" command now allows arguments and options
#     with an improved syntax replacing the cumbersome "clod" command.
#     A new -v option shows exactly how the LOD is calculated.
#
# 7.  "option discreteorder" can be used to reverse ordering for
#      multivariate discrete, or print out ordered data to file
#      discrete.out.
#
# 8.  The -cparm and -ctparm options of multipoint now permit embedded
#     expressions as well as parameters to be printed to the output.
#     Also, a -se option has been added to multipoint and linkmod to
#     enable the computation of standard errors, which may be output
#     using either -cparm or -ctparm option.  See multipoint documentation
#     for an example.
#
# 9. "loadbinary" command added to load Tcl binary packages.  (Most Tcl
#     applications use a "load" command for this, but we already use "load"
#     for other things.)  Many Tcl binary packages are available, including
#     one which provides command line editing.
#
# 10. selectrecords command added to select records which match one or more
#     conditions and copy them to a new file.
#
# 11. Properly handle c2 (house effect) boundaries in bivariate models during
#     multipoint (where they should float), do_boundaries, and perturb.  Previously,
#     c2 boundaries were untouched.  Identify null model as "household
#     polygenic" in bivariate multipoint (as for univariate multipoint).
#
# 12.  SNP covariate names (after the snp_ or hap_ prefix) can now be
#      descriptive alphanumeric names, and can be mapped to their location
#      through a snp.map file handled by the snpmap command.  See details in
#      the documentation for snpmap and qtnm.  The qtnm.out file now has a
#      fifth column to allow for the SNP descriptive name, though plotqtn will
#      also accept the original 4 column qtnm.out files.  The descriptive names
#      will appear at the top of the qtn plot.
#
# 13. Correctly report null0 values for bivariate models at the
#     beginning of the multipoint command, as for univariate.  Previously,
#     if multipoint were started without having the null0 model in memory,
#     the values would not have been correctly reported.
#
# 14. Multivariate mixed and discrete trait types are now detected
#     properly when there are multiple phenotypes files.
#
# 15. Bogus warnings from perturb are now suppressed.  When there is one
#     fewer constraint than there are variance components (such as in
#     sporadic models) perturb is not possible anyway, so perturb now
#     doesn't complain about it.
#
# 16. Attempting to re-load pedindex.out as a pedigree is disallowed and
#     there are some comments about this in the help for pedigree.
#
# 17. There is a new "snpdir" that works like ibddir and mibddir for snp
#     related information (though this is not supported yet by the snp
#     command).
#
# 18. Fields named PEDNO and GEN in the marker file are ignored, and the
#     help for marker now lists all ignored fields.
#
# 19. The snp command now creates "covariates" using the snp names instead
#     of locations.
#
# ****   New in version 3.0.1   *****
#
# 1.  Support for multivariate up to 20 traits
#
# ****   New in version 3.0.0   *****
#
# 1.  Added basic support for trivariate models in polymod, linkmod, maximize,
#     and multipoint (using -cparm) commands.
#
# 2.  Added basic support for multivariate in polymod command.
#
# 3.  Added basic support for multivariate in linkmod command.
#
# 4.  Improved rho "tracking" by linkmod command for trivariate linkage
#     models
#
# ****   New in version 2.1.4   *****
#
# 1.  A major advance in maximization: most convergence errors are "fixed"
#     internally by the mechanism previously used to handle very large
#     negative changes in likelihood: a step is taken 0.1 times the
#     previous stepsize.  (This applies to quantitative traits; a similar
#     change was made last year for discrete traits, but it seems to
#     work even better for quantitative traits.)
#
# 2.  New -ctparm option for multipoint works more reliably than -cparm
#     in some cases by rebuilding every linkage model from the prototype
#     model.
#
# 3.  Maximum number of constraints increased from 99 to 999.
#
# 4.  Bug in sorting probands when there is more than one proband per
#     pedigree (or group, if "house" command is used) is now fixed.
#     PREVIOUSLY YOU WOULD HAVE GOTTEN INCORRECT RESULTS IF THERE WAS
#     MORE THAN ONE PROBAND PER PEDIGREE OR "HOUSE" GROUP!  This bug
#     had been present in SOLAR for many years.
#
# 5.  joinfiles command added to join data files horizontally.
#
# 6.  Non-existant mibddir or ibddir doesn't cause error message on startup
#     or cause mibddir.info file to get deleted.  Error is detected
#     and gives nice message when twopoint or multipoint is run.
#
# 7.  Correctly report null0 values for bivariate models at the
#     beginning of the multipoint command, as for univariate.  Previously,
#     if multipoint were started without having the null0 model in memory,
#     the values would not have been correctly reported.
#
# 8.  When bivariate sporadic models don't converge on the first attempt,
#     the "perturb" operator now doesn't violate constraints.  It properly
#     checks for constraints when the constrained parameters are <> quoted.
#
# ****   New in version 2.1.3   *****
#
# 1.  Clarification in the documentation that lodadj is not (yet) applicable
#     to bivariate models, and that lodadj is ignored when computing LODs
#     for bivariate models.  Previously, lodadj was ignored when computing
#     1dF equivalent LODs for bivariate models, now it is ALWAYS ignored
#     when computing LODs for bivariate models, and a warning message is
#     shown when previously the LOD adjustment factor would have been
#     reported, such as in the multipoint.out file.
#
# 2.  -font option has been added to string plot.  For example,
#
#     plot -string -font *bold-r*
#
#     There is also a -titlefont option to just change the font used in
#     the string plot title.  Also new: if the script "stringplotk" is
#     copied into the working directory and modified, it supercedes the
#     standard one in the SOLAR bin directory.
#
#     Available fonts are listed with the command "xlsfonts | more".
#
# 3.  "residual" command now works properly with phenotypes files having
#     "ego" as ID specifier.  "residual" command now also works properly
#     for phenotype names including "." (period) and "-" (hyphen).
#
# 4.  As a result of the above change, the residual kurtosis is now
#     computed properly when "ego" is used as ID specifier.  In any
#     case where residual kurtosis is NOT computed, a warning message
#     is given.
#
# 5.  If marker file does not have FAMID field, but pedigree file does,
#     error messages from "load marker" will not attempt to include
#     FAMID.
#
# 6.  Allele names are mapped to consecutive integers for input to
#     Merlin.
#
# 7.  The power command will now automatically re-do a bad replicate
#     (when maximization had convergence failure) rather than just skipping
#     it.
#
# 8.  A bug in the rewrite of the "relate" code (introduced in version
#     2.0.8 Beta 1) was fixed.  In an inbred pedigree, this bug could have
#     resulted in an incomplete enumeration of the ways in which a pair
#     of individuals are related.  For example, a parent-offspring pair
#     who are also 1st cousins once removed might have been classified as
#     simply parent-offspring.  The effect of such a misclassification would
#     have been limited to an incorrect tally of relative classes by the
#     "mibd relate" command.   Since SOLAR is not able to compute its
#     approximate Multipoint IBD's for an inbred pedigree anyway, this
#     bug could not have produced incorrect linkage results.
# 
#  9. Multiple solar jobs running in parallel from the same working directory
#     used to have a crash caused by "phenotypes.info" file being deleted and
#     recreated on startup.  This now no longer happens, phenotypes.info
#     file is only deleted IF there is an error with an explicit "load
#     phenotypes" command, and it is only re-written if when a different
#     phenotypes file is selected.  IT IS STILL NOT GOOD TO CHANGE phenotypes
#     file loaded IN A PARALLEL JOB, but simply re-specifying the phenotypes
#     file already loaded is harmless.
#
# 10. Likewise, use of mibddir or ibddir command in a script would cause
#     multiple jobs running from the same working directory to fail because
#     state file was always being deleted and rewritten.  Now it is deleted
#     ONLY if there is an error, and re-written only if changed.  IT IS
#     STILL NOT GOOD TO CHANGE ibddir OR mibddir IN A PARALLEL JOB, but
#     simply re-specifying the directory already selected is harmless.
#     
# 11. Coefficients for class 49 (Double 2nd Cousins, 1 rem) have been added.
#
# 12. Bug in polygenic fixed: if all covariates are removed by screening,
#     "residual kurtosis" computation failed.
#
# 13. "mibd prep merlin" now correctly handles alpha allele names (i.e.
#     A/B).
#
# 14. "mibd prep simwalk" now handles consecutive integer locations in
#     the map file correctly.
#
# 15. If an imported mibd contains an ID which is NOT in the SOLAR pedigree
#     file, a helpful error messages is given.
#
# 16. "hap_" prefix has been added to qtn (in addition to "snp_") but
#     is not completely supported in this release because "noscale" is
#     not correctly being defaulted for "hap_".  This will be corrected
#     in 2.1.4.
#
# 17. Buffer limitation in handling map files fixed.
#
# 18. Maximum number of markers per chromosome (or marker file) is
#     increased from 500 to 2000.
#
# ****   New in version 2.1.2   *****
# 
# 1.  Fixed bug which caused simqtl to compute a value of rhoe (for a
#     bivariate trait simulation) which was too small.
#
# 2.  Fixed bug which caused SOLAR to crash whenever the omega includes
#     a x_ min_ max_ prefixed variable, or an _i _j suffixed variable when
#     that variable doesn't exist in the phenotypes file.  Now you get an
#     explanatory error message, and another SOLAR prompt.
#
# ****     New in version 2.1.1 *****
# 
# 1.  All chapters of the documentation have been reviewed, clarified,
#     corrected, updated, enhanced, reformatted to the new style (if
#     necessary).  The documentation for some commands has also been
#     clarified.
# 
# 2.  The change-notes have been updated.
# 
# ****     New in version 2.1.0 Beta 3 *****
# 
# 1.  The restriction about giving matrices only lowercase names has
#     been removed.  (This refers to their identifiers in the "matrix"
#     command, the actual filenames don't matter.  None of the built-in
#     commands such as "multipoint" will do this anyway, but if you
#     are building up a model by hand you just might.)  This bug had been
#     introduced in 2.1.0 Beta 1.  You might notice the bug if you
#     manually tried to load an imported MIBD file with a command like
#     this (note uppercase "MIBD"):
# 
#         load matrix mibddir/mibd.10.1.gz MIBD
# 
# ****     New in version 2.1.0 Beta 2 *****
# 
# 1.  Documentation updates.  The "help" command message summary and html
#     command index have been cleaned up.  Chapter 7, which introduces SOLAR
#     scripting, has been largely rewritten and reformatted.  The new commands
#     toscript and and showproc are described.  There is an all new section 7.7
#     which explains how to break-out of SOLAR to run a Unix command with
#     wildcards.  There is also a new section 7.8 describing commands which
#     might be useful in writing scripts.  A number of previously undocumented
#     commands are now documented (with more intuitive names and interfaces in
#     some cases.  The newly documented commands are:
# 
# read_model           Read a parameter value or likelihood from a saved model
# read_output          Read variable statistics from a maximization output file
# read_arglist         Read hyphenated optional arguments
# solarfile            Read datafile allowing for optional field name mapping
# is_nan               Check if a value is NaN (Not a Number)
# if_parameter_exists  Check if a parameter exists
# if_global_exists     Check if a Tcl global variable exists
# remove_global        Remove a Tcl global variable
# catenate             Concatenate strings
# string_imatch        Case insensitive string match testing
# remlist              Remove element from a list by name
# setappend            Append only new elements to a list (keeping it like a set)
# stringsub            Simple verbatim string substitution
# 
# 
#     
# ****     New in version 2.1.0 Beta 1 *****
# 
# 1.  When using imported MIBD files using the "-1" convention to indicate
#     pedigrees with no genotypic data, you would get invalid LOD scores
#     because the -1 convention was not being applied to MIBD files.
# 
#     Warning: at this time, you must not give matrices uppercase names such
#     as IBD or MIBD1 when using imported matrix files.  (This applies to the
#     matrix identifier name used in the model files, not the matrix filenames,
#     which have always been case sensitive.)  None of the built-in commands
#     will do this anyway, but you might do this if you were editing scripts or
#     models by hand.  There is now a case sensitivity bug bug in applying only
#     the -1 convention.  This will be fixed in a future update.
# 
# ****     New in version 2.0.9 Beta 2 *****
# 
# 1.  Starting lower boundary for bivariate e2 parameters is now 0.03 (or
#     "e2lower") just as for univariate.  This artificial boundary prevents
#     some convergence problems by preventing heritabilities from shooting up
#     to 1.0 (where there are often singularities) in one go.  If maximization
#     stops on this artificial boundary, the boundary is moved, ultimately all
#     the way to zero.
# 
# ****     New in version 2.0.9 Beta 1 *****
# 
# 1.  Fixed Memory leak in "twopoint", and also any "load model/maximize"
#     sequence.  Expansion per "load model" was equivalent to the length
#     of all ID's and FAMID's in the sample, typically about 20kB per
#     maximization.  This could possibly lead to 100Mb or more of wasted
#     space in a long running analysis, and therefore also to
#     paging/swapping and a major slowdown.  "multipoint" had not been
#     as much affected by this leak, since it doesn't much reload models
#     except when there are convergence problems.  This leak had been
#     present since 2.0.0 in 2002.
# 
# 2.  Alphanumeric entries in phenotype fields are now detected as errors.
#     Previously, if you had an alphameric string such as "null" it would
#     get read as "0", leading to incorrect results.  Also, invalid
#     exponents (such as "D4") and other alphameric suffixes will get
#     detected as errors.  Previously, invalid exponents and alphameric
#     suffixes would be ignored, having the same effect as "e0".
# 
# 3.  Tabs in most comma delimited files, including the phenotypes and
#     pedigree files, are now ignored just as spaces are.  (Previously,
#     because of the bug fixed by change 2, trailing tabs were
#     inadvertently ignored but not leading tabs.)  Note that the commas
#     are still required; tabs are not (as yet) read as delimiters in
#     phenotypes and pedigree files.
#     
# 4.  Clarified comments regarding kurtosis (see: stats) and the warning
#     about high residual kurtosis (see: polygenic note 5).  Also updated
#     html documenation to include 2.0.7 and 2.0.8 updates.
# 
# 5.  Updated classes.tab and relate.tab to reflect some new relationship
#     names and indexing.
# 
# 6.  Re-write of relate code so it handles all possible relative classes
#     correctly. Previously, some complex relationships could have been
#     incorrectly characterized. However, since such relationships fall
#     into the "Unknown relationship" category and are not supported for
#     computing SOLAR's multipoint IBDs, the problems with the old relate
#     code did not have any impact on previously obtained results.
# 
# 
# ****     New in version 2.0.8 Beta 2 *****
# 
# 1.  With all pedigree individuals typed, "ibd" would choose the "Monte Carlo
#     method" even with the "Xlinked" ibdoption.  Now it correctly chooses the
#     Curtis and Sham method if you select the Xlinked ibdoption.
# 
# 2.  Memory violation with "ped show all" under certain circumstances fixed.
# 
# 
# ****     New in version 2.0.8 Beta 1 ******
# 
# 1.  Bug in freq mle (usually only seen on linux systems) fixed
# 2.  Added -nostop option for bayesavg -qtn
# 3.  Added c0v to bayesavg_cov.win file for bayesavg -qtn, showing posterior
#     probability of cov0.
# 
# ****     New in version 2.0.7 Beta 3 ******
# 
# 1.  Bug in use of simqtl for bivariate traits fixed (caused crash)
# 2.  Bug in loading haldane map fixed
# 3.  Corrected max bits for Merlin and Genehunter
# 4.  Enforced "ibddir or mibddir must be declared in session when writing
#     new ibd/mibd files" rule consistently.
# 5.  Documentation Chapters 1-6 and 9 have been revised and formatted to
#     an improved style for clarity.
# 6.  If simqtl genotypes were read from a file, that file would not get
#     closed.  After multiple runs, user could run out of available files.
#     This bug has been fixed.
# 7.  Output frequency of 10 chosen for Loki, which makes using Loki to
#     calculate MIBDs faster than it (Loki) was previously.  This change
#     was recommended by Simon Heath, the author of Loki.
# 
# *****    New in version 2.0.7 Beta 2 ******
# 
# 1.  Support for Merlin and Genehunter for mibd prep and mibd import
# 2.  Added -byloc for mibd export to create a separate mibd export file
#     for each solar mibd (otherwise, the file can get so large as to
#     make file management difficult)
# 
# *****    New in version 2.0.7 Beta 1 ******
# 
# 1. Plotting for power calculation (plot -power) is now available which
#      plots power vs QTL heritability.
# 
# 2. The restart for power calculations (power -restart) now permits changing
#      the number of replicates and grid sizes.
# 
# 3. Dominance matrices (delta7 and d7) are now supported as intended by
#    twopoint and multipoint.  Previously these "second column" matrices were
#    ignored by the custom parameterization (-cparm) options of twopoint
#    and multipoint.  
# 
# 4. Other problems with "second column" matrices are fixed, including one which
#    caused SOLAR to crash if you used one second column matrix to override
#    a previously loaded second column matrix, then did "model new".
#    Now, when you override a matrix though a second column entry, the old
#    one is deleted (or renamed if it is the first column and the second
#    column is loaded also) as required so that the result is unambiguous.
# 
# 5. The -cparm options of twopoint and multipoint and twopoint automatically
#    seek out the highest indexed ibd* or mibd* matrix for scanning, which
#    means you can scan custom parameterized models with multiple linkage
#    elements.
# 
# 6. The linkmod -noparm option has been renamed "-cparm" for consistency
#    with twopoint and multipoint.  This has the new behavior required
#    to support dominance and other second column matrices, and multiple
#    linkage elements.
# 
# 7. The broken "-saveall" option of multipoint has been fixed, and a
#    -saveall option has been added to twopoint.  These options save
#    all the linkage models tested.
# 
# 8. The "Advanced Modeling Topics" Chapter 9 of the documentation has
#    been updated, clarified, and the examples presented (regarding
#    dominance, household groups, and custom parameterization)
#    have been tested to actually work as presented.  There is now
#    advice as to handle covariates with multiple "classes."
#    The command documentation for related commands has also been
#    clarified and updated.
# 
# *****    New in version 2.0.6 Beta 3 ******
# 
# 1. It is now possible to do "polygenic -testrhoe" and "polygenic -testrhog"
#    on bivariate household polygenic models.  Also there is a new test,
#    "polygenic -testrhoc" that tests the rhoc parameter.
# 
# 2. "twopoint -parm" has been renamed "twopoint -cparm" for consistency
#    with multipoint and the documentation.
# 
# *****    New in version 2.0.6 Beta 2 ******
# 
# 1. A "saved state" has been added to the field, mibddir, and ibddir
#    commands.  This means that once you give one of these commands, it
#    remains in effect in all futures sessions within the same working
#    directory.  The state is saved in the files field.info, mibddir.info,
#    and ibddir.info, respectively.
# 
#    In the case of mibddir and ibddir, the saved state only works for
#    reading purposes, not writing new ibd or mibd files.  In order to
#    write ibd or mibd files, you must explicity enter the ibddir or
#    mibddir commands within the current session.  This is to prevent
#    you from accidentally overwriting previous files.
# 
#    For the field command, it is now possible to restore a field to it's
#    "full default" state using the -default argument.  For the ID field,
#     for example, the "full default" allows either ID or EGO.
# 
# 1. For the polygenic command, the "Residual Kurtosis" test now works as
#    intended.  Previously, you were only getting the stats on the trait itself,
#    and no test was done if there were no covariates.  Now you are
#    actually getting the stats on the residuals, unless there
#    are no covariates, in which case you get the stats on the trait
#    itself.  Also there was a bug in writing the first line of the residuals
#    file if the -q option (used by polygenic) was used.  And the warning
#    about residual kurtosis being too high points you to some helpful
#    suggestions which have been added to the "help polygenic" message.
# 
# 2. sporadic command now properly does variance due to all covariates and
#    computes sporadic residual kurtosis.  A bug in the last update broke
#    these.  However, sporadic does NOT save or overwrite null0.out.  It
#    is not appropriate to use a sporadic model as the null in linkage
#    testing.
# 
# 3. rhoc is used for bivariate household models in place of rhoh (since
#    household parameter is c2).
# 
# 4. The residual command defaults to using the output file null0.out
#    rather than poly.out.  This makes more sense.
# 
# 5. The "residual" command will not run on discrete models.  It is not
#    appropriate.  Likewise, the polygenic command will not attempt to
#    report the residual kurtosis for discrete models.
# 
# 6. If you are analyzing a discrete trait as quantitative, polygenic
#    will not now attempt to compute the Kullback-Leibler R squared.
# 
# 7. The note about loglikelihoods and chi^2's being written to
#    polygenic.logs.out was been made neater and is written to the terminal
#    as well as the polygenic.out file.
# 
# *****    New in version 2.0.6  ******
# 
# 1. Bivariate household (household polygenic, etc.) models are now supported.
#    (VERY EXPERIMENTALLY...NOT BETA TESTED BY REAL USERS YET.)
#    Previously, you would get a cryptic error message if you attempted
#    to add household effects to a bivariate model.  That was because
#    none of the "house" and "polygenic" code had any inkling of how to
#    deal with bivariate household models.  Much work was needed to add
#    this capability, it kept falling to the bottom of our priority
#    list.  Also there is a new "-keephouse" option which forces the C2
#    parameter into models even if C2 is estimated at 0 in the household
#    polygenic model.
# 
# 2. "polygenic" now permits covariate beta variables to be constrained
#    to particular values.  When a model contains such beta variables,
#    the "variance due to all covariates" or the "Kullback-Leibler R-squared"
#    cannot be computed.  Also, covariates with constrained betas cannot
#    be screened, but other covariates in effect may be.
# 
# 3. One unanticipated form of convergence error could cause multipoint
#    scans to crash.  This convergence error would take the form of
#    constraints not being satisfied.  It was hoped to be able to fix
#    the problem altogether, but that turned out not to be possible within
#    current numerical limits.  But now the problem is correctly handled
#    as an individual convergence error, and multipoint scans continue.
# 
# 4. Various bugs and inconsistencies in the "stats" command have been fixed.
#    It should not crash in any eventuality.  In some cases, uncomputable stats
#    are correctly shown as NaN (not 0, or crash).  Alphanumeric fields are
#    identified as such.  Discrete fields are identified as such, and discrete
#    coding errors are identified correctly following the same algorithm as
#    during maximization.
# 
# 5. Discrete trait handling improved: An inconsistency in the identification
#    of discrete variables has been fixed.  zscore is permitted for "discrete"
#    variables when they are analyzed quantitatively (by setting option
#    enablediscrete to 0).  When discete variables are being analyzed
#    quantitatively, a warning is given by polygenic.  The discrete notes
#    have been updated to remove some areas of confusion.  The warning
#    about discrete traits being analyzed quantitatively appears (just)
#    once in the multipoint.out file, and not in between every line of
#    multipoint scanning.
# 
# 6. A bug in the use of the "field" command to permit a different label
#    to be used for famid has been fixed.
# 
# 7. simqtl now properly closes input file after each use (eventually, you
#    could have lost the ability to open more files).  map unloads current
#    map before loading new one.  "mibd prep" checks for markers in map but
#    not in marker file, and also checks for zero recombination between
#    markers.
# 
# 8. sporadic command has been merged with polygenic command to be more
#    consistent (and easier for us to update!).  Some of the polygenic
#    output messages have been clarified.
# 
# 
# 
# ******   New in version 2.0.5 or before *****
# 
# (Some of the following changes might actually have been made in releases
# prior to 2.0.5, but were only documented for the 2.0.5 release.)
# 
# 1.  QTN
# 
# Quantitative Trait Nucleotide (QTN) analysis is provided for with the
# "bayesavg -qtn" command and the qtnm command to do marginal testing and
# plot results.
# 
# 2.  SOLAR user identification (matching key)
# 
#        SOLAR KEY consistent with "su -c" (but possibly not "su")
#        Previous versions of SOLAR based the KEY verification on the
#        "username" or "login name" you ACTUALLY logged in on (regardless
#        of any "su" change) and the key found in the directory pointed
#        to by the HOME environment variable.  In many cases, this worked
#        with "su" (because that doesn't update your HOME directory), but
#        not "su -c" (which updates your HOME directory).  After much
#        agonizing, I decided it was more consistent to support "su -c".
#        So, the key can now match the identity you assume with with
#        "su -c" or a similar command, so long as the matching key is found in
#        the current HOME directory (which is pointed to by the environment
#        variable HOME...use the shell command "echo $HOME" to see it).
# 
# 3.  Cryptic errors when using the residual command when there are special
#     characters such as % in the phenotypes file have been fixed.
# 
# A bug involving the usage of special characters (such as %) in
#     the phenotypes file and the "residual" command
# 
# 
# ******   New in version 2.0.5  ******
# 
# Note: This is the 3rd release of SOLAR 2.0.5 (beta), properly known
# as SOLAR 2.0.5 "c".
# 
# Here is a summary of the updates in version 2.0.5 as compared with 2.0.4
# (details follow):
# 
# * trait command changed (-noparm not needed)
# * Univariate models compatible with 1.7.3
# * Alternate parameterization incompatible with 2.0.4
# * ibd,mibd import and export
# * zscore command
# * stats command
# * empp command
# * New Batch Interface
# * multipoint -saveall
# * showproc command (really new)
# * bad script identified
# * missing FAMID checked for
# * "undefined" phenotypes file
# * multipoint -cparm
# * multipoint -link
# * esd,gsd,qsd parameterization
# * spurious multipoint LODs fixed
# * polygenic command tests residual kurtosis
# * gridh2r
# * d7 convergence problem fixed
# * Automatically updates "last updated on" date
# * multipoint -plot fixed on linux
# 
# Now, here are the details (some of them, anyway):
# 
# * trait command changed (-noparm not needed)
# 
# It is no longer necessary to give the "-noparm" option to the trait
# command when setting up models with non-standard parameterization.
# The trait command no longer adds or removes parameters in your model
# to correspond with the trait(s).  If you are attempting to change from
# a univariate to a bivariate model or vice versa, and trait dependent
# parameters have already been created, you are now given an error
# message telling you that you must use the "model new" command first.
# At that point, you cannot inadvertantly continue with invalid
# parameters or the previous trait; any attempt to continue with a
# maximization will repeat the message about the need for using the
# "model new" command first.  (The trait takes on a special value
# "Must_give_command_model_new" in order to assure this.  If you need to
# recover this invalid model anyway, for the sake of previously maximized
# parameters, you can save it to a file and edit it.)
# 
# You can still change from one univariate trait to another if desired,
# but this is only recommended if the two traits have similar behavior.
# During a univariate trait change, some parameter values and boundaries
# will be adjusted. The documentation for the trait command gives more
# details.
# 
# If you wish to set up the standard parameters (mean, sd, e2, h2r) for
# the currently selected trait(s), you may use the "polygenic" command,
# the "polymod" command (to set up the parameters without maximization),
# or you may set up the parameters, constraints, and omega required by
# hand.  This was how it worked in earlier releases of SOLAR, but the
# original solar2 assumed you wanted the standard parameters and set
# them up for you whenever the trait command was given.  I thought this
# would simplify things for most people, but in the end this assumption
# proved to cause too many problems and misunderstandings.
# 
# * Univariate models compatible with 1.7.3
# 
# As a result of the changes to the trait command, univariate models from
# SOLAR 2.0.5 are now compatible with SOLAR version 1.7.3 and earlier releases.
# 
# * Alternate parameterization incompatible with 2.0.4
# 
# If non-standard parameters (such as esd, gsd, and qsd) are in your
# 2.0.5 model, it will not be compatible with 2.0.4, since that version
# required the "trait -noparm" option for such models to be permitted.
# 
# * ibd,mibd import and export
# 
# The ibd and mibd command now have new options designed to permit the use
# of ibd and mibd files from other software packages to be used in SOLAR, and
# for SOLAR ibd and mibd files to be used in other software packages.  The
# new commands are:
# 
#         ibd export
#         ibd import
#         mibd export
#         mibd import
#         mibd import <program>
#         mibd prep
# 
# See the documentation for further details.
# 
# * zscore command
# 
# The zscore command will compute statistics about your selected trait
# and then automatically "zscore" it during maximization.  This is a
# transformation in which the trait mean is subtracted from each trait
# and then the result is divided by the standard deviation.
# 
#     zscored = (value - Mean) / SD
# 
# The zscore status is stored in several model options (zscore, zmean1,
# zsd1, zmean2, and zsd2, the latter two being for a second trait) and
# can be adjusted if desired.
# 
# * stats command
# 
# The stats command can report basic statistics including mean, minimum,
# maximum, standard deviation, skewness, and kurtosis about any (or all)
# phenotypic variables.  The default variable is the current trait.  For
# use in scripts, a helper procedure stats_get pulls named statistics out
# of the record returned by stats.
# 
# * empp command
# 
# empp computes an empirical p-value from lodadj results.
# 
# * New Batch Interface
# 
# There is a new "batch" interface to solar.  From the shell, you can now
# invoke SOLAR with solar command(s) or the name of your script procedure
# followed by its arguments, just as you would do at the "solar>" prompt:
# 
#     solar2 [<command> [<args>]+ ]
# 
# For example, you could invoke the tutorial script "makemibd" like this:
# 
#     solar2 makemibd
# 
# You can even string together multiple SOLAR commands using the semicolon (;)
# separator:
# 
#     solar2 trait q4 \; covar age \; polygenic -s
# 
# You need to put backslash in front of semicolons because otherwhise
# the shell will interpret those as requiring the execution of new shell
# commands.
# 
# You can still do things like this (as many Unix experts know):
# 
#     solar2 << EOF
#     trait q4
#     covar age
#     polygenic -s
#     EOF
# 
# That is a feature of Unix shells and has nothing to do with SOLAR.
# 
# This is a change from how the argument(s) to solar2 were handled.
# Previously, the arguments only allowed you to specify the NAME OF A
# FILE containing a LIST OF SOLAR COMMANDS to process, not the name of a
# SOLAR procedure ("proc").  This meant that if you wanted to execute a
# predefined SOLAR proc, you had to create a second file simply
# containing the name of that proc.  This was very cumbersome and also
# very hard to explain to people, who just wanted to use commands like
# they do at the SOLAR prompt.  Now you can do that.
# 
# * multipoint -saveall
# 
# The -saveall option of multipoint saves all models tested in a multipoint
# analysis.  It was present before but not documented.
# 
# * showproc command (was writeproc)
# 
# There is now a "showproc" command which will display any built-in or
# user-defined procedure ("proc" or what most people call "scripts")
# that is implemented in Tcl.  You can also copy the script to a file.
# The proc name will be suffixed with .copy so as not to collide with
# the built-in script name.
# 
#     showproc <procname> [<filename>]
# 
# If you do not specify a filename, you will view the procedure in your terminal
# window using the more pager.  For example, if you want to view the
# "polygenic" procedure (which is very complicated!), you could give the
# command:
# 
#     showproc polygenic
# 
# Even if you don't fully understand the built-in procedure, it is
# sometimes useful to look at the code.  Unfortunately showproc does not
# show comments or the exact "pretty" formatting of the code.  The real
# code looks a little nicer than what gets shown.
# 
# * bad script identified
# 
# When SOLAR finds a seriously bad script when starting, it will identify
# the bad script by name (rather than giving a cryptic message like it
# used to).
# 
# * missing FAMID field in phenotypes file now checked for
# 
# Often people use ID's which are only unique in each family.  When
# doing this, they must also include a FAMID field to disambiguate ID's
# in both the pedigree and phenotypes files.  Previously, however, only
# pedigree files would be checked for having unique ID's.  Sometimes people
# would forget to include FAMID in the phenotypes file, or forget to
# name the applicable field in the phenotypes file as FAMID.  Now
# SOLAR checks for missing FAMID when loading the phenotypes file by
# searching for duplicate ID's.  If FAMID is already present in the
# phenotypes file, or if the pedigree file does not have FAMID, this
# check is not required.  It is usually very fast.
# 
# * "undefined" phenotypes file
# 
# If there is an error loading the phenotypes file, the phenotypes file
# status becomes "undefined," even if you exit from and restart SOLAR.
# Previously, if you restarted SOLAR this "undefined" status would be
# lost, and you might have continued with an obsolete phenotypes file.
# 
# * multipoint -cparm
# 
# The "-parm" option for multipoint has been renamed "-cparm" which signifies
# "custom parameterization".  See below for an example.
# 
# * multipoint -link
# 
# The -link option to multipoint allows you to specify a custom "linkage
# model creation" procedure.  This is useful if you use a custom
# parameterization (such as esd,gsd,qsd) which is not supported by the
# built-in linkage model creation procedure linkmod which uses the
# standard parameterization.  Two such procedures are "linkqsd" and
# "linkqsd0" (see below).
# 
# * esd,gsd,qsd parameterization
# 
# The custom parameterization using esd, gsd, and qsd parameters (in
# place of the standard e2, h2r, and h2q1 parameters) is now
# EXPERIMENTALLY supported by built-in procedures polygsd, linkqsd,
# linkqsd0, and gsd2h2r.  Here is a sample analysis:
# 
#     model new
#     trait q4
#     covar sex
#     polygsd       ;# sets up esd and gsd parameters
#     maximize
#     save model q4/null0
#     gsd2h2r       ;# reports equivalent h2r value
#     chromosome 9
#     interval 5
#     mibddir gaw10mibd
#     multipoint -link linkqsd0 -cparm {esd gsd qsd}
# 
# * spurious multipoint LODs fixed; polygenic h2r over 0.99
# 
# In some multipoint scans, positive LOD scores were being shown where
# the h2q1 was zero.  The true culprit was that the h2r in some polygenic
# models was being improperly estimated as 0.99, which was not the
# highest likelihood estimate.  These invalid estimates were being
# caused by singularities in the covariance matrix as h2r gets very
# close to 1.0 when there are monozygotic twins.
# 
# This has been addressed in two ways.  The default e2lower has been
# raised from 0.01 to 0.03 which prevents the sum of all heritabilities
# from being estimated over 0.97 on the first attempt.  This stopped the
# problem in the dataset at hand because h2r doesn't get close enough to
# 1.0 to cause singularity problems.  Also, if h2r is estimated above
# 0.90 during the "polygenic" command, you now get a warning:
# 
#     Warning.  Unexpectedly high heritabilities might result from
#     numerical problems, especially if mztwins are present.
# 
# * polygenic command tests residual kurtosis
# 
# During a univariate polygenic analysis in which there are covariates, the
# kurtosis of the residual is tested and reported.  If it is above 0.8, you
# get a special warning:
# 
#     WARNING!  Residual Kurtosis is 0.81, which is too high
# 
# * gridh2r
# 
# You run gridh2r after running polygenic to take a look at the
# likelihoods for fixed h2r values around estimated h2r if you are
# suspicious there was a problem with the h2r estimation.  You can set
# the upper bound, lower bound, and steps, or simply use the defaults.
# 
# * d7 convergence problem fixed
# 
# There was a bug in the code which was supposed to default missing d7 to the
# delta7 value.  This caused misconvergence in some dominance linkage
# models.
# 
# * Automatically updates "last updated on" date
# 
# When starting, SOLAR now scans all the SOLAR binary files and the solar.tcl
# file to determine the correct "last updated on" date shown in the
# copyright message.
# 
# * multipoint -plot fixed on linux
# 
# Previously, multipoint -plot did not work on linux.  In fact, once you did
# a plot on linux, many things wouldn't work.  That has now been fixed.
# 
# ******   SPECIAL 2.0.4f update ******
# 
# 1) h2q1 values for bivariate models were too low by an unpredictable
#    amount (some reports say a very tiny amount, but we're not sure
#    that's always true).  LOD scores are unchanged.  Sometimes convergence
#    is better.
# 
# 2) hlod fixed for bivariate models or other models where phi2.gz is
#    explicitly included.
# 
# ******   SPECIAL 2.0.4b update ******
# 
# This distribution includes the 2.0.4b bugfix update which fixes the following
# bugs.  When running, this version will identify itself as version 2.0.4,
# but if you check the solar.tcl file it will be dated March 28.
# 
# 1) p values for rhoe and rhog being different from zero were not being
#    calculated correctly.  The previous values were too high.  Better
#    (and correct) values are reported now.
# 
# 2) If C2 in household polygenic models went to zero, the polygenic
#    command crashed.  That has been fixed, and now also the
#    documentation clarifies that if C2 is 0, it is removed from the
#    model, regardless of whether the -screen option is in effect.
# 
# 3) Some standard errors might be reported as 0.0000000.  Now they are
#    reported as "not computable."
# 
# 4) The Kullback-Leibler R^2 might be reported for non-discrete models
#    when the user has constrained SD to zero.  Fixed.
# 
# 
# ******   NEW in SOLAR 2.0.4   *******
# 
# 1. A bug disabling the variance component boundary management for bivariate
#    models, which was introduced in version 2.0.2, has been fixed in this
#    release.  THIS MIGHT HAVE PRODUCED INVALID *BIVARIATE* MODELS and results
#    related to them.  If you have any important models that were created by
#    release 2.0.2 or 2.0.3, you should do one of the following:
# 
#       1) recreate those models using version 2.0.4
#       2) check all variance component parameters (e2 h2r h2q1 rhoe rhog rhoq1)
#          that they are not sitting on an artificial boundary.  If you don't
#          know what this means, see option 3.
#       3) Reload those models in the new SOLAR 2.0.4, and run the
#          check_artificial_boundaries command.  It returns a list of errors.
#          If it returns nothing, your model is OK.
# 
# 2. Maximization of troublesome discrete models has been improved,
#    partly by relaxing the required convergence precision slightly, and
#    partly by handling "over the cliff" problems consistently in a
#    useful way.  There may be very insignificant changes (in the third
#    or lesser signficant digit) in typical cases, but that is a small
#    price to pay for much greater chance of proper convergence in the
#    difficult cases.  (I can still concoct some model which don't
#    converge, but it's fairly difficult.)  If you are still getting
#    convergence errors with any type of model, please bring them to the
#    attention of the SOLAR developers at solar@txbiomedgenetics.org.  We are
#    getting a better handle on these things.
#    
# 3. Unsatisfied constraints involving more than one parameter (such as
#    e2 + h2r + h2q1 = 1) are now identified clearly and conveniently during
#    maximization and don't cause bouts of unnecessary retries and are not
#    mistakenly identified as "CONVERGENCE FAILURE."  (Unsatisfied
#    constraints involving only one parameter are automatically fixed...that
#    has been a feature of SOLAR for some time now.  The parameter is simply
#    set to the value it is constrained to.  But with multiple parameters, it
#    is non-obvious how this should be done, and is probably best left to the
#    user, though I may invent some script to make it easier for the user to
#    deal with in the future.)
# 
# 4. Bugs in multipoint scanning on linux have been fixed.  Any convergence
#    error would abort the scan, and proper sorting of the output files wasn't
#    done.
# 
# 5. "marker discrep" and "freq mle" will complete an entire list of markers
#    rather than terminating at the first error.
# 
# 6.  detection and reporting of various errors has been improved for pedigree, 
#     marker, ibd, and mibd commands.
# 
# 7.  New relative classes have been added.
# 
# 8.  Internal Tcl search PATH has been simplified, which may improve efficiency
#     when NFS is used and eliminate conflicts on sites where other Tcl
#     versions have been installed locally.
# 
# 9.  Bivariate LOD now automatically adjusted for constraint of rhoq1 (or
#     rhoqN) if present.  Minor improvements to clod and lodp interface.
#     Buggy auto-constraint of rhoq1 for convergence purposes eliminated.
# 
# 10  Bug in perturb fixed.  Perturb also now has multiple phases to deal with
#     difficult cases where there isn't much "room" between the boundaries.
# 
# 11. -key <key> argument to SOLAR permits operation without a permanent
#     .solar_reg file.  This is useful for remote job execution under some
#     circumstances, and reduces related NFS traffic.
# 
# 12. install_solar has been improved, now permitting you to give special
#     names to solar releases.  Internally we call SOLAR version 2.0.x
#     "solar2".
# 
# 
# *******  NEW in SOLAR 2.0.3  ******
# 
# SOLAR 2.0.3 now allows the same flexibility for bivariate models which
# had been available for univariate models.  In addition, there is a new
# command "toscript" which automatically writes your commands into
# scripts, and support of arbitrary non-standard parameterization and
# better "mu" editing.  Details are discussed below.
# 
# 1.  toscript command (writes commands to a script file)
# 2.  Trait-specific covariates
# 3.  Null covariates (useful for locking-in phenotypes)
# 4.  Trait change cleanup
# 5.  Constraint cleanup
# 6.  Arbitrary parameterization
# 7.  -parm option for multipoint and twopoint (and linkmod -noparm)
# 8.  Omega "(ti)" and "(tj)" variables generalized
# 9.  Bivariate (and Univariate) Mu's
# 10. Bugs fixed
# 
# 1. toscript
# 
# The new command "toscript" will automatically create a script based on
# the commands previously entered in this session or a set of ranges of
# those command.  For example:
# 
#     solar> trait q1 q2
#     solar> covar age sex
#     solar> toscript begin
# 
# Will create a script named "begin" including the two previous
# commands.  You may begin using the new script immediately, and it is
# also saved in a file (begin.tcl in this example) in the current
# directory.  You may also specify ranges of commands to use in the
# script.  To see the numbered list of all commands (up to 200) in the
# current session, use the Tcl "history" command.  To overwrite a
# previous script, use the "-ov" argument.  For example:
# 
#     solar> history
#         1  trait q1 q2
#         2  covar age sex
#         3  toscript begin
# 
#     solar> toscript -ov begin 1-2
# 
# You can use any number of single command numbers or hypenated ranges:
# 
#     solar> toscript analysis 1-5 11-15 25 20
# 
# As the script is being created, you also see it displayed on your terminal.
# 
# 
# 2. Trait specific covariates
# 
# It is now possible to specify exactly which trait a particular covariate
# should be applied to.  Previously, all covariates were "generic" and applied
# to all traits, which was not always desired.  To specify the trait to which a
# covariate should be applied, include the trait name in parentheses following
# the covariate.  For example:
# 
#     solar> covariate sex age(q1) age*sex(q2)
# 
# Here "sex" is a "generic" covariate to be applied to all traits, "age"
# is only applied to trait q1, and age*sex is only applied to trait q2.
# If you change trait(s), the specific covariate betas will change
# automatically (see section 3).  The help documentation for the covariate
# command has been updated and has more detail.
# 
# 
# 3. Null covariates (useful for locking-in phenotypes).
# 
# In addition to trait specific covariates, it is now possible to create
# "null covariates" which don't apply to any trait.  These are not true
# covariates in the usual sense, but serve the same function of
# "locking-in" a phenotype so that only individuals including that
# phenotype are included in the analysis.  For example:
# 
#     solar> covariate dob()
# 
# This forces the requirement that all invididuals in the analysis have
# the variable dob, but no beta variable is created or estimated.
# 
# In the past, people have tried using "suspended" covariates (which
# were really intended only for temporary hypothesis testing) and other
# problematic ways to do this.  "Null covariates" accomplish what is
# required in an elegant way, and it fits in perfectly with how
# trait specific covariates are handled when the trait(s) are changed.
# 
# 
# 
# 4.  Automatic trait change parameter cleanup
# 
# Previously if you ever changed trait(s) without giving the "model new"
# command you could get into serious trouble, including fatal errors.
# Now pretty much everything you absolutely need to do is taken care of
# automatically.  Covariates are "renewed" automatically.  This means
# that the old beta parameters are removed and new beta parameters are
# created which correspond to the new traits.  The covariate beta values
# and boundaries are also reset (which is likely to be appropriate).
# Finally, note that beta parameters are created only for those
# covariates which are applicable to the new trait(s).  Covariates which
# were declared as specific to trait(s) not in the current model will not
# have betas.  For example:
# 
#     solar> trait sex age(q1) age*sex(q2)
#     solar> trait q1
# 
# Since trait q1 is now in effect, beta parameters are created for sex and
# age, but not age*sex.
# 
#     solar> trait q2
# 
# Since trait q2 is now in effect, beta parameters are created for sex and
# age*sex, but not age.
# 
#     solar> trait q1 q2
# 
# Since traits q1 and q2 are now in effect, all three covariates are
# applicable, and sex is applicable to both traits.
# 
# Even if a covariate is "inapplicable" to the current trait, the underlying
# phenotypic variable is "locked-in."  This is appropriate to most analysis
# situations.  If this is not what you want, you can always delete the
# covariate or start from scratch with "model new".
# 
# The variance parameters, trait parameters, constraint, and omega are
# also re-created as needed for the new trait(s).  Optional constraints
# on any of the above parameters are removed when the parameters are
# deleted and then re-constructed.  (The removal of optional constraints
# might not be what you want in all cases, but it does prevent a number
# of problems.  For example, if a previous univariate trait was discrete
# and the new trait is quantitative, the old constraint on parameter SD
# would get you into trouble.)
# 
# If you don't want the trait command messing with any of your
# parameters, omega, or constraint, use the -noparm option.  In that
# case you had better know what you are doing.  You could end up with
# models that will not maximize or worse.
# 
# Another alternative is that you can still use "model new" to start from
# scratch when you change the trait.
# 
# 
# 5. Automatic constraint cleanup
# 
# Whenever a parameter is removed, contraints on that parameter are removed
# also.  
# 
# 
# 6. Arbitrary Parameterization
# 
# SOLAR is now designed so that the "standard" parameterization is no
# longer required.  Parameters also need not necessarily appear in a
# particular order (as was also previously required).  If you want to create
# a model with arbitrary parameterization, be sure to use the -noparm
# option of the trait command when specifying traits.  You will notice that
# models also include this option because they might or might not include
# non-standard parameterization.
# 
#     solar> trait -noparm q1
# 
# Otherwise the standard parameters will be created in the standard order,
# which is more convenient for most people most of the time.
# 
# It is not even necessary to have "mean" and "SD" parameters in models
# anymore.  Of course, all models must still have "omega" and "mu"
# equations; those are actually required for variance analysis to work.
# The "mean" could simply be a constant in the mu.
# 
# To facilitate the use of linkage models with arbitrary parameterization,
# the linkmod command (which builds linkage models) now has a -noparm
# option.  With this option, linkmod adds a linkage matrix to the current model,
# replacing the previous matrix (if applicable), but does not touch the
# the parameters, constraints, and omega in the model.  It then becomes your
# responsibility to set those things up correctly.
# 
# 
# 7.  -parm option for twopoint and multipoint
# 
# To enable linkage scanning for models with non-standard parameterization,
# the twopoint and multipoint commands now include a "-parm" option which
# allows you to specify a list of alternative parameters to be displayed for
# each model.  For example:
# 
#     solar> multipoint -parm {esd gesd}
# 
# This would print the values of parameters esd and gesd for each linkage model
# (in addition to the loglikelihood and LOD, which are always included).
# If you are using non-standard parameters, but there are no particular
# parameters you need in the output file, you can indicate this with the
# "empty list":
# 
#     solar> multipoint -parm {}
# 
# When you use the -parm option, a non-standard parameterization is assumed.
# The starting model MUST BE A CORRECTLY CONFIGURED LINKAGE MODEL.  Scanning
# is done simply by substituting the one ibd or mibd matrix after another
# into the model (using the "linkmod -noparm" command).  There must also be
# a maximized null0.mod model in the output directory, but it is only read
# to find the null loglikelihood for LOD calculation.  It is not used as
# a basis for creating new linkage models, since SOLAR only knows how to do
# that for the standard parameterization.
# 
# 
# 8. Omega "(ti)" and "(tj)" variables generalized
# 
# Bivariate models require (ti) and (tj) subscripted variables in the
# omega for each variance component.  Previously these (ti) and (tj)
# variables were provided for only e2, h2r, and h2q1.  Now they are
# available for ANY parameter for which there is a subscripted version
# for each trait.  This allows for household effect parameters and any
# other variance terms you can imagine.  For example:
# 
#     solar> trait q1 q2
#     solar> parameter c2(q1)
#     solar> parameter c2(q2)
# 
# After defining c2 for each trait, you can use variables c2(ti) and
# c2(tj) in the omega.
# 
# 
# 9.  Bivariate and Univariate Mu's
# 
# Previously there was no practical way to use mu in bivariate models.
# They were broken in several different ways.  Now there are "t1" and
# "t2" variables which reflect the trait being estimated at the time.
# (For example, t1 is 1 if the first trait is current, 0 otherwise).
# 
#     solar> trait q1 q2
#     solar> covar sex
#     solar> mu
# mu = \{t1*(<Mean(q1)>+<bsex(q1)>*Female) + t2*(<Mean(q2)>+<bsex(q2)>*Female)\}
# 
# There is now a much better explanation in the help message for the
# mu command, but I'll go over it briefly here.
# 
# First notice that the default mu has separate terms corresponding to
# traits q1 and q2, and that they are activated by the t1 and t2
# variables.  Following this example, you can create custom mu's for
# bivariate models.  (You don't actually have to follow this example
# precisely, you could repeat the t1 and t2 terms as many times as you
# like.  But I thought this clear division of the parts for each trait
# is easier to read.  I would have preferred to create entirely separate
# mu's for each trait, but that turned out to be much harder to do.  I
# hope to do that in a future release.)
# 
# Second I'll explain why you see the "\{" and "\}" (quoted curly brace)
# delimiters around the entire default mu.  YOU DO NOT NEED TO ENTER
# THESE DELIMITERS IF YOU ARE BUILDING UP A NEW MU FROM SCRATCH.  They
# merely delimit the "default" portion of the mu which is determined by
# the mean parameters and covariates.  Previously I used "[" and "]" to
# delimit the default mu, but that made it impossible to use verbatim.
# You always had to edit those characters out.  Now, you can keep them,
# and the default portion of the mu in, if you would like, simply by
# enclosing that portion between "\{" and "\}" delimiters as shown.  It
# is convenient to leave the default portion of the mu in because it
# automatically changes based on the covariates selected.  (I suspect
# people are using "residuals" and other features because they have
# never known they could do this.)
# 
# Suppose I want to keep the default part of the mu, but add an additional
# "log(g)" term.  I can do that like this:
# 
#     solar> parameter g
#     solar> mu = mu + log(g)
#     solar> mu
# mu = \{t1*(<Mean(q1)>+<bsex(q1)>*Female) + t2*(<Mean(q2)>+<bsex(q2)>*Female)\}+log(g)
# 
# This will *augment* the default mu, which is (now) still shown when you
# display the mu.  (Previously it became the cryptic "mu" term, and most
# people didn't understand what was going on.)  You can keep on adding terms
# to the mu in this way.  You can also cut and paste the entire mu shown and
# edit it as desired, for example, imagine that I have pasted then edit the
# previous line as shown to change log(g) into log(1-g):
# 
# solar> mu = \{t1*(<Mean(q1)>+<bsex(q1)>*Female) + t2*(<Mean(q2)>+<bsex(q2)>*Female)\}+log(1-g)
# solar> mu
# mu = \{t1*(<Mean(q1)>+<bsex(q1)>*Female) + t2*(<Mean(q2)>+<bsex(q2)>*Female)\}+log(1-g)
# 
# SOLAR will accept this, and continue to understand that the portion between
# the quoted curly brace delimiters is the default part which is dependent on the
# covariates chosen.  Editing that part will have no effect.  If you want to
# edit the entire mu, you must remove the delimiters.
# 
# 
# 10.  HLOD now available.
# 
# Homogeneity/heterogeneity testing is now available in SOLAR.  See the
# documentation for the "hlod" command.
# 
# 
# 11.  Bugs fixed
# 
# An cryptic error occurred when special characters (such as %) were included
# in phenotype names and the "residual" command was used.  An appropriate
# error message is now given.  Also there *may* be special characters in
# the data file (for example, in strings) and the residual command will now
# handle those properly.
# 
# Other bugs have been fixed also, but this is a very big update and it is
# unfortunately quite likely that new bugs have been added.
# 
# Please report all bugs.
# 
# 
# ******  NEW in SOLAR 2.0.1 ******
# 
# Bivariate LODS are now reported as "1df equivalent" LODs which are comparable
# to univariate LODS.  See documentation for clod and lodp commands.  You
# should use the lodp command if you are intentionally constraining rhoq1
# also.
# 
# pedlod and pedlike now provide accurate results so that when all
# numbers are added up they should equal scores for the entire pedigree set
# (except for small differences due to rounding errors).  They are also
# *much* faster, especially for large number of pedigrees.
# 
# There are now -testrhoe and -testrhog options for "polygenic" for
# bivariate models.
# 
# ibd and mibd code is now fully updated with latest fixes (for x-linked
# markers and some new relationship classes).
# 
# Probabilities are now reported with scientific notation if they get very
# small.
# 
# linkmod -2p option for setting up twopoint models (replacing linkmod2p).
# (Or, you can simply use the "twopoint" command; this now works).
# 
# To set traits with no parameters use "-noparam" option; this permits
# arbitrary parameterization.  Models are also saved this way, which is
# why they can't be read by earlier releases.
# 
# Bivariate with FAMID fixed.
# 
# Do not use new "hlod" command yet.  It is only experimental now.
# 
# Changes from the earliest distributions of 2.0.0:
# 
# Documentation is somewhat updated.  The tutorial (Chapter 3) is revised
# and corrected in a number of important places.
# 
# Too many fixes to remember.
# 
# **** New in version 2.0.0 ****
# 
# BIVARIATE ANALYSIS
# 
# Two traits may now be specified by the trait command, and the
# resulting bivariate models may be used in standard SOLAR commands
# as with univariate models.
# 
# "Unbalanced" traits (individuals having one trait but not the other) are
# supported by default.  However, there is an "UnbalancedTraits" option to
# turn this support off is required.
# 
# X-LINKED MARKERS ARE NOW WORKING
# 
# Note: Changes between beta versions prior to version 1.7.3 are NOT
# listed.
# 
# Changes to SOLAR 1.7.3 from 1.6.6
#
#       MUCH FASTER IBD CALCULATION
#       The Monte Carlo method of IBD computation (used when the Curtis and
#       Sham method is inappropriate) has been made faster by use of a
#       local likelihood approximation.  In the imputation step, possible
#       genotypes are weighted by probabilities conditional on the
#       genotypes of immediate family members (parents, siblings, and
#       offspring) rather than conditional on the entire pedigree.  For
#       large, complicated pedigrees the speedup can be quite substantial
#       (many fold).  Though tiny differences might be observed in IBD
#       files, we have not found the approximation to change our analysis
#       results, which is the bottom line.  Curtis and Sham IBD computation
#       has also been made faster through various efficiency measures.
#
#       MULTIPLE MARKERS MAY BE SPECIFIED
#       Multiple marker names may now be specified in IBD and MARKER
#       commands.
#      
#       MATRIX MEMORY USAGE GREATLY REDUCED...FIXING SOME MATRIX BUGS TOO
#       When loaded into SOLAR, some matrices could become quite large:
#       hundreds of megabytes or more.  Under some circumstances, you would
#       get an error about a matrix "not found" or "empty" when in reality
#       the problem was insufficient memory for loading.  Improvements were
#       made to greatly reduce memory usage by matrices.  In one typical
#       case, memory usage is reduced from 300 megabytes to 20 megabytes.
#       (The greatest reduction of memory usage comes when you have a large
#       number of separate families.)  Also, if there is a memory shortage
#       when loading matrices, it is now correctly reported.  In addition,
#       the Delta7 and D7 matrices are not loaded unless Delta7 or D7 is
#       actually included in the Omega.  (This cuts matrix memory usage in
#       half regardless of family sizes.)
#
#       MORE DISCRETE MODELS NOW WORK
#       The changes made for matrices seem to have made some previously
#       uncomputable discrete models now maximize properly.  Also, some work
#       was done to prevent discrete models from "hanging."  It is still
#       true, however, that not all discrete models can be maximized by
#       SOLAR.
#
#       REDUCED NETWORK LOADING AND FASTER MAXIMIZATION 
#       We found that dozens of large simultaneous SOLAR jobs were running
#       our network into the ground.  Several changes have been made to
#       SOLAR to greatly reduce network loading.  Some of these concern how
#       SOLAR finds its libraries and anciliary programs; it is now done
#       without unnecessarily searching directories (which on our network
#       and maybe yours are NFS mounted).  File operations are now done
#       internally rather than using external programs such as "cp" and
#       "rm" which also caused much path searching over NFS.  A very big
#       improvement was also made by eliminating the storage of pedigree
#       and phenotypic data in "scratchfiles" during analysis.  This was a
#       carryover from core SOLAR routines which had been written in the
#       1980's to run on memory strapped PC's.  These scratchfiles might be
#       read over and over hundreds of times for a single maximization,
#       resulting in tens of megabytes of file I/O.  On a single machine,
#       this was not much of a problem because of Unix file caching.  But,
#       across a network using NFS mounted directories, it was disasterous.
#       Elimination of the scratchfiles has also improved maximization
#       speed by about 20% even when the working directory is not NFS
#       mounted.
#
#       MUCH FASTER AND GENERALLY IMROVED BAYESIAN MODEL AVERAGING
#       The bayesavg algorithm has changed so that it is not necessary to
#       compute standard errors for every model, particularly for the
#       "saturated" model (for which standard errors might not even be
#       computable).  This doubles speed in typical cases.  Also, there is
#       now a -max option to limit the maximum subset size, and this can
#       reduce computation time astronomically (!) when it is applicable.
#       Models are now built in an intuitive bottom up order.  Much less
#       time and memory are required to generate the subsets
#       ("combinations").  (These changes are also reflected in the
#       combinations command.)  Additional changes include: starting models
#       with interactions or household effects can be used, all models in
#       the window are saved by default, saved models use constrained betas
#       instead of "suspended" covariates, the "best" model is loaded at
#       conclusion, the log_n and other information is shown during
#       operation.  If you have used the bayesavg command before, it may be
#       a good idea to re-read the help documentation before using the
#       new and improved version in this release.  Also, the numerical
#       results may differ slightly from previous versions of SOLAR.
#
#       SPORADIC command
#       A new "sporadic" command has been added having most of the same
#       features (including optional covariate screening) as the
#       "polygenic" command.  For some data, it is preferable to do
#       covariate screening with a sporadic model.
#
#       SIMQTL
#       simqtl genotypes are now written to a file named simqtl.qtl.
#       Previously they weren't being written out at all.  simqtl has been
#       upgraded to handle MZ twins and multiple QTL's.  A linux-specific
#       bugfix (also included in linux release 1.6.7) is added.  AGE
#       covariate no longer required (nor is a dummy covariate).
#
#       ALPHANUMERIC CHROMOSOME NAME BUGS FIXED
#       Alphanumeric chromosome names (e.g. 2p and 2q) now work properly
#       with multipoint analysis and plotting.  In multipoint output files,
#       a leading zero is now automatically prepended to single digit
#       chromosome names so that ordering is done properly with or without
#       the alphanumeric suffixes.  However, you need not specify leading
#       zeros in your commands.
#
#       SCALE command
#       There is now a command "scale" to change the adjustment of covariate
#       variables.  Previously they were always adjusted to the sample
#       mean, and there was no way you could change that.
#
#       PLOT -STRING
#       Options -colorname, -linewidth, and -datestamp added.
#
#       MU NOW EXPLICIT
#       The MU (Mean Equation) is now written to all saved models.
#       It also shows all the actual covariate adjustments used.
#
#       CONSTRAINT DELETION MORE CONSISTENT
#       You can now delete a constraint by specifying its full left hand
#       side.  Previously, you had to specify either an internal index, or
#       (possibly ambigously) specify just one left hand term.
#
#
#       TWOPOINT WORKS WITH PEDLOD
#       TWOPOINT saves and loads best model when finished.  As a result,
#       PEDLOD or PEDLIKE may be run afterwards.
#       
#       999 MZ TWINS SUPPORTED (previously only 99)
#
#       BUG FIXES:
#       RELATIVES (and RELPAIRS) command returns correct counts (previously
#            counts were off by one per family so the error could be quite
#            large for analyses based on nuclear families or sib pairs).
#       PEDLOD and PEDLIKE work when FAMID is used.
#       MULTIPOINT -EPISTASIS fixed for discrete traits.
#       BAYESAVG gets better results because boundary problems fixed.
#       PEDLOD may be run after TWOPOINT.
#       Convergence errors now shown on string plots (previously crashed).
#       Fewer than 10 chromosomes may be string plotted (previously garbled).
#       Better handling of files passed through Microsoft Windows.
#
#
# Changes to SOLAR 1.6.6 from 1.5.7
#
#    DISCRETE TRAITS:
#    SOLAR now supports a single discrete trait using the liability threshold
#    model.  A discrete trait is automatically detected as two integer values
#    separated by 1.  THIS IS CURRENTLY EXPERIMENTAL, AND IS KNOWN TO PRODUCE
#    INCORRECT RESULTS IN SOME CASES.  There are two discrete trait
#    implementations included whose results can be compared (the default one
#    is more reliable but possibly less accurate due to numerical problems).
#
#    STRING PLOTS AND MINI PLOTS:
#    A "string plot" is a condensed method of showing the results of a
#    genome scan (one pass) on one page.  The command is "plot -string."
#    Miniplots are ordinary one-chromosome plots shrunken down so that an
#    entire genome scan can fit on one page (even if some of the legends,
#    etc, are unreadable).  The command is "plot -all."  Miniplots
#    require that Python 1.5.2 or later be installed on the users system.
#    We prefer string plots, for which no external software is required.
#
#    EMPIRICAL LOD ADJUSTMENTS can now be calculated and applied to
#    all LOD scores.  See "help lodadj" for more details.  (This feature
#    may have been present but was not supported and was incomplete in
#    earlier releases.)  There is also a command "madj" to apply a
#    LOD adjustment to previously computed multipoint files.
#
#    DOCUMENTATION BROWSER:
#    The documentation has been restructured and formatted in html.  The
#    "doc" command starts a Netscape browser (if available) to read the
#    documentation.  The "example" command copies the standard example
#    to the working directory.
#
#    TRAIT SIMULATION:
#    "simqtl" command added to simulate one or more quantitative traits
#    controlled by a QTL with (optionally) a linked marker locus.  This
#    is an early release and does not yet handle MZ twins correctly,
#    multiple QTLs are not implemented, and QTL genotypes cannot be read
#    in from a file.
#
#    MARKER TESTING:
#    Marker files can now be checked using the "markertest" command.  If
#    errors are found, markertest will try to find the discrepant
#    individual, family, pedigree, or pair.
#
#    MATH FUNCTIONS IN OMEGA AND MU EQUATIONS: 
#    Math functions (all of C's math library including exp, log, sin, and
#    many more) now work in both Omega and Mu equations.  (This was an
#    advertised feature that didn't work previously.)
#
#    PHENOTYPIC VARIABLES AVAILABLE IN OMEGA
#    Phenotypic variables now allowed in the omega equation.  Must specify
#    varname_i or varname_j for individual i or j.  Also may use
#    x_varname, min_varname, max_varname.  For sex, use female_i, male_i,
#    female_j or male_j.
#
#    PER-PEDIGREE LODS:
#    Added new command "pedlod" which computes pedigree-specific LOD's.
#
#    PER-PEDIGREE LIKELIHOODS:
#    Added new command "pedlike" which computes pedigree-specific
#    loglikelihoods.
#
#    RELATIVE PAIRS:
#    Added new command "relatives" which lists relationships of relative
#    pairs included in an analysis (those having all variables).
#
#    PLOTTING LIABILITY:
#    "plot -liability" command plots discrete trait liability vs. age.  This
#    is automatically separated in to male and female curves if there is
#    a sex or age*sex covariate.
#
#    MITOCHONDRIAL IBDS:
#    Added new command "ibd mito" for computing mitochondrial IBDs.
#
#    MORE NEW PLOT FEATURES:
#    Added -min and -max arguments to ordinary "plot" command to zoom in on
#    a desired range.  If no chromosome is specified, the one with the
#    highest LOD score is plotted.  Plot files may be written to postscript
#    files with the -write option.  (See also new types of plots above.)
#    Plot command checks for most errors before putting up ACE/gr window.
#    Error checks now include check for presence of specified chromosome
#    number in multipoint run.  On Solaris and Alpha platforms up to
#    1000 marker ticks can be shown.  Marker labels and/or ticks can be
#    optionally supressed.
#
#    USING VARIABLE STATISTICS:
#    Basic statistics (mean, min, max, std deviation) about variables
#    can be retrieved from maximization output files using the "getvar"
#    command.  (There has long been a "oldmodel" command to retrieve
#    parameter information from model files.)  This makes it easier to
#    write certain kinds of scripts (like the "residual" command).
#
#    UNKNOWN RELATIONSHIPS:
#    Pedigrees containing "unknown" relationships (usually involving
#    inbreeding) are now permitted for twopoint analyses.  The user is
#    warned that multipoint analyses are not possible.
#
#    CODING MISSING GENOTYPES:
#    Missing genotypes may now be coded as '-/-' as well as blank and '0/0.'
#
#    IDENTITY-BY-STATE:
#    Added a new command, ibs, which computes IBS (identity-by-state) matrices
#    for markers in the ibddir directory.  This command should be considered
#    preliminary.
#
#    HALDANE MAPPING FUNCTION:
#    By default, the locations in the map file are mapped to recombination
#    fractions using the Kosambi mapping function.  It is now possible to
#    use the Haldane mapping function instead (see "file-map").
#
#    CASE-INSENSITIVE MARKER NAMING
#    Marker names are now treated as case insensitive in commands.  The
#    name as specified in the marker file is the "official" name used by
#    the IBD files created.  The map file may also use a different case.
#
#    MIBD WINDOW:
#    You can now specify an MIBD window.  The MIBDs at a given chromosomal
#    location will depend only on those markers that lie within the window.
#    This may give a large speedup when computing MIBDs but should be used
#    with caution.
#
#    RESIDUAL COMMAND FIXED:
#    The "residual" command had several bugs which were fixed.  In some
#    cases you might have gotten only a few lines of results or no results
#    at all.
#
#    ID "BUG" FIXED:
#    It is permitted now for the ID fields in pedigree and marker files
#    to have different max widths.  (This can happen if either file contains
#    more ID's than the other.)  Differences in formatted width (e.g. in
#    a PEDSYS file) are ignored.
#
#    NEW TRAIT "BUG" FIXED:
#    Scripts resetting trait (without doing a full "model new") will now work
#    although this is still not recommended procedure unless traits are
#    similar in genetic properties (variance component parameters are not
#    reset).
#
#    IMPROVED BOUNDARY HANDLING:
#    To handle problems with difficult likelihood spaces better, existing
#    artificial boundary handling has been improved and there are new
#    options to the "boundary" command: "boundary wide," "boundary null,"
#    and "boundary quadratic tol."  In some cases the quadratic values
#    were tested when they shouldn't have been leading to premature
#    "convergence error" failures, in other cases restarts occurred on 
#    boundaries (leading to infinite runs that appeared like SOLAR was
#    stuck), and so on.  There are now two tiers of boundary "crunching"
#    instead of just one.  Loglikehood must remain with 9 digits of
#    accuracy from "best" iteration to last one or warning is given
#    (for "maximize" or with "verbosity max").
#    
#    COVARIATE BOUNDARY ERRORS FIXED:
#    In some cases, covariate boundaries errors were occurring.  The defaults
#    for the covariate boundary adjustment and retry mechanism (from 1.4.x)
#    were increased to 10 retries with an expansion factor of 5.  These
#    values are now adjustable through the 'boundary cov retries' and
#    'boundary cov incr' commands.  The new default values handle currently
#    known problem cases without further adjustment.
#
#    ALPHANUMERIC CHROMOSOME LABELS:
#    Chromosome labels may now include alphabetic and numeric characters
#    and underscore.  Alphanumeric labels may be used in the "chromosome"
#    command and plot command, but may not be included in chromosome "ranges"
#    (e.g. instead of chromosome 1-23, now say chromosome 1 2 2p 3-23).
#
#    HEADER SPACING ALLOWED:
#    Comma delimited files with padded 'ID' columns now work OK
#    PEDSYS files with padded 'ID' columns (matched with unpadded ID C.D.
#    marker files) now work OK
#
#    COMMENTS ALLOWED IN DATA FILES:
#    Comments (leading #) and blank lines now OK in comma delimited files
#
#    MISSING PEDIGREE FILES HANDLED NICELY:
#    Improved handling of corrupted/missing state files (pedigree.info,
#    freq.info, marker.info).  In particular, pedindex.out isn't deleted
#    if one of these files is missing or corrupt.
#
#    MARKER FILES MAY CONTAIN PEDIGREE FIELDS:
#    The marker load command now ignores fields in a marker file which
#    are named FA, MO, SEX, MZTWIN, HHID, or AGE.  Previously, all fields
#    other than ID and FAMID were considered to be genotype fields.
#
#    MULTIPOINT PROMPTING
#    You can now enter the "multipoint" command and be prompted to enter
#    chromosome, mibddir, and interval rather than just getting error
#    messages for each item you forgot to enter.
#
#    IBD BUGS FIXED:
#    A bug in the computation of IBD's for some MZ twins was fixed.  (This
#    bug had not been reported by any users.)  Portability is improved by
#    use of printf in the domcibd script.
#
#    IBD CALCULATION MORE EFFICIENT:
#    IBD calculation (command: 'ibd') has been made more efficient when the
#    Monte Carlo method is used.
#
#    IBD MESSAGES FIXED:
#    If parental IDs were longer than ego IDs, this caused 'load pedigree' to
#    give an misleading error message.  The message is now helpful.
#
#    "LOAD MARKER" MESSAGES IMPROVED:
#    The "load marker" command now displays a message indicating whether the
#    marker allele frequencies were fread from a previously loaded freq
#    file or are being estimated from the marker data.  Also, if the
#    allele freqs are MLEs computed from old (now unloaded) marker data,
#    a warning to that effect is given.  Such allele freqs are now called
#    "stale MLEs" rather than "old MLEs" as before.
#
#    FREQ BUGS FIXED:
#    If allele freqs are loaded from a freq file rather than being estimated
#    from the marker data, it is possible to have frequency data for more
#    alleles than are actually present in the marker data.  (The converse
#    would be an error - every allele in the marker data must have an entry
#    in the freq file.)  If MLE allele frequencies are then computed, the
#    alleles not present in the marker data will have frequencies of 0.
#    This 0 frequencies caused problems with other commands, such as "ibd"
#    when using the Monte-Carlo method.  These problems have been fixed.
#
#    MIBD CHECKING:
#    The mibd command now ensures that the mean IBD file (mibdchrN.mean) is
#    newer than the merged marker-IBD file (mibdchrN.mrg.gz).  If the mean
#    file is older (or not found), it is recomputed.  The mibd command also
#    ensures that the merged marker-IBD file is newer than any of the IBD
#    files for markers in the map file.  If the merged marker-IBD file is out
#    of date, an error message is displayed, and "mibd" cannot be done until
#    "mibd merge" is done (which is described in the message).  IF, in fact,
#    the IBD files have not changed, but merely have a more recent date due
#    to copying, the user can simply run 'mibd merge' and proceed.  There is
#    also more extensive checking for "mibd merge."
#
#    RESOURCE LEAKS FIXED:
#    File descriptor "leak" in polygenic command fixed.  (If you ran
#    polygenic 50 times, you would run out of file descriptors.)
#
#    "FILE" COMMANDS
#    The file formats are described by "file-*" commands (e.g. file-freq)
#    replacing the old "notes-*" commands.
#
#    HOUSEHOLD-PEDIGREE MERGING INFORMATION
#    If household-pedigree groups were merged, this information is shown in
#    the results of the "polygenic" command.  A household-pedigree
#    group merging bug (unusual case) was fixed.  There is an option to
#    show the merged groups ("option HouseGroupShow on").
#
#    CONSTRAINTS EASIER TO MODIFY AND DELETE
#    You can now "replace" a constrant with a new one...the obsolete one
#    is automatically removed.  Parameters having embedded *'s may be
#    included in constraints using <>.
#
#    ALNORM command added to evaluate the tail of a normal curve.
#
#    AND MANY MORE BUG FIXES AND CHANGES
#    And no doubt some new bugs too.
#
# Changes to SOLAR 1.5.7 (from 1.5.6)
#
# 1.  Faster twopoint performance
#     A bug was fixed which had been causing 4x slower twopoint analyses in
#     version 1.5.5 than in 1.4.0.
#
# Changes to SOLAR 1.5.6 (from 1.5.5)
#
# 1. DEC Alpha zombie bug fixed
#    A bug was fixed (for DEC Alpha systems only) in which SOLAR would create
#    large numbers of zombie processes.
#
# Changes from SOLAR 1.5.5 to 1.5.4
#
# 1. Twin Bug fixed
#    Linkage analyses could not be done for pedigrees including MZ Twins.
#    The table-driven ibd code (introduced in version 1.5.0) did not handle
#    mztwins at all, due to a bug in multipnt.c and classes.tab, now fixed in
#    SOLAR 1.5.5.
#
# 2. Linux edition IBD bugs fixed
#    Bugs making IBD files using the "Curthis and Sham" method (the default
#    in most cases) fixed on Linux (bugs existed only in early Linux versions).
#
# Changes from SOLAR 1.5.5 to 1.5.4
#
# 1. Twin Bug fixed
#    Linkage analyses could not be done for pedigrees including MZ Twins.
#    The table-driven ibd code (introduced in version 1.5.0) did not handle
#    mztwins at all, due to a bug in multipnt.c and classes.tab, now fixed in
#    SOLAR 1.5.5.
#
# 2. Linux edition IBD bugs fixed
#    Bugs making IBD files using the "Curthis and Sham" method (the default
#    in most cases) fixed on Linux (bugs existed only in early Linux versions).
#
# Changes from SOLAR 1.5.4 to 1.5.3
#
# 1. The documentation was updated and enhanced for version 1.5.x.  Notably
#    full documentation for the input file requirements (which used to only
#    be available in help messages in earlier releases) and matrix file
#    contents were added (ibd, mibd, phi2) for the benefit of those doing
#    more sophisticated analyses (dominance, etc.).
#
# 2. Attempting to do a quantitative analysis of a discrete trait results
#    in a warning (as intended for 1.5.3), not a fatal error (a bug that
#    crept into 1.5.3).
#
# 3. If the correct model parameters have not been set up, the user is now
#    advised to give the "polygenic" script first (previously automodel and
#    polymodel were advised...but "polygenic" is now the recommended approach.
#
# 4. Support for Linux (Intel) added.
#
# Changes from SOLAR 1.5.3 to 1.4.0
#
# 1.  Household effects
# 
# 	There is a new command 'house' which sets up a C2 parameter
# 	for any common environmental effect.  The pedigree file should have
# 	a 'HHID' field in order for the required 'house.gz' matrix file
# 	to be created during the 'load pedigree' command.
# 
# 	The 'polygenic' command will determine the significance of the
# 	household effect if the 'house' command was given.  Multipoint and
# 	other commands will create models including the household effect.
# 
# 	Note that in some cases individuals in different "pedigrees" may
# 	share the same household.  (This happens for "marry-ins," for
# 	example.)  In order to get the best estimate of the household
# 	effect, pedigrees sharing households are automatically merged
# 	during maximization.  (This feature was added after version 1.5.0.)
# 	This feature can be turned off with the MergeHousePeds option.
# 	Alternatively, all individuals may be included in the same
# 	pedigree with the 'MergeAllPeds' option.  Either kind of merging
# 	has little or no effect on hereditary estimates, but can make SOLAR
# 	run more slowly.
# 
# 2.  No arbitrary limits on sizes
# 
# 	Previously there were some hard-coded limits for the number of
# 	individuals in a pedigree, number of alleles, etc., particularly
# 	for commands related to creation of IBD and MIBD matrices.  The
# 	limits arose because some of the old FORTRAN programs used had
# 	fixed array sizes.  These programs have been rewritten to use
# 	dynamic memory allocation instead, so there are no arbitrary limits
# 	on any sizes (or if there are, they are way out there).  You should
# 	not run into any arbitrary limits any more.
# 
# 3.  Epistasis and other special kinds of covariance
# 
# 	Epistasis effects are automatically handled by the new -epistasis
# 	argument of multipoint.  In this case, the starting model is not
# 	a null0 polygenic model, but a model including one or more linkage
# 	elements.  One of those is chosen by the -epistasis argument to
# 	be applied to each QTL in the multipoint scan with an added epistatic
# 	interaction component (h2qe1).
# 
# 	The multipoint and twopoint procedures now preserves special
# 	elements in the covariance (omega) equation and constraints.  This
# 	makes possible the inclusion of household effects, epistasis, and
# 	other effects such as dominance (though dominance terms must still
# 	be set up manually by the user).  Previously, multipoint simply
# 	clobbered whatever special elements the user had set up with a
# 	standard e2, h2r, h2q1, h2q2, ... series.
#
# 4.  Grid command and twopoint -grid
# 
# 	The highest likelihood in the vicinity of a marker can be found with
# 	the new grid command which searches recombination fractions of
# 	the marker.  There is also a twopoint -grid option which will do
# 	this for every marker in a twopoint scan.
# 
# 5.  Multipoint and twopoint require 'polygenic' be run first
# 
# 	Multipoint and twopoint procedures now require that 'polygenic' is
# 	run first to do a polygenic analysis.  'polygenic' now creates a
# 	model named null0 which is now required by multipoint and twopoint.
# 	Most users were running polygenic first anyway.  If you forget,
# 	multipoint and twopoint will now remind you to run polygenic first.
# 
# 	Previously twopoint and multipoint would create a null0 model using
# 	a procedure similar to (but not necessarily identical with)
# 	polygenic and possibly clobbering the null0 model that had
# 	previously been created.
# 
# 6.  chi -inverse and better chi
# 
# 	There is now a chi -inverse option, and the chi procedure itself
# 	is a better one (you may notice changes in the last printed decimal
# 	place in reported p values).
# 
# 7.  Fully automatic array allocations during maximization
#
#     Memory allocation for maximization is now fully automatic.  Previously,
#     with some very large pedigrees or other input data, the user might have
#     to set some obscure options to increase array allocations, and to
#     make matters worse...the required allocations were sometimes
#     underestimated...resulting in fatal or other errors.  Now the arrays
#     are tested after maximization to be sure memory overwriting did not
#     occur.
#
# 8.  Support for Digital Alpha Unix added
#
# 9.  Relative-class info in tables
#
#     Information about the types of relatives handled by SOLAR used to
#     be hard-coded but is now read from tables.  This means that adding
#     new classes or modifying existing classes can be done by
#     updating these tables; it isn't necessary to recompile SOLAR.  A
#     few new relative classes have been added since 1.4.0, and we can
#     (relatively) easily add new classes if needed by any users.  SOLAR
#     will tell you if you need a new relative class.
#
# 10.  Quadratic tested
#
#     The final normalized quadratic (which should be close to 1.0) is
#     now checked.  With some data, maximization would abort prematurely
#     with poor quadratic values.  Now there is an automatic retry mechanism
#     to force the quadratic to a good value, or fail with an error message
#     if that proves impossible.  There is also a "quadratic" command to
#     get the last quadratic value.
#
# 11.  Comma Delimited Input File Support Improved
#
#     Comma delimited files may now have type:name pairs in the first (header)
#     line.  Segmentation violations related to reading some files fixed.
#
# 12.  PEDSYS Input File Support Improved
#
#     PEDSYS 'standard' mnemonics (e.g. INFERD) are now properly ignored in
#     any mnemonic position (previously, they were only ignored in the last
#     position).
#
#     PEDSYS mnemonic names in successive fields are concatenated even if
#     they are shorter than 6 characters.  Many users liked to segment their
#     field names this way.
#
# 13.  Pedigrees identified by first PID
#
#     Pedigrees skipped (because of missing data) or having errors detected
#     during the maximization phase (rare) are now identified by the first
#     PID rather than by the PEDINDEX number.  (The PEDINDEX number had no
#     connection to the user's pedigree numbering.)
#
# 14.  A warning is given if trait is discrete
#
#     The analysis of discrete traits by liability threshold model is not
#     yet fully integrated with the Tcl-based public version of SOLAR.
#     If the user attempts to analyze a discrete trait, a warning is now
#     given, though the analysis is done anyway without liability threshold
#     model.  (Previously SOLAR would ignore whether the trait was discrete
#     or not.)
#
# 15.  Added drand command to get random numbers
#
# 16.  linkmod and linkmod2p interfaces improved
#
#     'linkmod' is the command which sets up (but doesn't test or maximize)
#     a linkage model with a particular MIBD matrix.  linkmod2p does this
#     for a twopoint IBD matrix.  These commands have been made relatively
#     easy to use by users (they no longer require global variables set up
#     by the multipoint and twopoint procedures).  Thus it is now much easier
#     to write custom IBD/MIBD scanning procedures.
#
# 17.  Help documentation for SOLAR model options added
#
# 18.  Bugs in the summary statistics for excluded pedigrees fixed
#
# 19.  Solaris Workshop 5 C++ library check and other startup issues
#
#     SOLAR is now compiled (on Solaris SPARC) with Solaris Workshop 5
#     compiler.  This may provide better performance and reliability, but
#     it also required considerable recoding to comply with the new C++
#     standards.  Also, a C++ library patch is required from Sun for Solaris.
#     SOLAR checks for the required library patch on startup and tells you
#     what is needed.  The SOLAR main binary is now called 'solarmain' to
#     avoid confusion with the 'solar' shell script which starts SOLAR.
#
# 20.  Convergence, Boundary, and other maximization errors properly named.
#
# 21.  Output formatting of polygenic command improved.
#
# 22.  Fixed bug when not reloading phenotypes after loading pedigree.
#
# 23.  Fixed bug setting omega to 3 character expression.
#
# 24.  Fixed bugs with residual command: when phenotypes file has famid field,
#        case sensitivity, skipping too many individuals, and the expression
#        used to calculate residual was fundamentally wrong.
#
# Changes from SOLAR 1.4.0 to 1.3.0
#
# The changes in this version were considered so vital that the previously
# anticipated public release was delayed for a few weeks so that they
# could be included (rather than wait for the next major release in a few
# months).
#
# INCREASED LOD SCORES AND H2Q COMPONENT VALUES
#
# LOD scores might increase (in rare cases) and QTL positions might even
# change (in very rare cases) as a result of these changes.
#
# It turned out that the old retry mechanism did not detect all artificial
# boundary conditions.  In particular, supposedly maximized linkage models
# could hit an artificial lower bound for E2, silently preventing the H2Q
# component from reaching their maximum values.  This would cause reduced LOD
# scores as well.
#
# You can check earlier models to see if E2 is at an artificial lower bound
# (higher than 0.0).  If it is, then you need to re-maximize the model using
# version 1.4.0 or higher.
#
# Models with this problem would be likely to have unusually steep LOD score
# curves (and relatively high LOD scores) in the first place.  This is
# not going to cause new LOD score peaks to arise, just make very steep peaks
# even steeper (and possibly even change slightly the positions of the
# summits -- in other words -- the QTL positions).
#
# BOUNDARY COMMAND AND AUTOMATIC RETRIES
#
# A new 'boundary' command has been added, replacing the slew of various
# artificial boundary heuristics (h2qf h2rf e2lower and e2squeeze) used to
# assist in convergence control through the use of artificial boundaries.
#
# But more importantly, there is now a retry mechanism so that whenever an
# artificial variance component boundary is hit the boundary is automatically
# changed.  Because of this mechanism, it is believed that 'Boundary Errors'
# (at least the ones related to variance components) should never occur
# again, and that if 'Convergence Errors' ever occur they can be dealt with
# more easily.  Because of the retry mechanism, the default heuristic
# values have been set very low to make convergence errors very unlikely
# also.
#
# The heuristically set upper bound for h2q parameters now floats from
# one locus to the next.  This floating action (governed by the command
# 'boundary float upper') has replaced the old 'h2qf' command (which turned
# out to be very problematical in many cases...there was no one value which
# worked across the genome).  The h2qf command is now silently ignored.
# The other heuristic commands still exist, but they are best used through
# the more intuitive 'boundary' command interface.
#
# The new retry mechanism now applies to all maximization commands, including
# a single 'maximize' command (for which previously no heuristics or retries
# were applied in earlier versions).
#
# MULTIPOINT -RESTART FIXED
#
# It turned out that if a multipoint scan was restarted (with the command
# 'multipoint -restart'), it would ignore models for which a boundary or
# convergence error occurred.  This meant that if you wanted to redo those
# models, you needed to edit the files such as multipoint1.out and remove
# those models so they would be redone.  Now a restart will re-maximize all
# erroneous models automatically, and no file editing is needed.
#
# Changes from SOLAR 1.3.0 to 1.2.1
#
# COMMAND SHORTCUTS, USAGE, AND HELP
#
#   Shortcuts for command names may now be used in scripts.  For example, a
#   script may use the command 'mul' for 'multipoint' just as at the command
#   line.  (This applies only to SOLAR commands, not Tcl commands.)
#
#   There is now a 'usage' command which gives an abbreviated form of help for
#   any command.  Unlike 'help,'  the usage information stays on screen to
#   help the next command as a memory aid.
#
#   There is also a 'shortcut' command which shows the legal shortcuts allowed
#   for any SOLAR command.  The legal shortcuts will also be shown by the
#   'help' and 'usage' commands.
#
#   Previously some commands required plural forms (such as 'phenotypes') while
#   others required singular forms (such as 'parameter').  Now, both singular
#   or plural forms are allowed for many commands, and, since they all be
#   abbreviated in scripts anyway, you may stick with the singular forms.
#
# PLOT improvements
#
#   'plotmulti' is now officially the 'plot' command (though you may still use
#   'plotmulti.'
#
# RESIDUALS
#
#    There is now a 'residual' command which postprocesses a maximization
#    output file and a phenotypes file to get the residual after the 
#    application of covariates.  The result is a new phenotypes file giving
#    the residual value for each individual.
#
# PROBANDS
#
#   Use of the proband field may now be turned on and off without reloading the
#   phenotypes file using the commands 'field probnd -none' and
#   'field probnd probnd' (or whatever).
#
#   SOLAR now detects proband fields named 'proband,' and 'prband,' as well as
#   the PEDSYS standard PROBND.
#
# TDIST
#
#   There is now a 'tdist' commands which sets up the 't' distribution option
#   for robust estimation of mean and variance for non-normal distributions.
#
# SCORE TEST
#
#   'multipoint -score' uses a score based test (in place of maximum
#   likelihood' to find QTL's.  The resulting measure is SLOD (Score based
#   LOD) which is not a real LOD score but should be analogous to one.
#   'multipoint -scoredebug' gives additional information.
#
# NEW BAYESAVG OPTIONS and BUGFIXES
#
#   command 'bayesavg' now accepts a '-sporadic' option which forces the use
#   of sporadic models in cases where the polygenic models don't converge.
#
#   command 'bayesavg' now accepts a '-fix' option to fix some covariates.
#
#   cases where H2R=0 are now handled properly.
#
# IBD-related PROCESSING improvements
#
#   It is no longer necessary to load the pedigree file in every SOLAR run in
#   order to do IBD-related processing.  Pedigree data remain loaded until a
#   new 'load pedigree' command is entered.  This is now true for the marker
#   file and the freq file as well.  Two new SOLAR files, marker.info and
#   freq.info, have been added to preserve marker and frequency information
#   between SOLAR runs.
#
#   To be consistent, the term "locus-information file" has been changed to
#   "freq file".  Since 'load pedigree' loads a pedigree file, it makes sense
#   that 'load freq' loads a freq file.
#   
#   The marker file and the freq file no longer have to contain exactly the
#   same set of markers, nor is the order of the markers important.  As before,
#   if a marker is loaded and no allele frequency info is available from a
#   prior 'load freq', a simple counting method is used to estimate the allele
#   frequencies.  Now, however, frequency info can be loaded in advance for
#   some of the markers in the marker file and not others.  Or a single freq
#   file might contain allele frequencies for all the markers in a study, not
#   just those in a particular marker file.
#
#   When a marker is loaded for which no frequency info has been loaded in
#   advance, the simple-count allele frequencies are added to the file freq.info
#   and will be displayed by 'freq show'.  But the freq file itself is never
#   modified, even when MLE allele frequencies are computed.  Nor is a default
#   freq file created (previously the file locfile.dat was created.)  To save
#   frequency information, a 'freq save' command has been added which creates
#   a file that can be loaded later with 'freq load'.
#
#   There is now a 'marker unload' command.  When marker data are unloaded,
#   the allele frequencies for these markers are removed from freq.info, and
#   the marker-specific subdirectories created by 'marker load' are deleted.
#   If MLE allele frequencies have been computed for any of the markers but
#   have not been saved to a file, the unload will not proceed unless the
#   -nosave option is given in the unload command.  It is not necessary to
#   unload current marker data before loading a new marker file.  The unload
#   will be done automatically (unless MLE allele frequencies have not been
#   saved, in which case an explicit 'marker unload' with the -nosave option
#   is required.)
#
#   There is also a 'freq unload' command which removes all currently loaded
#   frequency information, except for any markers with currently loaded
#   genotype data.  That is, the allele frequencies for markers in the marker
#   file are not deleted.  They can only be removed by 'marker unload'.  It
#   is not necessary to unload current frequency info before loading a new
#   freq file; the unload will be done automatically.
#
#   When MLE allele frequencies are computed, this fact is recorded in the
#   file freq.info.  An attempt to run 'freq mle' on markers for which MLE
#   allele frequencies have already been computed simply returns a warning
#   message.  By default, MLE allele frequencies are now required before
#   marker-specific IBDs can be computed.  The ibd command has a -nomle option
#   to get around this.  Alternatively, a new IBD-processing option, NoMLE,
#   can be set with the ibdoption command.
#
#   The 'freq show' display has been updated to indicate markers for which
#   MLE allele frequencies have been computed, and whether the frequencies
#   have been saved to a file.  Also, X-linked markers are labeled as such.
#
#   The 'marker load' command now has a -xlinked option which can be used to
#   load X-linked marker data.  Alternatively, the XLinked option can be set
#   with the ibdoption command, as before.  Male genotypes for X-linked
#   markers can now be coded as one allele, e.g. " /A" or "A/ ", or as a
#   "homozygote", e.g. "A/A".
#
#   Previously, the method of IBD computation had to be chosen before loading
#   marker data.  This is no longer true.  If both methods are applicable,
#   i.e. there is no inbreeding and multiple loopbreakers are not required,
#   then either method, Monte Carlo or Curtis and Sham, can be chosen at any
#   time.  For performance reasons, the Monte Carlo method is now used
#   automatically for markers that are completely typed, i.e. markers for
#   which there is no missing genotype data.
#
#   The information displayed by 'pedigree show' now includes the number of
#   loopbreakers required for each pedigree, and whether the pedigree is
#   inbred.
#
#   IBD and multipoint IBD (MIBD) matrix files are no longer created in the
#   current working directory by default.  The directory in which to create
#   the IBD files must now be specified with the ibddir command.  Similarly,
#   the directory in which to create the MIBD files must be specified with
#   the mibddir command.  Also, since IBD files are used to compute MIBDs,
#   the ibddir command is now required before the mibd command can be run.
#
#   When MIBDs are computed, a copy of the map file is now placed in the
#   mibddir directory.  This file is used by the plot command.
#
#   After the relative-class file has been created by 'mibd relate', a new
#   command, 'pedigree classes', will display a tally of the relative classes.
#
#   Several IBD-related commands write status info to the screen while they
#   are running, namely 'freq mle', 'ibd', and 'mibd'.  For example, 'freq mle'
#   displays the current iteration and the change in the likelihood.  These
#   displays can cause SOLAR scripts to hang or abort when run as background
#   jobs.  Since the status info serves no purpose in a background job, the
#   'verbosity min' command can now be used to turn off these displays.
#
#   Another problem with IBD-related scripts has been the use of prompts.
#   For example, the 'load marker' command would not overwrite an existing
#   locinfo.dat file without an OK from the user.  These prompts have been
#   eliminated.
#
#   MZ twin IDs no longer have to be sequential integers, but can be any
#   unique identifier.
#
#   If the pedigree file contains a household ID field, a matrix file (named
#   'house.gz') will be created so that household effects can be incorporated
#   into a variance components analysis.  The household ID can be any unique
#   identifier.  The standard SOLAR name for this field is HHID, but this can
#   be changed with the field command.
#
#   In the pedigree file, sex can be coded "m/f" as well as "M/F" and "1/2".
#   The manual already made this claim, but the code did not allow it.
#
# TCL patchlevel
#
#   TCL patch level 5 (8.0.5) is now used, and the startup scripts forces the
#   use of the init files in SOLAR_LIB rather than using whatever happens to
#   be installed in the OS.
#
# Changes from SOLAR 1.2.1 to 1.2.0
#
# PLOT improvements
#
#   The 'Plot' command accepts the -map option which allows the use of a user
#   map file to specify marker locations.  The map file may be in the same
#   format as used for the 'load map' command (see file-map).
#
#   If there is a convergence error, this is plotted as a Star for each
#   non-convergence location, and a legend box is put up to identify the
#   Star.  The symbol used may be changed by editing multipoint.gr.
#
#   The chromosome number may simply be specified without the -chrom
#   identifier:
#
#       plot 6
#
#   will plot chromosome 6.
#
#   The -set and -graph arguments are no longer documented as they are probably
#   not going to be needed.
#
# Changes from SOLAR 1.2.0 to 1.1.2
#
#   PLOTMULTI and MULTIPOINT -PLOT
#
#   There is now a PLOTMULTI command to plot multipoint LOD scores vs.
#   chromosome position in Cm.  The plots also show marker locations if
#   the file mibdchr<N>.loc files are found in the mibddir (as they
#   should be).
#
#   Plots may be drawn during a multipoint scan by using the '-plot' argument
#   to multipoint.
#
#   Several plots may be overlayed by using the -overlay argument for each
#   subsequent PLOTMULTI command.  The color of each plot may be specified
#   with the -color argument.
#
#   The multipont pass and chromosome number may be specified with -pass and
#   -chrom arguments.
#
#   The multipoint files are read from the 'trait' or 'outdir' directory.
#
#   Example:
#
#       trait bmi
#       plotmulti -chrom 5
#       plotmulti -overlay -chrom 5 -pass 2 -color 1
#
#   A custom version of XMGR is used to do the plotting (included with SOLAR)
#   is used to do the plotting.  The plot may be modified using the XMGR
#   GUI interface after plotting has been done, or by modifying the file
#   'multipoint.gr' beforehand.  The file describes all the useful settings.
#   Plots may be printed or saved to postscript files.
#
# CONVERGENCE
#
#   The default value of H2QF has been changed to 1.25 which helps convergence
#   in many cases.
#
# BAYESAVG improvements
#
#   A 'strict' Occam's window is now the default.  The 'symmetric' Occam's
#   window (which used to be used) is now an option using the -symmetric
#   argument.  The models within the window are now reported
#
#   Only the most important models and output files are saved.  (Previously,
#   all the models and output files were saved, and this could require
#   gigabytes.)  The saturated and unsaturated models now have more mnemonic
#   names (e.g. c.sat.mod or cov.sat.mod).  Options -saveall and -savewindow
#   may be used to save more of the models.  A seperate command 'bayesmod'
#   may be used to regenerate any of the models.
#
#   Because only the most important models are saved, large changes had to
#   be made for the restart function.  There is now a -redo option to allow
#   for the case when non-converging models were edited out of the output
#   file.  The regular -restart begins after the last model processed.
#
# PEDIGREE Handling
#
#   A message is printed if pedigrees needed to be skipped because
#   there were no non-probands having a full set of the required variables.
#   A list of skipped pedigrees is printed, and statistics are provided for
#   the skipped as well as the unskipped pedigrees.  A bug which caused some
#   of the statistics to be incorrect if there were skipped pedigrees has
#   been fixed.  (Note: these messages do not appear at the low verbosity
#   levels used by 'multipoint' and 'polygenic,' but are written to the
#   maximization output files such as 'poly.out' and 'null0.out.'.)
#
# Changes from SOLAR 1.0.1 to 1.1.2
#
# Covariate interactions may now specify any number of quantitative and/or
# binary variables specified in any order.  The * operator is now used to
# signify an interaction between variables (replacing : used in version 1.0.1,
# however : is still permitted for compatibility).  For example, the following
# covariates could be specified:
#
#      covariate age^2*sex*diabetes
#      covariate waist*height^2*age
#
# Covariate Boundary Detection and Automatic Retries:  If a covariate is
# maximized to a boundary, there will be up to 3 retries (increasing the
# boundary each time) to correct the problem.  If the boundary problem
# persists after 3 retries, an error will be reported and the current command
# will terminate.
#
# The bayesavg command has been fixed to give correct results for covariates
# and restart properly for covariates.  Posterior probabilities are now shown
# in the final output.
#
# A default finemapping of 0.588 LOD is now in effect for multipoint.
# Previously, the default was simply to finemap around the single highest
# peak, which was decided to be not useful (and potentially misleading for
# inexperienced users).
#
# The default verbosity for commands such as multipoint and polygenic hides
# all the maximization detail (so you don't need to remember to do
# "verbosity min").  If you really want to see all the maximization detail
# there is a new verbosity level, "verbosity plus" which shows it.
# (verbosity max shows even more detail such as memory usage.)
#
# twopoint now runs sporadic and polygenic models and reports their results
# (see file twopoint0.out) before beginning to maximize linkage models.
#
# Field names EGO, SIRE, and DAM are permitted in place of ID, FA, MO.
#
# User errors involving field names are more fully reported and the user
# is told what field command to give.  Also, the following bug has been
# fixed: if a file hadn't been read because of a mislabeled field, you had
# to re-start SOLAR to read it (even after giving a filed command).
#
# There is now no limit on the length of parameter names.  Names shorter than
# 40 characters are recommended for neater output, however.
#
# Variable names longer than 6 characters specified in code files need not be
# divided by the spaces ordinarily required for PEDSYS mnemonics.  SOLAR now
# allows the names to be divided by spaces or not.  Warning: PEDSYS programs
# do not have this feature.  They will write write spaces into the 6th and
# 12th character positions whether you have spaces there or not.
#
# The 'maximize' command now writes to the current 'outdir' (which defaults
# to the name of the trait).  It accepts -quiet and -output options.  -output
# is required when giving a particular output file name, e.g.:
#
#      maximize -quiet -output nocov.out
#
# Probability levels are reported as '=' (not <) unless below reportable
# precision.
#
# The upgrade command now makes copies (*.old) of the model or script files
# which are upgraded.
#
#
# Changes from SOLAR 0.9.100 to 1.0.1:
#
#	The new SOLAR 1.0.1 has much more powerful and intuitive covariate
#	commands, better covariate interaction (e.g. age by sex) screening,
#	the new bayesavg (Bayesian Oligogenic Model Averaging) command, and
#	some changed command options (more self-explanatory).  It also has
#	a number of bug fixes.
#
#	The changes are described in the following sections of this
#	announcement:
#
#		1.  Improved help
#		2.  18 char variable names and 40 char parameter names
#		3.  Model and Script Upgrading
#		4.  New covariate command
#		5.  Bayesian Model Averaging
#		6.  Some changed commands and arguments
#		7.  Comma delimited file bugs
#
#
#			       Improved Help
#			       -------- ----
#
#	The 'help' command now lists all the available commands and scripts
#	and gives a one-line summary of each.  You may find it easier to
#	find the command you are looking for.  All help messages now use
#	unix 'more' to page through the documentation.
#
#	(Note: If you use Sun's Open Windows cmdtool terminal, you may lose
#	a line or two at the top of each help message because of a bug in
#	Sun's cmdtool which occurs only if you have scrolling turned on.
#	It is annoying but not critical.  If you have scrolling turned on,
#	you can always scroll back to see the top line, which is always a
#	'Purpose' description.)
#	
#
#
#       18 character variable names and 40 character parameter names
#       -- --------- -------- ----- --- -- --------- --------- ----
#
#	Data variable names are now useful up to 18 characters long and
#	model parameter names are useful up to 40 characters long.  They
#	may actually be as long as you like, but they must be unique within
#	the new specified limits.  In the most important SOLAR messages,
#	the full names of variables and parameters are printed.  In some
#	older messages, the names are truncated on display to fit nicely in
#	columns.
#
#	When using PEDSYS files, the first 3 'mnemonics' are concatenated
#	and used as variable names.  Previously, only the first mnemonic
#	was used.  
#
#	Spaces are not allowed in variable names (as before), so the name
#	is terminated by the first space.
#
#	The longer parameter names make possible more meaningful covariate
#	beta parameter names, which are described in a later section.
#
#
#			Model and Script Upgrading
#			----- --- ------ ---------
#
#	The new solar does not require any changes to pedigree or phenotype
#	data files.  But it does require that previously created models
#	be upgraded to use the new covariate command syntax.  SOLAR will
#	automatically detect when models need upgrading and tell you how
#	to use the 'upgrade' command, e.g.:
#
#	  solar> load model oldie
#	  Must use upgrade command to upgrade this model: upgrade oldie.mod
#	  solar> upgrade oldie
#          solar> load model oldie
#	  solar> 
#
#	Some command option names have changed (-f is now -overwrite and
#	-r is now -restart).  Existing scripts may need to be upgraded as
#	well.  To upgrade scripts, you may also use the upgrade command,
#	but you MUST include the ".tcl" suffix:
#
#		solar> upgrade doit.tcl
#
#	
#			 The New Covariate Command
#			 --- --- --------- -------
#	You can now:
#
#	    a.  create or delete many covariates in one command line
#	    b.  specify interactions between any quantitative variable
#		  and any binary variable (not just sex)
#	    c.  specify any exponent (1-9)
#		
#	The syntax is also designed to be more intuitive.  Some examples:
#
#	covariate age			age as a simple covariate
#	covariate age:sex		the age by sex interaction ONLY
#
#	covariate age#sex		age, sex, and the age:sex interaction
#					  i.e. both vars AND their interaction
#
#	covariate age^2			age squared as a simple covariate
#	covariate age^1,2		age and age^2
#	covariate age^1,2#sex		all combinations of age^1,2 and sex
#					  i.e., all of the following:
#				  	  age sex age:sex age^2 age^2:sex
#
#	covariate age^1,2,3:diabet	age^1,2,3 by diabetes interactions ONLY
#
#	All covariate beta parameters begin with 'b', followed by the
#	complete covariate name (including the exponent and interactor
#	variable name if applicable).
#
#	For example, covariate age:sex has a beta parameter named:
#
#	    bage:sex
#
#	Covariate age^2:diabetes has a beta parameter named:
#
#	    bage^2:diabetes.
#
#	This is made possible by the fact that variable names may now be up
#	to 18 characters long, and parameter names may now be up to 40
#	characters long.
#
#	If you enter 'covariate age#sex', there is one beta term (bage)
#	applied to males, and three (bsex, bage, and bage:sex) applied to
#	females.  'bage:sex' isn't a female age term, but rather the
#	age-related sex difference between males and females.
#
#	This new parameterization allows SOLAR to test variables and their
#	interactions separately, which is what will be done during
#	covariate screening.  'age' might be found to be significant while
#	its interaction with sex isn't, or vice versa.  Also, the squared
#	terms might drop out separately.  All tests use one degree of
#	freedom now.
#
#	If you by mistake enter something like:
#
#		covariate sex age^1,2#sex
#
#	the repetition of the sex covariate by itself is ignored silently.
#	This simplifies the entry of such more complex things as:
#
#		covariate age^1,2#sex weight^1,2#sex
#
#	Technically, sex by itself is being specified twice, but SOLAR
#	ignores that.
#	
#
#	
#
#			 Bayesian Model Averaging
#			 -------- ----- ---------
#
#	The bayesavg procedure performs a Baysian Oligogenic Model Averaging
#	analysis on the linkage components of the current model.
#
#	It tests each combination of the linkage components, and finds the
#	set of models within Occam's window based on their Bayesian
#	Information Criterion.  Then it computes weighted averages of the
#	linkage components based on their weighted average.  The summary
#	output files is named 'bayesavg.info,' while the final
#	weighted-average model is described by 'bayesavg.out.'
#
#
#		    Some Changed Commands and Arguments
#		    ---- ------- -------- --- ---------
#
#	Several commands and arguments have been changed to be more
#	consistent and intuitive.
#
#        'covariate <name> delete_fully' is now 'covariate delete <name>'
#	to be more consistent with the delete commands for parameters and
#	constraints.  The covariate suspend and restore commands are
#	similarly reordered, and any of these commands may list more than
#	one covariate to operate on.  If you happen to give the operator
#	after the variable name, that will be understood also, so long as
#	you are only operating on only one variable.
#
#		covariate delete sex         ;# this form is preferred now
#		covariate delete sex age ... ;# because you can have a list
#		covariate sex delete         ;# but this is still OK
#
#	All hyphenated arguments now have full readible names (such as
#	-restart and -overwrite).  The most common hyphenated arguments
#	also have shorter abbreviations (such as -r and -ov).  This
#	is the approach preferred by most Tcl programmers.  The confusing
#	"-f" arguments have been removed (replaced by -overwrite or -ov).
#
#	Some commands now have a few additional options.  Multipoint now
#	has -renew and -nullbase as options as well, clarifying some
#	special case behaviors explicitly.
#
#	Twopoint no longer requires a '-m' argument.  The default is to use
#	the previously stored null model, or the model currently in memory
#	if there is one.  This is similar to what is done by multipoint.
#
#	Polygenic now takes fully named arguments (such as -screen and
#	-fix) as well as the old abbreviated arguments (-s and -f).
#		
#
#			 Comma Delimited File Bugs
#			 ----- --------- ---- ----
#
#	A number of bugs have been fixed that might cause segmentation
#	violations under some circumstances.  The worst involved the
#	use of comma delimited files.  If you are using comma delimited
#	files (instead of PEDSYS files), the new version is strongly
#	recommended.
#
# Changes from Solar 0.9.16 to 0.9.100:
# 
# THIS IS A BIG CHANGE!  Please read carefully.
# 
# TRANSLAT is no longer needed!  SOLAR will now run directly from PEDSYS
# files or the Comma Delimited files.  At first, there are many changes you
# might need to learn about.  In the long run, these changes should make
# Solar much easier to use for everybody.
# 
# Here is an itemized summary of the changes.
# 
# 1) Do not run TRANSLAT or use phenfiles created by TRANSLAT.  Previously
#    created model files will need to have the 'phenfile load' command
#    removed.
# 
# 2) The LOAD PHENFILE command no longer exists.  It has been replaced by the
#    LOAD PHENOTYPES command, which should be used in conjunction with the
#    LOAD PEDIGREE command to load pedigree and phenotype information
#    separately:
#    
#        solar> load pedigree Pedfile
#        solar> load phenotypes Phenofile
# 
#     (Or, you can also use 'pedigree load' and 'phenotypes load.')
# 
#     Both Pedfile and Phenofile can now be Pedsys files.  They can also be
#     the same file, if that file contains both the pedigree and phenotype
#     information.  (They can also be comma delimited files: Solar
#     automatically figures out the file type.)  
# 
#     Once a load pedigree or load phenotype command has been done within a
#     particular working directory, it need not be done again, unless you
#     start working with a different pedigree or phenotype file.  Within a
#     particular directory, you will always default to the pedigree and
#     phenotypes files you used last.  The phenotypes file can contain all
#     the phenotypes; Solar lets you select which ones you want to use in
#     a particular analysis and excludes families when (and only when) that
#     is required.
# 
#     Since the phenotypes file can be the whole phenotype database, there is
#     no reason why it needs to be identified in models anymore.
# 
# 3)  The FIELD command lets you map the mnemonics of the fields in your
#     data files to the names that Solar requires in the Pedigree, Marker,
#     and Phenotype files (which can all now be PEDSYS or Comma Delimited
#     Files).
# 
#     Once you determine the field commands which will be required, it is
#     useful to put them in a .solar file in the working directory.
# 
#     By itself, the FIELD command lists the Fields that Solar expects:
# 
# 	solar> field
# 	ID:	 ID	    ;Individual Permanent ID
# 	FA:	 FA	    ;Father's Permanent ID
# 	MO:	 MO	    ;Mother's Permanent ID
# 	SEX:	 SEX	    ;Sex: M/F or 1/2
# 	PROBND:	 PROBND	    ;Proband Status (optional)
# 	MZTWIN:	 MZTWIN	    ;Monozygotic Twin Group (optional)
# 	FAMID:	 FAMID	    ;Family ID (optional)
# 
#     You can change the mapping using the FIELD command.  For example, if
#     your Twin field is called TWIN instead of MZTWIN, you can use the
#     following FIELD command:
# 
#         solar> field mztwin twin
# 
#     If your data has no probands or no twins, you should declare this with
#     commands like the following:
# 
# 	solar> field probnd -none
#         solar> field mztwin -none
# 
#     If you have a PROBND field, any value other than 0 (zero) or blank
#     makes the individual a proband.
# 
#     FAMID is only required if ID is not unique in the entire pedigree.
#     If there is no FAMID field, or field mapped to FAMID with a field
#     command, solar will assume it isn't necessary.
# 
# 
# 4)  Previous "fixed-width" pedigree and marker files can still be used if
#     PEDSYS code files are created to go along with them.
# 
#     Fields other than those listed above in the marker-data file are taken
#     to be genotypes, and the field names are taken to be the marker
#     names. Marker names and the order of markers must still agree with that
#     in the locus-info and map data files.
# 
# 5)  The names of the following files have been changed:
# 
#    old name        new name
#    -----------     ---------------
#    relate.in       mibdrel.in
#    prep.ped        mibdrel.ped
#    prep.cde        mibdrel.cde
#    chr<n>.ibd      mibdchr<n>.mrg
#    chr<n>.loc      mibdchr<n>.loc
#    chr<n>.mean     mibdchr<n>.mean
# 
#    Names of marker directories have been changed from  .d_<marker-name>
#    to  d_<marker-name> , i.e. the leading period has been dropped.
# 
# 6)  Bugs have been fixed in IBD calculations involving three or more
#     genetically identical individuals
# 
# 7)  Fields determined to be binary are marked in the description of
#     variables in the maximize command.
# 
# 8)  Error reporting is improved.
# 
# 9)  Many help messages have been improved.
# 
# 10) The AUTOMODEL and ALLCOVAR commands ignore most pedigree fields IF THEY
#     ARE CORRECTLY IDENTIFIED with field commands.  If this doesn't work as
#     intended, you may end up having some pedigree fields as covariates
#     which will cause Solar to behave badly.  You should check which
#     variables have been automatically selected as covariates.
# 
#     There is also a new command, EXCLUDE, which lets you specifically
#     exclude certain fields from being selected as covariates by automodel
#     or allcovar:
# 
# 	solar> exclude pedno groupno cseq
# 
# 11) AUTOMODEL no longer includes a LOADKIN command (which loaded a phi2.gz
#     file, if present).  If you are doing a special analysis which requires
#     the use of a hacked phi2.gz, you should give a MATRIX or LOADKIN command.
#     This is not the typical operation, and Fisher's built-in phi2 handling
#     is much faster than loading a matrix each time.
# 
# 12) TWOPOINT has been fixed to work with the way that twopoint files are
#     acutally compressed.
# 
# 13) .solar files are now read in only when Solar is started, not whenever
#     a model is loaded or created from scratch.  This means you can now load
#     models or do just about anything in a .solar file without causing an
#     infinite recursion.  You will also not have your session-specific changes
#     overwritten.  About the only negative impact might be to the 'model new'
#     command.  'model new' will take you back to a nearly empty model, lacking
#     any parameters you may have defined in your .solar file.  You will need
#     to load those back in yourself.  It would be best to define such things
#     as procedures in the .solar file so that they can be readily re-loaded.
# 
# -

proc change-notes {} {
    return [helpscript change-notes-data]
}

proc substar {sstr rchr} {
    if {-1 == [string first * $sstr]} {
	return $sstr
    }
    set indx [string first * $sstr]
    set beforei [expr $indx - 1]
    set afteri  [expr $indx + 1]
    return "[string range $sstr 0 $beforei]$rchr[string range $sstr $afteri end]"
}


# solar::power --
#
# Purpose:  Perform power calculations (or MathMatrix power operation)
#
# Usage:    power [-prev] [-h2t <h2t>] [-h2r <h2r>] [-data <fieldname>]
#                 [-grid {<from> <to> <incr>}] [-lod {<lod> ...}]
#                 [-freq <freq>] [-nreps <nreps>] [-seed <seed>]
#                 [-overwrite] [-plot]
#
#           power -restart [-grid {<from> <to> <incr>}]
#                 [-lod {<lod> ...}] [-nreps <nreps>] [-plot]
#
#           power <matrix> <integer-power>  ;# see help mathmatrix
#
#           This command performs a power calculation for the currently
#           loaded pedigree, with the following assumptions:
#
#               (1) the trait to be studied is either quantitative or
#                   dichotomous (e.g. affected/unaffected)
#
#               (2) the trait to be studied is influenced by a single
#                   bi-allelic QTL with, optionally, a residual additive
#                   genetic effect due to polygenes
#
#               (3) there will be fully informative marker genotype data
#                   available for all study subjects
#
#               (4) all study subjects will be phenotyped for the trait
#                   to be studied (unless the -data option is used to
#                   exclude those individuals who will not have phenotypic
#                   data; see the description of this option below)
#
#           Simulation is used to estimate the LOD score one would expect
#           to obtain for a QTL having a certain effect size (i.e. QTL
#           heritability).  The expected LOD is calculated for a range of
#           effect sizes.  The ELODs, in turn, are used to compute the power
#           to detect a QTL having these various effect sizes with, for
#           example, a LOD of 3.
#
#           The default is to perform 10 replicates of the simulation for
#           each effect size in the range .01, .02, .03, ..., .99.  For
#           each replicate, both a polygenic and a linkage model are fitted
#           to the simulated data and then compared.  The resulting QTL
#           heritability estimate and LOD score are recorded.  The observed
#           LODs are converted to power, i.e. the power to detect the
#           corresponding observed effect size with a specified LOD.
#
#           The following options give the user some control over the power
#           calculation procedure:
#
#               -prev     If the trait to be studied is dichotomous, SOLAR
#                         will assume the existence of an unobserved liability
#                         distribution. Individuals with liabilities above
#                         some threshold value will be "affected", i.e. they
#                         will have the larger of the two trait values (for
#                         example, a 1 for a 0/1 trait.) The -prev option
#                         is used to specify the "disease" prevalence, or
#                         fraction of individuals who are "affected", which
#                         in turn determines the liability threshold.
#
#               -grid     Specify the set of effect sizes for which ELODs
#                         will be computed. The grid is given by a set of
#                         three numbers enclosed in curly braces:
#
#                             {<from> <to> <incr>}
#
#                         where <from> is the starting effect size, <to>
#                         is the last effect size considered, and <incr>
#                         is the interval between grid points.  If the
#                         desired grid consists of a single effect size,
#                         the three-number list can be replaced by that
#                         single number and curly braces are not required.
#                         The default grid is from 0.05 through 0.5 by
#                         steps of 0.05.
#
#               -h2r      At each grid point, add a constant residual
#                         additive genetic heritability <h2r> to the
#                         QTL-specific heritability.
#
#               -h2t      Set the residual heritability so that the total
#                         heritability (QTL plus residual) is equal to a
#                         constant value <h2t>.
#
#               -data     Exclude individuals from the power calculation
#                         who are missing data for phenotype <fieldname>.
#
#               -lod      Specify the set of LODs for which power will be
#                         computed.  If more than one LOD is specified, the
#                         set of numbers must be enclosed in curly braces.
#                         The default set of LODs is { 3 2 }.  The order of
#                         the LODs is important since it is reflected in
#                         the output file power.out (see below).  The set
#                         of LODs can also be changed for a completed power
#                         calculation by using the -lod option in conjunction
#                         with the -restart option.
#
#               -freq     Specify the frequency of the first of the two
#                         alleles assumed to exist for the QTL.  The default
#                         allele frequency is 0.2113; this frequency results
#                         in the simulated trait having kurtosis = 0.
#
#               -nreps    Perform <nreps> simulations at each grid point.
#                         The default number of replicates is 100.
#
#               -seed     Set the random number generator seed.  The default
#                         is to set the seed based on the date and time.
#
#               -plot     At the end of the power calculations, display a
#                         plot of power versus QTL heritability.  To display
#                         this plot for a previously completed calculation,
#                         use the command "plot -power".
#
#               -overwrite (or -ov)  Overwrite the results of a previous
#                                    power calculation.
#
#               -restart (or -r)     Restart a power calculation.
#
#
# Notes:    It is possible to change the grid of effect sizes and the number
#           of replicates when restarting a calculation.  The calculation
#           will not be restarted if a grid is chosen that does not include
#           all the points in the previously specified grid unless the
#           -overwrite option is included, in which case the simulation
#           replicates for any extra grid points are discarded.  Similarly,
#           the -overwrite option is necessary if fewer replicates are
#           requested than were done previously, in which case any extra
#           replicates are discarded.  The set of LODs for which power
#           estimates are computed can also be changed in a restart.  The
#           other parameters, e.g. h2t, cannot be changed and are kept the
#           same as in the original run, with the exception of the seed for
#           the random number generator which is set based on the date and
#           time.
#
#           The plot of power versus QTL heritability is derived from a
#           smoothed estimate of the ELODs.  Smoothing is achieved with a
#           least-squares fit of a second degree polynomial to the ELODs as
#           a function of QTL heritability.  It is important to have a
#           sufficiently large number of replicates to produce a reasonable
#           curve fit.  The default of 100 replicates should suffice in most
#           cases.  To compute power as a function of the unsmoothed ELODs,
#           include the -nosmooth option.
#
#           The following files are created:
#
#               power.out    A space-delimited file containing a line for
#                            each grid point in the format X Y1 Y2 ..., which
#                            is suitable for input to plotting packages such
#                            as xmgr.  The first (or X) column contains the
#                            QTL heritability.  The succeeding columns hold
#                            the power estimates, each corresponding to a
#                            different LOD.  These columns are in the order
#                            given by the -lod option.
#
#               power.info   Stores the various options selected along with
#                            the ELODs, averaged over the replicates, at each
#                            grid point.
#
#               power.lods   Stores the results of the simulation replicates
#                            run at each grid point.  This file, along with
#                            power.info, is used to restart an interrupted
#                            power calculation.
#
#            During a power calculation, various files named "simqtl.*" are
#            created along with a trait directory named "simqt". These will
#            be removed at the end of the run.
#

proc power {args} {

    set eps 1e-7

    set data ""
    set h2t ""
    set h2r ""
    set grid ""
    set lod_list ""
    set h2q_from 0
    set h2q_to 0
    set h2q_incr .01
    set prev ""
    set freq ""
    set nreps ""
    set seed ""
    set overwrite 0
    set restart 0
    set plot 0
    set nosmooth 0
    set noavg 0

# jump to MathMatrix "power" if first arg is matrix
    if {[llength $args] >= 1 && 
	[string range [lindex $args 0] 0 3] == ".mm."} {
	return [eval matrixpower $args]
    }

    set badargs [ read_arglist $args -h2t h2t -h2r h2r -data data -grid grid \
                    -lod lod_list -freq freq -nreps nreps -seed seed \
                    -overwrite {set overwrite 1} -ov {set overwrite 1} \
                    -restart {set restart 1} -r {set restart 1} \
                    -plot {set plot 1 } -nosmooth {set nosmooth 1} \
                    -prev prev ]

    if {$badargs != ""} {
        error "Invalid power command: bad arguments: $badargs"
    }

    if {$restart} {
        if {[catch {set rstf [open power.info r]}]} {
            error \
            "Can't restart power calculations: file power.info not found"
        }

        if {$data != ""} {
            puts "The -data option is ignored on a restart."
            set data ""
        }
        if {$h2t != ""} {
            puts "The -h2t option is ignored on a restart."
            set h2t ""
        }
        if {$h2r != ""} {
            puts "The -h2r option is ignored on a restart."
            set h2r ""
        }
        if {$freq != ""} {
            puts "The -freq option is ignored on a restart."
            set freq ""
        }
        if {$seed != ""} {
            puts "The -seed option is ignored on a restart."
            set seed ""
        }

        set record [gets $rstf]
        if {[lindex $record 0] == "data"} {
            set data [lindex $record 2]
            set phenfile_name [lindex $record 8]
            set vnum [verbosity -number]
            verbosity min
            phenotypes load $phenfile_name
            verbosity $vnum
            set record [gets $rstf]
        }

        if {[lindex $record 0] == "nreps"} {
            set onreps [lindex $record 2]
            set discard_reps 0
            if {$nreps == ""} {
                set nreps $onreps
            }
        } else {
            error \
            "Can't restart power calculations: file power.info is corrupted"
        }

        set record [gets $rstf]
        if {[lindex $record 0] == "h2t"} {
            set h2t [lindex $record 2]
            set freq [lindex $record 5]
        } elseif {[lindex $record 0] == "h2r"} {
            set h2r [lindex $record 2]
            set freq [lindex $record 5]
        } else {
            error \
            "Can't restart power calculations: file power.info is corrupted"
        }
        if {[llength $record] == 9 && [lindex $record 6] == "prev"} {
            set prev [lindex $record 8]
        }

        set record [gets $rstf]
        if {[lindex $record 0] == "grid"} {
            set oh2q_from [lindex $record 2]
            set oh2q_to [lindex $record 3]
            set oh2q_incr [lindex $record 4]
            if {$oh2q_from == 0 && $oh2q_to == 0} {
                set oh2q_from .01
                set oh2q_to .99
                set oh2q_incr .01
                set noavg 1
            }
            if {$grid == ""} {
                set grid "$oh2q_from $oh2q_to $oh2q_incr"
            }
            if {$lod_list == ""} {
                set lod_list [lindex $record 7]
                for {set i 8} {$i < [llength $record]} {incr i} {
                    lappend lod_list [lindex $record $i]
                }
            } else {
                set record [lrange $record 0 6]
                for {set i 0} {$i < [llength $lod_list]} {incr i} {
                    lappend record [lindex $lod_list $i]
                }
            }
        } else {
            error \
            "Can't restart power calculations: file power.info is corrupted"
        }
        close $rstf

    } else {
        if {[file exists power.info] && !$overwrite} {
            error \
            "Power calculations have already been run. Use -overwrite option."
        }
    }

    if {$h2t != "" && $h2r != ""} {
        error \
        "You may specify a fixed total h2 or a fixed residual h2 but not both."
    }

    if {$h2t != "" && ([scan $h2t "%f" tmp] != 1 || $h2t < 0 || $h2t > 1)} {
        error "Total heritability (h2t) must be between 0 and 1."
    }

    if {$h2r != "" && ([scan $h2r "%f" tmp] != 1 || $h2r < 0 || $h2r > 1)} {
        error "Residual heritability (h2r) must be between 0 and 1."
    }

    if {$grid != ""} {
        if {[llength $grid] != 1 && [llength $grid] != 3} {
            error \
    "You must specify the h2q grid with a list {<from> <to> <incr>} or a single constant value."
        }

        set h2q_from [lindex $grid 0]
        if {[scan $h2q_from "%f" tmp] != 1 || $h2q_from < 0 || $h2q_from > 1} {
            error "Starting h2q in grid must be between 0 and 1."
        }

        if {[llength $grid] == 1} {
            set h2q_to $h2q_from
            set h2q_incr 1

        } else {
            set h2q_to [lindex $grid 1]
            if {[scan $h2q_to "%f" tmp] != 1 || $h2q_to < 0 || $h2q_to > 1} {
                error "Ending h2q in grid must be between 0 and 1."
            }
            if {$h2q_to < $h2q_from} {
                error "Ending h2q in grid cannot be less than the starting h2q."
            }

            set h2q_incr [lindex $grid 2]
            if {[scan $h2q_incr "%f" tmp] != 1 || $h2q_incr <= 0} {
                error "Increment in h2q grid must be greater than 0."
            }
        }

        if {$restart && !$overwrite} {
            set h2q $h2q_from
            set oh2q $oh2q_from
            set done 0
            while {!$done} {
                if {$oh2q < [expr $h2q - $eps]} {
                    error \
"The requested grid does not include all of the existing grid \{$oh2q_from $oh2q_to $oh2q_incr\}.
Add -overwrite if you really want to discard the extra grid points."
                } elseif {$oh2q > [expr $h2q_to + $eps]} {
                    error \
"The requested grid does not include all of the existing grid \{$oh2q_from $oh2q_to $oh2q_incr\}.
Add -overwrite if you really want to discard the extra grid points."
                } else {
                    set done 1
                    if {$oh2q > [expr $h2q - $eps] && $oh2q < [expr $h2q + $eps]} {
                        set oh2q [expr $oh2q + $oh2q_incr]
                        set done 0
                    }
                    if {$h2q < [expr $h2q_to - $eps]} {
                        set h2q [expr $h2q + $h2q_incr]
                        set done 0
                    }
                    if {$oh2q > [expr $oh2q_to + $eps]} {
                        set done 1
                    }
                }
            }
        }
    }

    if {$h2q_from == 0 && $h2q_to == 0} {
        set h2q_from .01
        set h2q_to .99
        set h2q_incr .01
        set noavg 1
        if {$nreps == ""} {
            set nreps 10
        }
    }

    if {$prev != ""} {
        if {[scan $prev "%f" tmp] != 1 || $prev <= 0 || $prev >= 1} {
            error "Prevalence must be greater than 0 and less than 1."
        }
    }

    if {$nreps == ""} {
        set nreps 100
    }

    if {[scan $nreps "%d" tmp] != 1 || $nreps <= 0} {
        error "Number of replicates must be a positive integer."
    }

    if {$seed == ""} {
        set seed 0
    }

    if {$h2t != "" && $h2q_to > $h2t} {
        set h2q_to $h2t
    }

    if {$h2r == ""} {
        set h2r 0
    }

    if {$freq == ""} {
        set freq 0.2113
    }

    if {[scan $freq "%f" tmp] != 1 || $freq < 0 || $freq > 1} {
        error "Allele frequency must be between 0 and 1."
    }

    if {$lod_list == ""} {
        set lod_list {3 2}
    }

    set crit ""
    for {set i 0} {$i < [llength $lod_list]} {incr i} {
        set lod [lindex $lod_list $i]
        if {[scan $lod "%f" tmp] != 1 || $lod <= 0} {
            error "LODs must be greater than 0."
        }
        set cval [expr 2*log(10)*$lod]
        lappend crit $cval
    }
    set ncrit [llength $crit]

    if {$restart && $nreps < $onreps && !$overwrite} {
        error \
"The requested number of replicates is less than the existing number ($onreps).
Add -overwrite if you really want to discard the extra replicates."
    }

    if {[scan $seed "%d" tmp] != 1 || $seed < 0} {
        error "Random number seed must be a non-negative integer."
    }

    set phenfile_name ""
    if {[catch {set infof [open phenotypes.info r]}]} {
        if {$data != ""} {
            error "A phenotypes file has not been loaded."
        }

    } else {
        gets $infof phenfile_name
        close $infof
        if {$data != ""} {
            if {$phenfile_name == "simqtl.phn"} {
                error \
"The currently loaded phenotypes file is named \"simqtl.phn\", which is a
filename reserved for use by the power command. You will have to rename
the phenotypes file and then reload it."
            }
            if {[catch {set phenfile [tablefile open $phenfile_name]}]} {
                error "Can't find phenotypes file $phenfile_name"
            }
            if {![tablefile $phenfile test_name $data]} {
                tablefile $phenfile close
                error "The phenotypes file does not contain a field named $data."
            }
            tablefile $phenfile start_setup
            tablefile $phenfile setup $data
            set navail 0
            while {"" != [set record [tablefile $phenfile get]]} {
                if {$record != "{}"} {
                    incr navail
                }
            }
            tablefile $phenfile close
        }
    }

    set outf [open power.info w]
    if {$data != ""} {
        puts $outf "data = $data  navail = $navail  phenf = $phenfile_name"
    }
    puts $outf "nreps = $nreps  seed = $seed"
    if {$h2t != ""} {
        puts -nonewline $outf "h2t = $h2t  freq = $freq"
    } else {
        puts -nonewline $outf "h2r = $h2r  freq = $freq"
    }
    if {$prev != ""} {
        puts -nonewline $outf "  prev = $prev"
    }
    puts $outf ""
    if {$noavg} {
        puts $outf "grid = 0 0 .01  lod = $lod_list"
    } else {
        puts $outf "grid = $h2q_from $h2q_to $h2q_incr  lod = $lod_list"
    }
    flush $outf

    drand $seed

    if {$restart} {
        exec mv power.lods power.lods.tmp
        set olodsf [open power.lods.tmp r]
        set oh2q $oh2q_from
    }

    set lodsf [open power.lods w]
    set h2q $h2q_from
    set nh2q 0
    set slod 0
    set sh2r 0
    set sh2q 0
    set rep1 0

    while {$h2q <= [expr $h2q_to + $eps]} {

        if {$restart && $oh2q < [expr $h2q - $eps]} {
            while {$rep1 < $onreps && {} != [set lodrec [gets $olodsf]]} {
                incr rep1
            }
            set rep1 0
            set oh2q [expr $oh2q + $oh2q_incr]
            continue

        } elseif {$restart && $oh2q > [expr $h2q - $eps] && \
                  $oh2q < [expr $h2q + $eps]} {
            while {$rep1 < $onreps && {} != [set lodrec [gets $olodsf]]} {
                if {$rep1 < $nreps} {
                    if {$noavg} {
                        set lods($nh2q) [lindex $lodrec 1]
                        set o_h2r($nh2q) [lindex $lodrec 2]
                        set o_h2q($nh2q) [lindex $lodrec 3]
                        puts $lodsf [format "%5d%12.6f%12.6f%12.6f" \
                                     [expr $nh2q + 1] [lindex $lodrec 1] \
                                     [lindex $lodrec 2] [lindex $lodrec 3]]
                        incr nh2q
                    } else {
                        set slod [expr $slod + [lindex $lodrec 1]]
                        set sh2r [expr $sh2r + [lindex $lodrec 2]]
                        set sh2q [expr $sh2q + [lindex $lodrec 3]]
                        puts $lodsf [format "%5d%12.6f%12.6f%12.6f" \
                                     [expr $rep1 + 1] [lindex $lodrec 1] \
                                     [lindex $lodrec 2] [lindex $lodrec 3]]
                    }
                }
                incr rep1
            }
            set th2q $h2q
            set nexth2q [expr $h2q + $h2q_incr]
            set nextoh2q [expr $oh2q + $oh2q_incr]

        } else {
            set th2q $h2q
            set nexth2q [expr $h2q + $h2q_incr]
            if {$restart} {
                set nextoh2q $oh2q
            }
        }

        if {$h2t != ""} {
            set h2r [expr $h2t - $th2q]
            if {$h2r < $eps} {
                set h2r 0
            }
        }

        set delta [expr 10*sqrt($th2q/(2*$freq*(1-$freq)*(1-$th2q)))]
        set means "[expr 100 - $delta] 100 [expr 100 + $delta]"
        set h2res [expr $h2r/(1 - $th2q)]

        if {$data != ""} {
            simqtl -freq $freq -mfreq {.5} -mean $means -sdev 10 -h2r $h2res \
                   -cov $data
        } else {
            simqtl -freq $freq -mfreq {.5} -mean $means -sdev 10 -h2r $h2res
        }

        for {set i $rep1} {$i < $nreps} {incr i} {
            if {$phenfile_name != ""} {
                phenotypes load $phenfile_name
            }
            simqtl -inform
            if {$prev != ""} {
                mkdisc $prev
            }
            phenotypes load simqtl.phn
            model new
            trait simqt

            polymod
            option standerr 0
            set errmsg [maximize_quietly last]
            if {$errmsg != ""} {
                if {[string compare [verbosity] "verbosity min"] != 0} {
                    puts "    *** Error maximizing, rep $i"
                }
                set i [expr $i - 1]
                continue
            }

            set plike [loglike]
            ibd -inform simqtl.mrk simqtl.ibd

            linkmod simqtl.ibd
            option standerr 0
            set errmsg [maximize_quietly last]
            if {$errmsg != ""} {
                if {[string compare [verbosity] "verbosity min"] != 0} {
                    puts "    *** Error maximizing, rep $i"
                }
                set i [expr $i - 1]
                continue
            }

            set lod [expr ([loglike] - $plike) / 2.302585]
            if {[string compare [verbosity] "verbosity min"] != 0} {
                if {$noavg} {
                    puts [format "%5d%12.6f%12.6f%12.6f" [expr $nh2q + 1] $lod \
                         [parameter h2r =] [parameter h2q1 =]]
                } else {
                    puts [format "%5d%12.6f%12.6f%12.6f" [expr $i + 1] $lod \
                         [parameter h2r =] [parameter h2q1 =]]
                }
            }
            if {$noavg} {
                puts $lodsf \
                     [format "%5d%12.6f%12.6f%12.6f" [expr $nh2q + 1] $lod \
                     [parameter h2r =] [parameter h2q1 =]]
            } else {
                puts $lodsf \
                     [format "%5d%12.6f%12.6f%12.6f" [expr $i + 1] $lod \
                     [parameter h2r =] [parameter h2q1 =]]
            }
            flush $lodsf

            if {$noavg} {
                set lods($nh2q) $lod
                set o_h2r($nh2q) [parameter h2r =]
                set o_h2q($nh2q) [parameter h2q1 =]
                incr nh2q
            } else {
                set slod [expr $slod + $lod]
                set sh2r [expr $sh2r + [parameter h2r =]]
                set sh2q [expr $sh2q + [parameter h2q1 =]]
            }
        }

        if {!$noavg} {
            set lods($nh2q) [expr $slod/$nreps]
            set o_h2q($nh2q) [expr $sh2q/$nreps]
            puts \
                "h2q = [format %g $th2q]  h2r = [format %g $h2r]  ELOD = [ \
                format %.6g $lods($nh2q)]"
            puts $outf \
                "h2q = [format %g $th2q]  h2r = [format %g $h2r]  ELOD = [ \
                format %.6g $lods($nh2q)]"
            flush $outf
            incr nh2q
            set slod 0
            set sh2r 0
            set sh2q 0
        }

        if {$restart} {
            set oh2q $nextoh2q
        }
        set h2q $nexth2q
        set rep1 0
    }

    if {$restart} {
        close $olodsf
        file delete power.lods.tmp
    }

    close $lodsf
    close $outf

    if {$nosmooth} {
        set outf [open "|[usort] -nu > power.out" w]
    } else {
        set xx 0
        set xy 0
    }

    set h2q $h2q_from
    set nh2q 0
    set last_h2q [expr $h2q_to + $eps]
    while {$h2q <= $last_h2q} {
        if {$noavg} {
            for {set i 0} {$i < $nreps} {incr i} {
                set lod $lods($nh2q)
                if {$lod < 0} {
                    set lod 0
                }
                set chi [expr 2*log(10)*$lod]
                if {$nosmooth} {
                    puts -nonewline $outf [format %.4g $o_h2q($nh2q)]
                    for {set j 0} {$j < $ncrit} {incr j} {
                        puts -nonewline $outf " [format %.6g \
                             [expr 1 - [chinc [lindex $crit $j] 1 $chi]]]"
                    }
                    puts $outf ""
                } elseif {$chi < 33.3} {
                    set xx [expr $xx + pow($o_h2q($nh2q),4)]
                    set xy [expr $xy + pow($o_h2q($nh2q),2)*$chi]
                }
                incr nh2q
            }
        } else {
            set lod $lods($nh2q)
            if {$lod < 0} {
                set lod 0
            }
            set chi [expr 2*log(10)*$lod]
            if {$nosmooth} {
                puts -nonewline $outf [format %.4g $h2q]
                for {set j 0} {$j < $ncrit} {incr j} {
                    puts -nonewline $outf " [format %.6g \
                         [expr 1 - [chinc [lindex $crit $j] 1 $chi]]]"
                }
                puts $outf ""
            } elseif {$chi < 33.3} {
                set xx [expr $xx + pow($o_h2q($nh2q),4)]
                set xy [expr $xy + pow($o_h2q($nh2q),2)*$chi]
            }
            incr nh2q
        }
        set h2q [expr $h2q + $h2q_incr]
    }

    if {!$nosmooth} {
        set outf [open power.out w]
        set last [expr int(100*$h2q_to)]
        for {set i 0} {$i < $last} {incr i} {
            set chi [expr pow($i*.01,2)*$xy/$xx]
            puts -nonewline $outf [format %.4g [expr $i*.01]]
            for {set j 0} {$j < $ncrit} {incr j} {
                puts -nonewline $outf " [format %.6g \
                     [expr 1 - [chinc [lindex $crit $j] 1 $chi]]]"
            }
            puts $outf ""
        }
    }
    close $outf

    if {$plot} {
        plot_power
    }

    exec rm -rf simqt
    delete_files_forcibly \
        simqtl.qtl simqtl.mrk simqtl.ibd.gz simqtl.par simqtl.dat

    if {$phenfile_name != ""} {
        delete_files_forcibly simqtl.phn
        set vnum [verbosity -number]
        verbosity min
        catch {phenotypes load $phenfile_name}
        verbosity $vnum
    }
# the magic LOD = 4.512385961872635
}


# solar::chinc --
#
# Purpose:  Compute probability for a noncentral chi-square value
# 
# Usage:    chinc <value> <df> <lambda>
#

proc chinc {value df lambda} {

    if {[scan $value "%f" tmp] != 1 || $value < 0} {
        error "The chi-square value must be >= 0."
    }
    if {[scan $df "%f" tmp] != 1 || $df <= 0} {
        error "The degrees of freedom must be > 0."
    }
    if {[scan $lambda "%f" tmp] != 1 || $lambda < 0} {
        error "The noncentrality parameter must be >= 0."
    }

    set g [expr double($lambda)/2]
    if {[catch {set r [expr exp(-$g)]}]} {
        return 1e-307
    }
    if {[catch {set tol [expr 1e-8/$r]}]} {
        return 1e-307
    }
    if {$tol > 0.5} {
        set tol 0.5
    }

    set termj [expr 1 - [chi -number $value $df]]
    set tot $termj
    set j 1
    while {1} {
        set prodc 1
        for {set k 1} {$k <= $j} {incr k} {
            set prodc [expr $prodc*$g/$k]
        }
        set termj [expr $prodc*(1 - [chi -number $value [expr $df+2*$j]])]
        set tot [expr $tot + $termj]
        incr j
        if {$termj < $tol} {
            break
        }
    }

    set p [expr $r*$tot]
    if {$p >= 0.00001} {
        set p [format %-9.7f $p]
    } elseif {$p > 1e-307} {
        set p [format %.8g $p]
    } else {
        set p 1e-307
    }

    return $p
}


# solar::qtld
#
# Purpose:  Association analysis for snps
#
# Usage:    qtld
#
# Notes:    Current model is used as starting point.  It is saved in output
#           directory as qtld.start.mod with standard errors turned off.
#
#           Snp association phenotypes are prefixed by b_ w_ b2_ and w2_ and
#           are taken from the currently loaded phenotypes files.  If there
#           is one matching phenotype, the other 3 are expected, and it is an
#           error if any are missing.
#
#           Main output is written to terminal and file qtld.out in the
#           output directory.  An additional file with detailed measured
#           genotype information is written to mgeno.out in output directory.
#           These output files are fixed column size and space delimited.
#
#           The output fields in the mgeno.out file (same as in the terminal
#           output) are:
#
#               Trait, SNP, Stratification, Measured Genotype, QTDT, QTLD
#
#           The output fields in the mgeno.out file (also listed at the top
#           of the file) are:
#
#                Trait, SNP, p(mg), h2m, muAA, se(muAA), muAB, se(muAB),
#                    muBB, se(muBB)
#
# -


proc qtld {args} {


# Set defaults and read arguments

    set prefixes {b_ w_ b2_ w2_}

    set mgoname mgeno.out
    set oname qtld.out
    set badargs [read_arglist $args  -prefix prefixes  -o oname  ]

    set deferr 0
    option standerr $deferr
    save model [full_filename qtld.start]
    file delete [full_filename $oname]
    file delete [full_filename $mgoname]

# Scan through phenotypes looking for standard prefixes

    set phens [string tolower [lrange [phenotypes] 1 end]]
    set snplist {}
    foreach phen $phens {
	foreach prefix $prefixes {
	    set plen [string length $prefix]
	    if {[string_imatch $prefix  [string range $phen 0 [expr $plen - 1]]]} {
		setappend snplist [string range $phen $plen end]
	    }
	}
    }

# Make sure all 4 variants are present for each label

    if {{} == $snplist} {
	error "qtld: No phenotypes match standard prefixes: $prefixes"
    }
    foreach snp $snplist {
	foreach pre $prefixes {
	    if {-1 == [lsearch -exact $phens [catenate $pre $snp]]} {
		error "qtld: Missing snip data: [catenate $pre $snp]"
	    }
	}
    }

    puts "---------------------------------------------------------------------"
    puts "                                           P-values"
    puts "                          -------------------------------------------"
    puts "                          Strati-    Measured             "
    puts "   Trait         SNP      fication   Genotype     QTDT       QTLD"
    puts "---------------------------------------------------------------------"

    putsout -q $oname "    Trait          SNP   Stratifcn  MGenotype     QTDT       QTLD"
    putsout -q $mgoname  "    Trait        SNP       p(mg)      h2m         muAA       se(muAA)      muAB       se(muAB)      muBB       se(muBB)"


# Cycle through all the snps


    for {set i 0} {$i < [llength $snplist]} {incr i} {
	set snp [lindex $snplist $i]

	load model [full_filename qtld.start]

	set errmes ""
	set alldone 0
	catch {
#
# Null model
#
	covariate b_$snp w_$snp
	noscale b_$snp
	noscale w_$snp
	constraint <bb_$snp> = 0
	constraint <bw_$snp> = 0
	maximize  -quiet -o [catenate $snp .null.test]
	set like1 [loglike]
	set sdmgeno0 [parameter SD =]
	save model [full_filename [catenate $snp .null]]

	constraint delete <bb_$snp>
	constraint delete <bw_$snp>
	maximize  -quiet -o [catenate $snp .qtdt.test]
	set like2 [loglike]
	save model [full_filename [catenate $snp .qtdt]]
#
# Measured genotype model
#
	parameter bw_$snp = [parameter bb_$snp =]
	constraint <bw_$snp> = <bb_$snp>
	option standerr 1
	maximize  -quiet -o [catenate $snp .mgeno.test]
	option standerr $deferr

	set like3 [loglike]
	set sdmgeno1 [parameter SD =]
	set mumg [parameter mean =]
	set bmg [parameter bb_$snp =]
	set semumg [parameter mean SE]
	set sebmg [parameter bb_$snp SE]

	save model [full_filename [catenate $snp .mgeno]]

	constraint delete <bw_$snp> = <bb_$snp>
	constraint <bw_$snp> = 0
	maximize  -quiet -o [catenate $snp .qtdtnull.test]
	set like4 [loglike]
	save model [full_filename [catenate $snp .qtdtnull]]
	
	load model [full_filename qtld.start]
	
	covariate b2_$snp w2_$snp
	noscale b2_$snp
	noscale w2_$snp
	constraint <bw2_$snp> = 0
	maximize  -quiet -o [catenate $snp .qtldnull.test]
	set like5a [loglike]
	save model [full_filename [catenate $snp .qtldnull]]

	constraint delete <bw2_$snp>
	maximize -quiet -o [catenate $snp .qtld.test]
	set like5b [loglike]
	save model [full_filename [catenate $snp .qtld]]

	set chi2strat [highest [expr 2*($like2 - $like3)] 0]
	set chi2mg [highest [expr 2*($like3 - $like1)] 0]
	set chi2qtdt [highest [expr 2*($like2 - $like4)] 0]
	set chi2qtld [expr 2*($like5b - $like5a)]

	putsout $oname [fformat "%9s %12s  %10.7y %10.7y %10.7y %10.7y" [trait]  $snp  [chi -number $chi2strat 1]  [chi -number $chi2mg 1]  [chi -number $chi2qtdt 1]  [chi -number $chi2qtld 1]]

	set alldone 1
#
# Calculate additional statistics
#
	set mgr_valid 0
	catch {
	    set mgr [getcor mean bb_$snp [full_filename [catenate $snp .mgeno.test.out]]]
	    set mgr_valid 1
	}
#
# p(mg)
#
	set gformat %11.5g
	set gthresh 0.001

	set pmg [chi -number $chi2mg 1]
	set fpmg [format %10.9f $pmg]
	if {$fpmg < $gthresh} {
	    set fpmg [format $gformat $pmg]
	}
#
# h2m
#
	if {0!=$sdmgeno0} {
	    set h2m [expr 1.0-(($sdmgeno1 * $sdmgeno1)/($sdmgeno0*$sdmgeno0))]
	    if {$h2m < 0} {
		set h2m 0
	    }
	    set fh2m [format %10.9f $h2m]
	    if {$fh2m < 0.00002} {
		set fh2m [format $gformat $h2m]
	    }
	} else {
# This should be impossible because it should only happen if first model
# fails to maximize, in which case we end up at "unrecoverable" output
# at bottom and not here.
	    set fh2m "   NaN    "
	}
#
# muAA, se(muAA)
#
	set muaa [expr $mumg - $bmg]
	set semuaas 0
	if {$sebmg>0 && $semumg>0 && $mgr_valid} {
	    set semuaas [expr $semumg*$semumg + $sebmg*$sebmg - (2.0*$mgr*$semumg*$sebmg)]
	}
	if {$semuaas < 0} {
	    set semuaas 0
	}
	set semuaa [expr sqrt ($semuaas)]
	set fsemuaa [fformat %11.6y $semuaa]
#
# muAB, se(muAB)
#
	set muab $mumg
	set semuabs 0
	if {$semumg>0} {
	    set semuabs [expr $semumg*$semumg]
	}
	set semuab [expr sqrt ($semuabs)]
	set fsemuab [fformat %11.6y $semuab]
#
# muBB, se(muBB)
#
	set mubb [expr $mumg + $bmg]
	set semubbs 0
	if {$sebmg>0 && $semumg>0 && $mgr_valid} {
	    set semubbs [expr $semumg*$semumg + $sebmg*$sebmg + (2.0*$mgr*$semumg*$sebmg)]
	}
	if {$semubbs < 0} {
	    set semubbs 0
	}
	set semubb [expr sqrt ($semubbs)]
	set fsemubb [fformat %11.6y $semubb]
#
# Write to auxiliary file
#
	putsout -q $mgoname [fformat "%9s %12s %10.6y %10.6y %12.6y %11s %12.6y %11s %12.6y %11s"  [trait] $snp $pmg $h2m $muaa $fsemuaa $muab $fsemuab $mubb $fsemubb]
	
	set alldone 2
    } errmes ;# end catch
#
# Output for unrecoverable errors
#
    if {$alldone != 2} {
	if {$alldone == 1} {
	    puts "Unable to compute means for $snp: $errmes"
	} else {
	    puts "Unable to compute anything for $snp: $errmes"
	}
	if {$alldone == 0} {
	    putsout $oname [fformat "%9s %12s" [trait] $snp]
	}
	putsout -q $mgoname [fformat "%9s %12s" [trait] $snp]
    }
    }
    return " "

}


# solar::getcor -- private
#
# Purpose: get Asymptotic Correlation of Parameters from SOLAR output file
#
# Usage:  getcor <parameter1> <parameter2> <filename>
# -

proc getcor {par1 par2 outfilename} {
    if {![file exists $outfilename]} {
	error "getcor: $outfilename not found to get correlation"
    }

    global errorInfo errorCode
    set catchcode [catch {
    set ofile [open $outfilename r]
    while {-1 != [gets $ofile line]} {
	set line [string tolower $line]
	if {-1 != [string first "asymptotic correlation matrix" $line]} {
	    gets $ofile line  ;# skip over following blank line
	    set pos1 0
	    set pos2 0
	    set cpos 0
	    while {-1 != [gets $ofile line]} {
		set line [string tolower $line]
		if {"" == $line} {
		    if {$pos1 == 0} {
			error "getcor: Unable to find correlation for $par1 in $outfilename"
		    } else {
			error "getcor: Unable to find correlation for $par2 in $outfilename"
		    }
		}
		foreach pname $line {
		    incr cpos
		    if {[string_imatch $pname $par1]} {
			set pos1 $cpos
		    }
		    if {[string_imatch $pname $par2]} {
			set pos2 $cpos
		    }
		}
		if {$pos1 && $pos2} {
		    break
		}
	    }
	    if {!$pos1 || !$pos2} {
		error "getcor:  correlation for $par1 and $par2 not found in $outfilename"
	    }
#
# Put larger index first because only half matrix is printed and we scroll
# down by first index
#
	    if {$pos1 < $pos2} {
		set ptemp $pos1
		set pos1 $pos2
		set pos2 $ptemp
	    }
#
# Now skip down to first line for par1
# Each data "line" (which could be several lines) is preceded by a blank line
# So, to skip over $pos1 blank lines to get to first line for par1
#
#	    puts "pos1: $pos1    pos2: $pos2"
	    set blank_lines 0
	    while {-1 != [gets $ofile line]} {
		if {"" == $line} {
		    incr blank_lines
		    if {$pos1 <= $blank_lines} {
			break
		    }
		}
	    }
#
# Now scan through data line(s) to position par2
#
	    set cpos 0
	    set r ""
	    while {-1 != [gets $ofile line]} {
		foreach par $line {
		    incr cpos
		    if {$cpos == $pos2} {
			set r $par
			break
		    }
		}
		if {"" != $r} {break}
	    }
	    if {"" != $r} {
#
# Change D exponent to E
#		
		set r [string tolower $r]
		set c [regsub d $r e newr]
		if {$c} {
		    set r $newr
		}
		return $r
	    } else {
		error "getcor: correlation for $par1 and $par2 not found in $outfilename"
	    }
	}
    }
    error "getcor: Correlation for $par1 and $par2 not found in $outfilename"
    } catchvalue]
    close $ofile
    set returncode 0
    if {$catchcode == 1} {
	set returncode 1
    }
    return -code $returncode -errorinfo $errorInfo -errorcode $errorCode \
	$catchvalue
}


# solar::mgassoc
# solar::mga
#
# Purpose:  Run Measured Genotype (MG) association analysis for every SNP
#
# Usage:    mga [-files [<gcovfile>]+ ] [-snps <snp-tcl-list>]
#               [-out <outfile>] [-snplists [<snplistfile>]+]
#               [-format csv | pedsys | fortran]  [-noevd] [-notsame]
#               [-saveall] [-slowse] [-evdse]
#               [-fixupper <boundary>] [-fixlower <boundary>]
#               [-ixsnp <SNP>]
#
# SPECIAL NOTE: no filenames or snp names should begin with hyphen (-)
#               SNPS should be specified by their actual names, but the
#               corresponding file variables should be prefixed by snp_
#               The snp_ prefix is added automatically by SOLAR command
#               snp covar.
#
# Before invoking mga, user should set up trait and covariate(s) for
# the null model.  It is not necessary to maximize the model prior to running
# mga.  You may choose to set up the modeltype and omega, but it is not
# necessary.  The omega will default to polygenic and the modeltype will be
# set to evd (evd1).
#
# When default evd1 models are used, mga can automatically detect when the
# sample has changed, and run new null models as needed.  This may be the
# fastest approach in most cases.
#
# To use evd2 instead, specify "option modeltype evd2" just before running mga.
# For evd2, both -evdse and -slowse produce the same fast evd2 standard
# errors.  Evd2 may be faster for individual pedigrees larger than 3000.
#
# At mga completion, mga_null will be the last null model created.
# If you wish to save more information, use the -saveall option
#(described below) to save all maximization output files.
#
# -ixsnp   Do an interaction test with the specified SNP and each one of
#          snps in the <snp-tcl-list> or <snplistfile>.  Additional
#          terms for chi's and p values related to the interaction will be
#          included in the output file.  DO NOT include this snp in the
#          starting model, it will be added as needed.
#
# -noevd   Do not use EVD fast maximization or sample checking.  Normally
#          this is not needed because mga tests whether EVD can be
#          used and changes to -noevd model when EVD cannot be run.  However,
#          sometimes EVD uses too much memory, so this can help.*
#
# -notsame  Samples not same, therefore always run models for each SNP!
#   Normally this is not needed because mga tests the sample automatically.
#
# If "option samplesametrustme 1" is given prior to running mga,
#   mga will only run one null model for the first sample, unless
#   EVD is used and fast checking is available.
#
#  By default (starting with version 7.4.2) standard errors are computed
#  using the formula sqrt(beta^2/chi^2).  The -slowse and -evdse options
#  are now considered obsolescent.  If standard errors cannot be computed
#  because chi is zero, the value 10e20 (10^20) will be returned.
#
# -slowse Estimate standard errors, not using EVD
# -evdse  Estimate standard errors, using EVD if possible
#
#  The -slowse option will disable EVD because standard errors computed when
#  using EVD in the current EVD1 implementation are sometimes inaccurate.
#
#  The -evdse option will compute standard errors and use EVD if
#  possible, which will be faster if EVD can be used.  A warning will be
#  given about possible SE inaccuracy.
#
# If no -snps or -snplists arguments are given, mga will process
# all snp_ prefixed covariates in the currently loaded phenotypes files.
#
# <gcovfile> is one or more snp.genocov file generated by 'snp covar' command.
# These files are scanned for snp_ prefixed covariates in addition to the
# initially loaded phenotypes files.
#
# <snp-tcl-list> is a tcl-list of snps.  If specified, the -snps list
# supercedes -snplists.
#
# <snplistfile> is a file listing the snps to be processed.  Each snp
# is listed on a separate line with no blanks or other characters
# The snp_ prefix is not required, but allowed. 
#
# Currently, no checking is done to see if any listed snp is found or
# not found, or duplicated in subsequent files.
#
# <outfile> defaults to mga.out in the current output directory, which is
# cleared out first, then one line of information is produced for each SNP.
# If another filename is specified, it is located relative to the output
# directory or full absolute pathname, and it is not erased first to permit
# accumulation.  Note: if the "mgassoc" (original) command name is used, the
# output file is named mgassoc.out by default.
#
# -debug      Print extra messages useful in debugging, including all
#             maximization output
#
# -format     csv, fortran, pedsys.  csv is default.  fortran
#             and pedsys formats are identical space delimited except
#             that fortran version has a header line at top, pedsys version
#             writes a code file (.cde).  Actually, both fortran and
#             pedsys options write the code file in case you need it later.
#
#             If you have written pedsys format, it can be converted to
#             comma delimited with the command ped2csv.  If you have written
#             a comma delimited format file, you can convert it to pedsys
#             format with the command mg_topedsys.
#
# -saveall    save all solar.out output files.  Each output file will be named
#             mga_<snpname>.out.  Null model output files will be named
#             mga_null_<snpname>.out based on the first <snpname> they
#             were created for (null will be re-used if sample unchanged).
#
# -fixlower   fix snp beta parameter lower boundaries to this value
# -fixupper   fix snp beta parameter upper boundaries to this value
#
# Notes:
#
#  The genotype covariates are numeric variables giving the observed
# (or imputed) number of rare alleles an individual has at a particular SNP.
#
#  These fields must be named'snp_<snp>' where <snp> is the SNP name. 
#  Ex: snp_rs12345
#
#  *EVD is not currently available for multivariate or discrete models.
#  However, discrete traits may be handled as quantitative (at some loss in
#  accuracy) and therefore used with EVD by using either
#  "option enablediscrete 0" or (generally preferred, to increase SD):
#  define qtrait = dtrait * 5
#
# -

proc mga {args} {
    return [eval mgassoc -mga $args]
}

proc mgassoc {args} {

    global SOLAR_mga_last_out
    global SOLAR_mga_header
    set SOLAR_mga_last_out ""
    set SOLAR_mga_header ""

    set fixupper ""
    set fixlower ""

    set ixsnp ""
    set mgacommand 0
    set nose 1  ;# this now means cse
    set seevd 0
    set notsame 0
    set null_made 0
    set ofile ""
    set lphenotypes [phenotypes -files]
    set nlphenotypes [llength $lphenotypes]
    set xphenotypes {}
    set snplists {}
    set nocheck 0
    set debug 0
    set debug0 0
    set written 0
    set quiet "-q"
    set quietsub 0
    set snptlist ""
    set moreargs $args
    set snpwide 12
    set format csv
    set noevd 0
    set samplesame 0
    set saveall 0
    set samplesametrustme 0
    if {[option samplesametrustme] == 1} {
	set samplesametrustme 1
    }
    set traits [trait]
    set ntraits [llength $traits]
#
# Process -file argument

    set fpos [lsearch $args "-files"]
    if {$fpos == -1} {
	set fpos [lsearch $args "-file"]
    }
    if {$fpos > -1} {
	set moreargs [remove_from_list_by_pos $args $fpos]
	set testchar [string index [lindex $moreargs $fpos] 0]
	while {"-" != $testchar} {
	    set thisarg [lindex $moreargs $fpos]
	    if {"" == $thisarg} {break}
	    lappend xphenotypes $thisarg
	    set moreargs [remove_from_list_by_pos $moreargs $fpos]
	    set testchar [string index [lindex $moreargs $fpos] 0]
	}
	if {$xphenotypes == ""} {
	    error "-files argument without phenotype files"
	}
    }
    ifdebug0 puts "-files are $xphenotypes"
    set spos [lsearch $moreargs "-snplists"]
    if {$spos > -1} {
	set moreargs [remove_from_list_by_pos $moreargs $spos]
	while {"-" != [string index [lindex $moreargs $spos] 0]} {
	    set thisarg [lindex $moreargs $spos]
	    if {"" == $thisarg} {break}
	    lappend snplists $thisarg
	    set moreargs [remove_from_list_by_pos $moreargs $spos]
	}
	if {$snplists == ""} {
	    error "-snplists argument without snpfiles"
	}
    }
    ifdebug0 puts "-snplists listfiles are $snplists"
    set badargs [read_arglist $moreargs \
		     -slowse {set nose 0} \
		     -evdse {set seevd 1} \
		     -notsame {set notsame 1} \
		     -nocheck {set nocheck 1} \
		     -debug {set debug 1} \
		     -debug0 {set debug0 1} \
		     -snps snptlist \
		     -noevd {set noevd 1} \
		     -out ofile \
		     -format format \
		     -saveall {set saveall 1} \
		     -mga {set mgacommand 1} \
                     -fixupper fixupper \
		     -fixlower fixlower \
		     -ixsnp ixsnp \
                     -q {set quietsub 1} \
		     ]
    if {!$nose && $seevd} {
	error "Incompatible -slowse and -evdse options"
    }
    if {$noevd && $seevd} {
	error "Incompatible -noevd and -evdse options"
    }
    if {!$nose} {
	set noevd 1
    }
    if {$seevd} {
	set nose 0
    }
    if {[option modeltype] == "evd2"} {
	set noevd 1
	if {!$nose || $seevd} {
	    set nose 0
	}
    }

    if {$noevd} {
	if {[option samplesametrustme] == 1 && !$notsame} {
	    puts "    ** Warning: option samplesametrustme specifed by user and -noevd"
	    puts "    ** Null model will be reused without verification"
	    puts "    ** Sample differences could cause erroneous p values"
	}
    }

    if {"/" != [string index $ofile 0]} {
	if {"" == $ofile} {
	    if {$mgacommand} {
		set ofile mga.out
	    } else {
		set ofile mgassoc.out
	    }
	    file delete -force [full_filename $ofile]
	}
	set ofile [full_filename $ofile]
	puts "    ** Output file is now $ofile"
    } else {
	puts "    ** User specified absolute output filename: $ofile"
    }


    if {$badargs != ""} {
	error "bad argument(s) $badargs"
    }
    if {$debug} {
	set quiet ""
	set debug0 1
    }
    if {$format != "csv" && $format != "pedsys" && $format != "fortran"} {
	error "-format must be csv, fortran, or pedsys"
    }
#
# Make list of user-listed snps
#
    set allsnps ""
    set snpfilter 0
    if {$snplists != ""} {
	foreach snpf $snplists {
	    set snpfilter 1
	    set allsnps [concat $allsnps [snplistfile $snpf]]
	}
    }
    if {$snptlist != ""} {
	set snpfilter 1
	set allsnps $snptlist
    }
#
# Build list of lists of snps found in each file
#
    global snpsinfile
    set snpsinfile {}
    set allphenf [concat $lphenotypes $xphenotypes]
    ifdebug0 puts "Phen files are $allphenf"
    set snpshere {}
    foreach phenf $allphenf {
	set tfile [tablefile open $phenf]
	set names [tablefile $tfile names]
	foreach nam $names {
	    if {"snp_" == [string range $nam 0 3]} {
		set testsnp [string range $nam 4 end]
		set pos -1
		if {!$snpfilter || -1 != [set pos [lsearch $allsnps \
							   $testsnp]]} {
		    lappend snpshere $testsnp
		    if {[string length $testsnp] > $snpwide} {
			set snpwide [string length $testsnp]
		    }
		}
	    }
	}
	lappend snpsinfile $snpshere
	tablefile $tfile close
    }
    ifdebug0 puts "snps in each file:\n$snpsinfile"
#
# setup starting model (not maximized yet until we get first snp)
#
    if {$ixsnp != ""} {
	covariate snp_$ixsnp
    }
    option dontallowsamplechange 1
    if {$nose} {
	option standerr 0
    } else {
	option standerr 1
	if {!$noevd} {
	    puts "  ** Warning.  EVD1 Standard Errors are sometimes inaccurate"
	}
    }
    set dontallow 0
    if {!$noevd} {
	option modeltype evd
	set dontallow 1
# memory need reduced, mergepeds no longer needed mostly
#	option mergeallpeds 1
    }
    if {$notsame} {
	set dontallow 0
    }
    save model [full_filename mga_start]
    set nulloutname mga_null
    set nullsnps {}
    set covs [covariate]
    foreach cov $covs {
	if {[string range $cov 0 3] == "snp_"} {
	    puts "snp $cov already included in null model"
	    lappend nullsnps [string range $cov 4 end]
	}
    }
#
# For each phen file
#   load the file (if not already loaded
#   first nlphenotypes are loaded
#
    set headerneeded [expr !([file exists $ofile])]
    set tphenfiles [llength $allphenf]
    set tested_model_type 0
    ifdebug0 puts "total files: $tphenfiles"
    for {set iphen 0} {$iphen < $tphenfiles} {incr iphen} {
	if {$iphen+1 > $nlphenotypes} {
	    eval load phenotypes $lphenotypes [lindex $xphenotypes \
				   [expr $iphen - $nlphenotypes]]
	}
	set snpshere [lindex $snpsinfile $iphen]
	if {{} != $snpshere || $debug0} {
	    puts "    ** Evaluating SNPs found in [lindex $allphenf $iphen]..."
	}
	foreach snp $snpshere {
#           puts "SNP is $snp"
	    if {$snp == $ixsnp} {
		puts "skipping $snp X $ixsnp because they are the same"
		continue
	    }
	    if {[lsearch -exact $nullsnps $snp] != -1} {
		continue
	    }
	    set done 0
	    if {$format != "csv"} {
		mga_codefile $ofile $snpwide $ntraits
	    }
#
# This is the main null vs test maximization section.
# If all samples the same, we run null once.
# If -notsame specified, we run null for each covariate.
# If we discover sample error after skipping null, we have to go back and
# do null.  That's why this section is in an infinite loop, which breaks
# after both test and required null are done.
#
	    while {1} {
		set ifwho ""
		if {$noevd && !$notsame} {
		    set ifwho -runwho
		}
		set used_previous_null 1
		if {$notsame || !$null_made} {

# make the null model (first time or again)

		    load model [full_filename mga_start]
		    covariate snp_$snp\(\)
		    set null_snp $snp
		    if {$saveall} {
			set nulloutname mga_null_$snp
		    }
		    puts "   maximizing null for $snp..."
		    set errmsg ""
		    if {[catch {eval maximize $quiet -out $nulloutname $ifwho} \
			     errmsg]} {
			if {-1 != [string first "Undefined omega." $errmsg]} {
			    puts "Defaulting to polygenic model type"
			    load model [full_filename mga_start]
			    polymod
			    save model [full_filename mga_start]
			    continue
			} elseif {-1 != [string first "Error setting constraint sd" $errmsg]} {
			    puts "Defaulting to discrete polygenic type"
			    load model [full_filename mga_start]
			    option modeltype Default
			    polymod -d
			    save model [full_filename mga_start]
			    set dontallow 0
			    set noevd 1
			    continue
			} else {
			    error $errmsg
			}
		    }

# null model is now maximized
#   save likelihood and sd values

		    set null_lnlik [loglike]
		    if {$ntraits == 1} {
			set null_sdev [parameter sd =]
		    } else {
			set null_sdev {}
			foreach tr $traits {
			    lappend null_sdev [parameter sd($tr) =]
			}
		    }
		    if {!$noevd && !$tested_model_type} {
			set no_evd_error [catch {exec grep "Evd Processing is Not Applicable" [full_filename $nulloutname.out]}]
			if {!$noevd && !$no_evd_error} {
			    puts "   ** EVD1 is not available for these models"
			    puts "   ** Switching to Default maximization"
			    load model [full_filename mga_start]
			    option modeltype Default
			    if {$samplesametrustme && !$notsame} {
	    puts "     ** Warning: option samplesametrustme specifed and EVD not available"
	    puts "     ** Null model will be reused without verification"
	    puts "     ** Sample differences could cause erroneous p values"
			    }
			    save model [full_filename mga_start]
			    set dontallow 0
			    set noevd 1

# EVD1 can't be used so repeat null without it

			    continue
			}
			set tested_model_type 1
		    }
		    save model [full_filename mga_null]
		    if {$noevd && !$notsame} {
			set nullids [lsort -integer [listfile [full_filename who.out]]]
		    }

#    store the number of inividuals included in the analysis

		    set navail [lindex \
				[exec fgrep "sample size including probands" \
				     [full_filename $nulloutname.out]] 7]
		    puts "   Sample Size: $navail  Loglikelihood: $null_lnlik  SD: $null_sdev"
		    set used_previous_null 0
		    set null_made 1
		} else {
		    load model [full_filename mga_null]
		    if {$noevd && !$notsame} {
#
# check sample the old fashioned way, with maximize -who
#   (this is not a real maximize)
#
# replace dummy snp with the real one needed this time
#
			covariate delete snp_$null_snp\(\)
			covariate snp_$snp
			parameter bsnp_$snp = 0.001
			maximize -who -quiet
			set testids [lsort -integer [listfile \
						   [full_filename who.out]]]
			if {$nullids != $testids} {
			    set null_made 0
#
# null sample needs to reflect THIS snp so do it again
#
			    continue
			}
			load model [full_filename mga_null]
		    }
		}
#
# TEST MODEL
#
		option dontallowsamplechange $dontallow

		covariate delete snp_$null_snp\(\)
		covariate snp_$snp
		if {$ixsnp != ""} {
		    set ixname snp_$ixsnp*snp_$snp
		    covariate $ixname
		}
		if {"" != $fixlower} {
		    parameter bsnp_$snp fixlower $fixlower
		}
		if {"" != $fixupper} {
		    parameter bsnp_$snp fixupper $fixupper
		}
		set ifsave ""
		set testoutname solar.out
		if {$saveall} {
		    set testoutname mga_$snp.out
		    set ifsave "-out mga_$snp"
		}
		set test_error 0
		if {[catch {eval maximize $quiet $ifsave} errmsg]} {
#
# check for sample changed error (if we are doing it that way)
#
		    if {-1 != [string first "Sample" $errmsg]} {
			if {$samplesametrustme} {
			    error "mga sample changed error"
			}
			set null_made 0
#		        puts "   Re-running SNP null because of sample change"
			continue
		    }
		    puts "SNP $snp test model failed to maximize:\n\t$errmsg"
		    set test_error 1
		}
#
# NOW we have null and test models with the same sample (or test_error)
#
#               Compute the chi-square test statistic. If this stat is less
#               than zero, then we must not be at a likelihood meximum since
#               the likelihood of the more general model must always improve.
#
#               However, if the stat is just a tiny bit below zero, this is
#               probably a numerical artifact and we reset the stat to zero.
#               But I'm not sure how tiny 'tiny' has to be to be acceptable!
#
                if {!$test_error} {
		    set test_lnlik [loglike]
		    set rawchi [expr 2*($test_lnlik - $null_lnlik)]
		    set chi $rawchi
		    if {$chi < 0} {
			set chi 0
		    }
#
#               Compute the proportion of variance explained by this SNP.
#               If the proportion is negative, reset it to zero.
#
		    set varexp {}
		    if {$ntraits == 1} {
			set test_sdev [parameter sd =]
			set varexp [highest 0 [expr 1 - \
					pow($test_sdev/$null_sdev,2)]]
		    } else {
			set tindex 0
			set test_sdev ""
			foreach tr $traits {
			    lappend test_sdev [parameter sd($tr) =]
			  lappend varexp [highest 0 [expr 1 - \
		 pow([parameter sd($tr) =]/[lindex $null_sdev $tindex],2)]]
			    incr tindex
			}
		    }
#
#               Output the test stat, nominal p-value (a 1 df test), the
#               regression coefficient for the SNP and its standard error,
#               variance explained, and number in the analysis (which doesn't
#               change in this case, but I output anyway!).
#
		    if {$ixsnp == ""} {
			set pval [chi -number $chi $ntraits]
		    } else {
			set pval [chi -number $chi [expr 2 * $ntraits]]
		    }
		    set navail [expr round([lindex [exec fgrep "sample size including probands" [full_filename $testoutname]] 7])]
		    if {$ntraits == 1} {
			set bsnp [parameter bsnp_$snp =]
			set se [parameter bsnp_$snp se]
		    } else {
			set bsnp {}
			set se {}
			foreach tr $traits {
			    lappend bsnp [parameter bsnp_$snp\($tr\) =]
			    lappend se [parameter bsnp_$snp\($tr\) se]
			}
		    }
#
# Calculate SNP dosage stats
#
		    set snp_mean [read_output $testoutname snp_$snp -mean]
		    set snp_sd [read_output $testoutname snp_$snp -std]
		    set snp_sample_size [outfile_sample_size $testoutname]
		    set snp_maf [expr $snp_mean / 2.0]
		    set snp_mac [expr $snp_mean * $snp_sample_size]

# If -ixsnp then do third model, with interaction only constrained to zero

		    if {$ixsnp != ""} {

# in case of failure, set results to null values

			set ixchi ""
			set ixpval ""
			set ix ""
			set ixse ""
			set ixsd ""
			set ixvar ""

# rather than actually constraining covariate
# covariate is replaced with suspended covariate to ensure same sample
# this approach is more stable because there are fewer parameters
			save model [full_filename mga_ixsnp]

			catch {

			covariate delete $ixname
			if {$ifsave == ""} {
			    maximize $quiet
			} else {
			    eval maximize $quiet $ifsave\_ix
			}
			set xtest_lnlik [loglike]
			if {$ntraits == 1} {
			    set ixsd [parameter sd =]
			} else {
			    set ixsd ""
			    foreach tr $traits {
				lappend ixsd [parameter sd($tr) =]
			    }
			}
			load model [full_filename mga_ixsnp]

# compute chi square and p values

			set xrawchi [expr 2*($test_lnlik - $xtest_lnlik)]
			set ixchi $xrawchi
			if {$ixchi < 0} {
			    set ixchi 0
			}
			set ixpval [chi -number $ixchi $ntraits]
			if {$ntraits == 1} {
			    set ix [parameter b$ixname =]
			    set  ixse [parameter b$ixname se]
			    set ixvar [highest 0 [expr 1 - \
						      pow($test_sdev/$ixsd,2)]]
			} else {
			    set ix ""
			    set ixse ""
			    set ixvar ""
			    set tindex 0
			    foreach tr $traits {
				lappend ix [parameter b$ixname\($tr\) =]
				lappend ixse [parameter b$ixname\($tr\) se]
				lappend ixvar [highest 0 [expr 1 - \
	            pow([lindex $test_sdev $tindex]/[lindex $ixsd $tindex],2)]]
				incr tindex
			    }
			}

		    } ;# end catch
# if error occurred in constrained model, test model was not reloaded
			load model [full_filename mga_ixsnp]
		    }
		}

# Now output header if not output yet
# snp rawchi pval bsnp bsnpse varexp n
		    if {$headerneeded} {
			puts " "

			if {$format == "fortran"} {
			    set outheader [fformat "%-$snpwide\s %5s %10s \
%10s" SNP  "NAv" "chi  " "p(SNP) "]
			    if {$ntraits==1} {
				set outheader "$outheader [fformat \
                                "%12s %12s %12s" "bSNP  " "bSNPse " "Varexp "]"
			    } else {
				set tno 1
				foreach tr $traits {
				    set outheader "$outheader [fformat \
"%12s %12s %12s" "bSNP.$tno " "bSNPse.$tno " "Varexp.$tno " ]"
				    incr tno
				}
			    }
			    if {$ixsnp != ""} {
				set outheader "$outheader[fformat \
"%12s %12s" ix_chi p(IX)]"
				if {$ntraits == 1} {
				    set outheader "$outheader [fformat \
"%12s %12s %12s" bIX bIXse VarIX]"
				} else {
				    foreach tr $traits {
					set outheader "$outheader [fformat \
"%12s %12s %12s" bIX($tr) bIXse($tr) VarIX($tr)]"
				    }
				}
			    }
			   set outheader "$outheader est_maf est_mac dosage_sd"

			} elseif {$format == "csv"} {
			    set outheader "SNP,NAv,chi,p(SNP)"
                            if {$ntraits==1} {
				set outheader "$outheader,bSNP,bSNPse,Varexp"
			    } else {
				foreach tr $traits {
				    set outheader \
				 "$outheader,bSNP($tr),bSNPse($tr),Varexp($tr)"
				}
			    }
			    if {$ixsnp != ""} {
				set outheader "$outheader,ix_chi,p(IX)"
				if {$ntraits==1} {
				    set outheader "$outheader,bIX,bIXse,VarIX"
				} else {
				    foreach tr $traits {
					set outheader \
					    "$outheader,bIX($tr),bIXse($tr),VarIX($tr)"
				    }
				}
			    }
			    set outheader \
				"$outheader,est_maf,est_mac,dosage_sd"
			} else {
			    set outheader ""
			}
			if {"" != $outheader} {
			    set SOLAR_mga_header $outheader
			    putsout -d. $ofile $outheader
			}
			set headerneeded 0
		    }

# compute SE
		if {!$test_error} {
		    if {$nose} {
			set beta [lindex $bsnp 0]
			set se_ 10e20
			if {$chi > 0} {
			    catch {
				set se_ [expr sqrt ($beta*$beta/$chi)]
			    }
			}
			if {$ixsnp != ""} {
			    if {$ntraits==1} {
				set ixse_ 10e20
				if {$ixchi > 0} {
				    catch {
					set ixse_ [expr sqrt \
					       ($ix*$ix/$ixchi)]
				    }
				}
				if {$ixse_ != 10e20} {
				    set ixse $ixse_
				}
			    }
			}
		    } else {
			set se_ [lindex $se 0]
		    }
#
# Now output results for this SNP
#
# csv format
		    if {$format == "csv"} {

			if {[option ExpNotation]} {
			    set fs e
			} else {
			    set fs z
			}

			set outline [fformat \
 "%s,%d,%10.6$fs,%10.6$fs,%12.6$fs,%12.6$fs,%12.6$fs" \
 $snp $navail $chi $pval [lindex $bsnp 0] $se_ [lindex $varexp 0]]

			for {set i 1} {$i < $ntraits} {incr i} {
			    set outline "$outline,[fformat \
 "%12.6$fs,%12.6$fs,%12.6$fs" \
 [lindex $bsnp $i] [lindex $se $i] [lindex $varexp $i]]"
			}

			if {$ixsnp != ""} {

			    set outline "$outline,[fformat \
 "%10.6$fs,%12.6$fs,%12.6$fs,%12.6$fs,%12.6$fs" \
 $ixchi $ixpval [lindex $ix 0] [lindex $ixse 0] [lindex $ixvar 0]]"

			    for {set i 1} {$i < $ntraits} {incr i} {
				set outline "$outline,[fformat \
 "%12.6$fs,%12.6$fs,%12.6$fs" \
 [lindex $ix $i] [lindex $ixse $i] [lindex $ixvar $i]]"

			    }
			}
			set outline "$outline,[fformat \
"%12.6$fs,%12.6$fs,%12.6$fs" \
$snp_maf $snp_mac $snp_sd]"

# tab delimited format

		    } else {

			set outline [fformat \
 "%-$snpwide\s %5d %10.6y %10.6y %12.6y %12.6y %12.6y" \
$snp $navail $chi $pval [lindex $bsnp 0] $se_ [lindex $varexp 0]]

			for {set i 1} {$i < $ntraits} {incr i} {
			    set outline "$outline [fformat \
 "%12.6y %12.6y %12.6y" \
 [lindex $bsnp $i] [lindex $se $i] [lindex $varexp $i]]"
			}

                        if {$ixsnp != ""} {
			    set outline "$outline [fformat \
"%12.6y %12.6y" $ixchi $ixpval]"
			    for {set i 0} {$i < $ntraits} {incr i} {
                                set outline "$outline [fformat \
"%12.6y %12.6y %12.6y" [lindex $ix $i] [lindex $ixse $i] [lindex $ixvar $i]]"
                            }
}
			set outline "$outline [fformat \
"%12.6y %12.6y %12.6y" \
$snp_maf $snp_mac $snp_sd]"
                    }

                    putsat $ofile $outline
                    set SOLAR_mga_last_out $outline
		    set done 1
		    set written 1
                }    
		if {!$done} {
		    if {$format != "csv"} {
			putsat $ofile [fformat %-11s $snp]
		    } else {
			putsat $ofile "$snp,,,,,,,,,"
		    }
		    set written 1
		}
		break ;# breaks out of while for null/test maximizations
	    }
	}
    }
    if {$written && !$quietsub} {
	puts "\n    ** results written to $ofile"
    }
}

proc snplistfile {filename} {
    set retlist {}
    set infile [open $filename]
    while {-1 != [gets $infile line]} {
	if {"" != $line} {
	    set trimmed [string trim $line]
	    if {"snp_" == [string range $trimmed 0 3]} {
		set trimmed [string range $trimmed 4 end]
	    }
	    lappend retlist $trimmed
	}
    }
    close $infile
    return $retlist
}

proc remove_from_list_by_pos {inlist pos} {
    return [concat [lrange $inlist 0 [expr $pos - 1]] \
		[lrange $inlist [expr $pos + 1] end]]
}

proc ifdebug0 {args} {
    upvar debug0 debug0
    if {$debug0} {
	return [uplevel $args]
    }
    return ""
}

proc mga_codefile {filename snpwidth {ntraits 1}} {
    set fext [file extension $filename]
    if {[string tolower $fext] == ".out"} {
	set newfilename [file rootname $filename].cde
    } else {
	set newfilename $filename.cde
    }
    set o [open $newfilename w]
    puts $o $filename
    puts $o [format_codefile_record $snpwidth SNP SNP C]
    puts $o [format_codefile_record BLANK]
    puts $o [format_codefile_record 5 NAv NAv I]
    puts $o [format_codefile_record BLANK]
    puts $o [format_codefile_record 10 CHI CHI C]
    puts $o [format_codefile_record BLANK]
    puts $o [format_codefile_record 10 p(SNP) p(SNP) R]
    if {$ntraits == 1} {
    puts $o [format_codefile_record BLANK]
    puts $o [format_codefile_record 12 bSNP bSNP R]
    puts $o [format_codefile_record BLANK]
    puts $o [format_codefile_record 12 bSNPse BsnpSE R]
    puts $o [format_codefile_record BLANK]
    puts $o [format_codefile_record 12 Varexp Varexp R]
    } else {
	for {set i 1} {$i <= $ntraits} {incr i} {
	    puts $o [format_codefile_record BLANK]
	    puts $o [format_codefile_record 12 bSNP.$i bSNP.$i R]
	    puts $o [format_codefile_record BLANK]
	    puts $o [format_codefile_record 12 bSNPse.$i BsnpSE.$i R]
	    puts $o [format_codefile_record BLANK]
	    puts $o [format_codefile_record 12 Varexp.$i Varexp.$i R]
	}
    }
    puts $o [format_codefile_record BLANK]
    puts $o [format_codefile_record 12 snp_mac snp_mac R]
    puts $o [format_codefile_record BLANK]
    puts $o [format_codefile_record 12 snp_maf snp_maf R]
    puts $o [format_codefile_record BLANK]
    puts $o [format_codefile_record 12 dosage_sd dosage_sd R]
    close $o
    return ""
}

proc format_codefile_record {count args} {
    if {[string tolower $count] == "blank"} {
	if {$args == ""} {
	    return [format_codefile_record 1 BLANK BLANK I]
	}
    }
    set name [lindex $args 0]
    set mnemonics [lindex $args 1]
    set type [lindex $args 2]
    set outline [fformat %2s $count]
    set outline "$outline [fformat %-21s $name]"
    set outline "$outline [fformat %-27s $mnemonics]"
    set outline "$outline $type"
    return $outline
}

# solar::mg_topedsys
#
# Purpose: Convert comma delimited mga.out to pedsys format
#
# Usage: mg_topedsys <input_filename> <output_filename>
#
#-

proc mg_topedsys {inname outname {ntraits 1}} {
    set snpwide 12
    set infile [open $inname]
    while {-1 != [gets $infile line]} {
	set lline [split $line ,]
	set snp [lindex $lline 0]
	set wide [string length $snp]
	if {$wide > $snpwide} {
	    set snpwide $wide
	}
    }
    close $infile
    set infile [open $inname]
    set outfile [open $outname w]
    gets $infile
    while {-1 != [gets $infile line]} {
	set lline [split $line ,]
	set snp [lindex $lline 0]
	set chi [lindex $lline 1]
	set pval [lindex $lline 2]
	set bsnp [lindex $lline 3]
	set se [lindex $lline 4]
	set varexp [lindex $lline 5]
	set navail [lindex $lline 6]
#	puts "navail is $navail"
#	puts "varexp is $varexp"
	set outline [fformat \
 "%-$snpwide\s %10.6y %10.6y %12.6y %12.6y %12.6y %5d" \
			 $snp $chi $pval $bsnp $se $varexp $navail]
	puts $outfile $outline
    }
    close $infile
    close $outfile
    mga_codefile $outname $snpwide $ntraits
}

# solar::snp --
#
# Purpose:  Process SNP data.
#
# Usage:    load snp [-xlinked] <genofile> [<locfile>] ; loads SNP data
#           snp show                        ; displays summary of SNP data
#           snp covar [-nohaplos] [-impute] ; prepare genotype covariates file
#           snp qtld                        ; prepare QTLD covariates file
#           snp ld [-window <window>] [-overwrite]
#                  [-plot [-absrho] [-file <psfile>]
#                   [-title <title>] [-date] [-gray]]
#                                           ; compute linkage disequilibrium
#           snp effnum [<method>] [-alpha <alpha>]
#                                           ; use <method> to compute the
#                                             effective number of SNPs
#           snp unload                      ; unload SNP data
#
#           SNP genotype data is treated as a special case of marker data.
#           The file containing SNP genotypes must be in the same format as
#           any SOLAR marker data file, and the SOLAR 'marker' and 'freq'
#           commands can be used to process the SNP genotype data.  However,
#           the following restriction applies to SNP genotype data: there
#           must be exactly two allelic types present in the data for each
#           SNP.  If a SNP has only a single allele, i.e. the SNP is not
#           polymorphic, it will be loaded but cannot be used for further
#           analysis.  If a SNP with more than two alleles is encountered,
#           the 'load snp' command will fail.  After a successful load, a
#           file named 'snp.typed' is created, which contains a field, named
#           nGTypes, giving the number of SNPs genotyped for each pedigree
#           member.  This field is empty for untyped individuals.
#
#           The locations of the SNPs can be read from a standard SOLAR map
#           file by the 'load snp' command.  Each location must be given as
#           an integer number of basepairs.  Only those SNPs that appear in
#           the map file will be included in SNP processing commands.  While
#           it is not necessary that every SNP in the marker file be present
#           in the map, it is required that genotype data be loaded for all
#           SNPs in the map file.  If the user does not specify a map file
#           in the 'load snp' command but a map file has already been loaded,
#           that map file will be used.  Otherwise, a dummy map file, named
#           'snp.map', will be created in which consecutive basepair locations
#           have been assigned to the SNPs, starting at zero.
#
#           The 'snp show' command displays a summary of the SNP genotype
#           data.  The contents of this display are also saved in the file
#           'snp.show.txt'.  The information displayed includes SNP name,
#           location, number of typed individuals, and allele frequencies.
#           The frequency information is ordered so that the common (higher
#           frequency) allele is listed first.  If available, the standard
#           error of the allele frequency estimates and the p-value for a
#           test of Hardy-Weinberg equilibrium are also displayed.  The
#           allele frequency standard errors are computed by the 'freq mle'
#           command.  The HWE test is performed when the '-hwe' option is
#           included in the 'freq mle' command.
#
#           The 'snp covar' command produces a file, 'snp.genocov', in which
#           the SNP genotypes have been recoded to be suitable for use as
#           covariates in a SOLAR measured genotype, QTLD, or Bayesian
#           QTN analysis.  This file includes a field for each SNP, named
#           snp_<name>, where <name> is the SNP name.  Genotypes are coded
#           as the number of copies of the rarer allele: 0, 1, or 2.  The
#           'snp covar' command can be run in one of three different ways.
#           By default, SNP haplotypes generated by a haplotype estimation
#           procedure, e.g.  SimWalk2, are used to impute as many missing
#           genotypes as possible.  The haplotypes are read from the file
#           'snp.haplotypes', which is created from the output of the
#           haplotype estimation procedure using the 'snphap' command.
#
#           If SNP haplotypes are not available, the '-nohaplos' option
#           can be given to the 'snp covar' command, in which case the
#           covariates are generated solely from the genotype data.
#
#           The third method for generating covariates, invoked with the
#           '-impute' option, is to extend the genotype imputation of the
#           default method.  In this case, a missing genotype which cannot
#           be assigned exactly from the haplotypes, is imputed from the
#           weighted average of all haplotypes which are consistent with
#           that individual's genotype and estimated haplotype data, where
#           the weights are the estimated haplotype frequencies.  These
#           frequencies are read from the file 'snp.haplofreqs', which is
#           created by the 'snphap' command.  This method of generating
#           the covariates ensures that each individual has complete data,
#           i.e. there are no missing covariates.  Because covariates will
#           have been imputed for all pedigree members, whether they were
#           genotyped or not, it may be desirable to include in an analysis
#           only those individuals for whom at least some minimum number
#           of SNPs were typed.  This can be done by selecting on the
#           nGTypes field, which is taken from the file 'snp.typed' and
#           automatically joined to the covariates file.
#
#           In addition to the file 'snp.genocov', the 'snp covar' command
#           creates a file 'snp.geno-list', which contains the names of
#           the covariates, one per line.  This file can be used to
#           specify the covariates to be included in various association
#           analyses, e.g. the 'bayesavg' command.
#
#           The 'snp qtld' command generates another type of covariates
#           file, in this case the covariates required for a SOLAR QTLD
#           analysis.  This file, which is named 'snp.qtldcov', contains
#           four covariates for each SNP: b_<name>, w_<name>, b2_<name>,
#           and w2_<name>, where <name> is the name of the SNP.  As with
#           the genotypes covariates file, the nGTypes field from the
#           file 'snp.typed' is automatically included and can be used
#           to exclude untyped individuals from an analysis.
#
#           The 'snp ld' command computes the pairwise correlation among
#           the SNP genotypes.  This measure can be used to identify
#           those SNPs which are in linkage disequilibrium.  The signed
#           pairwise correlations are written to the file 'snp.ld.dat'.
#           If 'snp.ld.dat' already exists, the correlations are not
#           recomputed unless the '-overwrite' option is specified.
#
#           The '-window' option limits the number of pairwise correlations
#           that will be computed by the 'snp ld' command.  Only the pairs
#           of SNPs separated by no more than the number of basepairs in
#           the window will be considered.  A map file containing basepair
#           locations must be loaded in order to use this option.
#
#           When the '-plot' option is added to the 'snp ld' command, a
#           PostScript LD plot will be displayed.  If the file 'snp.ld.dat'
#           already exists, the genotypic correlations are not recomputed.
#           The plot is saved in the file specified by the '-file' option',
#           or, by default, in the file 'snp.ld.ps'.  The LD measure shown
#           in the plot is the square of the genotypic correlation (rho^2),
#           or, if the '-absrho' option is specified, the absolute value of
#           the correlation.  A plot title, enclosed in quotes, can be
#           supplied with the '-title' option.  The '-date' option adds a
#           date stamp to the plot.  The '-gray' option produces a gray-scale
#           version of the plot.
#
#           The 'snp effnum' command uses the specified method to estimate
#           the effective number of SNPs based on the pairwise genotypic
#           correlations.  This is an estimate of the number of independent
#           tests in an association analysis utilizing these SNPs, which can
#           be used to determine an appropriate significance level for an
#           association analysis utilizing these SNP data.  Currently, the
#           following methods are implemented:
#
#             mosk   Moskvina & Schmidt    (default)
#             liji   Li & Ji
#
#           For example: snp effnum liji
#
#           The method of Moskvina & Schmidt is the more conservative of the
#           two and is the default.  The Li & Ji method entails computing
#           the eigenvalues of the genotypic correlation matrix.  The number
#           of SNPs cannot exceed the number of genotyped individuals (i.e.,
#           the number of rows in the correlation matrix) or the matrix will
#           be singular.
#
#           The 'snp effnum' command also returns the p-value required for
#           a result to be considered statistically significant after the
#           correction for multiple testing is applied.  This p-value is a
#           function of the effective number of SNPs and the experiment-wide
#           significance threshold (target alpha).  The '-alpha' option
#           specifies the target alpha (default value = .05).
#
#-

proc snp {args} {
#           snp ld [[-plot | -ps] [-absrho] [-file <psfile>]
#           If the '-ps' option is given instead of '-plot', the LD plot
#           is generated but not displayed.  The plot is saved in the file
#           specified by the '-file' option', or, by default, in the file
#           'snp.ld.ps'. The LD measure shown in the plot is the square of
#           the genotypic correlation (rho^2), or, if the '-absrho' option
#           is specified, the absolute value of the correlation.  A plot
#           title, enclosed in quotes, can be supplied with the '-title'
#           option.  The '-date' option adds a date stamp to the plot. The
#           '-gray' option produces a gray-scale version of the plot.
    set plot 0
    set ps_only 0
    set psfile ""
    set gray 0
    set title ""
    set date 0
    set nfont ""
    set lfont ""
    set ldwindow ""
    set alpha .05
    set moskwin 20
    set overwrite 0
    set use_locn 0
    set absrho 0
    set nohaplos 0
    set impute 0
    set xlinked 0

    set arglist [ read_arglist $args \
                      -plot {set plot 1} -ps {set ps_only 1} -file psfile \
                      -window ldwindow -alpha alpha -moskwin moskwin \
                      -overwrite {set overwrite 1} -ov {set overwrite 1} \
                      -title title -gray {set gray 1} \
                      -nfont nfont -lfont lfont \
                      -locn {set use_locn 1} \
                      -date {set date 1} -absrho {set absrho 1} \
                      -nohaplos {set nohaplos 1} -impute {set impute 1} \
                      -xlinked {set xlinked 1} ]

    proc desc_sort_by_freq {a b} {
        if {[lindex $b 1] < [lindex $a 1]} {
            return -1
        } elseif {[lindex $b 1] > [lindex $a 1]} {
            return 1
        } elseif {[lindex $a 0] < [lindex $b 0]} {
            return -1
        } elseif {[lindex $a 0] > [lindex $b 0]} {
            return 1
        }
        return 0
    }

    proc sort_by_pos {a b} {
        if {[lindex $a 1] < [lindex $b 1]} {
            return -1
        } elseif {[lindex $a 1] > [lindex $b 1]} {
            return 1
        }
        return 0
    }

    set arg1 [lindex $arglist 0]

    if {$arg1 == "unload"} {
        if {[catch {marker unload} errmsg]} {
            puts $errmsg
        }
        if {![catch {map test}]} {
            puts "Unloading map data ..."
            map unload
            file delete snp.map
        }
        return
    }

    if {$arg1 == "load"} {
        if {[catch {pedigree_loaded} errmsg]} {
            error $errmsg
        }

        if {![catch {map test}]} {
            if {[cmap fname] == "snp.map"} {
                map unload
            }
        }
        file delete snp.map

        if {[llength $arglist] == 3} {
            if {[catch {map load -basepair [lindex $arglist 2]} errmsg]} {
                error $errmsg
            }

        } elseif {[llength $arglist] == 2} {
            if {![catch {map test}]} {
                puts "Using currently loaded map file [map fname] ..."
                if {[cmap func] != "b"} {
                    error \
        "Load failed: currently loaded map does not contain basepair locations."
                }
            }

        } else {
            error "Invalid snp command"
        }

        if {![catch {map test}]} {
            set maplist [cmap names]
            if {[catch {set mrkfile [tablefile open [lindex $arglist 1]]}]} {
                error "Cannot open marker file [lindex $arglist 1]"
            }
            set missing {}
            for {set i 0} {$i < [cmap nloci]} {incr i} {
                if {![tablefile $mrkfile test_name [lindex $maplist $i]]} {
                    lappend missing [lindex $maplist $i]
                }
            }
            tablefile $mrkfile close
            if {$missing != {}} {
                set errmsg "Load failed: SNP(s) in map but not in marker file:"
                for {set i 0} {$i < [llength $missing]} {incr i} {
                    set errmsg "$errmsg\n  [lindex $missing $i]"
                }
                error $errmsg
            }
        }

        if {$xlinked} {
            if {[catch {marker load -xlinked [lindex $arglist 1]} errmsg]} {
                error $errmsg
            }
        } else {
            if {[catch {marker load [lindex $arglist 1]} errmsg]} {
                error $errmsg
            }
        }

        set mrklist [marker names]
        if {![catch {map test}]} {
            for {set i 0} {$i < [llength $mrklist]} {incr i} {
                set snp [lindex $mrklist $i]
                if {[catch {cmap locn $snp}]} {
                    puts "Warning: SNP $snp is in marker file but not in map."
                }
            }
        } else {
            puts "Creating and loading map file snp.map ..."
            set fp [open snp.map w]
            puts $fp "SNP basepair"
            set i 0
            foreach snp $mrklist {
                puts $fp "$snp $i"
                incr i
            }
            close $fp
            map load -basepair snp.map
            set maplist [cmap names]
        }

        set infofile [open pedigree.info r]
        gets $infofile line
        gets $infofile line
        gets $infofile line
        set nind [lindex $line 2]
        close $infofile

        for {set i 1} {$i <= $nind} {incr i} {
            set ngtypes($i) 0
        }

        set errmsg ""
        for {set i 0} {$i < [llength $maplist]} {incr i} {
            set snp [lindex $maplist $i]
            set nall [cfreq nall $snp]
            if {$nall > 2} {
                set errmsg "$errmsg\n  SNP $snp: more than 2 alleles"
                continue
            }
            if {$nall < 2} {
                puts \
            "Warning: SNP $snp not polymorphic so it can't be used for analysis."
                continue
            }
            set genfname d_$snp/translat.tab
            if {[catch {set genfile [open $genfname r]}]} {
                error "Cannot open file $genfname"
            }
            gets $genfile genrec
            gets $genfile genrec
            set ibdid 0
            while {-1 != [gets $genfile genrec]} {
                set n [lindex $genrec 0]
                for {set j 0} {$j < $n} {incr j} {
                    incr ibdid
                    gets $genfile genrec
                    set a1 [string range $genrec 19 21]
                    if {$a1 != "   "} {
                        incr ngtypes($ibdid)
                    }
                }
            }
            close $genfile
        }

        if {$errmsg != ""} {
            marker unload
            if {[cmap fname] == "snp.map"} {
                map unload
                file delete snp.map
            }
            error "Load failed:$errmsg"
        }

        set pedfile [tablefile open pedindex.out]
        set need_famid 0
        if {[tablefile $pedfile test_name FAMID]} {
            set need_famid 1
        }
        tablefile $pedfile close

        set outf [open snp.typed w]
        if {$need_famid} {
            puts -nonewline $outf "FAMID,"
        }
        puts $outf "ID,nGTypes"
        for {set i 1} {$i <= $nind} {incr i} {
            set id [get_id $i]
            if {$need_famid} {
                puts -nonewline $outf "[lindex $id 0],[lindex $id 1]"
            } else {
                puts -nonewline $outf $id
            }
            if {$ngtypes($i)} {
                puts $outf ",$ngtypes($i)"
            } else {
                puts $outf ","
            }
        }
        close $outf

        return
    }

    if {$arg1 == "show"} {
        if {[catch {pedigree_loaded} errmsg]} {
            error $errmsg
        }

        if {[catch {set mrkfile [marker fname]}]} {
            error "SNP genotypes have not been loaded."
        }

        if {[catch {set mapfname [map fname]}]} {
            error "SNP locations have not been loaded."
        }
        if {[catch {set mapfile [open $mapfname r]}]} {
            error "Cannot open map data file $mapfname"
        }
        gets $mapfile record
        set maplist ""
        while {-1 != [gets $mapfile record]} {
            lappend maplist "[lindex $record 0] [lindex $record 1]"
        }
        close $mapfile

        set maxnamelen 3
        set maxlocsize 8
        set allintlocs 1
        set nostderr 1
        set nohwetest 1
        for {set i 0} {$i < [llength $maplist]} {incr i} {
            set elem [lindex $maplist $i]
            set snp [lindex $elem 0]
            if {[cfreq nall $snp]} {
                set mle_se [cfreq mle_se $snp 0]
                if {$mle_se >= 0} {
                    set se($snp) $mle_se
                    set nostderr 0
                }
                set chi2 [cfreq chi2_hwe $snp]
                if {$chi2 >= 0} {
                    set hwe_p($snp) [chi -number $chi2 1]
                    set nohwetest 0
                }
            }
            if {[string length $snp] > $maxnamelen} {
                set maxnamelen [string length $snp]
            }
            set loc [lindex $elem 1]
            if {[expr int($loc)] != $loc} {
                set allintlocs 0
            } else {
                set loc [expr int($loc)]
            }
            if {[string length $loc] > $maxlocsize} {
                set maxlocsize [string length $loc]
            }
        }

        set fmt "%-${maxnamelen}s  %${maxlocsize}s"
        set sfile snp.show.txt
        set fp [open $sfile w]
        puts $fp "\ngenotype file: $mrkfile\nlocation file: [map fname]\n"
        puts -nonewline $fp [format $fmt "snp" "locn(bp)"]
        if {$nostderr} {
            if {$nohwetest} {
                puts $fp "  #typed  %typed  alleles"
                set dashes "------------------"
            } else {
                puts $fp "  #typed  %typed  alleles             HWE p-val"
                set dashes "------------------  ---------"
            }
        } else {
            if {$nohwetest} {
                puts $fp "  #typed  %typed  alleles             SE(freq)"
                set dashes "------------------  --------"
            } else {
                puts $fp \
                    "  #typed  %typed  alleles             SE(freq)  HWE p-val"
                set dashes "------------------  --------  ---------"
            }
        }
        puts -nonewline $fp "[string range $dashes 0 [expr $maxnamelen - 1]]  "
        puts -nonewline $fp "[string range $dashes 0 [expr $maxlocsize - 1]]  "
        puts $fp "------  ------  $dashes"

        set nonpolym 0
        for {set i 0} {$i < [llength $maplist]} {incr i} {
            set elem [lindex $maplist $i]
            set snp [lindex $elem 0]
            set loc [lindex $elem 1]
            if {$allintlocs} {
                set loc [expr int($loc)]
            }
            set mrkinfo [cmarker show $snp]
            set ntyped [lindex $mrkinfo 1]
            set pctyped [lindex $mrkinfo 2]
            set nall [cfreq nall $snp]

            set alllist ""
            for {set j 0} {$j < $nall} {incr j} {
                 lappend alllist \
                     "[cfreq name $snp $j] [cfreq freq $snp $j]"
            }
            set alllist [lsort -command desc_sort_by_freq $alllist]
            puts -nonewline $fp [format $fmt $snp $loc]
            puts -nonewline $fp [format "  %6d  %6.1f" $ntyped $pctyped]
            for {set j 0} {$j < [llength $alllist]} {incr j} {
                set elem [lindex $alllist $j]
                puts -nonewline $fp \
                    [format "  %s %6.4f" [lindex $elem 0] [lindex $elem 1]]
            }
            if {$nall == 0} {
                puts -nonewline $fp "                    "
                set nonpolym 1
            }
            if {$nall == 1} {
                puts -nonewline $fp "          "
                set nonpolym 1
            }
            if {!$nostderr} {
                if {[info exists se($snp)]} {
                    puts -nonewline $fp [format "  %8.6f" $se($snp)]
                } else {
                    puts -nonewline $fp "          "
                }
            }
            if {!$nohwetest} {
                if {[info exists hwe_p($snp)]} {
                    puts -nonewline $fp [format "  %s" $hwe_p($snp)]
                } else {
                    puts -nonewline $fp "           "
                }
            }
            if {$nall < 2} {
                puts -nonewline $fp " **"
            }
            puts $fp ""
        }

        if {$nonpolym} {
            puts $fp "\n** Non-polymorphic SNPs will not be included in SNP analyses."
        }

        close $fp
        exec >&@stdout <@stdin more $sfile
        puts "\nThis output is saved in file snp.show.txt.\n"
        return
    }

    if {$arg1 == "covar"} {
        if {[catch {pedigree_loaded} errmsg]} {
            error $errmsg
        }

        if {[catch {marker fname}]} {
            error "SNP genotypes have not been loaded."
        }

        if {[catch {map test}]} {
            error "SNP locations have not been loaded."
        }

        if {![polym_snps]} {
            error \
       "Cannot create covariates file because there are no polymorphic SNPs."
        }

        if {$nohaplos} {
            set pedfile [tablefile open pedindex.out]
            set need_famid 0
            if {[tablefile $pedfile test_name FAMID]} {
                set need_famid 1
            }
            tablefile $pedfile start_setup
            tablefile $pedfile setup FIBDID
            tablefile $pedfile setup MIBDID
            tablefile $pedfile setup SEX
            tablefile $pedfile setup MZTWIN
            tablefile $pedfile setup PEDNO
            set nindt 0
            while {{} != [set record [tablefile $pedfile get]]} {
                incr nindt
                set fibdid($nindt) [lindex $record 0]
                set mibdid($nindt) [lindex $record 1]
                set sex($nindt) [lindex $record 2]
                set mztwin($nindt) [lindex $record 3]
                if {$mztwin($nindt) != 0} {
                    if {![info exists twinid($mztwin($nindt))]} {
                        set twinid($mztwin($nindt)) $nindt
                    } else {
                        lappend twinid($mztwin($nindt)) $nindt
                    }
                }
                set pedno($nindt) [lindex $record 4]
                set gtype($nindt) {}
            }
            tablefile $pedfile close

            set frqfile [open freq.info r]
            gets $frqfile record
            while {-1 != [gets $frqfile record]} {
                set snp [lindex $record 0]
                set alllist($snp) {}
                set frqlist($snp) {}
                for {set i 4} {$i < [llength $record]} {incr i} {
                    set allname [lindex $record $i]
                    if {$allname == "se=" || $allname == "hwe="} {
                        incr i
                        continue
                    }
                    lappend alllist($snp) $allname
                    incr i
                    lappend frqlist($snp) [lindex $record $i]
                }
            }
            close $frqfile

            set mapfile [open [map fname] r]
            gets $mapfile record
            set maplist {}
            while {-1 != [gets $mapfile record]} {
                set snp [lindex $record 0]
                if {[cfreq nall $snp] < 2} {
                    continue
                }
                lappend maplist $snp
                set genfname d_$snp/translat.tab
                if {[catch {set genfile [open $genfname r]}]} {
                    error "Cannot open file $genfname"
                }
                gets $genfile genrec
                gets $genfile genrec
                set ibdid 0
                while {-1 != [gets $genfile genrec]} {
                    set n [lindex $genrec 0]
                    for {set i 0} {$i < $n} {incr i} {
                        incr ibdid
                        gets $genfile genrec
                        set a1 [string range $genrec 19 21]
                        set a2 [string range $genrec 22 24]
                        if {$a1 != "   "} {
                            set gtype($ibdid,$snp) [format %d/%d $a1 $a2]
                        } else {
                            set gtype($ibdid,$snp) "*"
                        }
                    }
                }
                close $genfile
            }
            close $mapfile

            set outf [open snp.genocov w]
            set outf2 [open snp.geno-list w]
            if {$need_famid} {
                puts -nonewline $outf "FAMID,"
            }
            puts -nonewline $outf "ID"
            for {set i 0} {$i < [llength $maplist]} {incr i} {
                set snp [lindex $maplist $i]
                puts -nonewline $outf ",snp_$snp"
                puts $outf2 "snp_$snp"
            }
            close $outf2
            puts $outf ""
            for {set ibdid 1} {$ibdid <= $nindt} {incr ibdid} {
                set id [get_id $ibdid]
                if {$need_famid} {
                    puts -nonewline $outf "[lindex $id 0],[lindex $id 1]"
                } else {
                    puts -nonewline $outf $id
                }
                for {set i 0} {$i < [llength $maplist]} {incr i} {
                    set snp [lindex $maplist $i]
                    set swap 0
                    if {[lindex $frqlist($snp) 0] < [lindex $frqlist($snp) 1]} {
                        set swap 1
                    }
                    if {$gtype($ibdid,$snp) == "1/1"} {
                        if {$swap} {
                            puts -nonewline $outf ",2"
                        } else {
                            puts -nonewline $outf ",0"
                        }
                    } elseif {$gtype($ibdid,$snp) == "1/2"} {
                        puts -nonewline $outf ",1"
                    } elseif {$gtype($ibdid,$snp) == "2/2"} {
                        if {$swap} {
                            puts -nonewline $outf ",0"
                        } else {
                            puts -nonewline $outf ",2"
                        }
                    } else {
                        puts -nonewline $outf ","
                    }
                }
                puts $outf ""
            }
            close $outf

        } elseif {$impute} {
            if {![file exists snp.haplotypes]} {
                error "SNP haplotypes have not been loaded."
            }
            if {![file exists snp.haplofreqs]} {
                error "SNP haplotype frequencies have not been computed."
            }
            csnp covar -impute

        } else {
            if {![file exists snp.haplotypes]} {
                error "SNP haplotypes have not been loaded."
            }
            csnp covar
        }

        if {[catch {joinfiles snp.typed snp.genocov}]} {
            error "Failed to join files snp.typed and snp.genocov."
        }
        exec mv joinfiles.out snp.genocov

        return
    }

    if {$arg1 == "effnum"} {
        if {![file exists snp.ld.dat]} {
            error \
        "The file snp.ld.dat was not found. Run \"snp ld\" to create it."
        }

        if {[catch {set genfile [solarfile open snp.genocov]} errmsg]} {
            error "Cannot open snp.genocov: $errmsg"
        }
        set nsnp 0
        foreach fld [solarfile $genfile names] {
            if {[string range $fld 0 3] == "snp_"} {
                incr nsnp
            }
        }
        solarfile $genfile close

        set method "mosk"
        if {[llength $arglist] > 1} {
            set method [lindex $arglist 1]
        }

        if {$method == "liji"} {
            set ifp [open snp.ld.dat r]
            gets $ifp record
            set nrec 0
            while {-1 != [gets $ifp record]} {
                incr nrec
            }
            close $ifp

            if {$nrec != $nsnp*($nsnp+1)/2} {
                error \
"Cannot use the method of Li & Ji because the genotypic correlation matrix\n\
(snp.ld.dat) does not include all pairs of SNPs."
            }

            csnp $method $nsnp $alpha

        } elseif {$method == "mosk"} {
            set ifp [open snp.ld.dat r]
            gets $ifp record
            while {-1 != [gets $ifp record]} {
                set r([lindex $record 0],[lindex $record 1]) [lindex $record 2]
            }

            close $ifp
            set x [expr -1.31*log($alpha)/log(10)]
            set neff 1
            for {set i 2} {$i <= $nsnp} {incr i} {
                set j1 [expr $i - $moskwin]
                if {$j1 < 1} {
                    set j1 1
                }
                set rmax 0
                for {set j $j1} {$j < $i} {incr j} {
                    if {[info exists r($j,$i)] && $r($j,$i) > $rmax} {
                        set rmax $r($j,$i)
                    }
                }
                set neff [expr $neff + sqrt(1 - pow($rmax, $x))]
            }

            set newalpha [expr 1 - exp(log(1-$alpha)/$neff)]
            puts [format "Experiment-Wide Alpha = %g" $alpha]
            puts "Number of SNPs = $nsnp"
            puts [format "Effective Number of SNPs = %g" $neff]
            puts \
[format "Effective Number/Total Number = %g" [expr double($neff)/$nsnp]]
            puts [format "Required Alpha = %g" $newalpha]
            puts [format "Target -log(p) = %g" [expr log($newalpha)/log(10)]]
            puts \
"Effective number of SNPs and required alpha written to the file snp.effnum."

            set ofp [open snp.effnum w]
            puts $ofp "method = Moskvina&Schmidt"
            puts $ofp "nsnp = $nsnp"
            puts $ofp [format "effnum = %g" $neff]
            puts $ofp [format "newalpha = %g" $newalpha]
            close $ofp

        } else {
            error \
"Multiple testing correction method [lindex $arglist 1] not implemented."
        }

        return
    }

    if {$arg1 == "ld"} {
        if {$use_locn} {
            puts \
    "-locn option deprecated, SNP names will appear in output file snp.ld.dat"
        }

        if {![file exists snp.ld.dat] || $overwrite} {
            if {![file exists snp.genocov]} {
                error \
    "SNP genotype covariates file not found. Run \"snp covar\" to create it."
            }

            if {[catch {set genfile [solarfile open snp.genocov]} errmsg]} {
                error "Cannot open snp.genocov: $errmsg"
            }
            set nsnp 0
            foreach fld [solarfile $genfile names] {
                if {[string range $fld 0 3] == "snp_"} {
                    incr nsnp
                }
            }
            solarfile $genfile close
            if {$nsnp == 0} {
                error "No SNP covariates were found in the file snp.genocov!"
            }

            if {$ldwindow != ""} {
		if {![regexp {^0*[1-9][0-9]*$} $ldwindow]} {
                    error "Window size must be a positive integer."
                }
                if {[catch {map test}]} {
                    error "A map file must be loaded to use the -window option."
                }
                puts "Computing genotypic correlations (window = $ldwindow bp) ..."
                csnp ld -window $ldwindow
            } else {
                puts "Computing genotypic correlations ..."
                csnp ld
            }
        } elseif {!$plot && !$ps_only} {
            puts \
"LD has already been calculated. Use the -overwrite option to recompute."
            return
        }

        if {$plot || $ps_only} {
            set nomap 0
            if {[catch {map test}]} {
                set nomap 1
                puts \
"A map file has not been loaded, so dummy SNP locations will appear in LD plot."
            }

            if {[catch {set genfile [solarfile open snp.genocov]} errmsg]} {
                error "Cannot open snp.genocov: $errmsg"
            }

            set nsnp 0
            set poslist {}
            foreach fld [solarfile $genfile names] {
                if {[string range $fld 0 3] == "snp_"} {
                    set snp [string range $fld 4 end]
                    if {$nomap} {
                        lappend poslist "$snp $nsnp"
                    } else {
                        lappend poslist "$snp [cmap locn $snp]"
                    }
                    incr nsnp
                    set covsnp($nsnp) $snp
                }
            }
            solarfile $genfile close

            set poslist [lsort -command sort_by_pos $poslist]

            set ofp [open snp.ld.pos w]
            puts $ofp "MARKERID NAME LOCATION"
            for {set i 0} {$i < $nsnp} {incr i} {
                puts $ofp "\
[expr $i + 1] [lindex [lindex $poslist $i] 0] [lindex [lindex $poslist $i] 1]"
            }
            close $ofp

            set ifp [open snp.ld.dat r]
            gets $ifp record
            set nrec 0
            while {-1 != [gets $ifp record]} {
                set snpi $covsnp([lindex $record 0])
                set snpj $covsnp([lindex $record 1])
                set ld($snpi,$snpj) [lindex $record 2]
                set ld($snpj,$snpi) $ld($snpi,$snpj)
                incr nrec
            }
            close $ifp

            if {$nrec != $nsnp*($nsnp+1)/2} {
                error \
"Cannot generate LD plot because the genotypic correlation matrix (snp.ld.dat)\n\
does not include all pairs of SNPs."
            }

            set ofp [open snp.ld.tmp w]
            puts $ofp "M1 M2 DISEQ"
            for {set i 0} {$i < $nsnp} {incr i} {
                set snpi [lindex [lindex $poslist $i] 0]
                for {set j $i} {$j < $nsnp} {incr j} {
                    set snpj [lindex [lindex $poslist $j] 0]
                    if {$absrho} {
                        set tld [expr abs($ld($snpi,$snpj))]
                    } else {
                        set tld [expr pow($ld($snpi,$snpj),2)]
                    }
                    puts $ofp "[expr $i + 1] [expr $j + 1] $tld"
                }
            }
            close $ofp

            set plot_args "-i snp.ld.tmp -m snp.ld.pos"
            if {$psfile != ""} {
                set plot_args "$plot_args -o $psfile"
            } else {
                set plot_args "$plot_args -o snp.ld.ps"
            }
            if {$absrho} {
                set plot_args "$plot_args -absrho"
            }
            if {$title != ""} {
                set plot_args "$plot_args -title \"$title\""
            }
            if {$date} {
                set plot_args "$plot_args -date"
            }
            if {$gray} {
                set plot_args "$plot_args -gray"
            }
            if {$nfont != ""} {
                set plot_args "$plot_args -nfont $nfont"
            }
            if {$lfont != ""} {
                set plot_args "$plot_args -lfont $lfont"
            }
            if {$ps_only} {
                set plot_args "$plot_args -ps_only"
            }
            eval exec snpplotk $plot_args &
        }

        return
    }

    if {$arg1 == "qtld"} {
        if {[catch {pedigree_loaded} errmsg]} {
            error $errmsg
        }

        if {![file exists snp.genocov]} {
            error \
    "SNP genotype covariates file not found. Run \"snp covar\" to create it."
        }

        set pedfile [tablefile open pedindex.out]
        set need_famid 0
        if {[tablefile $pedfile test_name FAMID]} {
            set need_famid 1
        }
        tablefile $pedfile start_setup
        tablefile $pedfile setup FIBDID
        tablefile $pedfile setup MIBDID
        tablefile $pedfile setup SEX
        tablefile $pedfile setup MZTWIN
        tablefile $pedfile setup PEDNO
        set nindt 0
        while {{} != [set record [tablefile $pedfile get]]} {
            incr nindt
            set fibdid($nindt) [lindex $record 0]
            set mibdid($nindt) [lindex $record 1]
            if {$fibdid($nindt)} {
                if {![info exists sibs($fibdid($nindt),$mibdid($nindt))]} {
                    set sibs($fibdid($nindt),$mibdid($nindt)) $nindt
                } else {
                    lappend sibs($fibdid($nindt),$mibdid($nindt)) $nindt
                }
            }
            set sex($nindt) [lindex $record 2]
            set mztwin($nindt) [lindex $record 3]
            if {$mztwin($nindt)} {
                if {![info exists twinid($mztwin($nindt))]} {
                    set twinid($mztwin($nindt)) $nindt
                } else {
                    lappend twinid($mztwin($nindt)) $nindt
                }
            }
            set pedno($nindt) [lindex $record 4]
        }
        tablefile $pedfile close

        if {[catch {set genfile [solarfile open snp.genocov]} errmsg]} {
            error "Cannot open snp.genocov: $errmsg"
        }
        solarfile $genfile start_setup
        if {$need_famid} {
            solarfile $genfile setup FAMID
        }
        solarfile $genfile setup ID
        set maplist {}
        foreach fld [solarfile $genfile names] {
            if {$fld == "famid" || $fld == "id" || $fld == "nGTypes"} {
                continue
            }
            if {[string range $fld 0 3] != "snp_"} {
                continue
            }
            set snp [string range $fld 4 end]
            lappend maplist $snp
            solarfile $genfile setup snp_$snp
            for {set i 1} {$i <= $nindt} {incr i} {
                set gtype($i,$snp) "*"
            }
        }

        while {{} != [set record [solarfile $genfile get]]} {
            if {$need_famid} {
                set ibdid [get_ibdid [lindex $record 0] [lindex $record 1]]
            } else {
                set ibdid [get_ibdid [lindex $record 0]]
            }
            for {set m 0} {$m < [llength $maplist]} {incr m} {
                set snp [lindex $maplist $m]
                if {$need_famid} {
                    set mm [expr $m + 2]
                } else {
                    set mm [expr $m + 1]
                }
                if {[lindex $record $mm] != ""} {
                    set gtype($ibdid,$snp) [expr [lindex $record $mm] - 1]
                } else {
                    set gtype($ibdid,$snp) "*"
                }
            }
        }
        solarfile $genfile close

        for {set m 0} {$m < [llength $maplist]} {incr m} {
            set snp [lindex $maplist $m]
            for {set i 1} {$i <= $nindt} {incr i} {
                if {!$fibdid($i) && $gtype($i,$snp) != "*"} {
                    set b($i,$snp) $gtype($i,$snp)
                    set w($i,$snp) 0
                    set b2($i,$snp) 0
                    set w2($i,$snp) $gtype($i,$snp)
                } elseif {[info exists b($fibdid($i),$snp)] \
                          && [info exists b($mibdid($i),$snp)] \
                          && $gtype($i,$snp) != "*"} {
                    set b($i,$snp) [expr .5*($b($fibdid($i),$snp) \
                                             + $b($mibdid($i),$snp))]
                    set w($i,$snp) [expr $gtype($i,$snp) - $b($i,$snp)]
                    set b2($i,$snp) $b($i,$snp)
                    set w2($i,$snp) $w($i,$snp)
                } elseif {$gtype($i,$snp) != "*"} {
                    set b($i,$snp) 0
                    set n 0
                    set j 0
                    while {$j < [llength $sibs($fibdid($i),$mibdid($i))]} {
                        set s [lindex $sibs($fibdid($i),$mibdid($i)) $j]
                        if {$gtype($s,$snp) != "*"} {
                            set b($i,$snp) [expr $b($i,$snp) + $gtype($s,$snp)]
                            incr n
                        }
                        incr j
                    }
                    if {$n} {
                        set b($i,$snp) [expr double($b($i,$snp))/$n]
                    }
                    set w($i,$snp) [expr $gtype($i,$snp) - $b($i,$snp)]
                    set b2($i,$snp) $b($i,$snp)
                    set w2($i,$snp) $w($i,$snp)
                }
            }
        }

        set outf [open snp.qtldcov w]
        if {$need_famid} {
            puts -nonewline $outf "FAMID,"
        }
        puts -nonewline $outf "ID"
        for {set i 0} {$i < [llength $maplist]} {incr i} {
            puts -nonewline $outf ",b_[lindex $maplist $i]"
        }
        for {set i 0} {$i < [llength $maplist]} {incr i} {
            puts -nonewline $outf ",w_[lindex $maplist $i]"
        }
        for {set i 0} {$i < [llength $maplist]} {incr i} {
            puts -nonewline $outf ",b2_[lindex $maplist $i]"
        }
        for {set i 0} {$i < [llength $maplist]} {incr i} {
            puts -nonewline $outf ",w2_[lindex $maplist $i]"
        }
        puts $outf ""
        for {set ibdid 1} {$ibdid <= $nindt} {incr ibdid} {
            set id [get_id $ibdid]
            if {$need_famid} {
                puts -nonewline $outf "[lindex $id 0],[lindex $id 1]"
            } else {
                puts -nonewline $outf $id
            }
            for {set i 0} {$i < [llength $maplist]} {incr i} {
                set snp [lindex $maplist $i]
                if {[info exists b($ibdid,$snp)]} {
                    puts -nonewline $outf ",[format %.10g $b($ibdid,$snp)]"
                } else {
                    puts -nonewline $outf ","
                }
            }
            for {set i 0} {$i < [llength $maplist]} {incr i} {
                set snp [lindex $maplist $i]
                if {[info exists w($ibdid,$snp)]} {
                    puts -nonewline $outf ",[format %.10g $w($ibdid,$snp)]"
                } else {
                    puts -nonewline $outf ","
                }
            }
            for {set i 0} {$i < [llength $maplist]} {incr i} {
                set snp [lindex $maplist $i]
                if {[info exists b2($ibdid,$snp)]} {
                    puts -nonewline $outf ",[format %.10g $b2($ibdid,$snp)]"
                } else {
                    puts -nonewline $outf ","
                }
            }
            for {set i 0} {$i < [llength $maplist]} {incr i} {
                set snp [lindex $maplist $i]
                if {[info exists w2($ibdid,$snp)]} {
                    puts -nonewline $outf ",[format %.10g $w2($ibdid,$snp)]"
                } else {
                    puts -nonewline $outf ","
                }
            }
            puts $outf ""
        }
        close $outf

        return
    }

    error "Invalid snp command"
}


proc polym_snps {} {
    set mapfile [open [map fname] r]
    gets $mapfile record
    set polym 0
    while {-1 != [gets $mapfile record]} {
        if {[cfreq nall [lindex $record 0]] > 1} {
            set polym 1
        }
    }
    close $mapfile
    return $polym
}


# solar::snphap --
#
# Purpose:  Compute SNP haplotypes and haplotype frequencies.
#
# Usage:    snphap show               ; displays summary of SNP haplotypes
#           snphap prep <program>     ; prepares input files needed to compute
#                                     ; SNP haplotypes using <program>, where
#                                     ; <program> is simwalk (sw) or merlin
#           snphap import <program> [-file <filename>] [-overwrite]
#                                     ; imports SNP haplotypes from an output
#                                     ; file computed by <program>, where
#                                     ; <program> is simwalk (sw) or merlin
#           snphap freq prep          ; prepares input file needed to compute
#                                     ; SNP haplotype frequencies using the
#                                     ; program snphap
#           snphap freq import [-file <filename>] [-overwrite]
#                                     ; imports SNP haplotype frequencies
#                                     ; from an output file generated by the
#                                     ; program snphap
#           snphap count              ; computes SNP haplotype frequencies by
#                                     ; counting haplotypes present in data
#           snphap covar              ; prepare haplotype covariates file
#
#           The 'snphap' command assumes that the 'snp load' command has been
#           used to load SNP genotype data.  The main purpose of this command
#           is to prepare the SNP haplotypes and haplotype frequencies files
#           used by the 'snp covar' command.  SOLAR does not do haplotyping
#           or haplotype frequency estimation itself, but rather relies on
#           these functions being provided by external programs.
#
#           The 'snphap prep' command is used to generate the input files for
#           a haplotyping run using either SimWalk2 or Merlin.  The output
#           file created by the haplotyping procedure is then imported into
#           SOLAR with the 'snphap import' command, which creates the file
#           'snp.haplotypes'.  The '-overwrite' option guards against the
#           unintentional replacement of an existing haplotypes file.
#
#           Haplotype frequencies can be generated in two ways.  The program
#           SNPHAP can be used to compute frequency estimates using an EM
#           algorithm with the assumption that all individuals are unrelated.
#           The 'snphap freq prep' command prepares the input file required
#           by SNPHAP.  The 'snphap freq import' command processes the SNPHAP
#           output to create the file 'snp.haplofreqs'.  Alternatively, the
#           'snphap count' command generates the haplotype frequencies file
#           by simply counting the haplotypes present in the haplotypes file
#           'snp.haplotypes'.  The haplotype frequencies file is sorted in
#           descending order of frequency, so that the most common haplotype
#           appears first.
#
#           The 'snphap covar' command generates a haplotype-based covariates
#           file suitable for use in a SOLAR analysis.  This file, which is
#           named 'snp.haplocov' includes a covariate field for each of the
#           haplotypes present in the file 'snp.haplotypes'.  These fields are
#           named hap_<hap#> where <hap#> is the position of the corresponding
#           haplotype in the file 'snp.haplofreqs'.  For example, the field
#           corresponding to the most frequent haplotype is named hap_1 and
#           the value in this field is the number of copies of this haplotype
#           that an individual possesses.  Covariates are defined only for
#           those individuals with two complete haplotypes in 'snp.haplotypes'
#           are included in the haplotype covariates file.
#
#-

proc snphap {args} {

    proc desc_sort_by_freq {a b} {
        if {[lindex $b 1] < [lindex $a 1]} {
            return -1
        } elseif {[lindex $b 1] > [lindex $a 1]} {
            return 1
        } elseif {[lindex $a 0] < [lindex $b 0]} {
            return -1
        } elseif {[lindex $a 0] > [lindex $b 0]} {
            return 1
        }
        return 0
    }

    set overwrite 0
    set fname ""
    set byped 0
    set nocode 0

    set arglist [ read_arglist $args \
                      -overwrite {set overwrite 1} -ov {set overwrite 1} \
                      -fname fname -f fname \
                      -nocode {set nocode 1} ]

    set arg1 [lindex $arglist 0]

    if {[llength $arglist] && [lindex $arglist 0] == "show"} {
        if {[catch {marker fname}]} {
            error "SNP genotypes have not been loaded."
        }

        if {[catch {set mapfname [map fname]}]} {
            error "SNP locations have not been loaded."
        }

        if {![file exists snp.haplofreqs]} {
            error "SNP haplotype frequencies have not been loaded."
        }
        if {[catch {set frqfile [tablefile open snp.haplofreqs]}]} {
            error "Cannot open file snp.haplofreqs."
        }

        if {[catch {set mapfile [open $mapfname r]}]} {
            error "Cannot open map data file $mapfname"
        }
        gets $mapfile record
        set maplist ""
        while {-1 != [gets $mapfile record]} {
            set snp [lindex $record 0]
            if {[cfreq nall $snp] < 2} {
                continue
            }
            lappend maplist $snp
        }
        close $mapfile

        set nsnp [llength $maplist]
        set maxnamelen 3
        for {set i 0} {$i < $nsnp} {incr i} {
            set snp [lindex $maplist $i]
            if {[string length $snp] > $maxnamelen} {
                set maxnamelen [string length $snp]
            }
        }

        for {set i 0} {$i < $nsnp} {incr i} {
            set snp [lindex $maplist $i]
            set alllist ""
            for {set j 0} {$j < [cfreq nall $snp]} {incr j} {
                 lappend alllist \
                     "[cfreq name $snp $j] [cfreq freq $snp $j]"
            }
            set alllist [lsort -command desc_sort_by_freq $alllist]
            for {set j 0} {$j < [llength $alllist]} {incr j} {
                set elem [lindex $alllist $j]
                set allele($i,$j) [lindex $elem 0]
            }
        }

        set fp [open snphap.show.tmp w]
        puts $fp ""
        for {set l 0} {$l < $maxnamelen} {incr l} {
            if {$l < [expr $maxnamelen - 1]} {
                puts -nonewline $fp "                      "
            } else {
                puts -nonewline $fp "Frequency  Cum.Freq.  "
            }
            for {set i 0} {$i < $nsnp} {incr i} {
                set snp [lindex $maplist $i]
                if {$l < [string length $snp]} {
                    puts -nonewline $fp "[string index $snp $l]"
                } else {
                    puts -nonewline $fp " "
                }
            }
	    puts $fp ""
        }
	puts $fp ""

        tablefile $frqfile start_setup
        for {set i 0} {$i < $nsnp} {incr i} {
            tablefile $frqfile setup [lindex $maplist $i]
        }
        tablefile $frqfile setup freq

        set nhap 0
        set cumfreq 0
        set hapdiv 1
        while {{} != [set record [tablefile $frqfile get]]} {
            incr nhap
            set freq [lindex $record $nsnp]
            puts -nonewline $fp " [format %8.6f $freq]   "
            set cumfreq [expr $cumfreq + $freq]
            if {$cumfreq > 1} {
                set cumfreq 1
            }
            set hapdiv [expr $hapdiv - pow($freq, 2)]
            foreach frac { .8 .9 .95 .99 } {
                if {![info exists cover($frac)] && $cumfreq >= $frac} {
                    set cover($frac) $nhap
                }
            }
            puts -nonewline $fp "[format %8.6f $cumfreq]  "
            for {set i 0} {$i < $nsnp} {incr i} {
                set all [lindex $record $i]
                if {$nocode} {
                    puts -nonewline $fp "$all"
                } elseif {$all == $allele($i,0)} {
                    puts -nonewline $fp "1"
                } else {
                    puts -nonewline $fp "2"
                }
            }
            puts $fp ""
        }

        close $fp
        tablefile $frqfile close

        set sfile snphap.show.txt
        set outfp [open $sfile w]
        puts $outfp ""
        puts $outfp "Total #Haplotypes:    $nhap"
        puts $outfp "Haplotype Diversity:  [format %g $hapdiv]"
        puts $outfp ""
        puts -nonewline $outfp "Per Cent Coverage: "
        foreach frac { .8 .9 .95 .99 } {
            puts -nonewline $outfp "   [format "%2d%%" [expr int(100*$frac)]]"
        }
        puts $outfp ""
        puts -nonewline $outfp "#Haplotypes Needed:"
        foreach frac { .8 .9 .95 .99 } {
            puts -nonewline $outfp "[format %6d $cover($frac)]"
        }
        puts $outfp ""

        set fp [open snphap.show.tmp r]
        while {-1 != [gets $fp record]} {
            puts $outfp $record
        }
        close $fp
        close $outfp
        delete_files_forcibly snphap.show.tmp

        exec >&@stdout <@stdin more $sfile
        puts "\nThis output is saved in file snphap.show.txt.\n"
        return
    }

    if {$arg1 == "covar"} {
        if {[catch {marker fname}]} {
            error "SNP genotypes have not been loaded."
        }

        if {[catch {map test}]} {
            error "SNP locations have not been loaded."
        }

        if {![file exists snp.haplotypes]} {
            error "SNP haplotypes have not been loaded."
        }

        csnp hapcov

        if {[catch {joinfiles snp.typed snp.haplocov}]} {
            error "Failed to join files snp.typed and snp.haplocov."
        }
        exec mv joinfiles.out snp.haplocov

        return
    }

    if {[llength $arglist] && [lindex $arglist 0] == "prep"} {
        if {[catch {marker fname}]} {
            error "SNP genotypes have not been loaded."
        }

        if {[catch {map test}]} {
            error "SNP locations have not been loaded."
        }

        if {![polym_snps]} {
            error "Cannot do haplotyping because there are no polymorphic SNPs."
        }

        if {[llength $arglist] == 1} {
            error "You must specify a haplotyping program."
        }

        if {[string tolower [lindex $arglist 1]] == "merlin"} {
            set snpfmt "merlin"
            puts "Preparing input files for SNP haplotyping using Merlin ..."
        } elseif {[string tolower [lindex $arglist 1]] == "simwalk" || \
                  [string tolower [lindex $arglist 1]] == "simwalk2" || \
                  [string tolower [lindex $arglist 1]] == "sw" || \
                  [string tolower [lindex $arglist 1]] == "sw2"} {
            set snpfmt "sw"
            puts "Preparing input files for SNP haplotyping using SimWalk2 ..."
        } else {
            error "SNP haplotyping using [lindex $arglist 1] is not supported."
        }

        if {[catch {pedigree_loaded} errmsg]} {
            error $errmsg
        }

        set infofile [open pedigree.info r]
        gets $infofile record
        gets $infofile record
        gets $infofile record
        scan $record "%d %d %d %d" nped nfamt nindt nfout
        set is_inbred 0
        set max_nbits 0
        for {set i 1} {$i <= $nped} {incr i} {
            set typed($i) 0
            gets $infofile record
            scan $record "%d %d %d %d %s" \
                 nfam($i) nind($i) nfou($i) nlbrk($i) inbred($i)
            if {$inbred($i) == "y"} {
                set is_inbred 1
            }
            set nbits($i) [expr 2*$nind($i) - 3*$nfou($i)]
            if {$nbits($i) > $max_nbits} {
                set max_nbits $nbits($i)
            }
        }
        close $infofile

        if {$is_inbred} {
            if {$snpfmt == "merlin"} {
                error \
"SNP haplotyping for inbred pedigrees using Merlin is not supported by SOLAR."
            }
        }

        set pedfile [tablefile open pedindex.out]
        tablefile $pedfile start_setup
        tablefile $pedfile setup FIBDID
        tablefile $pedfile setup MIBDID
        tablefile $pedfile setup SEX
        tablefile $pedfile setup MZTWIN
        tablefile $pedfile setup PEDNO
        for {set i 1} {$i <= $nindt} {incr i} {
            set record [tablefile $pedfile get]
            set fibdid($i) [lindex $record 0]
            set mibdid($i) [lindex $record 1]
            set sex($i) [lindex $record 2]
            set mztwin($i) [lindex $record 3]
            if {$mztwin($i) != 0} {
                if {![info exists twinid($mztwin($i))]} {
                    set twinid($mztwin($i)) $i
                } else {
                    lappend twinid($mztwin($i)) $i
                }
            }
            set pedno($i) [lindex $record 4]
            set gtype($i) {}
        }
        tablefile $pedfile close

        if {[catch {set frqfile [open freq.info r]}]} {
            error "SNP genotypes have not been loaded."
        }
        gets $frqfile record
        while {-1 != [gets $frqfile record]} {
            set snp [lindex $record 0]
            set alllist($snp) {}
            set frqlist($snp) {}
            for {set i 4} {$i < [llength $record]} {incr i} {
                set allname [lindex $record $i]
                if {$allname == "se=" || $allname == "hwe="} {
                    incr i
                    continue
                }
                lappend alllist($snp) $allname
                incr i
                lappend frqlist($snp) [lindex $record $i]
            }
        }
        close $frqfile

        if {[catch {set mapfname [map fname]}]} {
            error "SNP locations have not been loaded."
        }
        if {[catch {set mapfile [open $mapfname r]}]} {
            error "Cannot open map data file $mapfname"
        }
        gets $mapfile record
        set chrnum [lindex $record 0]
        set maplist ""
        while {-1 != [gets $mapfile record]} {
            set snp [lindex $record 0]
            if {[cfreq nall $snp] < 2} {
                continue
            }
            lappend maplist $snp
        }
        close $mapfile

        for {set i 0} {$i < [llength $maplist]} {incr i} {
            set snp [lindex $maplist $i]
            set genfname d_$snp/translat.tab
            if {[catch {set genfile [open $genfname r]}]} {
                error "Cannot open file $genfname"
            }
            gets $genfile genrec
            gets $genfile genrec
            set ibdid 0
            while {-1 != [gets $genfile genrec]} {
                set n [lindex $genrec 0]
                for {set j 0} {$j < $n} {incr j} {
                    incr ibdid
                    gets $genfile genrec
                    set a1 [string range $genrec 19 21]
                    set a2 [string range $genrec 22 24]
                    if {$a1 != "   "} {
                        if {$snpfmt == "merlin"} {
                            eval lappend gtype($ibdid) [format "%d %d" $a1 $a2]
                        } else {
                            lappend gtype($ibdid) [format %d/%d $a1 $a2]
                        }
                    } else {
                        if {$snpfmt == "merlin"} {
                            eval lappend gtype($ibdid) "0 0"
                        } else {
                            lappend gtype($ibdid) "*"
                        }
                    }
                }
            }
            close $genfile
        }

        if {$snpfmt == "merlin"} {
            set outf [open mlsnphap.map w]
            for {set i 0} {$i < [llength $maplist]} {incr i} {
                set snp [lindex $maplist $i]
                puts $outf "$chrnum $snp 0"
            }
            close $outf

            set outf [open mlsnphap.frq w]
            set outf2 [open mlsnphap.dat w]
            for {set i 0} {$i < [llength $maplist]} {incr i} {
                set snp [lindex $maplist $i]
                puts $outf "M $snp"
                puts $outf2 "M $snp"
                for {set j 0} {$j < [llength $alllist($snp)]} {incr j} {
                    puts $outf "A [expr $j + 1] [lindex $frqlist($snp) $j]"
                }
            }
            close $outf2
            close $outf

            if {$byped} {
                set currped 0
            } else {
                set outf [open mlsnphap.ped w]
            }
            for {set i 1} {$i <= $nindt} {incr i} {
                if {$mztwin($i) && $i != [lindex $twinid($mztwin($i)) 0]} {
                    continue
                }
                if {$byped && $pedno($i) != $currped} {
                    if {$currped} {
                        close $outf
                    }
                    set currped $pedno($i)
                    set outf [open mlsnphap.ped.$currped w]
                }
                set fa $fibdid($i)
                if {$fa != 0 && $mztwin($fa)} {
                    set fa [lindex $twinid($mztwin($fa)) 0]
                }
                set mo $mibdid($i)
                if {$mo != 0 && $mztwin($mo)} {
                    set mo [lindex $twinid($mztwin($mo)) 0]
                }
                puts $outf "$pedno($i) $i $fa $mo $sex($i) $gtype($i)"
            }
            close $outf

            set outf [open mlsnphap.cmd w]
            puts $outf \
"merlin -dmlsnphap.dat -pmlsnphap.ped -fmlsnphap.frq -mmlsnphap.map --best --bits $max_nbits"
            close $outf

            puts "The following files have been created:"
            puts "    mlsnphap.cmd              - Merlin haplotyping command"
            puts "    mlsnphap.dat              - data description file"
            puts "    mlsnphap.ped              - pedigree/genotype data"
            puts "    mlsnphap.frq              - allele frequency file"
            puts "    mlsnphap.map              - map file"

        } elseif {$snpfmt == "sw"} {
            set maplist8 ""
            for {set i 0} {$i < [llength $maplist]} {incr i} {
                set snp [lindex $maplist $i]
                lappend maplist8 [string range $snp 0 7]
            }
            set maplist8 [lsort $maplist8]
            set snp [lindex $maplist8 0]
            for {set i 1} {$i < [llength $maplist8]} {incr i} {
                if {[lindex $maplist8 $i] == $snp} {
                    error \
            "For SimWalk2, SNP names must be unique in first 8 characters."
                }
                set snp [lindex $maplist8 $i]
            }

            set outf [open BATCH2.DAT w]
            puts $outf ""
            puts $outf "01"
            puts $outf "1"
            puts $outf ""
            puts $outf "02"
            puts $outf "1"
            puts $outf ""
            puts $outf "03"
            puts $outf "SNP Haplotyping"
            puts $outf ""
            puts $outf "09"
            puts $outf "swsnphap.map"
            puts $outf ""
            puts $outf "10"
            puts $outf "swsnphap.loc"
            puts $outf ""
            puts $outf "11"
            puts $outf "swsnphap.ped"
            puts $outf ""
            puts $outf "12"
            puts $outf "2"
            puts $outf "1"
            puts $outf ""
            puts $outf "13"
            puts $outf "y"
            close $outf

            set outf [open swsnphap.map w]
            puts $outf AFFECTED
            puts $outf [format "        %8.5f" 0.5]
            puts $outf [lindex $maplist 0]
            for {set i 1} {$i < [llength $maplist]} {incr i} {
                set snp [lindex $maplist $i]
                puts $outf [format "        %8.5f" 0.]
                puts $outf $snp
            }
            close $outf

            set outf [open swsnphap.loc w]
            puts $outf "AFFECTEDAUTOSOME 2 2"
            puts $outf "       1 0.99990"
            puts $outf "       2 0.00010"
            puts $outf " 1       1"
            puts $outf "  1/  1"
            puts $outf " 2       1"
            puts $outf "  1/  2"
            for {set i 0} {$i < [llength $maplist]} {incr i} {
                set snp [lindex $maplist $i]
                puts $outf \
                    [format "%-8.8sAUTOSOME%2d 0" $snp [llength $alllist($snp)]]
                for {set j 0} {$j < [llength $alllist($snp)]} {incr j} {
                    puts $outf [format "%5d   %8.5f" [expr $j + 1] \
                                [lindex $frqlist($snp) $j]]
                }
            }
            close $outf

            if {!$byped} {
                set outf [open swsnphap.ped w]
                puts $outf "(I6,2X,A8)"
                puts $outf [format "(3A5,A1,A3,A1,%dA5)" [llength $maplist]]
            }
            set ibdid 0
            for {set i 1} {$i <= $nped} {incr i} {
                if {$byped} {
                    if {$i > 1} {
                        close $outf
                    }
                    set outf [open swsnphap.ped.$i w]
                    puts $outf "(I6,2X,A8)"
                    puts $outf [format "(3A5,A1,A3,A1,%dA5)" [llength $maplist]]
                }
                set n $nind($i)
                set ii $ibdid
                for {set j 0} {$j < $n} {incr j} {
                    incr ii
                    if {$mztwin($ii) && \
                            $ii != [lindex $twinid($mztwin($ii)) 0]} {
                        set n [expr $n - 1]
                    }
                }
                puts $outf [format "%6d  FAM%05d" $n $i]
                for {set j 0} {$j < $nind($i)} {incr j} {
                    incr ibdid
                    if {$mztwin($ibdid) && \
                            $ibdid != [lindex $twinid($mztwin($ibdid)) 0]} {
                        continue
                    }
                    set fa ""
                    if {$fibdid($ibdid) != 0} {
                        set fa $fibdid($ibdid)
                        if {$mztwin($fa)} {
                            set fa [lindex $twinid($mztwin($fa)) 0]
                        }
                    }
                    set mo ""
                    if {$mibdid($ibdid) != 0} {
                        set mo $mibdid($ibdid)
                        if {$mztwin($mo)} {
                            set mo [lindex $twinid($mztwin($mo)) 0]
                        }
                    }
                    set twin ""
                    puts -nonewline $outf [format \
                        "%5s%5s%5s%1s%3s2" $ibdid $fa $mo $sex($ibdid) $twin]
                    for {set k 0} {$k < [llength $maplist]} {incr k} {
                        set snp [lindex $maplist $k]
                        if {[lindex $gtype($ibdid) $k] == "*"} {
                            set geno ""
                        } else {
                            scan [lindex $gtype($ibdid) $k] "%d/%d" a1 a2
                            set geno [format "%2d/%2d" $a1 $a2]
                        }
                        puts -nonewline $outf [format "%5s" $geno]
                    }
                    puts $outf ""
                }
            }
            close $outf

            puts "The following files have been created:"
            puts "    BATCH2.DAT                - control file"
            puts "    swsnphap.map              - map file"
            puts "    swsnphap.loc              - locus file"
            puts "    swsnphap.ped              - pedigree/genotype data"
        }
        return
    }

    if {$arg1 == "import"} {
        if {[catch {marker fname}]} {
            error "SNP genotypes have not been loaded."
        }

        if {[llength $arglist] == 1} {
            error "You must specify a haplotyping program."
        }

        if {[file exists snp.haplotypes] && !$overwrite} {
            error \
            "SNP haplotypes have already been imported. Use -overwrite option."
        }

        if {[string tolower [lindex $arglist 1]] == "merlin"} {
            set snpfmt "merlin"
            puts "Importing SNP haplotypes from Merlin ..."
        } elseif {[string tolower [lindex $arglist 1]] == "simwalk" || \
                  [string tolower [lindex $arglist 1]] == "simwalk2" || \
                  [string tolower [lindex $arglist 1]] == "sw" || \
                  [string tolower [lindex $arglist 1]] == "sw2"} {
            set snpfmt "sw"
            puts "Importing SNP haplotypes from SimWalk2 ..."
        } else {
            error "SNP haplotyping using [lindex $arglist 1] is not supported."
        }

        if {[catch {pedigree_loaded} errmsg]} {
            error $errmsg
        }

        set pedfile [tablefile open pedindex.out]
        set need_famid 0
        if {[tablefile $pedfile test_name FAMID]} {
            set need_famid 1
        }
        tablefile $pedfile close

        if {[catch {set frqfile [open freq.info r]}]} {
            error "SNP genotypes have not been loaded."
        }
        gets $frqfile record
        while {-1 != [gets $frqfile record]} {
            set snp [lindex $record 0]
            set alllist($snp) ""
            for {set i 4} {$i < [llength $record]} {incr i} {
                set allname [lindex $record $i]
                if {$allname != "se=" && $allname != "hwe="} {
                    lappend alllist($snp) $allname
                }
                incr i
            }
        }
        close $frqfile

        if {[catch {set mapfname [map fname]}]} {
            error "SNP locations have not been loaded."
        }
        if {[catch {set mapfile [open $mapfname r]}]} {
            error "Cannot open map data file $mapfname"
        }
        gets $mapfile record
        set maplist ""
        while {-1 != [gets $mapfile record]} {
            set snp [lindex $record 0]
            if {[cfreq nall $snp] < 2} {
                continue
            }
            lappend maplist $snp
        }
        close $mapfile
        set nsnp [llength $maplist]

        if {$snpfmt == "merlin"} {
            if {$fname == ""} {
                set fname merlin.chr
            }
            if {[file exists $fname]} {
                if {[file extension $fname] == ".gz"} {
                    set hapfile [open "|gunzip -c $fname" r]
                } else {
                    set hapfile [open $fname r]
                }
            } elseif {[file exists $fname.gz]} {
                set hapfile [open "|gunzip -c $fname" r]
            } else {
                error "Cannot open file $fname"
            }

            set outfp [open snp.haplotypes w]
            if {$need_famid} {
                puts -nonewline $outfp "FAMID,"
            }
            puts -nonewline $outfp "ID"
            for {set i 0} {$i < $nsnp} {incr i} {
                puts -nonewline $outfp ",[lindex $maplist $i]"
            }
            puts $outfp ""

            while {-1 != [gets $hapfile record]} {
                if {[llength $record] && [lindex $record 0] != "FAMILY"} {
                    set n [expr [llength $record] / 2]
                    for {set j 0} {$j < $n} {incr j} {
                        set ibdid [lindex $record [expr $j * 2]]
                        set id($j) [get_id $ibdid]
                    }
                    for {set i 0} {$i < $nsnp} {incr i} {
                        set snp [lindex $maplist $i]
                        gets $hapfile record
                        for {set j 0} {$j < $n} {incr j} {
                            set hap [lindex $record [expr $j*3]]
                            if {$hap == "?"} {
                                set hap1($j,$i) ""
                            } elseif {[string match {[1-9],[1-9][A-Z]} $hap]} {
                                set hap [string index $hap 0]
                                set hap1($j,$i) \
                                    [lindex $alllist($snp) [expr $hap - 1]]
                            } elseif {[string match {[A-Z][1-9],[1-9]} $hap]} {
                                set hap [string index $hap 1]
                                set hap1($j,$i) \
                                    [lindex $alllist($snp) [expr $hap - 1]]
                            } else {
                                set hap1($j,$i) \
                                    [lindex $alllist($snp) [expr $hap - 1]]
                            }
                            set hap [lindex $record [expr $j*3 + 2]]
                            if {$hap == "?"} {
                                set hap2($j,$i) ""
                            } elseif {[string match {[1-9],[1-9][A-Z]} $hap]} {
                                set hap [string index $hap 0]
                                set hap2($j,$i) \
                                    [lindex $alllist($snp) [expr $hap - 1]]
                            } elseif {[string match {[A-Z][1-9],[1-9]} $hap]} {
                                set hap [string index $hap 1]
                                set hap2($j,$i) \
                                    [lindex $alllist($snp) [expr $hap - 1]]
                            } else {
                                set hap2($j,$i) \
                                    [lindex $alllist($snp) [expr $hap - 1]]
                            }
                        }
                    }
                    for {set j 0} {$j < $n} {incr j} {
                        if {$need_famid} {
                            puts -nonewline $outfp \
                                "[lindex $id($j) 0],[lindex $id($j) 1]"
                        } else {
                            puts -nonewline $outfp $id($j)
                        }
                        for {set i 0} {$i < $nsnp} {incr i} {
                            puts -nonewline $outfp ",$hap1($j,$i)"
                        }
                        puts $outfp ""
                        if {$need_famid} {
                            puts -nonewline $outfp \
                                "[lindex $id($j) 0],[lindex $id($j) 1]"
                        } else {
                            puts -nonewline $outfp $id($j)
                        }
                        for {set i 0} {$i < $nsnp} {incr i} {
                            puts -nonewline $outfp ",$hap2($j,$i)"
                        }
                        puts $outfp ""
                    }
                }
            }
            close $hapfile
            close $outfp

        } elseif {$snpfmt == "sw"} {
            if {$fname == ""} {
                set fname HAPLO-01.001
            }
            if {[file exists $fname]} {
                if {[file extension $fname] == ".gz"} {
                    set hapfile [open "|gunzip -c $fname" r]
                } else {
                    set hapfile [open $fname r]
                }
            } elseif {[file exists $fname.gz]} {
                set hapfile [open "|gunzip -c $fname" r]
            } else {
                error "Cannot open file $fname"
            }

            set outfp [open snp.haplotypes w]
            if {$need_famid} {
                puts -nonewline $outfp "FAMID,"
            }
            puts -nonewline $outfp "ID"
            for {set i 0} {$i < $nsnp} {incr i} {
                puts -nonewline $outfp ",[lindex $maplist $i]"
            }
            puts $outfp ""

            set which 0
            set missing 1
            while {-1 != [gets $hapfile record]} {
                if {$nsnp <= 60} {
                    set char1 [string range $record 0 0]
                    if {$char1 == "M" || $char1 == "P"} {
                        set n [llength $record]
                        if {$n == [expr 4 + $nsnp]} {
                            set ibdid [lindex $record 1]
                            set fibdid 0
                            set mibdid 0
                            set sex [lindex $record 2]
                            set affect [lindex $record 3]
                            set ihap 4
                        } elseif {$n == [expr 6 + $nsnp]} {
                            set ibdid [lindex $record 1]
                            set fibdid [lindex $record 2]
                            set mibdid [lindex $record 3]
                            set sex [lindex $record 4]
                            set affect [lindex $record 5]
                            set ihap 6
                        } else {
                            error \
    "Error reading haplotypes file: incorrect haplotype length (IBDID = [lindex $record 1])"
                        }
                        for {set i 0} {$i < $nsnp} {incr i} {
                            if {[lindex $record $ihap] != "@"} {
                                set missing 0
                            }
                            if {$which} {
                                lappend hap2 [lindex $record $ihap]
                            } else {
                                lappend hap1 [lindex $record $ihap]
                            }
                            incr ihap
                        }
                        if {$which} {
                            if {!$missing} {
                                set id [get_id $ibdid]
                                if {$need_famid} {
                                    puts -nonewline $outfp \
                                        "[lindex $id 0],[lindex $id 1]"
                                } else {
                                    puts -nonewline $outfp $id
                                }
                                for {set i 0} {$i < $nsnp} {incr i} {
                                    set snp [lindex $maplist $i]
                                    set hap [lindex $hap1 $i]
                                    if {$hap == "@"} {
                                        puts -nonewline $outfp ","
                                    } else {
                                        puts -nonewline $outfp \
                                            ",[lindex $alllist($snp) [expr $hap - 1]]"
                                    }
                                }
                                puts $outfp ""
                                if {$need_famid} {
                                    puts -nonewline $outfp \
                                        "[lindex $id 0],[lindex $id 1]"
                                } else {
                                    puts -nonewline $outfp $id
                                }
                                for {set i 0} {$i < $nsnp} {incr i} {
                                    set snp [lindex $maplist $i]
                                    set hap [lindex $hap2 $i]
                                    if {$hap == "@"} {
                                        puts -nonewline $outfp ","
                                    } else {
                                        puts -nonewline $outfp \
                                            ",[lindex $alllist($snp) [expr $hap - 1]]"
                                    }
                                }
                                puts $outfp ""
                            }
                            set which 0
                            set missing 1
                            set hap1 {}
                            set hap2 {}
                        } else {
                            set which 1
                        }
                    }
                } else {
                    if {[regexp {[a-zA-Z_:]} $record]} {
                        continue
                    }
                    regsub {[\\]} $record / newrec
                    set n [llength $newrec]
                    if {$n != 3 && $n != 5 && \
                            ![regexp {[|/+]} [lindex $newrec 1]]} {
                        continue
                    }
                    if {[regexp {[|/+]} [lindex $newrec 1]]} {
                        lappend hap1 [lindex $newrec 0]
                        lappend hap2 [lindex $newrec 2]
                        incr isnp
                    } else {
                        if {[info exists isnp]} {
                            if {$isnp != $nsnp} {
                                error \
    "Error reading haplotypes file: incorrect haplotype length (IBDID = $ibdid)"
                            }
                            if {$need_famid} {
                                puts -nonewline $outfp \
                                    "[lindex $id 0],[lindex $id 1]"
                            } else {
                                puts -nonewline $outfp $id
                            }
                            for {set i 0} {$i < $nsnp} {incr i} {
                                set snp [lindex $maplist $i]
                                set hap [lindex $hap1 $i]
                                if {$hap == "@"} {
                                    puts -nonewline $outfp ","
                                } else {
                                    puts -nonewline $outfp \
                                        ",[lindex $alllist($snp) [expr $hap - 1]]"
                                }
                            }
                            puts $outfp ""
                            if {$need_famid} {
                                puts -nonewline $outfp \
                                    "[lindex $id 0],[lindex $id 1]"
                            } else {
                                puts -nonewline $outfp $id
                            }
                            for {set i 0} {$i < $nsnp} {incr i} {
                                set snp [lindex $maplist $i]
                                set hap [lindex $hap2 $i]
                                if {$hap == "@"} {
                                    puts -nonewline $outfp ","
                                } else {
                                    puts -nonewline $outfp \
                                        ",[lindex $alllist($snp) [expr $hap - 1]]"
                                }
                            }
                            puts $outfp ""
                        }
                        set ibdid [lindex $newrec 0]
                        set id [get_id $ibdid]
                        if {$n == 3} {
                            set fibdid 0
                            set mibdid 0
                            set sex [lindex $newrec 1]
                            set affect [lindex $newrec 2]
                        } else {
                            set fibdid [lindex $newrec 1]
                            set mibdid [lindex $newrec 2]
                            set sex [lindex $newrec 3]
                            set affect [lindex $newrec 4]
                        }
                        set isnp 0
                        set hap1 {}
                        set hap2 {}
                    }
                }
            }

            if {$nsnp > 60} {
                if {$isnp != $nsnp} {
                    error \
    "Error reading haplotypes file: incorrect haplotype length (IBDID = $ibdid)"
                }
                if {$need_famid} {
                    puts -nonewline $outfp "[lindex $id 0],[lindex $id 1]"
                } else {
                    puts -nonewline $outfp $id
                }
                for {set i 0} {$i < $nsnp} {incr i} {
                    set snp [lindex $maplist $i]
                    set hap [lindex $hap1 $i]
                    if {$hap == "@"} {
                        puts -nonewline $outfp ","
                    } else {
                        puts -nonewline $outfp \
                            ",[lindex $alllist($snp) [expr $hap - 1]]"
                    }
                }
                puts $outfp ""
                if {$need_famid} {
                    puts -nonewline $outfp "[lindex $id 0],[lindex $id 1]"
                } else {
                    puts -nonewline $outfp $id
                }
                for {set i 0} {$i < $nsnp} {incr i} {
                    set snp [lindex $maplist $i]
                    set hap [lindex $hap2 $i]
                    if {$hap == "@"} {
                        puts -nonewline $outfp ","
                    } else {
                        puts -nonewline $outfp \
                            ",[lindex $alllist($snp) [expr $hap - 1]]"
                    }
                }
                puts $outfp ""
            }

            close $hapfile
            close $outfp
        }

        return
    }

    if {[llength $arglist] && [lindex $arglist 0] == "count"} {
        if {[catch {marker fname}]} {
            error "SNP genotypes have not been loaded."
        }

        if {[catch {set mapfname [map fname]}]} {
            error "SNP locations have not been loaded."
        }

        if {[catch {set mapfile [open $mapfname r]}]} {
            error "Cannot open map data file $mapfname"
        }
        gets $mapfile record
        set maplist ""
        while {-1 != [gets $mapfile record]} {
            set snp [lindex $record 0]
            if {[cfreq nall $snp] < 2} {
                continue
            }
            lappend maplist $snp
        }
        close $mapfile
        set nsnp [llength $maplist]

        if {![file exists snp.haplotypes]} {
            error "SNP haplotypes have not been loaded."
        }
        if {[catch {set hapfile [tablefile open snp.haplotypes]}]} {
            error "Cannot open file snp.haplotypes."
        }

        if {[file exists snp.haplofreqs] && !$overwrite} {
            error \
"SNP haplotype frequencies have already been estimated. Use -overwrite option."
        }

        tablefile $hapfile start_setup
        for {set i 0} {$i < $nsnp} {incr i} {
            tablefile $hapfile setup [lindex $maplist $i]
        }

        set nhap 0
        while {{} != [set record [tablefile $hapfile get]]} {
            set hap ""
            set missing 0
            for {set i 0} {$i < $nsnp} {incr i} {
                set all [lindex $record $i]
                if {$all == ""} {
                    set missing 1
                    break
                }
                set hap $hap${all}_
            }
            if {!$missing} {
                incr nhap
                if {![info exists cnt($hap)]} {
                    set cnt($hap) 1
                } else {
                    incr cnt($hap)
                }
            }
        }
        tablefile $hapfile close

        set haplist {}
        foreach hap [array names cnt] {
            lappend haplist "$hap [expr double($cnt($hap))/$nhap]"
        }
        set haplist [lsort -command desc_sort_by_freq $haplist]

        set outfp [open snp.haplofreqs w]
        for {set i 0} {$i < [llength $maplist]} {incr i} {
            puts -nonewline $outfp "[lindex $maplist $i],"
        }
        puts $outfp "freq"

        for {set i 0} {$i < [llength $haplist]} {incr i} {
            set elem [lindex $haplist $i]
            set hap [lindex $elem 0]
            for {set j 0} {$j < [string length $hap]} {incr j} {
                set char [string index $hap $j]
                if {$char == "_"} {
                    puts -nonewline $outfp ","
                } else {
                    puts -nonewline $outfp $char
                }
            }
            puts $outfp [lindex $elem 1]
        }

        close $outfp

        return
    }

    if {[llength $arglist] && [lindex $arglist 0] == "freq"} {
        if {[llength $arglist] < 2} {
            error "Invalid snphap command"
        }

        if {[catch {marker fname}]} {
            error "SNP genotypes have not been loaded."
        }

        if {[catch {set mapfname [map fname]}]} {
            error "SNP locations have not been loaded."
        }

        if {[lindex $arglist 1] == "prep"} {
            if {![polym_snps]} {
                error \
    "Cannot estimate haplotype frequencies because there are no polymorphic SNPs."
            }

            puts \
    "Preparing input files for haplotype frequency estimation using SNPHAP ..."

            if {[catch {set mapfname [map fname]}]} {
                error "SNP locations have not been loaded."
            }
            if {[catch {set mapfile [open $mapfname r]}]} {
                error "Cannot open map data file $mapfname"
            }
            gets $mapfile record
            set maplist ""
            while {-1 != [gets $mapfile record]} {
                set snp [lindex $record 0]
                if {[cfreq nall $snp] < 2} {
                    continue
                }
                lappend maplist $snp
                set revall 0
                if {[cfreq freq $snp 0] < [cfreq freq $snp 1]} {
                    set revall 1
                }
                set genfname d_[lindex $record 0]/translat.tab
                if {[catch {set genfile [open $genfname r]}]} {
                    error "Cannot open file $genfname"
                }
                gets $genfile genrec
                gets $genfile genrec
                set ibdid 0
                while {-1 != [gets $genfile genrec]} {
                    set n [lindex $genrec 0]
                    for {set i 0} {$i < $n} {incr i} {
                        incr ibdid
                        gets $genfile genrec
                        set a1 [string range $genrec 19 21]
                        set a2 [string range $genrec 22 24]
                        if {$a1 != "   "} {
                            if {$revall} {
                                if {$a1 == "1"} {
                                    set a1 "2"
                                } else {
                                    set a1 "1"
                                }
                                if {$a2 == "1"} {
                                    set a2 "2"
                                } else {
                                    set a2 "1"
                                }
                            }
                            eval lappend gtype($ibdid) [format "%d %d" $a1 $a2]
                        } else {
                            eval lappend gtype($ibdid) "0 0"
                        }
                    }
                }
                close $genfile
                set nindt $ibdid
            }
            close $mapfile

            set outf [open snphap.dat w]
            puts -nonewline $outf [lindex $maplist 0]
            for {set i 1} {$i < [llength $maplist]} {incr i} {
                puts -nonewline $outf " [lindex $maplist $i]"
            }
            puts $outf ""

            for {set i 1} {$i <= $nindt} {incr i} {
                puts $outf "$i $gtype($i)"
            }
            close $outf

        } elseif {[lindex $arglist 1] == "import"} {
            if {[catch {set mapfname [map fname]}]} {
                error "SNP locations have not been loaded."
            }
            if {[catch {set mapfile [open $mapfname r]}]} {
                error "Cannot open map data file $mapfname"
            }
            gets $mapfile record
            set maplist ""
            while {-1 != [gets $mapfile record]} {
                set snp [lindex $record 0]
                if {[cfreq nall $snp] < 2} {
                    continue
                }
                lappend maplist $snp
            }
            close $mapfile

            if {[catch {set frqfile [open freq.info r]}]} {
                error "SNP genotypes have not been loaded."
            }
            gets $frqfile record
            while {-1 != [gets $frqfile record]} {
                set snp [lindex $record 0]
                set alllist($snp) ""
                for {set i 4} {$i < [llength $record]} {incr i} {
                    set allname [lindex $record $i]
                    if {$allname != "se=" && $allname != "hwe="} {
                        lappend alllist($snp) $allname
                    }
                    incr i
                }
            }
            close $frqfile

            if {[file exists snp.haplofreqs] && !$overwrite} {
                error \
"SNP haplotype frequencies have already been estimated. Use -overwrite option."
            }

            if {$fname == ""} {
                set fname snphap.out
            }
            if {[catch {set infp [open $fname r]}]} {
                error "Cannot open file $fname"
            }
            while {-1 != [gets $infp record]} {
                set n [expr [llength $record] - 1]
                if {[lindex $record $n] == "Cumulative"} {
                    gets $infp record
                    break
                }
            }

            set nhap 0
            set sum 0.
            while {-1 != [gets $infp record]} {
                set hap($nhap) [lindex $record 0]
                set freq($nhap) [lindex $record 1]
                set sum [expr $sum + $freq($nhap)]
                incr nhap
            }
            close $infp
            set freq(0) [expr $freq(0) + 1. - $sum]

            set outfp [open snp.haplofreqs w]
            for {set i 0} {$i < [llength $maplist]} {incr i} {
                puts -nonewline $outfp "[lindex $maplist $i],"
            }
            puts $outfp "freq"
            for {set i 0} {$i < $nhap} {incr i} {
                for {set j 0} {$j < [llength $maplist]} {incr j} {
                    set snp [lindex $maplist $j]
                    set revall 0
                    if {[cfreq freq $snp 0] < [cfreq freq $snp 1]} {
                        set revall 1
                    }
                    set iall [string range $hap($i) $j $j]
                    if {$revall} {
                        if {$iall == 1} {
                            set iall 2
                        } elseif {$iall == 2} {
                            set iall 1
                        }
                    }
                    puts -nonewline $outfp \
                        "[lindex $alllist($snp) [expr $iall - 1]],"
                }
                puts $outfp [format %8.6f $freq($i)]
            }
            close $outfp
        } else {
            error "Invalid snphap command"
        }
        return
    }

    error "Invalid snphap command"
}


# solar::simqtl --
#
# Purpose:  Simulate a QTL and (optionally) a linked marker
#
# Usage:    simqtl [-seed <seed>] [-inform] [-gfile <genotype_file>]
#
#           simqtl -model
#
#           simqtl -freq {<f_1> ...} [-mfreq {<f_1> ...} [-theta <theta>]]
#                  [-ntrt <#traits>] -mean {{<m1_AA> <m1_Aa> <m1_aa>} ...}
#                  -sdev {<sd_1> ...} [-cov {<cov1> ...}]
#                  [-beta {{<b1_AA> <b1_Aa> <b1_aa>} ...}]
#                  [-cmean {<cov1_mean> ...}] [-mage {<mean_age>}]
#                  [-rhog {<gen_corr_2,1> <gen_corr_3,1> <gen_corr_3,2> ...}]
#                  [-rhoe {<env_corr_2,1> <env_corr_3,1> <env_corr_3,2> ...}]
#                  [-h2r {<h2r_1> ...}]
#
#           simqtl -nloc <#QTLs> -nall {<nall_1> ...}
#                  [-ntrt <#traits>] -mean {{<m1_AA> <m1_Aa> <m1_aa>} ...}
#                  -sdev {<sd_1> ...} [-cov {<cov1> ...}]
#                  [-beta {{<b1_AA> <b1_Aa> <b1_aa>} ...}]
#                  [-cmean {<cov1_mean> ...}] [-mage {<mean_age>}]
#                  [-rhog {<gen_corr_2,1> <gen_corr_3,1> <gen_corr_3,2> ...}]
#                  [-rhoe {<env_corr_2,1> <env_corr_3,1> <env_corr_3,2> ...}]
#                  [-h2r {<h2r_1> ...}]
#
#           There are two steps in the simulation process: (1) specifying
#           the simulation model, and (2) running the simulation. The first
#           form of the command shown is used to run the simulation, and
#           takes the following optional arguments:
#
#             -seed    An integer seed for the random number generator.
#
#             -inform  If this argument is given, the simulated marker
#                      genotypes will be fully informative.
#
#             -gfile   For models of the second type described below, QTL
#                      genotypes are read from a file rather than simulated.
#                      This argument specifies the name of this file.
#
#           The simulated trait values are written to the file "simqtl.phn".
#           A simulated trait value will not be assigned to any individual
#           who has an unknown age, or who is missing data for any other
#           covariate specified in the simulation model. If QTL genotypes
#           are simulated, they will be written to the file "simqtl.qtl".
#           If the model includes a linked marker, the simulated marker
#           genotypes are written to "simqtl.mrk". Two additional files are
#           created and used by this command: "simqtl.dat" and "simqtl.par",
#           which contain pedigree/covariate data and the model parameters,
#           respectively. All of these files are created in the current
#           working directory.
#
#           If QTL genotypes are read from a file, that file must contain
#           an ID field, a FAMID field (if required - see the documentation
#           for marker files), and, for each QTL, a field containing the
#           QTL genotype. Unlike SOLAR marker genotypes in general, the
#           QTL genotypes must have integer alleles numbered consecutively
#           beginning with 1. Also, if there are multiple QTLs, the position
#           of the alleles is significant. For example, the QTL genotypes
#           1/3 and 2/1 are combined to form the two-locus genotype 12/31,
#           while genotypes 3/1 and 2/1 yield the two-locus genotype 32/11.
#
#           The second form of the command displays the simulation model
#           parameters.
#
#           The remaining forms of the command are for the two types of
#           simulation model that may be specified. In the first model, a
#           single QTL and, optionally, a single linked marker are simulated.
#           One or more correlated quantitative traits will be generated,
#           along with a polygenic background. Adjustments may be made to
#           the trait means for covariate effects. The covariates sex and
#           age are always included although no adjustments need be specified
#           for these covariates. The sex field is a required SOLAR field
#           and so it is guaranteed to be available. The age field is taken
#           from the phenotypes file, if one has been loaded. The name of
#           the age field must be "AGE". It is not an error if there is no
#           age field in the phenotypes file. The model will still contain
#           age correction terms (which should be set to zero), but obviously
#           no adjustment to the trait mean involving age can be made.
#           If adjustments to the mean are made for sex and/or age, then
#           betas must be specified for each of 5 effects: sex, male age,
#           female age, male age^2, and female age^2, in that order. The
#           parameters for this model are:
#
#             -freq    The frequency of QTL alleles 1, 2, ,,,, N-1 where
#                      the QTL has N alleles.
#
#             -mfreq   The frequency of marker alleles 1, 2, ..., N-1 where
#                      the marker has N alleles.
#
#             -theta   The recombination fraction between the QTL and the
#                      marker. The default value is 0, i.e. the QTL and marker
#                      are fully linked.
#
#             -ntrt    The number of traits controlled by the QTL. The default
#                      value is 1.
#
#             -mean    For each trait, a list of genotypic means. Genotypes
#                      are ordered as follows: 1/1, 2/1, 2/2, 3/1, 3/2, ...
#                      That is, the mean for genotype i/j, i >= j, is the
#                      k-th element in the list, where k is given by
#
#                           k = i*(i - 1)/2 + j
#
#                      Because phase is not considered, genotypes i/j and j/i
#                      are the same.
#
#             -sdev    For each trait, the within-genotype phenotypic standard
#                      deviation.
#
#             -cov     A list of covariates, in addition to sex and age, for
#                      which adjustments to the trait mean(s) will be made.
#
#             -beta    For each trait, a set of lists, one for each covariate
#                      including sex and age, of genotype-specific adjustments
#                      to the trait mean. Genotype order is the same as for
#                      genotypic means. If no betas are specified, they will
#                      all default to 0, i.e. no covariate effects. As noted
#                      above, sex and age together require 5 betas for each
#                      genotype. If the betas for a particular trait and
#                      covariate are not genotype-specific, the corresponding
#                      list can be shortened to a single value; this value
#                      will be used for every genotype.
#
#             -cmean   For each covariate other than sex and age, a mean
#                      value to subtract from the covariate before applying
#                      a covariate correction to the trait means.
#
#             -mage    A mean age to be subtracted before applying the age
#                      correction to the trait means.
#
#             -rhog    For each pair of traits, the genetic correlation
#                      between those two traits. If there are N traits, the
#                      order of the pairs is (2,1), (3,1), (3,2), ..., (N,1),
#                      (N,2), ..., (N,N-1). The default is no genetic
#                      correlation.
#
#             -rhoe    For each pair of traits, the environmental correlation
#                      between those two traits. The default is no
#                      environmental correlation.
#
#             -h2r     For each trait, the residual heritability expressed as
#                      the fraction of trait variance after the QTL effect has
#                      been accounted for. The default is no residual
#                      heritability.
#
#
#           In the second type of model, there may be multiple QTLs. The QTL
#           genotypes are read from a file rather than simulated. Parameters
#           unique to this model are:
#
#             -nloc    The number of QTLs.
#
#             -nall    A list of the number of alleles at each QTL.
#
#           The remaining parameters are the same as in the first model type.
#           The order of multi-locus genotypes is analogous to the single
#           locus case. The multi-locus genotype i/j, i >= j, is in the k-th
#           position, where k is given by
#
#               k = i*(i - 1)/2 + j
#
#           The i and j refer to the i-th and j-th multi-locus haplotypes.
#           Haplotypes are ordered so that the alleles of the last locus
#           vary the fastest, while the alleles of the first locus vary the
#           slowest. For example, given three bi-allelic loci, the order of
#           the three-locus haplotypes is
#
#               111, 112, 121, 122, 211, 212, 221, 222
#
#           The order of the three-locus genotypes is then
#
#               111/111, 112/111, 112/112, 121/111, 121/112, 121/121, ...
#
#
# Examples:
#
#        1. Simulate a QTL with two alleles, call them A and a, where the
#           frequency of allele A is 0.7. A single trait will be simulated
#           for which the mean of genotype AA is 90, the mean of genotype
#           Aa is 100, and the mean of genotype aa is 120. The trait will
#           have a within-genotype standard deviation of 10, and a residual
#           heritability of 0.3. A marker with 5 alleles of equal frequency
#           will be generated which has a recombination fraction of 0.05
#           with the QTL. The required commands are shown below - one to
#           create the simulation model, another to actually perform the
#           simulation. The first command has been broken into two lines
#           to avoid line-wrapping, but must actually be entered as a
#           single line.
#
#               solar> simqtl -freq .7 -mfreq {.2 .2 .2 .2} -theta .05
#                      -mean {90 100 120} -sdev 10 -h2r .3
#               solar> simqtl
#
#
#        2. Simulate a QTL with 3 alleles; the allele frequencies are 0.5,
#           0.3, and 0.2. There is no linked marker. There are two traits
#           associated with this QTL. Sex and age have an effect on the
#           first trait; there is no sex-by-age interaction or second-order
#           age effect. The traits are correlated both genetically and
#           environmentally. A mean population age of 40 is subtracted
#           prior to the age correction to the mean of the first trait.
#           When the simulation is run, the random number generator is
#           seeded with the integer value 12345.
#
#               solar> simqtl -freq {.5 .3} -ntrt 2
#                      -mean {{10 15 12 20 18 30} {50 55 60 60 55 80}}
#                      -sdev {2.5 10} -h2r {.2 .65}
#                      -beta {{{-1.2 .1 -.5 1.4 2 -.5} {2.4 3 1.6 -4 0 -1}
#                              {2.4 3 1.6 -4 0 -1} {0} {0}}
#                             {{0} {0} {0} {0} {0}}}
#                      -mage 40 -rhog .7 -rhoe .4
#               solar> simqtl -seed 12345
#
#
#        3. Simulate a quantitative trait controlled by two QTLs. The
#           first QTL has 3 alleles, and the second QTL has 2 alleles.
#           There are 6 two-locus haplotypes, so we have a total of 21
#           two-locus genotypes in the order
#
#               11/11, 12/11, 12/12, 21/11, 21/12, 21/21, 22/11, 22/12,
#               22/21, 22/22, 31/11, 31/12, 31/21, 31/22, 31/31, 32/11,
#               32/12, 32/21, 32/22, 32/31, 32/32
#
#           When the simulation is run, the two-locus genotypes are read
#           from the file "2locgtyp".
#
#               solar> simqtl -nloc 2 -nall {3 2}
#                      -mean {26 31 36 28 33 30 33 38 35 40 31 36 33 38 36
#                             36 41 38 43 41 46}
#                      -sdev 2.8
#               solar> simqtl -gfile 2locgtyp
# -

proc simqtl {args} {

    set nloc 1
    set lnall ""
    set lfreq ""
    set ntrt 1
    set lcov ""
    set lbeta ""
    set mage 0
    set lcmean ""
    set lmean ""
    set lsdev ""
    set lh2r ""
    set lrhog ""
    set lrhoe ""
    set theta ""
    set lmfreq ""
    set gfile ""
    set model 0
    set seed ""
    set inform 0

    read_arglist $args -nloc nloc -nall lnall -freq lfreq -ntrt ntrt \
        -cov lcov -beta lbeta -mage mage -cmean lcmean -mean lmean \
        -sdev lsdev -h2r lh2r -rhog lrhog -rhoe lrhoe -theta theta \
        -mfreq lmfreq -gfile gfile -model {set model 1} -seed seed \
        -inform {set inform 1}

    if {($nloc != 1 || $lnall != "" || $lfreq != "" || $ntrt != 1 || \
            $lcov != "" || $lbeta != "" || $mage || $lcmean != "" || \
            $lmean != "" || $lsdev != "" || $lh2r != "" || $lrhog != "" || \
            $lrhoe != "" || $theta != "" || $lmfreq != "") && \
           ($gfile != "" || $model || $seed != "" || $inform)} {
        error "Invalid simqtl command"
    }

    if {($gfile != "" || $seed != "" || $inform) && $model} {
        error "Invalid simqtl command"
    }

    if {$model} {
        csimqtl -model
        return
    }

    if {[file exists simqtl.dat]} {
        if {[file mtime simqtl.dat] < [file mtime pedindex.out]} {
            file delete simqtl.dat
        }
    }

    if {$args == ""} {
        write_simqtl_data
        csimqtl
        return
    }

    if {$gfile != ""} {
        write_simqtl_data
        if {$seed != ""} {
            if {$inform} {
                csimqtl -gfile -seed $seed -inform
                return
            } else {
                csimqtl -gfile -seed $seed
                return
            }
        } else {
            if {$inform} {
                csimqtl -gfile -inform
                return
            } else {
                csimqtl -gfile
                return
            }
        }
    } elseif {$seed != ""} {
        write_simqtl_data
        if {$inform} {
            csimqtl -seed $seed -inform
            return
        } else {
            csimqtl -seed $seed
            return
        }
    } else {
        if {$inform} {
            write_simqtl_data
            csimqtl -inform
            return
        }
    }

    if {$nloc <= 0} {
        error "At least one QTL is required"
    }

    if {$nloc == 1 && $lfreq == ""} {
        error "QTL allele frequencies are required"
    }

    if {$nloc > 1 && [llength $lnall] != $nloc} {
        error "There should be allele counts for $nloc loci"
    }

    if {$ntrt <= 0} {
        error "At least one trait is required"
    }

    set ncov [expr [llength $lcov] + 2]
    if {$lbeta != ""} {
        if {$ntrt == 1 && [llength $lbeta] > 1} {
            set lbeta [list $lbeta]
        }
        if {[llength $lbeta] != $ntrt} {
            error \
            "There should be $ntrt sets of beta lists, one for each trait"
        }
    }

    if {$ntrt == 1 && [llength $lmean] > 1} {
        set lmean [list $lmean]
    }
    if {[llength $lmean] != $ntrt} {
        error \
        "There should be $ntrt genotypic mean lists, one for each trait"
    }
    if {[llength $lsdev] != $ntrt} {
        error \
    "There should be $ntrt phenotypic standard deviations, one for each trait"
    }

    if {$lrhog != "" && [llength $lrhog] != [expr $ntrt * ($ntrt - 1) / 2]} {
        error \
    "There should be [expr $ntrt * ($ntrt - 1) / 2] genetic correlations"
    }
    if {$lrhoe != "" && [llength $lrhoe] != [expr $ntrt * ($ntrt - 1) / 2]} {
        error \
    "There should be [expr $ntrt * ($ntrt - 1) / 2] environmental correlations"
    }

    set nmrk 0
    if {$theta != ""} {
        if {$nloc != 1} {
            error "Only 1 QTL and 1 linked marker can be simulated"
        }
        set nmall [llength $lmfreq]
        if {$nmall == 0} {
            error "Marker allele frequencies are required"
        }
        incr nmall
        set nmrk 1
    } elseif {$lmfreq != ""} {
        set theta 0
        set nmall [llength $lmfreq]
        incr nmall
        set nmrk 1
    }

    set outfile [open simqtl.par w]
    if {$nmrk} {
        set nall [expr [llength $lfreq] + 1]
        set ngen [expr $nall * ($nall + 1) / 2]
        puts $outfile "$nloc $nall $nmrk $nmall $ntrt $ncov"
        for {set ic 0} {$ic < [expr $ncov - 2]} {incr ic} {
            puts $outfile [lindex $lcov $ic]
        }
        puts $outfile $lfreq
        puts $outfile $lmfreq
        puts $outfile $theta
    } elseif {$nloc == 1 && [llength $lfreq]} {
        set nall [expr [llength $lfreq] + 1]
        set ngen [expr $nall * ($nall + 1) / 2]
        puts $outfile "$nloc $nall 0 $ntrt $ncov"
        for {set ic 0} {$ic < [expr $ncov - 2]} {incr ic} {
            puts $outfile [lindex $lcov $ic]
        }
        puts $outfile $lfreq
    } else {
        set ngen 1
        for {set il 0} {$il < $nloc} {incr il} {
            set nall [lindex $lnall $il]
            set ngen [expr $ngen * $nall]
        }
        set ngen [expr $ngen * ($ngen + 1) / 2]
        puts $outfile "$nloc $lnall 0 $ntrt $ncov"
        for {set ic 0} {$ic < [expr $ncov - 2]} {incr ic} {
            puts $outfile [lindex $lcov $ic]
        }
    }

    set ncov [expr $ncov + 3]
    set cname(0) sex
    set cname(1) "male age"
    set cname(2) "female age"
    set cname(3) "male age^2"
    set cname(4) "female age^2"

    for {set it 0} {$it < $ntrt} {incr it} {
        if {[llength [lindex $lmean $it]] != $ngen} {
            close $outfile
            file delete simqtl.par
            error \
            "There should be $ngen genotypic means for trait [expr $it + 1]"
        }
        puts $outfile [lindex $lmean $it]
        puts $outfile [lindex $lsdev $it]
        if {$lh2r != ""} {
            puts $outfile [lindex $lh2r $it]
        } else {
            puts $outfile 0
        }

        set lcmeant "0"
        for {set ic 0} {$ic < 4} {incr ic} {
            lappend lcmeant $mage
        }
        for {set ic 0} {$ic < [expr $ncov - 5]} {incr ic} {
            if {$lcmean != ""} {
                lappend lcmeant [lindex $lcmean $ic]
            } else {
                lappend lcmeant 0
            }
        }

        if {$lbeta != ""} {
            set lbetat [lindex $lbeta $it]
            if {[llength $lbetat] != $ncov} {
                close $outfile
                file delete simqtl.par
                error \
                "There should be $ncov beta lists for trait [expr $it + 1]"
            }
            for {set ic 0} {$ic < $ncov} {incr ic} {
                if {$ic < 5} {
                    set cstr $cname($ic)
                } else {
                    set cstr "covariate [lindex $lcov [expr $ic - 5]]"
                }
                set lbetatc [lindex $lbetat $ic]
                if {[llength $lbetatc] != $ngen} {
                    if {[llength $lbetatc] == 1} {
                        for {set i 1} {$i < $ngen} {incr i} {
                            lappend lbetatc [lindex $lbetatc 0]
                        }
                    } else {
                        close $outfile
                        file delete simqtl.par
                        error \
                "There should be $ngen betas for $cstr, trait [expr $it + 1]"
                    }
                }
                puts $outfile "[lindex $lcmeant $ic] $lbetatc"
            }
        } else {
            set lbetat ""
            for {set ig 0} {$ig < $ngen} {incr ig} {
                lappend lbetat 0
            }
            for {set ic 0} {$ic < $ncov} {incr ic} {
                puts $outfile "[lindex $lcmeant $ic] $lbetat"
            }
        }

        if {$it} {
            if {$lrhog != ""} {
                set lrhogt [lrange $lrhog [expr $it * ($it - 1) / 2] \
                                          [expr $it * ($it + 1) / 2 - 1]]
            }
            if {$lrhoe != ""} {
                set lrhoet [lrange $lrhoe [expr $it * ($it - 1) / 2] \
                                          [expr $it * ($it + 1) / 2 - 1]]
            }
            for {set jt 0} {$jt < $it} {incr jt } {
                if {$lrhog != "" && $lrhoe != ""} {
                    puts $outfile "[lindex $lrhogt $jt] [lindex $lrhoet $jt]"
                } elseif {$lrhoe != ""} {
                    puts $outfile "0 [lindex $lrhoet $jt]"
                } elseif {$lrhog != ""} {
                    puts $outfile "[lindex $lrhogt $jt] 0"
                } else {
                    puts $outfile "0 0"
                }
            }
        }
    }

    close $outfile
}


proc write_simqtl_data {} {

    upvar gfile gfile

    if {[catch {set parfile [open simqtl.par]}]} {
	error "A simulation model must be specified first."
    }

    if {[catch {set pedfile [tablefile open pedindex.out]}]} {
	error "Pedigree data have not been loaded."
    }

    set record [gets $parfile]
    set nloc [lindex $record 0]
    set nmrk [lindex $record [expr $nloc + 1]]
    set ncov [lindex $record [expr $nloc + $nmrk + 3]]
    set ncov [expr $ncov - 2]
    set lcov {}
    for {set i 0} {$i < $ncov} {incr i} {
        lappend lcov [gets $parfile]
    }
    close $parfile

    set phenfile ""
    if {[catch {set ifile [open phenotypes.info r]}]} {
        if {$ncov} {
            tablefile $pedfile close
            error "Phenotype data must be loaded if covariates are used"
        }
        set use_phenfile 0
    } else {
        gets $ifile phenfile_name
        close $ifile
        set use_phenfile 1
    }

    set need_famid 0
    if {$use_phenfile} {
        if {[catch {set phenfile [solarfile open $phenfile_name]}]} {
            tablefile $pedfile close
            error "Can't find phenotypes file $phenfile_name"
        }

        if {[tablefile $pedfile test_name FAMID]} {
            if {[solarfile $phenfile test_name FAMID]} {
                set need_famid 1
            }
        }

        solarfile $phenfile start_setup
        if {$need_famid} {
            solarfile $phenfile setup FAMID
        }
        solarfile $phenfile setup ID
        if {![solarfile $phenfile test_name AGE]} {
            set no_ages_avail 1
        } else {
            solarfile $phenfile setup AGE
            set no_ages_avail 0
        }
        for {set i 0} {$i < $ncov} {incr i} {
            if {![solarfile $phenfile test_name [lindex $lcov $i]]} {
                solarfile $phenfile close
                tablefile $pedfile close
                error "No [lindex $lcov $i] field found in phenotype file"
            } else {
                solarfile $phenfile setup [lindex $lcov $i]
            }
        }

        set id_table {}
        if {!$no_ages_avail} {
            set v_table(AGE) {}
        }
        for {set i 0} {$i < $ncov} {incr i} {
            set v_table([lindex $lcov $i]) {}
        }

        while {"" != [set record [solarfile $phenfile get]]} {
            lappend id_table [lrange $record 0 $need_famid]
            set ifld [expr $need_famid + 1]
            if {!$no_ages_avail} {
                lappend v_table(AGE) [lindex $record $ifld]
                incr ifld
            }
            for {set i 0} {$i < $ncov} {incr i} {
                lappend v_table([lindex $lcov $i]) [lindex $record $ifld]
                incr ifld
            }
        }
        solarfile $phenfile close
    } else {
        set no_ages_avail 1
        if {[tablefile $pedfile test_name FAMID]} {
            set need_famid 1
        }
    }

    if {$gfile != ""} {
        if {[catch {set genfile [solarfile open $gfile]}]} {
            tablefile $pedfile close
            error "Can't find genotypes file $gfile"
        }

        solarfile $genfile start_setup
        if {$need_famid} {
            solarfile $genfile setup FAMID
        }
        solarfile $genfile setup ID

        set fnames [solarfile $genfile names]
        set nl 0
        for {set i 0} {$i < [llength $fnames]} {incr i} {
            set fname [string tolower [lindex $fnames $i]]
            if {$fname != [string tolower [field ID]] && \
                    $fname != [string tolower EGO] && \
                    $fname != [string tolower [field FAMID]]} {
                solarfile $genfile setup $fname
                incr nl
            }
        }

        if {$nl != $nloc} {
            solarfile $genfile close
            tablefile $pedfile close
            error "There should be $nloc QTL genotypes in the genotypes file"
        }

        while {"" != [set gtyprec [solarfile $genfile get]]} {
            if {$need_famid} {
                set famid [lindex $gtyprec 0]
                set id [lindex $gtyprec 1]
            } else {
                set id [lindex $gtyprec 0]
            }
            for {set i 1} {$i <= $nloc} {incr i} {
                if {$need_famid} {
                    set gt($famid,$id,$i) [lindex $gtyprec [expr $i + 1]]
                } else {
                    set gt($id,$i) [lindex $gtyprec $i]
                }
            }
        }
        solarfile $genfile close
    }

    tablefile $pedfile start_setup
    tablefile $pedfile setup PEDNO
    tablefile $pedfile setup IBDID
    tablefile $pedfile setup FIBDID
    tablefile $pedfile setup MIBDID
    tablefile $pedfile setup SEX
    tablefile $pedfile setup MZTWIN
    if {$need_famid} {
        tablefile $pedfile setup FAMID
    }
    tablefile $pedfile setup ID

    set outfile [open simqtl.dat w]
    if {$need_famid} {
        puts -nonewline $outfile "FAMID,"
    }
    puts -nonewline $outfile "ID,PEDNO,IBDID,FIBDID,MIBDID,SEX,MZTWIN"
    if {!$no_ages_avail} {
        puts -nonewline $outfile ",AGE"
    }
    for {set i 0} {$i < $ncov} {incr i} {
        puts -nonewline $outfile [format ",%s" [lindex $lcov $i]]
    }
    if {$gfile != ""} {
        for {set i 1} {$i <= $nloc} {incr i} {
            puts -nonewline $outfile [format ",QTL%d" $i]
        }
    }
    puts $outfile ""

    while {"" != [set record [tablefile $pedfile get]]} {
        if {$need_famid} {
            set famid [lindex $record 6]
            set id [lindex $record 7]
            puts -nonewline $outfile [format "%s,%s,%s,%s,%s,%s,%s,%s" \
                $famid $id [lindex $record 0] [lindex $record 1] \
                [lindex $record 2] [lindex $record 3] [lindex $record 4] \
                [lindex $record 5] ]
        } else {
            set id [lindex $record 6]
            puts -nonewline $outfile [format "%s,%s,%s,%s,%s,%s,%s" \
                $id [lindex $record 0] [lindex $record 1] \
                [lindex $record 2] [lindex $record 3] [lindex $record 4] \
                [lindex $record 5] ]
        }

        if {$use_phenfile} {
            set target [lrange $record 6 [expr 6 + $need_famid]]
            set ndx [lsearch -exact $id_table $target]
        }
        if {!$no_ages_avail} {
            set age [lindex $v_table(AGE) $ndx]
            puts -nonewline $outfile [format ",%s" $age]
        }
        for {set i 0} {$i < $ncov} {incr i} {
            set covar [lindex $v_table([lindex $lcov $i]) $ndx]
            puts -nonewline $outfile [format ",%s" $covar]
        }

        if {$gfile != ""} {
            for {set i 1} {$i <= $nloc} {incr i} {
                if {$need_famid} {
                    if {![info exists gt($famid,$id,$i)]} {
                        tablefile $pedfile close
                        close $outfile
                        error \
            "No genotype record was found for individual FAMID=$famid ID=$id"
                    }
                    if {$gt($famid,$id,$i) == ""} {
                        tablefile $pedfile close
                        close $outfile
                        error \
                    "Individual FAMID=$famid ID=$id has no genotype for QTL $i"
                    }
                    puts -nonewline $outfile [format ",%s" $gt($famid,$id,$i)]
                } else {
                    if {![info exists gt($id,$i)]} {
                        tablefile $pedfile close
                        close $outfile
                        error \
                        "No genotype record was found for individual ID=$id"
                    }
                    if {$gt($id,$i) == ""} {
                        tablefile $pedfile close
                        close $outfile
                        error "Individual ID=$id has no genotype for QTL $i"
                    }
                    puts -nonewline $outfile [format ",%s" $gt($id,$i)]
                }
            }
        }
        puts $outfile ""
    }

    tablefile $pedfile close
    close $outfile
}

# solar::polyvoxel --
#
# Purpose: polygenic analysis of image data
#
# Usage: trait ...
#        covariate ...
#        polyvoxel <maskname> <outname>
#
# <maskname> the filename of mask image file to use
# <outname> the filename of image file to write as output
#
# Notes:  Trait and covariate should be selected first.  The number of
# layers in the output image will be adjusted to match the number of
# traits and covariates
#
# -

proc polyvoxel {maskname outname} {

    set traits [trait]
    set covs [covariates]
    set ntrait [llength $traits]
    set ncov [llength $covs]

# start mask

    mask $maskname

# create imout object for output

    imout $outname -ntrait $ntrait -ncovar $ncov
#
#    imout -nxyz 182:218:182
#    imout -dxyz 1:1:1
#    imout -nvol 30
#    imout -orient XYZ-++

# note: the default is all non-zero voxels in mask
# otherwise use -intensity for specific intensity

# compute polygenic h2r etc. for each voxel in mask

    set ongoing 1
    while {$ongoing} {

	model new
	eval trait $traits
	eval covar $covs
	if {$ncov > 0} {
	    polygenic -screen
	} else {
	    polygenic
	}
	
	if {[catch {mask -next}]} {
	    set ongoing 0
	}
    }
    mask -delete
    imout -write
    imout -close
}


# solar::polyimout -- private
#
# Purpose: explain binary output for polygenic command
#
# Usage:   mask <maskfile> -intensity <intensity>  OR  voxel x:y:z
#          imout [-r] <filename> [-nxyz <MaxX>:<MaxY>:<MaxZ> ...]
#          trait x
#          covariate a ...
#          polygenic ...
#
#          ;# next interation
#          mask -next
#          model new
#          trait x
#          ...
#
#          ;# write and close
#          imout -write
#          imout -close
#
# Notes:
# The mask command creates a mask, which also determines the current voxel.
# The current voxel can also be set explicitly with the voxel command.
#
# The imout command sets up binary object for output.  If a new object is
# created, the user must provide all needed information.  If an existing
# object is loaded with -r, information is retrived from that object.
# For more information see "help mask" and "help imout".
#
# When the mask or voxel and imout commands are given before running polygenic,
# polygenic will write binary output to the specified binary file.
#
# Each "volume" of the binary file will be populated with applicable results.
# The volumes are assigned as follows for univariate models:
#
# Volume   Result
# 0        status (0=not tested, 1=success, -1=error)
# 1        h2r
# 2        h2r standard error
# 3        p(h2r)
# 4        log likelihood
# 5        proportion of variance due to covariates
# 6        residual kurtosis
# 7        standard deviation (SD)
# 8        SD se
# 9        Mean
# 10       Mean se
# 11       Sample size (N)
# 12-18    Reserved for linkage or association
# 20       beta for 1st covariate
# 21       beta se for 1st covariate
# 22       p value for 1st covariate (if calculated)
# 23       chi value for 1st covariate (if calculated)
# 24+      (repeat items 20-23 for each covariate if more than one)
#
# The folder assignments have not yet been made for bivariate models.
#
# If covariate screening is done "polygenic -s" without fixing all the
# covariates, some covariates will be dropped from the final model.  Any
# such dropped covariates will have p and chi values, but not beta and
# beta se values.  The beta and beta se values will then be zero.
#-


# solar::imout -- private
#
# Purpose: Define and write binary image output
#
# Usage:   imout [[-r] <filename>] [-ignoremask]
#          imout -nxyz <nx>:<ny>:<nz>   ; set limits
#          imout -dxyz <dx>:<dy>:<dz>   ; set spacing
#          imout -nvol <nvol>
#          imout -ntrait <# of traits>
#          imout -ncovar <# of covariates>
#          imout -orient <orientation string> ;# quotes required if blanks
#
#          imout -puts <number> -vol <volume>  ;# put data into image
#          imout -write  ;# actually writes file
#          imout -close  ;# frees memory without writing file
#          imout -valid  ;# returns 1 if imout is ready-to-use or 0
#
# Notes:
#
# The number of volumes may be specified either with the -nvol argument or
# the -ntrait and -ncovar options.  The latter options create the number
# of volumes that would be required for a polygenic analysis of a model
# with that many traits and covariates.  Currently only one trait is
# supported and the -ntrait option defaults to one, but -ncovar has
# no default.  For more information about the volume assignments for a
# polygenic model, see 'help polyimout'.
#
# If a mask file is loaded (see mask command), and if -r is not specified,
# imout will be set to the same dimensions as the mask UNLESS overridden in
# the initial command line.  In this case, it will not be possible to change
# dimensions or orientation after the first imout command is given, unless
# the -ignoremask option is used.  The number of volumes *must* be specified
# in the initial command line as well (since this can't be changed later)
# either by using the -nvol or -ncovar options, -ntrait defaults to 1.
#
# Otherwise the  top group of commands initialize a binary output object.
# The filename must be given first, possibly preceded by a "-r" to start by
# loading a pre-existing file.  If a file is loaded with -r, all the
# required limits and other information is read from the file itself.
# If -r is not specified, and therefore a new file will be written (the file
# is actually written by the imout -write command, and not before) the
# user must specify all required information before either -puts or -write
# can be used.
#
# The other commands may be used in any order,
# except that the imout -orient command should be last because it checks
# that all other commands have given sufficient information that the object
# is ready to be created.  All the options shown could actually be included
# in one line, but with the same restriction that the <filename> must come
# first and <orientation string> must be last.
#
# The second group of commands are used for writing to the binary volume that
# is currently defined.  The imout -write command writes a file but does not
# close the object, so that additional writing could be done later.  The
# imout -puts command writes a scalar value to the volume specified using
# the current voxel (see voxel and mask commands about the voxel).  If no
# voxel has been defined either by the mask or voxel commands, no -puts is
# possible.
#-

# solar::mask --
#
# Purpose:  To read image mask file and set current voxel
#
# Usage:    mask [<filename>] [-intensity <intensity>] [-index <index>]
#           mask -next
#           mask -delete
#
#           <filename> is the name of the file containing the mask
#           <intensity> is the integer value that defines this mask
#           <index> is position within the set of mask-defined voxels
#           -next specifies to advance to the next mask-defined voxel
#           -delete deletes the mask and frees all related storage
#
# Currently, the file must be NIFTI/RicVolumeSet.  The mask is assumed to
# be volume zero.  If intensity is unspecified, the mask is defined as all
# non-zero intensities.  The default starting index is zero, meaning the first
# voxel with the specified intensity.  Indexes are advanced in the order
# x-first and z-last.
#
# The value returned is the x:y:z voxel value, which is also set as the
# current voxel value, which is saved to model files.  The voxel value
# determines the voxel obtained when reading image files for image traits.
# See the voxel command for more information about the voxel and how
# image traits are included in csv files.
#
# The mask remains in effect during the current session and unless cleared by
# -delete.  It is not cleared by 'model new'.  The intent is that successive
# models can use successive voxels advanced with 'mask -next' which is the
# most efficient way to advance to the next voxel.  It is possible though not
# required to specify filename, intensity, and index in one command.
# It is also possible to change the -intensity or -index in subsequent
# commands.  If the intensity is changed, we go back to the first voxel with
# that intensity. The index is always absolute relative to the first voxel at
# current intensity specification.  'mask -next' starts from the current voxel
# which is normally set by the mask command itself, but could also be
# re-specified with the voxel command.  If the <filename> has not been
# specifed in the current mask command or a previous mask command, all
# other options are invalid.
# -

# solar::voxel --
#
# Purpose:  To set and save current voxel position
#
# Usage:    voxel [<voxel-value>]
#
#           <voxel-value> is 3 coordinates delimited by colons as x:y:z
#              for example, 12:8:23
#
#           If no voxel-value is specified, the current voxel is returned.
#           If no current voxel has been defined, an error is raised.
#           If a voxel has been defined, it is written to model files.
#           The current voxel can also be set with the mask command, and
#             that is the general way it should be done.
#
# Notes:
#
# 1) Image traits are qualified as to type in the header of the phenotypes
#    file, which must be comma-delimited type, and the type qualification
#    follows the trait name separated by colon. The only type currently
#    supported is NIFTI/RicVolumeSet which is called "nifti".  Such a
#    header could look like this:
#
#       ID,age,count:nifti
#
#    Then, each corresponding data field consists of a NIFTI filename
#    followed by a colon and the volume number for that person.  For example:
#
#       A001,19,images.gz:1
#
#-


# solar::toscript --
#
# Purpose:  Write previous commands to a script
#
# Usage:    toscript [-ov] <name> [<first>[-<last>]]*
#
#           -ov      Force overwrite of previous script
#           <first>  First command number to be included
#           <last>   Last command number in sequence to be included
#
# Example:  toscript analysis 1 3 9-20  ;# include commands 1, 3, and 9-20
#
# Notes:    Command numbers are displayed with the tcl "history" command.
#
#           If no numbers are specified, all previous commands in this SOLAR
#           session will be included in script.
#
#           Script will be saved in file named <name>.tcl in the current
#           directory.  After saving, newtcl will automatically be invoked
#           so that the script can be used immediately.
#
#           The script <name> defaults to being the first argument, but may
#           also be the last argument if the <name> is not a number or range
#           of numbers so there is no ambiguity.  For example:
#
#           toscript 1-10 startscript     ;# OK
#           toscript startscript 1-10     ;# OK
#           toscript 2 1-10               ;# OK script named 2.tcl
#           toscript 1-10 2               ;# OK script named 1-10.tcl
# -

proc toscript {args} {

    set done 0

# Check for -ov (or, undocumented unix-like -f) argument

    set ov 0

    set more_args [read_arglist $args \
		       -ov {set ov 1} \
		       -overwrite {set ov 1} \
		       -f {set ov 1}]
    set args $more_args

# Determine which argument is script name and put it first

    proc test_range {range} {
	if {1 == [scan $range "%d %s" a b]} {
	    return 1
	}
	if {2 == [scan $range "%d-%d %s" a b c]} {
	    return 1
	}
	return 0
    }

    set a0 [lindex $args 0]
    set a1 [lindex $args end]
    set t0 [test_range $a0]
    set t1 [test_range $a1]
    if {$t0==1 && $t1==0} {
	set newargs [concat $a1 [lrange $args 0 [expr [llength $args] - 2]]]
	set args $newargs
    }
    if {$t0==0 && $t1==0} {
	error "toscript requires command numbers or ranges such as 1-10"
    }


#   puts "args list after ordering: $args"

# Get name, convert to filename, and check for existing file

    set name [lindex $args 0]
    if {"" == $name} {
	error "toscript requires <name>"
    }

    if {".tcl" == [file extension $name]} {
	set filename $name
	set name [file rootname $name]
    } else {
	set filename $name.tcl
    }
    puts "\nWriting script file $filename\n"
    if {[file exists $filename]} {
	if {$ov} {
	    file delete $filename
	} else {
	    error "Script file $filename already exists; use -ov to overwrite"
	}
    }
    set tempname [catenate $filename .tmp]

    file delete -force $tempname

    putsout -d. $tempname "proc $name \{\} \{"

# Parse range(s) or default range

    set ranges [lrange $args 1 end]
    if {"" == $ranges} {
        set ranges "1-[expr [history nextid] - 2]"
    }
    foreach seq $ranges {
	set first $seq
	set last $seq
	if {-1 != [string first - $seq]} {
	    set ptr [string first - $seq]
	    set first [string range $seq 0 [expr $ptr - 1]]
	    set last [string range $seq [expr $ptr + 1] end]
	}
	ensure_integer $first
	ensure_integer $last
	
# Copy range of commands to temporary output file

	for {set i $first} {$i <= $last} {incr i} {
	    if {[catch {putsout -d. $tempname "    [history event $i]"}]} {
		file delete $tempname
		error "Unable to access command $i; Only [history keep] commands retained"
	    }
	}
    }
    
# Put closing brace and invoke newtcl

    putsout -d. $tempname "\}"
    set done 1

# end catch

    if {!$done} {
	file delete $tempname
    } else {
	exec mv $tempname $filename
    }

    newtcl
    return ""
}


# The motivation for the following section, #2, is now obsolete so this
# comment has been removed from the standard documentation for the time being.
#
# Purpose:  Notes regarding SOLAR accuracy.
#
# 1) Of course, there is NO WARRANTY, express or implied, regarding the
# performance or accuracy of SOLAR, or its fitness for any purpose.
#
# 2) Starting with version 2.0.4, parameter accuracy may be reduced slightly
# to achive convergence after all other efforts at achieving convergence
# have failed.  When this happens, a message will appear indicating that
# accuracy has been reduced.  The exact amount of inaccuracy is unknown, but
# is probably quite small, say, 1% or less of the indicated values.  This
# may actually be one of the smaller sources of error in a real analysis.
# There are two options that control parameter estimation tolerances.
# One is "option Conv" and the other is "Option Tol".  Currently the
# "Option Conv" may be increased from 1e-6 to 1e-5 to force convergence
# during twopoint and multipoint scans.  This is not done automatically
# during individual maximizations, so you may try doing this yourself then.
#
# 3) Always check parameter standard errors (if available) before
# drawing conclusions regarding parameter values.  These are are shown
# in models and model files.  Sometimes standard errors are not
# available by design or because of computational difficulty.
# Inability to compute standard errors does not indicate that the
# parameter estimates are faulty, but it does suggest that the
# parameter estimates themselves were hard to achieve, and that the
# true standard error may be quite high.  The option which controls
# the computation of parameter standard errors is "option StandErr".
# For linkage models, this is set to "0" to disable computation of
# standard errors for higher speed during scans.  At the completion of a
# multipoint scan, the model with the highest LOD score is reloaded and
# re-maximized with Option StandErr set to "1" to enable computation
# of standard errors.
#
# 4) There is a selection bias toward high locus-specific
# heritabilities (e.g. h2q1) in genomic scans since high LOD scores
# correspond with loci having high computed heritability, that being
# the difference between the null and test models.  SOLAR does not attempt
# to correct for this bias, about which there has been some discussion
# in the literature of Statistical Genetics.  In general, the LOD score
# is of more importance than the h2q1 value.
#-

# SOLAR::dumpomega
#
# Purpose: Display omega values on terminal during maximization
#
# Usage:   dumpomega [<count>]
#
#          <count> is number of omega values to show.
#          -1 means show all omega values (Note: there can be millions,
#               but if necessary, you can stop SOLAR session with CTRL-C.)
#           0 means turn off dumpomega
#               (Any non-numeric string will also be interpreted as zero,
#                so you could enter "dumpomega off" for example.)
#
#           If no argument is given, current value is shown.
#
# The output during maximization looks like this:
#
#    Omega[52,51]<1,1>{ 0.500000, 0.250000} =  0.048751575375
#
# The first two numbers in [] are the IBDID's for individuals 1 and 2.
# The second two numbers in <> are the effective trait numbers.
# The third two numbers in {} are the phi2 and delta7 values.
# The final number is the covariance ("omega") that has been calculated
# for this pair of individuals.
#
# IBDID's are the sequential indexes assigned to individuals in the
# file pedindex.out created by the "load pedigree" command.
# -

proc dumpomega {args} {
    global SOLAR_DumpOmega
    if {"" == $args} {
	set currently 0
	if {[if_global_exists SOLAR_DumpOmega]} {
	    set currently $SOLAR_DumpOmega
	}
	return $currently
    }
    set SOLAR_DumpOmega $args
    return ""
}

    
proc solardebug {args} {
    global SOLAR_Debug
    if {{} == $args} {
	if {[if_global_exists SOLAR_Debug]} {
	    return $SOLAR_Debug
	}
	return 0
    }
    if {"on" == $args || $args == 1} {
	set SOLAR_Debug 1
    } elseif {"off" == $args || $args == 0} {
	set SOLAR_Debug 0
    } else {
	error "Usage: solardebug \[on | 1 | off | 0\]"
    }
    return ""
}

proc prompt {args} {
    if {"" == $args} {
	set args "\nEnter <RETURN> to continue or q to quit:"
    }
    puts $args
    set foobar ""
    gets stdin foobar
    foreach test {q quit e exit} {
	if {$test == $foobar} {
	    error "\nexiting at user request\n"
	}
    }
	    
    return $foobar
}

proc ifdebug {args} {
    if {[solardebug]} {
	return [uplevel $args]
    }
    return ""
}

proc notquiet {args} {
    upvar quiet quiet
    if {!$quiet} {
	return [uplevel $args]
    }
    return ""
}

proc get_id {ibdid} {
    if {[catch {pedigree_loaded} errmsg]} {
        error $errmsg
    }
    set ndx [expr $ibdid - 1]
    if {![if_global_exists PedNdxIDList]} {
        setup_PedNdxIDList
    }
    global PedNdxIDList
    return [lindex $PedNdxIDList $ndx]
}

proc get_ibdid {args} {
    if {[catch {pedigree_loaded} errmsg]} {
        error $errmsg
    }
    if {[llength $args] == 2} {
        set id "[lindex $args 0] [lindex $args 1]"
    } else {
        set id [lindex $args 0]
    }
    if {![if_global_exists PedNdxIDList]} {
        setup_PedNdxIDList
    }
    global PedNdxIDList
    set ibdid [expr [lsearch -exact $PedNdxIDList $id] + 1]
    if {$ibdid} {
        return $ibdid
    } elseif {[llength $args] == 2} {
        set famid [lindex $args 0]
        set id [lindex $args 0]
        error " individual FAMID = $famid ID = $id not found in pedindex.out"
    } else {
        error " individual ID = [lindex $args 0] not found in pedindex.out"
    }
}

proc setup_PedNdxIDList {} {
    if {[catch {pedigree_loaded} errmsg]} {
        error $errmsg
    }
    global PedNdxIDList
    set PedNdxIDList {}
    set pedfile [tablefile open pedindex.out]
    tablefile $pedfile start_setup
    tablefile $pedfile setup IBDID
    tablefile $pedfile setup ID
    set need_famid 0
    if {[tablefile $pedfile test_name FAMID]} {
        set need_famid 1
        tablefile $pedfile setup FAMID
    }
    while {{} != [set record [tablefile $pedfile get]]} {
        if {$need_famid} {
            lappend PedNdxIDList "[lindex $record 2] [lindex $record 1]"
        } else {
            lappend PedNdxIDList [lindex $record 1]
        }
    }
    tablefile $pedfile close
}

proc pedigree_loaded {} {
    if {![file exists pedindex.out]} {
        error "Pedigree data have not been loaded."
    } else {
        if {![file exists pedigree.info]} {
            error \
            "Cannot open pedigree.info\nReloading the pedigree data is advised."
        }
    }
}

proc fastmod {} {
    fastpolymod
    option modeltype evd
    option standerr 0
    verbosity min
    return ""
}

proc fastpolymod {} {
    polymod
}

proc setevd2omega {} {
    global SOLAR_EVD2_omega
    set SOLAR_EVD2_omega [omega]
}

# solar::retext --
# solar::retextpc --
#
# Purpose: Translate files with Old Mac line terminators to unix terminators
#
# Usage:  retext <input-file> [<output-file>]    ;# for Old Mac files
#         retextpc <input-file> [<output-file>]  ;# for PC files (if needed)
#
# Notes:  retextpc is for converting Dos/Windows files to the unix
#         format SOLAR likes best (however this is not usually needed for
#         csv files from PC's which generally SOLAR can handle asis).
#
#         SOLAR now identifies the old style Mac termination when opening
#         phenotypes files.  These files MUST be translated because they
#         are incompatible with the C programming library.  If you need to
#         run the retext command, you will get this message:
#
#             File old.txt has unsupported text line terminators
#             Use retext command to fix file before using
#
#         The full input filename must be given, including ".txt" extension
#         if present.  If no output filename is specified, an additional
#         ".tr" extension is added to the input filename.  If a file with
#         the .tr extension already exists, it is overwritten.
#
#-

proc retext {filename {outfilename ""}} {

    if {0 == [file exists $filename]} {
	error "File not found: $filename"
    }
    if {"" == $outfilename} {
	set outfilename $filename.tr
    }
    exec tr '\015' '\012' <$filename >$outfilename
    return ""
}

proc retextpc {filename {outfilename ""}} {

    if {0 == [file exists $filename]} {
	error "File not found: $filename"
    }
    if {"" == $outfilename} {
	set outfilename $filename.tr
    }
    exec col -b <$filename >$outfilename
    return ""
}

proc housematrix {} {

    set all [matrix]
    set hindex [lsearch $all house]
    if {$hindex == -1} {return ""}
    if {$hindex < 3} {return ""}
    if {![string compare [lindex $all [expr $hindex - 3]] matrix]} {
	set startt -3
    } elseif {![string compare [lindex $all [expr $hindex - 4]] matrix]} {
	set startt -4
    }
    set rval [lrange $all [expr $hindex + $startt] $hindex]
    if {[lindex $rval 0] == "matrix" && [lindex $rval 1] == "load"} {
	return $rval
    }
    return ""
}


# solar::vcfselect --
#
# Purpose: extract genotype data from vcf file (HIGHLY EXPERIMENTAL!)
#
# Usage: vcfselect <filename> <chrom> <pos>
#
# See also vcfinfo to extract genotype meta information.
#
# Notes: (1) output file is named as input file with .gz and .vcf extensions
#        removed and .<chrom>.<pos>.csv added.
#
#        (2) File is returned with genotypes as originally coded, and also
#            with 0,1,2 SOLAR coding if there are no blanks.  Fields are
#            named sample_id, <chrom>.<pos> and snp_<chrom>.<pos>.
#
#        (3) Errors in decoding to 0,1,2 are flagged with "error" in the
#            output lines, and an error message.  However the output file
#            is still written completely with the original genotype coding.
# -

proc vcfselect {vcffilename chrom pos} {
    set found 0
    if {[file extension $vcffilename] == ".gz"} {
	set outfilename [file rootname $vcffilename]
	set infile [open "|gunzip -c $vcffilename" r]
    } else {
	set outfilename $vcffilename
	set infile [open $vcffilename]
    }
    if {[file extension $outfilename] == ".vcf"} {
	set outfilename [file rootname $outfilename]
    }
    set outfilename $outfilename.$chrom.$pos.csv
#
# Find CHROM line and get names of samples
#
    while {1} {
	if {-1 != [gets $infile line]} {
	    if {[string range $line 0 5] != "#CHROM"} {
		continue
	    }
	    break
	} else {
	    fconfigure $infile -blocking f
	    close $infile
	    error "No header found in vcf file $vcffilename"
	}
    }
    set names [lrange $line 9 end]
#
# Find line with same chrom, loc
#
    while {1} {
	if {-1 != [gets $infile line]} {
	    if {[lindex $line 0] != $chrom || \
		    [lindex $line 1] != $pos} {
		continue
	    }
	    set found 1
	    break
	} else {
	    fconfigure $infile -blocking f
	    close $infile
	    error "Chrom $chrom Pos $pos not found in $vcffilename"
	}
    }
    fconfigure $infile -blocking f
    close $infile
#
# Find GT in genotype format
#
    set formatstr [lindex $line 8]
    set formatlist [split $formatstr :]
    set gtindex [lsearch $formatlist GT]
    if {$gtindex < 0} {
	error "GT data not found for $chrom + $pos"
    }
#
# Write output header
#
    set outfile [open $outfilename w]
    puts $outfile "sample_id,$chrom.$pos,snp_$chrom.$pos"
    set i 9
    set errorcount 0
    set fullblank 0
    set partblank 0
    foreach name $names {
	set sampledata [lindex $line $i]
	set samplelist [split $sampledata :]
	set genotype [lindex $samplelist $gtindex]
#
# translate genotype to solar value, if possible
#
	set len [string length $genotype]
#
# first rewrite as "genocode" which always uses slash for easier comparisons
#
	set genocode "[string index $genotype 0]/[string index $genotype 2]"
	set snpval ""

	if {$len != 3} {
	    set snpval ERROR
	    incr errorcount
	} else {
	    if {-1 == [string first . $genocode]} {
		if {$genocode == "0/0"} {
		    set snpval 0
		} elseif {$genocode == "0/1" || $genocode == "1/0"} {
		    set snpval 1
		} elseif {$genocode == "1/1"} {
		    set snpval 2
		} else {
		    set snpval ERROR
		    incr errorcount
		}
	    } else {
		if {$genocode == "./."} {
		    incr fullblank
		} elseif {[string index $genocode 0] == "." || \
			      [string index $genocode 1] == "."} {
		    incr partblank
		} else {
		    set snpval ERROR
		    incr errorcount
		}
	    }
	}
	puts $outfile $name,$genotype,$snpval
	incr i
    }
    close $outfile
    set count [expr $i - 9]

    if {$errorcount} {
	error "ERROR: $errorcount errors, $count total lines written to $outfilename"
    }
    return "OK $count written to $outfilename, $fullblank full blanks, $partblank partial blanks"
}

# solar::faketraits --
#
# Purpose: Generate fake pedigree/phenotypes file for testing memory required
#
# Usage:   faketraits <n_individuals> <n_traits>
#
# faketraits.out is written with 4-person families. Trait data is random.
#
# Note: The intended use is testing whether memory is sufficient to handle
# a particular number of traits/individuals/pedigree-size.
#
# -

proc faketraits {nind ntrait} {

    set ofile [open faketraits.out w]
    set header "id,fa,mo,sex,mztwin"
    for {set i 1} {$i <= $ntrait} {incr i} {
	set header "$header,t$i"
    }
    puts $ofile $header

    set last_n 0
    for {set n 1} {$n <= $nind} {incr n 4} {
	set record "$n,,,1,"
	for {set i 1} {$i <= $ntrait} {incr i} {
	    set record $record,[expr rand()]
	}
	puts $ofile $record
	set last_n $n

	if {$n + 1 > $nind} {break}
	set record "[expr $n+1],,,2,"
	for {set i 1} {$i <= $ntrait} {incr i} {
	    set record $record,[expr rand()]
	}
	puts $ofile $record
	set last_n [expr $n + 1]

	if {$n + 2 > $nind} {break}

	set record "[expr $n+2],$n,[expr $n+1],1,"
	for {set i 1} {$i <= $ntrait} {incr i} {
	    set record $record,[expr rand()]
	}
	puts $ofile $record
	set last_n [expr $n+2]

	if {$n + 3 > $nind} {break}

	set record "[expr $n+3],$n,[expr $n+1],2,"
	for {set i 1} {$i <= $ntrait} {incr i} {
	    set record $record,[expr rand()]
	}
	puts $ofile $record
	set last_n [expr $n+3]
    }
    close $ofile
    return "$last_n records written to faketraits.out"
}

proc faketraits_bigped {nind ntrait} {

    set ofile [open faketraits.out w]
    set header "id,fa,mo,sex,mztwin"
    for {set i 1} {$i <= $ntrait} {incr i} {
	set header "$header,t$i"
    }
    puts $ofile $header

    set ffa "1,,,1,"
    set fmo "2,,,2,"
    for {set i 1} {$i <= $ntrait} {incr i} {
	set ffa $ffa,[expr rand()]
	set fmo $fmo,[expr rand()]
    }
    puts $ofile $ffa
    puts $ofile $fmo

    for {set n 3} {$n <= $nind} {incr n} {
	set record "$n,1,2,1,1"
	for {set i 1} {$i <= $ntrait} {incr i} {
	    set record $record,[expr rand()]
	}
	puts $ofile $record
    }
    close $ofile
    return OK
}

# solar::vcfinfo --
#
# Purpose: extract per-genotype meta information from vcf file
#
# Usage: vcfinfo <filename> [-chrom <chrom>] [-info] [-all]
#
#        -chrom  restrict output to chromosome <chrom>
#        -info   include INFO and FORMAT fields
#        -all    include everything, including sample genotypes
#                (WARNING!  OUTPUT CAN BE VERY LARGE!)
#
# See also vcfselect to extract genotype/sample data only
#
# Notes: (1) output file is named as input file with .gz and .vcf extensions
#        removed and .vcfinfo.csv or .<chrom>.vcfinfo.csv appended
#
#        (2) vcf files permit commas within fields, which is not permitted
#            in csv files.  vcfinfo converts these commas to spaces.
#
# -

proc vcfinfo {filename args} {
    set found 0
    set chrom 0
    set info 0
    set all 0
    if {$args != ""} {
	set badargs [read_arglist $args \
			   -chrom chrom \
			   -info {set info 1} \
			   -all {set all 1} \
			   ]
	if {$badargs != ""} {
	    error "invalid arguments to vcfinfo: $badargs"
	}
    }
    if {[file extension $filename] == ".gz"} {
	set outfilename [file rootname $filename]
	set infile [open "|gunzip -c $filename" r]
    } else {
	set outfilename $filename
	set infile [open $filename]
    }
    if {[file extension $outfilename] == ".vcf"} {
	set outfilename [file rootname $outfilename]
    }
    if {$chrom} {
	set outfilename $outfilename.$chrom.vcfinfo.csv
    } else {
	set outfilename $outfilename.vcfinfo.csv
    }

#
# Find CHROM line and get names of samples
#
    while {1} {
	if {-1 != [gets $infile line]} {
	    if {[string range $line 0 5] != "#CHROM"} {
		continue
	    }
	    break
	} else {
	    fconfigure $infile -blocking f
	    close $infile
	    error "No header found in vcf file $filename"
	}
    }
    set names [lrange $line 9 end]

    set outfile [open $outfilename w]
    set hline "chrom,pos,id,ref,alt,qual,filter"
    set maxfield 6
    if {$info || $all} {
	set hline "$hline,info,format"
	set maxfield 8
    }
    if {$all} {
	foreach name $names {
	    set hline "$hline,$name"
	    incr maxfield
	}
    }
    puts $outfile $hline
#
# 
#
    set linecount 0
    while {-1 != [gets $infile line]} {
	set chromo [lindex $line 0]
	set oline $chromo
	if {$chrom == $chromo || !$chrom} {
	    for {set i 1} {$i <= $maxfield} {incr i} {
		set fields [split [lindex $line $i] ,]
		set oline "$oline,[join $fields]"
	    }
	}
	puts $outfile $oline
	incr linecount
    }
    fconfigure $infile -blocking f
    close $infile
    close $outfile
    return "$linecount data lines written to $outfilename"
}

# solar::fakedata --
#
# Purpose: Generate fake pedigree/phenotypes file for testing memory required
#
# Usage:   fakedata <n_individuals> <n_traits>
#
# fakedata.out is written with 4-person families. Trait data is random.
#
# Note: The intended use is testing whether memory is sufficient to handle
# a particular number of traits/individuals/pedigree-size.
#
# -

proc fakedata {nind ntrait} {

    set ofile [open fakedata.out w]
    set header "id,fa,mo,sex,mztwin"
    for {set i 1} {$i <= $ntrait} {incr i} {
	set header "$header,t$i"
    }
    puts $ofile $header

    set last_n 0
    for {set n 1} {$n <= $nind} {incr n 4} {
	set record "$n,,,1,"
	for {set i 1} {$i <= $ntrait} {incr i} {
	    set record $record,[expr -1+2*rand()]
	}
	puts $ofile $record
	set last_n $n

	if {$n + 1 > $nind} {break}
	set record "[expr $n+1],,,2,"
	for {set i 1} {$i <= $ntrait} {incr i} {
	    set record $record,[expr -1+2*rand()]
	}
	puts $ofile $record
	set last_n [expr $n + 1]

	if {$n + 2 > $nind} {break}

	set record "[expr $n+2],$n,[expr $n+1],1,"
	for {set i 1} {$i <= $ntrait} {incr i} {
	    set record $record,[expr -1+2*rand()]
	}
	puts $ofile $record
	set last_n [expr $n+2]

	if {$n + 3 > $nind} {break}

	set record "[expr $n+3],$n,[expr $n+1],2,"
	for {set i 1} {$i <= $ntrait} {incr i} {
	    set record $record,[expr -1+2*rand()]
	}
	puts $ofile $record
	set last_n [expr $n+3]
    }
    close $ofile
    return "$last_n records written to fakedata.out"
}

proc fakedata_bigped {nind ntrait} {

    set ofile [open fakedata.out w]
    set header "id,fa,mo,sex,mztwin"
    for {set i 1} {$i <= $ntrait} {incr i} {
	set header "$header,t$i"
    }
    puts $ofile $header

    set ffa "1,,,1,"
    set fmo "2,,,2,"
    for {set i 1} {$i <= $ntrait} {incr i} {
	set ffa $ffa,[expr rand()]
	set fmo $fmo,[expr rand()]
    }
    puts $ofile $ffa
    puts $ofile $fmo

    for {set n 3} {$n <= $nind} {incr n} {
	set record "$n,1,2,1,1"
	for {set i 1} {$i <= $ntrait} {incr i} {
	    set record $record,[expr rand()]
	}
	puts $ofile $record
    }
    close $ofile
    return OK
}

# solar::fphi
#
# Purpose: Fast test and heritability approximation (Experimental)
#
# Usage: fphi [-p] [-np <nP>] [-debug] [-X <matrix>] [-Y <matrix>]
#             [-Z <matrix>] [-indicator]
#
# Returns: indicator h2r pvalue
#
#        -p             Get P value
#        -np <nP>       Number of Permutations (default: 5000)
#        -debug         Print informative messages
#        -X <x-matrix>  Use this X matrix (default is SOLAR EVD)
#        -Y <y-matrix>  Use this Y matrix (default is SOLAR EVD)
#        -Z <z-matrix>  Use this Z matrix (default is SOLAR EVD)
#        -indicator      Only compute and return the indicator variable
#                         (only a single scalar value is returned)
#
# Notes: Do "mathmatrix reset" frequently, preferably before or after this
#        procedure, to clear out all mathmatrix storage.  fphi will create
#        large matrices, especially for P value estimation.
#
#        Trait and covariates must be specified beforehand.  However actual
#        maximization is not done so there will be no parameters created.
#
#        If X, Y, and Z matrices are all specified using the -X -Y -Z options,
#        there need be no trait and covariates specified, as the trait and
#        covariates are only needed for SOLAR EVD to create the matrices
#
# Example:
#
#          model new
#          trait q4
#          covar age
#          set results [fphi -p]
#          puts "H2r is [lindex $results 1]"
#          puts "pvalue is [lindex $results 2]"
#          mathmatrix reset
# -
proc fphi {args} {
    set indicator_only 0
    set debug0 0
    set use_x ""
    set use_y ""
    set use_z ""
    set get_pvalue 0
    set nP 5000

# process optional arguments

    set badargs [read_arglist $args \
		     -debug {set debug0 1} \
		     -nodebug {set debug0 0} \
		     -X use_x \
		     -Y use_y \
		     -Z use_z \
		     -p {set get_pvalue 1} \
		     -np nP \
		     -indicator {set indicator_only 1}]

    if {[llength $badargs]} {
	error "Invalid arguments to fphi: $badargs"
    }

    if {$indicator_only && $get_pvalue} {
	error "fphi: -indicator and -p arguments incompatible"
    }


# Do SOLAR EVD -> evddata.out

    if {"" == $use_x || "" == $use_y || "" == $use_z} {
	evdout -all 
    }

# Get X, Y, Z matrices from evddata.out OR user matrices

    if {"" == $use_x} {
	set X [evdinx]
    } else {
	set X $use_x
    }

    if {"" == $use_y} {
	set Y [evdiny]
    } else {
	set Y $use_y
    }

    set zcols {{1} lambda}
    if {"" == $use_z} {
	set Z [load matrix -cols $zcols [full_filename evddata.out]]
    } else {
	set Z $use_z
    }

# Use OLS to get B

    set B [ols $Y $X]

    ifdebug0 puts "X=$X, Y=$Y, B=$B, Z=$Z"

# Get Lamda (2nd column of Z) and Lambda transformed (Lt)

    set Lambda [col $Z 2]
    set Lt [transpose $Lambda]

# calculate squared residuals F=(Y* - X*B)^2

    set XB [times $X $B]
    set R [minus $Y $XB]
    set F [power $R 2]
    ifdebug0 puts "F=$F"

# calculate mean squared residuals,
#   then test statistic
#     and quit if statistic is non-positive or all that was asked for

    set rsig [expr 1.0 / [mean $F]]
    set test1m [times [times $rsig $Lt] [minus [times $rsig $F] 1]]
    set Indicator [show $test1m 1 1]
    if {$Indicator <= 0} {
	if {$indicator_only} {
	    return 0
	}
	return {0 0}
    }
    if {$indicator_only} {
	return $Indicator
    }
    ifdebug0 puts "Indicator Variable: $Indicator"

# 1(g)i. and 1(g)ii.
# Regress F on Z
#    (matrix numbers shown assume starting from .mm.1)

    set theta [ols $F $Z]
    set ztheta2 [power [times $Z $theta] 2]
    set SigmaHatOLS [diagonal $ztheta2]
    ifdebug0 puts "Theta is $theta, SigmaHatOLS is $SigmaHatOLS"

# 1(g)iii.
# Calculate estimated heritability

    set Zt [transpose $Z]
    set SigmaHinv [dinverse $SigmaHatOLS]
    set term1 [inverse [times [times $Zt $SigmaHinv] $Z]]
    set Sigma2wls [times [times [times $term1 $Zt] $SigmaHinv] $F]
    ifdebug0 puts "SigmaVector is\n[show $Sigma2wls]\n"

    set SigmaE [show $Sigma2wls 1 1]
    set SigmaG [show $Sigma2wls 2 1]
    if {$SigmaE < 0} {
	set SigmaE 0
    }
    if {$SigmaG < 0} {
	set SigmaG 0
    }
    set denom [expr $SigmaE + $SigmaG]
    if {$denom == 0} {
	set H2r 0
    } else {
		set H2r [expr $SigmaG / $denom]
#		set weights [times $Z $Sigma2wls]
#		set weights [power $weights -2]
#		set a [expr [rows $weights ]*[mean $weights]]
#		set c [times  [power [transpose $Lambda ] 2] $weights]
#		puts "here"
#		set det [expr (($a*[show $c 1 1]) - ([show $b 1 1]*[show $b 1 1]))]
		
#		set E [expr $SigmaE/(($SigmaG + $SigmaE)*($SigmaG + $SigmaE))]
#		set G [expr $SigmaG/(($SigmaG + $SigmaE)*($SigmaG + $SigmaE))]
	#	set Var [expr ((2*$G*$G*[show $c 1 1]) + (2*($G*$E)*[show $b 1 1]) +(($E*$E)*$a))/$det]
	#	set SE $Var
	#	puts "$SE"
		
	}

    if {$get_pvalue==0} {
	return "$Indicator $H2r"
    }

# ***********************************************************************
    ifdebug0 puts "Computing P Value"
# ***********************************************************************


# August 3 rewrite:
#   The P-value is calculated using a permutation test.

# Section 3(b) Create Ys (shuffled Y matrix)
#     First column is XB + R (reconstructed Y)
#     Subsequent columns 2...nP+1 are XB + shuffled R
#       R is "square root of F" i.e. (Y - XB)
#     Using custom function PermuteY

    set Ys [permutey $XB $R $nP]

# Section 3(b)
# If there are covariates
#   F3 = (HxY) etimes (HxY)
#     Hx = (I - X(X'X)^-1 X')
#     (May 9 pdf was wrong, corrected on July 9)

    if {[llength [covariates]]} {
	set Xt [transpose $X]
	set Xp [times [times $X [inverse [times $Xt $X]]] $Xt]
	set Ip [identity [rows $Xp]]
	set Hx [minus $Ip $Xp]

	ifdebug0 puts "beginning to multiply Hx times Ys"
	set HxY [times $Hx $Ys]
	ifdebug0 puts "end multiplication"
	ifdebug0 puts "HxY is $HxY"

	set F3 [times -e $HxY $HxY]

    } else {

# If no covariates
#   F3 = Y etimes Y

	set F3 [times -e $Ys $Ys]
    }
    ifdebug0 puts "F3 is $F3"
#
# Section 3(c)
#
    set SigmaP [mean $F3]  ;# SigmaP is scalar
    set SigmaPi [expr 1.0 / $SigmaP]
    set Fs [minus [times -e $F3 $SigmaPi] 1]
    ifdebug0 puts "Fs is $Fs"
#
# Section 3(d)
#   Calculate Ts
#
    set ZtZi [inverse [times $Zt $Z]]
    set Fst [transpose $Fs]

    set TsM [times 0.5 [power [times [times $Fst $Z] $ZtZi] 2]]

    ifdebug0 puts "TsM is $TsM"

    set twoones [matrix new {{1} {1}}]
    set Ts [times $TsM $twoones]
    ifdebug0 puts "Ts is $Ts"

    set SigmaPi2 [expr $SigmaPi * 0.5]
    set Score [times $SigmaPi2 [times $Lt $F3]]
    ifdebug0 puts "Score is $Score"

# 3(d)(3) Apply positive indicator function on Score to Ts
# Ts has dimension (np+1) rows and 1 col
# Score has dimension 1 row and (np+1) col
    
    for {set i 1} {$i <= $nP + 1} {incr i} {
	set scorei [show $Score 1 $i]
	if {$scorei < 0} {
	    insert $Ts $i 0
	}
    }

# Section 4(d)
#   Calculate P-value
#
    set Ts1 [show $Ts 1 1]
    set tsn 0
    for {set i 2} {$i < $nP+1} {incr i} {
	if {[show $Ts $i 1] >= $Ts1} {
	    incr tsn
	}
    }
    ifdebug0 puts "tsn is $tsn, nP is $nP\n"
    set Pvalue [expr double($tsn + 1)/($nP + 1)]
    return [list $Indicator $H2r $Pvalue]
}


# solar::gpu_fphi
#
# Purpose: Fast test and heritability approximation performed on the GPU
#
# Usage: gpu_fphi [-trait_list <trait list file name>] [-p] [-np <nP>]
# [-output <output file name>]
#
# Returns: Outputs scores and H2r values to <output file name>.  Optionally outputs
# pvalues as well.
#
# Requirements: A NVIDIA CUDA Capable GPU with architecture greater than equal to 3.5
#
#        -trait_list <trait list file name> File containing all the traits that gpu_fphi will process
#        -np <nP>       					Number of Permutations (default: 5000)
#        -p         						Get pvalue
#        -output <output file name>			CSV file where output is written
#
# Notes: The trait list file name should be setup as a file in which all the trait are listed 
# and separated by spaces, not commas or newlines.  If you are experiencing difficulty in running
# this program please try the following:
# 	-Make sure LD_LIBRARY_PATH includes the cuda library, i.e. /usr/local/lib/cuda-7.5.
#   -Ensure your GPU/GPUs are NVIDIA CUDA GPUs that have a device architecture greater than equal
# 	to 3.5.
#   -If you wish to run more than one GPU at once be sure your driver's compute mode is set to DEFAULT or 0.  
#   This can be checked using the command nvidia-smi -c.
#   -Try closing all processes currently running on the GPU to free up memory and device occupancy.  
#	
#   If none of the above works please feel free to email me at bdono09@gmail.com.
#
# Example:
#
#          load ped pedigree.csv
#          load pheno phenotype.csv
#          cov age sex
#          gpu_fphi -trait_list trait_list.header -p -output results.csv
# -
proc gpu_fphi {args} {
	
    set get_pvalue 0
    set trait_header_file {}
    set nP 5000
    set output_file_name {}
# process optional arguments
    
    set badargs [read_arglist $args \
             -trait_list trait_header_file \
		     -p {set get_pvalue 1} \
		     -np nP \
		     -output output_file_name 
		     ]

    if {[llength $badargs]} {
		error "Invalid arguments to fphi: $badargs"
	}
	
    set pheno_file_name [phenotypes -files]  
   
    if {$pheno_file_name == {}} {
		error "No phenotype loaded"
	}
	 
	if {$trait_header_file == {}} {
		error "No trait file list loaded"
	}
	
	if {$output_file_name == {}} {
		error "No output file specified"
	}

	set trait_file [open $trait_header_file]
	set trait_string [split [read $trait_file]]
	close $trait_file
	set trait_matrix {}
	set covars [covar]
	set list $trait_string
	set trait_list {}
	putsnew $output_file_name
	foreach item $list {
		if {$item == {}} { 
			continue 
		} else {
		catch {
			trait $item
			lappend trait_list $item
			putsa $output_file_name $item
			}
		}
	}
	
	set keepgoing 0
	set index 0
	
	while {$keepgoing == 0} {
		catch {
			
			set first_trait [lindex $trait_list $index]
			model new 
			foreach var $covars {
				cov var
			}
			trait $first_trait
			evdout -evectors -all
			set evectors [load matrix "$first_trait/evectors.mat.csv"] 
			incr keepgoing
			
		}
		incr index
		
		if {$index == [expr [llength $trait_list] - 1] && $keepGoing == 0} {
			error "Eigenvectors couldn't be created with data given"
		}
		
	}
	set idlist {}
	set data_in [open "$pheno_file_name"]
	gets $data_in line
	set title_line [split $line ","]
	set index [lsearch $title_line "id"]
	if  {$index == -1} {
		set index [lsearch $title_line "ID"]
		if {$index == -1} {
			error "Cannot find ID field in $pheno_file_name"
		}
	} 
	while {[gets $data_in line] >= 0} {
		set line_list [split $line ","]
		lappend idlist [lindex $line_list $index]
	}
	close $data_in
	ped2csv "pedindex.out" "pedindex.csv"
	joinfiles "pedindex.csv" $pheno_file_name -o "ped_pheno.csv"
	
	set ped_pheno_in [open "ped_pheno.csv"]
	gets $ped_pheno_in line
	set title_line [split $line ","]
	set ped_pheno_out "pedindex_$pheno_file_name"
	putsnew $ped_pheno_out
	putsa $ped_pheno_out "$line"
	while {[gets $ped_pheno_in line] >= 0} {
		set line_list [split $line ","]
		set current_id [lindex $line_list 0]
		set index [lsearch $idlist $current_id]
		if { $index != -1 } {
			putsa $ped_pheno_out "$line"
		}
	}
	
	close $ped_pheno_in
	
	

   

    set Y [load matrix -cols "$trait_list" "$ped_pheno_out"]
    output $Y "Y.mat.csv"
    
    set Z [evdinz]
    output $evectors "evectors.mat.csv"
    output $Z "aux.mat.csv"
    if {[llength [covariates]]} {
		set X [evdinx]
        output $X "X.mat.csv"
        set I [identity [rows $X]] 
        set XT [transpose $X]
        set XTXI [inverse [times $XT $X]]
        set XTXIXT [times $XTXI $XT]
        set hat [minus $I [times $X $XTXIXT]] 
        output $hat "hat.mat.csv"
        
        cuda_fphi "Y.mat.csv" "aux.mat.csv" "evectors.mat.csv" "$output_file_name" "$nP" "$get_pvalue" "hat.mat.csv"
	} else {
		cuda_fphi "Y.mat.csv" "aux.mat.csv" "evectors.mat.csv" "$output_file_name" "$nP" "$get_pvalue" 
	}
	puts "Done"
		
}

# solar::polyclass_normalize --
#
# Purpose:  Runs sporadic model and inormal on a phenotype with or without a pedigree
#
# Usage:    polyclass_normalize [-create_pedigree] [-class <class values seperated by comma>] [-out <output filename>]
#           
#           Example:
#             load phenotypes <phenotypes file>
#             covariates <covariate list>
#             trait  <trait to be analyized>
#             polyclass_normalize -create_pedigree -class 1,2,3
#
#           Optional arguments:
#            create_pedigree         ;# Option to be used if no pedigree is loaded
#            class                   ;# Option to select classes to run inorm on separated by commas
#            out                     ;# Option to set output name to a name not used as default
#
#
#   Polyclass normalization function will perform normalization of datasets collected across different 
# studies in preparation for mega-analysis. See Jahanshad an Kochunov Neuroimage. 2014 Apr 15;90:470-1. for details. 
# In short, the data for each dataset should be coded by class variable with one class value (0, 1, 2, 3 ..) codding the individual datasets. 
# The polyclass normalization will perform regression of the covarariates for each dataset and then inverse Gaussian normalization of the residuals. 
# The outputs will be written in the file that can specified by the option -out.
#
# The polyclass normalization function will use the existing pedigree structure to regress residuals using mixed model analysis. 
# The mega-pedigree is expected with non-overlapping subject ids. See the paper above for details on coding mega-pedigree structure. 
# Alternatively -create_pedigree option can be used where subjects are #treated as unrelated individuals and linear model is used.
#
# A consistent set of covariates is expected for all datasets (classes), if covariate structure varies by dataset, 
# make an inclusive list of covariates and fill "1" values for the datasets where the covariate is not present.  
# -
proc polyclass_normalize {args} {   
	set create_pedigree 0
        set classes ""
        set optional_output_name ""
        set proc_args [read_arglist $args -create_pedigree {set create_pedigree 1} -class classes -out optional_output_name]

        if {[llength $proc_args]} {
	   error "Invalid arguments to polyclass_normalize: $proc_args"
          }

        set phenofile [phenotypes -files]
        
        set covlist [cov]
        set traitlist [trait]
        
        
        if {$phenofile == {}} {
            error "Phenotype file not found"
        }
  
        if {$covlist == {}} {
           
            error "No covariates found"
        }
        
        if {$create_pedigree == 1} {
               create_fake_pedigree $phenofile
               
         }
         
         
        
        set inorm "inorm"

        set name_list $inorm
        
 
        lappend name_list $traitlist
        lappend name_list $phenofile
        set total_out_filename [join $name_list "_" ]
        if {$classes == ""} {            
            model new
            phenotypes $phenofile
            trait $traitlist
            foreach c $covlist {
				cov $c
			}
           eval polygenic  -all  -sporadic -s  
         
          if {$optional_output_name != ""} { 
			 set out_filename $optional_output_name
			 inormal -trait "residual" -file [full_filename "sporadic.residuals"] -out $out_filename
          } else { 
			 set out_filename $total_out_filename
			 inormal -trait "residual" -file [full_filename "sporadic.residuals"] -out $out_filename
          }
          
          set file_out [open [full_filename "sporadic.residuals"]]
          set lineNumber 0

          puts "Saving normalized trait in $out_filename_filename"
         
        } else {
          set list_of_classes [split $classes ","]

          set comb_file 0
          if {$optional_output_name != ""} { 
             putsnew $optional_output_name
             putsa $optional_output_name "id,$traitlist,class"
          } else { 
             putsnew $total_out_filename
             putsa $total_out_filename "id,$traitlist,class"
          }
          split_class_file $phenofile
          foreach i $list_of_classes {
			set class_phenofile_list $i
			lappend class_phenofile_list $phenofile
			set class_phenofile [join $class_phenofile_list "_"]
            model new

            phenotypes $class_phenofile
            trait $traitlist
            foreach c $covlist {
				cov $c
			}
            eval polygenic  -all  -sporadic -s               
            set name_list $inorm
            
            lappend name_list $i
           
            lappend name_list $traitlist
           
            lappend name_list $phenofile
            
           
            set out_filename [join $name_list "_"]
            
            inormal -trait "residual" -file [full_filename "sporadic.residuals"] -out $out_filename
            
            if {$optional_output_name != ""} { 
              set file_out [open $out_filename]
              set lineNumber 0
              while {[gets $file_out line] >= 0} {
                if {$lineNumber == 0} {
                  incr lineNumber
                } else {
                  putsa $optional_output_name "$line,$i"
                  incr lineNumber
                }
          
              }
              close $file_out
              
           } else {
              set file_out [open $out_filename]
              set lineNumber 0
              while {[gets $file_out line] >= 0} {
                if {$lineNumber == 0} {
                  incr lineNumber
                } else {
                  putsa $total_out_filename "$line,$i"
                  incr lineNumber
                }
          
              }
              close $file_out
              
             }

              
          }
          
          pheno load $phenofile

          if {$optional_output_name != ""} {
            puts "Saving normalized trait in $optional_output_name"
          } else {
            puts "Saving normalized trait in $total_out_filename"
          }
           
        }
        
        
	}

  proc create_unique_pedigree {args} {
	
    set get_pvalue 0
    set trait_header_file ""
    set nP 5000
    set output_file_name ""
    set pedfilename ""
# process optional arguments

    set badargs [read_arglist $args \
             -trait_list trait_header_file \
		     -output output_file_name \
		     -pedigree_file_name pedfilename]

    if {[llength $badargs]} {
		error "Invalid arguments to fphi: $badargs"
	}
	
    set pheno_file_name [phenotypes -files]  
   
    if {$pheno_file_name == {}} {
		error "No phenotype loaded"
	}
	 
	if {$trait_header_file == {}} {
		error "No trait file list loaded"
	}
	
	
	
	set trait_file [open $trait_header_file]
	set trait_string [split [read $trait_file]]
	close $trait_file
	set trait_matrix {}
	
	set list $trait_string
	set trait_list {}
	foreach item $list {
		if {$item == {}} { 
			continue 
		} else {
		catch {
			trait $item
			lappend trait_list $item
			}
		}
	}
	
	close $trait_file
	
	selectfields $pheno_file_name $pedfilename ID  MO FA Sex MZTWIN HHID -o ped_$pedfilename
	
	
	
	
	
}     

# solar::create_fake_pedigree --
# "Purpose: This command creates a pedigree file given a phenotype file taken as input. 
#
#  Usage: create_fake_pedigree <phenotype filename> [-o output pedigree filename
#       <phenotype filename> Phenotype filename to be used to create pedigree
#	[-o <output pedigree filename>] Option to name output pedigree filename



# solar::mathmatrix --
#
# Purpose: Create and use matrix objects in algebraic computations
#
# Usage:  For matrices m, n, y, x, vector v, and scalar s:
#             (vector is a 1 dimensional matrix either row or column)
#
#         times [-e] $m $n   ;# m or n can be scalar, -e means elementwise
#         plus $m $n         ;# elementwise addition, or m and/or n can be
#                            ;#   scalars added to each element
#         plus $v            ;# unary plus is vector sum for v (only vectors)
#         minus $m $n        ;# elementwise subtraction, or m and/or n can be
#                            ;#   scalars
#         transpose $m       ;# matrix transpose
#         inverse $m         ;# matrix inverse
#         power $m $s        ;# raise or lower each element to power $s
#         dinverse $m        ;# Fast inverse for diagonal matrix
#                            ;#  Matrix must be diagonal!  This is not checked!
#
#         ols $y $x  ;# ordinary least squares (lldt fastest) for y=xb+e
#         solve [<method>] $y $x   ;# other methods, default is
#                                  ;# FullPivHouseholderQR
#         evalues [$x]  ;# eigenvalues ;# if no x, take from last evectors
#         evectors [$x] ;# eigenvectors ;# if no x, take from last evalues
#
#         mean $v  ;# mean of all elements
#         min $m|-where ;# returns min of all elements
#         max $m|-where ;# returns max of all elements
#                       ;# -where returns {x y} for last min or max
#         max $m $s     ;# returns matrix with all values less than $s
#                       ;# changed to $s...new matrix only if values changed
#         insert $m <row> <column> <number>   ;# note one based indexing
#         concatenate <1 | 2>  [$m]* [$v]* ;#concatenate matrices and/or
#                                          ;#vectors along specified dimension
#              ;# in place of 1 | 2 may use vertical | horizontal
#
#         show $m [<row> <column>]    ;# echo element or entire matrix
#              if <row> and <column> specified, scalar element is returned
#              if no <row> and <column>, return entire matrix pretty printed
#                                        as a tcl nested list (can be input to
#                                        matrix new)
#         output $m <filename>        ;# write out matrix as csv file
#         
#         row $m <row>       ;# extract row as a column vector: 1,2,...
#         col $m <col>       ;# extract column as a column vector: 1,2,...
#         diagonal $m [<offset>] ;# extract diagonal as a column vector
#                                ;#   offset to superdiagonal or subdiagonal
#         diagonal $v            ;# create matrix with v as diagonal (v must
#                                ;#   be a Nx1 or 1xN matrix aka vector)
#         rows $m            ;# number of rows
#         cols $m            ;# number of columns
#         shuffle $v         ;# shuffle the elements of vector v
#         shuffle $v n       ;# shuffle the elements of vector v into n-1 cols
#                            ;# retaining first column unshuffled
#         identity <rows>    ;# create an identity matrix
#
# Matrix commands for creating, loading, and deleting matrixes
#
#         matrix new { <row>* }
#            <row> = { <number>* }
#        load matrix [<option>]* <csvfile>
#            <option> = <column-list>|-hoheader
#            <column-list> = -cols { <column> }
#            <column> = [<name>|'<name>'|<index>|<repeating-sequence>|end]+
#            <repeating-sequence> = { <number>+ }
#
# Examples of new and load:
#
#        matrix new {{1 2 3} {4 5 6}}
#        load matrix -noheader design.mat.csv  ;# .mat.csv are headerless
#        load matrix -cols {{1} 1 age bmi '2020' end} phen.csv
#                           ;# first column is all 1's
#
# Delete and reset commands:
#
#         matrix delete $m
#         matrix reset         ;# free all MathMatrix storage and ID's
#                              ;# do reset as much as possible to free memory
#
# MathMatrix commands
#
#         mathmatrix debug on  ;# print messages as operations are performed
#
# Notes
#
# You can save each returned matrix in a Tcl variable as you do with
# other operations, such as:
#
#   set X [load matrix -cols {{1} sex age} phen.csv]
#   set Xt [transform X]
#
# Matrices are returned as internally numbered identifiers, starting from
#
#   .mm.1
#
# As shown, you may assign these to variables like any other returns in
# SOLAR.  Or you can simply remember the identifiers themselves and use them
# directly, skipping the assignments and referencing.  This can be convenient
# for interactive use as it may reduce typing, but normal assignment is better
# for scripting.
#
#   solar> matrix reset
#   solar> load matrix -cols {{1} sex age} phen.csv
#   .mm.1
#   solar> rows .mm.1
#   1009
#
# Actual matrix objects are stored internally in C++.  The storage use
# will continue to increase as more operations are done, as every
# intermediate matrix created is saved.  As soon as convenient you
# should delete matrices or do "matrix reset" to clear
# all matrix storage and identifiers.
#
# *matrix reset frees all the memory used by MathMatrix objects and
# resets the MathMatrixID to start at .mm.1.  It is recommended to
# do this at the beginning and/or end of matrix calculations.
#
# The -cols option may be used in several ways.  You can specify the
# columns to be taken from a CSV file, and in what order.  You can
# specify the columns by name (the natural way), or by column number
# (the first column is defined as column 1) which would be required for
# headerless CSV files as produced by other math programs.  In addition
# to taking columns from the file, you can specify single numbers or
# or sequences of numbers which will be repeated to fill up the column.
#
# Here is an example of loading a Design Matrix out of two named columns from
# a CSV file (age and bmi) with a leading column of all 1's:
#
# load matrix -cols {{1} age bmi} phen.csv
#
# The entire column list must be enclosed in curly braces, and sequences
# of numbers to be repeated must be further enclosed in an inner set of
# curly braces.
#
# Here is an example of loading a headerless csv file having 4 columns in
# reverse column order:
#
# load matrix -cols {4 3 2 1} -noheader oldmat.csv
#
# Numbers within the column list are assumed to be column indexes, starting
# from 1 for the first column.  If you wish to identify columns in a CSV
# file which happen to have numbers as their names, you must enclose them
# in single apostrophes.
#
# For example, if phen.csv has a column named "2020" you could use it like
# this:
#
# load matrix -cols {age bmi '2020'} phen.csv
#
#
# The evalues and evectors commands allow you to compute a new real pseudo
# eigen decomposition, or simply echo the eigvenvalues or eigenvectors of
# the previous decomposition.  This way you can get either one first,
# or only if needed.  Generally you should extract your eigenvalues
# or eigenvectors as quickly as possible and certainly before calling any
# other routines which might also use eigen decompositions.
#
# (The pseudoeigendecomposition is the only type currently supported as
#  it guarantees real numbers.  A general eigendecomposition would require
#  complex numbers and is not yet supported.)
#
# For example, starting with matrix x, you could get the eigenvalues with
# the evalues command, then get the eigenvectors:
#
# set values [evalues $x]
# set vectors [evectors]
#
# Or you could do this in reverse order:
#
# set vectors [evectors $x]
# set values [evalues]
#
# If a matrix filename ends in ".mat.csv" it will automatically be handled as
# headerless matrix file and the -noheader argument is not required.
#-


# solar::split_class_file -- 
#
# Purpose: Splits a csv file based on a class column
#          
# Usage: split_class_file <csv file name>
#
# Example:  split_class_file pheno.csv
#	pheno.csv is split into 0_pheno.csv, 1_pheno.csv,....,i_pheno.csv
#
# -


# solar::plink_converter -- 
#
# Purpose: Converts plink .bed, .fam, and .bim to .csv file
#          
# Usage: plink_converter -i <input base name> -o <output base name> optional:<-bin>
#							 optional:<-max> <maximum snps per file>
#
# Example: plink_converter -i test -o test -bin -max 50000
#          
#			-bin Prints the number between 0 and 2 instead of alleles combination
#			-max Sets the maximum number of snps in a file.  For example if a file
#            contains 500 snps and the option -max 100 is used then five .csv files
#            labeled <output_base_name>_<file number>.csv will be created.
# -

